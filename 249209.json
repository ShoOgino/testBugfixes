{"path":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","commits":[{"id":"0f080986da691a3bba7b757f43ab72cdc82b57ce","date":1273069619,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28427ef110c4c5bf5b4057731b83110bd1e13724","date":1276701452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    Terms terms = MultiFields.getTerms(reader, fieldName);\n    if (terms != null) {\n      TermsEnum te = terms.iterator();\n      BytesRef text;\n      while ((text = te.next()) != null) {\n        if (te.docFreq() > maxDocFreq) {\n          stopWords.add(text.utf8ToString());\n        }\n      }\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    Terms terms = MultiFields.getTerms(reader, fieldName);\n    if (terms != null) {\n      TermsEnum te = terms.iterator();\n      BytesRef text;\n      while ((text = te.next()) != null) {\n        if (te.docFreq() > maxDocFreq) {\n          stopWords.add(text.utf8ToString());\n        }\n      }\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4d7a70709df2f3ba961939504c75098e92da8b99","date":1286120517,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    Terms terms = MultiFields.getTerms(reader, fieldName);\n    if (terms != null) {\n      TermsEnum te = terms.iterator();\n      BytesRef text;\n      while ((text = te.next()) != null) {\n        if (te.docFreq() > maxDocFreq) {\n          stopWords.add(text.utf8ToString());\n        }\n      }\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    @SuppressWarnings(\"unchecked\")\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    Terms terms = MultiFields.getTerms(reader, fieldName);\n    if (terms != null) {\n      TermsEnum te = terms.iterator();\n      BytesRef text;\n      while ((text = te.next()) != null) {\n        if (te.docFreq() > maxDocFreq) {\n          stopWords.add(text.utf8ToString());\n        }\n      }\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    Terms terms = MultiFields.getTerms(reader, fieldName);\n    if (terms != null) {\n      TermsEnum te = terms.iterator();\n      BytesRef text;\n      while ((text = te.next()) != null) {\n        if (te.docFreq() > maxDocFreq) {\n          stopWords.add(text.utf8ToString());\n        }\n      }\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    @SuppressWarnings(\"unchecked\")\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    Terms terms = MultiFields.getTerms(reader, fieldName);\n    if (terms != null) {\n      TermsEnum te = terms.iterator();\n      BytesRef text;\n      while ((text = te.next()) != null) {\n        if (te.docFreq() > maxDocFreq) {\n          stopWords.add(text.utf8ToString());\n        }\n      }\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1b3a24d5d9b47345473ff564f5cc127a7b526b4","date":1306277076,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    final Terms terms = MultiFields.getTerms(reader, fieldName);\n    final CharsRef spare = new CharsRef();\n    if (terms != null) {\n      final TermsEnum te = terms.iterator();\n      BytesRef text;\n      while ((text = te.next()) != null) {\n        if (te.docFreq() > maxDocFreq) {\n          stopWords.add(text.utf8ToChars(spare).toString());\n        }\n      }\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    @SuppressWarnings(\"unchecked\")\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    Terms terms = MultiFields.getTerms(reader, fieldName);\n    if (terms != null) {\n      TermsEnum te = terms.iterator();\n      BytesRef text;\n      while ((text = te.next()) != null) {\n        if (te.docFreq() > maxDocFreq) {\n          stopWords.add(text.utf8ToString());\n        }\n      }\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    @SuppressWarnings(\"unchecked\")\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    final Terms terms = MultiFields.getTerms(reader, fieldName);\n    final CharsRef spare = new CharsRef();\n    if (terms != null) {\n      final TermsEnum te = terms.iterator();\n      BytesRef text;\n      while ((text = te.next()) != null) {\n        if (te.docFreq() > maxDocFreq) {\n          stopWords.add(text.utf8ToChars(spare).toString());\n        }\n      }\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    @SuppressWarnings(\"unchecked\")\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    Terms terms = MultiFields.getTerms(reader, fieldName);\n    if (terms != null) {\n      TermsEnum te = terms.iterator();\n      BytesRef text;\n      while ((text = te.next()) != null) {\n        if (te.docFreq() > maxDocFreq) {\n          stopWords.add(text.utf8ToString());\n        }\n      }\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    @SuppressWarnings(\"unchecked\")\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    final Terms terms = MultiFields.getTerms(reader, fieldName);\n    final CharsRef spare = new CharsRef();\n    if (terms != null) {\n      final TermsEnum te = terms.iterator();\n      BytesRef text;\n      while ((text = te.next()) != null) {\n        if (te.docFreq() > maxDocFreq) {\n          stopWords.add(text.utf8ToChars(spare).toString());\n        }\n      }\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    @SuppressWarnings(\"unchecked\")\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    Terms terms = MultiFields.getTerms(reader, fieldName);\n    if (terms != null) {\n      TermsEnum te = terms.iterator();\n      BytesRef text;\n      while ((text = te.next()) != null) {\n        if (te.docFreq() > maxDocFreq) {\n          stopWords.add(text.utf8ToString());\n        }\n      }\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    @SuppressWarnings(\"unchecked\")\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"193a8346fc41165af561d084bb5f40a62125a878","date":1315971230,"type":4,"author":"Christopher John Male","isMerge":false,"pathNew":"/dev/null","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":null,"sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    final Terms terms = MultiFields.getTerms(reader, fieldName);\n    final CharsRef spare = new CharsRef();\n    if (terms != null) {\n      final TermsEnum te = terms.iterator();\n      BytesRef text;\n      while ((text = te.next()) != null) {\n        if (te.docFreq() > maxDocFreq) {\n          stopWords.add(text.utf8ToChars(spare).toString());\n        }\n      }\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    @SuppressWarnings(\"unchecked\")\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"4d7a70709df2f3ba961939504c75098e92da8b99":["28427ef110c4c5bf5b4057731b83110bd1e13724"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["4d7a70709df2f3ba961939504c75098e92da8b99","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"193a8346fc41165af561d084bb5f40a62125a878":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"5f4e87790277826a2aea119328600dfb07761f32":["0f080986da691a3bba7b757f43ab72cdc82b57ce","28427ef110c4c5bf5b4057731b83110bd1e13724"],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["4d7a70709df2f3ba961939504c75098e92da8b99"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","4d7a70709df2f3ba961939504c75098e92da8b99"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["193a8346fc41165af561d084bb5f40a62125a878"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["4d7a70709df2f3ba961939504c75098e92da8b99","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"]},"commit2Childs":{"4d7a70709df2f3ba961939504c75098e92da8b99":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","a1b3a24d5d9b47345473ff564f5cc127a7b526b4","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["4d7a70709df2f3ba961939504c75098e92da8b99","5f4e87790277826a2aea119328600dfb07761f32"],"193a8346fc41165af561d084bb5f40a62125a878":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","193a8346fc41165af561d084bb5f40a62125a878","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["28427ef110c4c5bf5b4057731b83110bd1e13724","5f4e87790277826a2aea119328600dfb07761f32"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[]},"heads":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}