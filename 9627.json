{"path":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    final LineFileDocs docs = new LineFileDocs(random);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random.nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random, 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    final LineFileDocs docs = new LineFileDocs(random);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random.nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random, 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"86365ce8db75e42ebe10805e99e92c463fef63b6","date":1330370408,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    final LineFileDocs docs = new LineFileDocs(random, defaultCodecSupportsDocValues());\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random.nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random, 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    final LineFileDocs docs = new LineFileDocs(random);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random.nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random, 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    final LineFileDocs docs = new LineFileDocs(random, defaultCodecSupportsDocValues());\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random.nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random, 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    final LineFileDocs docs = new LineFileDocs(random);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random.nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random, 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, defaultCodecSupportsDocValues());\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    final LineFileDocs docs = new LineFileDocs(random, defaultCodecSupportsDocValues());\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random.nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random, 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"57ae3024996ccdb3c36c42cb890e1efb37df4ce8","date":1338343651,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, defaultCodecSupportsDocValues());\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1c6875fb4ae52dc6f55e9bc34eb8152666c41a6f","date":1338915218,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","date":1341839195,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d19974432be9aed28ee7dca73bdf01d139e763a9","date":1342822166,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":["7b91922b55d15444d554721b352861d028eb8278"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a199589f8815be5b2c960d0a591ca1ddad8b52b1","date":1342833673,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newMockFSDirectory(tempDir); // some subclasses rely on this being MDW\n    dir.setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["527cc14542789f47d75da436cb4287d1ab887e34"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","date":1343059585,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newMockFSDirectory(tempDir); // some subclasses rely on this being MDW\n    dir.setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newMockFSDirectory(tempDir); // some subclasses rely on this being MDW\n    dir.setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newMockFSDirectory(tempDir); // some subclasses rely on this being MDW\n    dir.setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newMockFSDirectory(tempDir); // some subclasses rely on this being MDW\n    dir.setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ed7ba385535ed5109fa3082c791190945b382538","date":1380021240,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newMockFSDirectory(tempDir); // some subclasses rely on this being MDW\n    dir.setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newMockFSDirectory(tempDir); // some subclasses rely on this being MDW\n    dir.setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"527cc14542789f47d75da436cb4287d1ab887e34","date":1391705548,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newMockFSDirectory(tempDir); // some subclasses rely on this being MDW\n    dir.setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":["a199589f8815be5b2c960d0a591ca1ddad8b52b1"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = TestUtil.getTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7810a02fa5c6b863ba7dbe926e1c790f80430a9","date":1394119528,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = TestUtil.getTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = TestUtil.getTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6b51e891605604cf911ab579fb28c49b26749f93","date":1394126258,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = TestUtil.getTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = TestUtil.getTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = TestUtil.getTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = TestUtil.getTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        new MockAnalyzer(random())).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0d579490a72f2e6297eaa648940611234c57cf1","date":1395917140,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = TestUtil.createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = TestUtil.getTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"be04a7534a8a0679382542b62556ea5fba6cfb7f","date":1396613089,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = TestUtil.createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = TestUtil.createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["cbc3688252d4a8045d69a164236b2cf87b721f17"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c","date":1396633078,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = TestUtil.createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2a0f5bb79c600763ffe7b8141df59a3169d31e48","date":1396689440,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = TestUtil.getTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    writer.shutdown(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"151e3584e1250f9d055922452701114a33302e49","date":1399403957,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    writer.shutdown(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, \n        analyzer).setInfoStream(new FailOnNonBulkMergesInfoStream());\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    writer.shutdown(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    writer.shutdown(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    writer.shutdown(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    writer.shutdown(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cbc3688252d4a8045d69a164236b2cf87b721f17","date":1409846185,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    TestUtil.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":["be04a7534a8a0679382542b62556ea5fba6cfb7f"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f4abec28b874149a7223e32cc7a01704c27790de","date":1410644789,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final File tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(AtomicReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6cfa34a40cb8b737559ddb09f664924b522aedb2","date":1420468534,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c55941686ce4a07d295ac4bf8993f170b6dff731","date":1427841851,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6c576bf71df117a2003cac1787df5a9a5de44eb6","date":1427849700,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final StoredDocument doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final StoredDocument doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6bfe104fc023fadc9e709f8d17403d2cc61133fe","date":1454446396,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b470f36a9372c97283360b1304eacbde22df6c0d","date":1454765175,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e6acbaae7af722f17204ceccf0f7db5753eccf3","date":1454775255,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a207d19eac354d649c3f0e2cce070017c78125e","date":1454776470,"type":3,"author":"Erick Erickson","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n    IOUtils.rm(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c265f9b8ca00c60bfea494ac277670b45827bcbd","date":1457462048,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader, false);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"221076a44effb5561a3b799974ba1a35119fbcc0","date":1457468497,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader, false);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random, true);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader, false);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3dcdf0501ecb64dde73646fe1f7533c1586deac1","date":1507107556,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer((reader) -> {\n      if (VERBOSE) {\n        System.out.println(\"TEST: now warm merged reader=\" + reader);\n      }\n      warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n      int sum = 0;\n      final int inc = Math.max(1, maxDoc/50);\n      for(int docID=0;docID<maxDoc;docID += inc) {\n        if (liveDocs == null || liveDocs.get(docID)) {\n          final Document doc = reader.document(docID);\n          sum += doc.getFields().size();\n        }\n      }\n\n      IndexSearcher searcher = newSearcher(reader, false);\n      sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n      }\n    });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(LeafReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader, false);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"83788ad129a5154d5c6562c4e8ce3db48793aada","date":1532961485,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer((reader) -> {\n      if (VERBOSE) {\n        System.out.println(\"TEST: now warm merged reader=\" + reader);\n      }\n      warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n      int sum = 0;\n      final int inc = Math.max(1, maxDoc/50);\n      for(int docID=0;docID<maxDoc;docID += inc) {\n        if (liveDocs == null || liveDocs.get(docID)) {\n          final Document doc = reader.document(docID);\n          sum += doc.getFields().size();\n        }\n      }\n\n      IndexSearcher searcher = newSearcher(reader, false);\n      sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits.value;\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n      }\n    });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits.value != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits.value);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits.value);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits.value);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits.value != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits.value + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer((reader) -> {\n      if (VERBOSE) {\n        System.out.println(\"TEST: now warm merged reader=\" + reader);\n      }\n      warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n      int sum = 0;\n      final int inc = Math.max(1, maxDoc/50);\n      for(int docID=0;docID<maxDoc;docID += inc) {\n        if (liveDocs == null || liveDocs.get(docID)) {\n          final Document doc = reader.document(docID);\n          sum += doc.getFields().size();\n        }\n      }\n\n      IndexSearcher searcher = newSearcher(reader, false);\n      sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n      }\n    });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"feb4029567b43f074ed7b6eb8fb126d355075dfd","date":1544812585,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer((reader) -> {\n      if (VERBOSE) {\n        System.out.println(\"TEST: now warm merged reader=\" + reader);\n      }\n      warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n      int sum = 0;\n      final int inc = Math.max(1, maxDoc/50);\n      for(int docID=0;docID<maxDoc;docID += inc) {\n        if (liveDocs == null || liveDocs.get(docID)) {\n          final Document doc = reader.document(docID);\n          sum += doc.getFields().size();\n        }\n      }\n\n      IndexSearcher searcher = newSearcher(reader, false);\n      sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits.value;\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n      }\n    });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits.value != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits.value);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits.value);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits.value);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits.value != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits.value + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.getDocStats().numDocs);\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer((reader) -> {\n      if (VERBOSE) {\n        System.out.println(\"TEST: now warm merged reader=\" + reader);\n      }\n      warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n      int sum = 0;\n      final int inc = Math.max(1, maxDoc/50);\n      for(int docID=0;docID<maxDoc;docID += inc) {\n        if (liveDocs == null || liveDocs.get(docID)) {\n          final Document doc = reader.document(docID);\n          sum += doc.getFields().size();\n        }\n      }\n\n      IndexSearcher searcher = newSearcher(reader, false);\n      sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits.value;\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n      }\n    });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits.value != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits.value);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits.value);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits.value);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits.value != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits.value + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"71da933d30aea361ccc224d6544c451cbf49916d","date":1579874339,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer((reader) -> {\n      if (VERBOSE) {\n        System.out.println(\"TEST: now warm merged reader=\" + reader);\n      }\n      warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n      int sum = 0;\n      final int inc = Math.max(1, maxDoc/50);\n      for(int docID=0;docID<maxDoc;docID += inc) {\n        if (liveDocs == null || liveDocs.get(docID)) {\n          final Document doc = reader.document(docID);\n          sum += doc.getFields().size();\n        }\n      }\n\n      IndexSearcher searcher = newSearcher(reader, false);\n      sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits.value;\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n      }\n    });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_MSEC = LuceneTestCase.TEST_NIGHTLY ? 300000 : 100 * RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_MSEC;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits.value != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits.value);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits.value);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits.value);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits.value != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits.value + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.getDocStats().numDocs);\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer((reader) -> {\n      if (VERBOSE) {\n        System.out.println(\"TEST: now warm merged reader=\" + reader);\n      }\n      warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n      int sum = 0;\n      final int inc = Math.max(1, maxDoc/50);\n      for(int docID=0;docID<maxDoc;docID += inc) {\n        if (liveDocs == null || liveDocs.get(docID)) {\n          final Document doc = reader.document(docID);\n          sum += doc.getFields().size();\n        }\n      }\n\n      IndexSearcher searcher = newSearcher(reader, false);\n      sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits.value;\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n      }\n    });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits.value != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits.value);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits.value);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits.value);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits.value != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits.value + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.getDocStats().numDocs);\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":["7b91922b55d15444d554721b352861d028eb8278"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"81b6049b4a65a8926e3a66746bfcff7a00a668c5","date":1587710166,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    ensureSaneIWCOnNigtly(conf);\n\n    conf.setMergedSegmentWarmer((reader) -> {\n      if (VERBOSE) {\n        System.out.println(\"TEST: now warm merged reader=\" + reader);\n      }\n      warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n      int sum = 0;\n      final int inc = Math.max(1, maxDoc/50);\n      for(int docID=0;docID<maxDoc;docID += inc) {\n        if (liveDocs == null || liveDocs.get(docID)) {\n          final Document doc = reader.document(docID);\n          sum += doc.getFields().size();\n        }\n      }\n\n      IndexSearcher searcher = newSearcher(reader, false);\n      sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits.value;\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n      }\n    });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_MSEC = LuceneTestCase.TEST_NIGHTLY ? 300000 : 100 * RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_MSEC;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits.value != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits.value);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits.value);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits.value);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits.value != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits.value + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.getDocStats().numDocs);\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n      // when running nightly, merging can still have crazy parameters, \n      // and might use many per-field codecs. turn on CFS for IW flushes\n      // and ensure CFS ratio is reasonable to keep it contained.\n      conf.setUseCompoundFile(true);\n      mp.setNoCFSRatio(Math.max(0.25d, mp.getNoCFSRatio()));\n    }\n\n    conf.setMergedSegmentWarmer((reader) -> {\n      if (VERBOSE) {\n        System.out.println(\"TEST: now warm merged reader=\" + reader);\n      }\n      warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n      int sum = 0;\n      final int inc = Math.max(1, maxDoc/50);\n      for(int docID=0;docID<maxDoc;docID += inc) {\n        if (liveDocs == null || liveDocs.get(docID)) {\n          final Document doc = reader.document(docID);\n          sum += doc.getFields().size();\n        }\n      }\n\n      IndexSearcher searcher = newSearcher(reader, false);\n      sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits.value;\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n      }\n    });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_MSEC = LuceneTestCase.TEST_NIGHTLY ? 300000 : 100 * RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_MSEC;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits.value != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits.value);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits.value);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits.value);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits.value != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits.value + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.getDocStats().numDocs);\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e84628e1e00d06556b67af150a13dbfb1849a818","date":1588191102,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    ensureSaneIWCOnNightly(conf);\n\n    conf.setMergedSegmentWarmer((reader) -> {\n      if (VERBOSE) {\n        System.out.println(\"TEST: now warm merged reader=\" + reader);\n      }\n      warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n      int sum = 0;\n      final int inc = Math.max(1, maxDoc/50);\n      for(int docID=0;docID<maxDoc;docID += inc) {\n        if (liveDocs == null || liveDocs.get(docID)) {\n          final Document doc = reader.document(docID);\n          sum += doc.getFields().size();\n        }\n      }\n\n      IndexSearcher searcher = newSearcher(reader, false);\n      sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits.value;\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n      }\n    });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_MSEC = LuceneTestCase.TEST_NIGHTLY ? 300000 : 100 * RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_MSEC;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits.value != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits.value);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits.value);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits.value);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits.value != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits.value + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.getDocStats().numDocs);\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    Random random = new Random(random().nextLong());\n    final LineFileDocs docs = new LineFileDocs(random);\n    final Path tempDir = createTempDir(testName);\n    dir = getDirectory(newMockFSDirectory(tempDir)); // some subclasses rely on this being MDW\n    if (dir instanceof BaseDirectoryWrapper) {\n      ((BaseDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    }\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    final IndexWriterConfig conf = newIndexWriterConfig(analyzer).setCommitOnClose(false);\n    conf.setInfoStream(new FailOnNonBulkMergesInfoStream());\n    if (conf.getMergePolicy() instanceof MockRandomMergePolicy) {\n      ((MockRandomMergePolicy)conf.getMergePolicy()).setDoNonBulkMerges(false);\n    }\n\n    ensureSaneIWCOnNigtly(conf);\n\n    conf.setMergedSegmentWarmer((reader) -> {\n      if (VERBOSE) {\n        System.out.println(\"TEST: now warm merged reader=\" + reader);\n      }\n      warmed.put(((SegmentReader) reader).core, Boolean.TRUE);\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n      int sum = 0;\n      final int inc = Math.max(1, maxDoc/50);\n      for(int docID=0;docID<maxDoc;docID += inc) {\n        if (liveDocs == null || liveDocs.get(docID)) {\n          final Document doc = reader.document(docID);\n          sum += doc.getFields().size();\n        }\n      }\n\n      IndexSearcher searcher = newSearcher(reader, false);\n      sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits.value;\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n      }\n    });\n\n    if (VERBOSE) {\n      conf.setInfoStream(new PrintStreamInfoStream(System.out) {\n          @Override\n          public void message(String component, String message) {\n            if (\"TP\".equals(component)) {\n              return; // ignore test points!\n            }\n            super.message(component, message);\n          }\n        });\n    }\n    writer = new IndexWriter(dir, conf);\n    TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random().nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = TestUtil.nextInt(random(), 2, 4);\n\n    final int RUN_TIME_MSEC = LuceneTestCase.TEST_NIGHTLY ? 300000 : 100 * RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_MSEC;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start \" + NUM_INDEX_THREADS + \" indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(Thread thread : indexThreads) {\n      thread.join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n\n    assertFalse(failed.get());\n\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits.value != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits.value + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits.value != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits.value);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits.value);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits.value);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits.value != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits.value + \"; delIDs=\" + delIDs);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.getDocStats().numDocs);\n\n    doClose();\n\n    try {\n      writer.commit();\n    } finally {\n      writer.close();\n    }\n\n    // Cannot close until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    TestUtil.checkIndex(dir);\n    dir.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3dcdf0501ecb64dde73646fe1f7533c1586deac1":["221076a44effb5561a3b799974ba1a35119fbcc0"],"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4":["1c6875fb4ae52dc6f55e9bc34eb8152666c41a6f"],"e84628e1e00d06556b67af150a13dbfb1849a818":["81b6049b4a65a8926e3a66746bfcff7a00a668c5"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["6613659748fe4411a7dcf85266e55db1f95f7315","e7810a02fa5c6b863ba7dbe926e1c790f80430a9"],"6b51e891605604cf911ab579fb28c49b26749f93":["6613659748fe4411a7dcf85266e55db1f95f7315","e7810a02fa5c6b863ba7dbe926e1c790f80430a9"],"e7810a02fa5c6b863ba7dbe926e1c790f80430a9":["6613659748fe4411a7dcf85266e55db1f95f7315"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["e7810a02fa5c6b863ba7dbe926e1c790f80430a9","a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","86365ce8db75e42ebe10805e99e92c463fef63b6"],"a199589f8815be5b2c960d0a591ca1ddad8b52b1":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5a207d19eac354d649c3f0e2cce070017c78125e":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","b470f36a9372c97283360b1304eacbde22df6c0d"],"221076a44effb5561a3b799974ba1a35119fbcc0":["c265f9b8ca00c60bfea494ac277670b45827bcbd"],"151e3584e1250f9d055922452701114a33302e49":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["f4abec28b874149a7223e32cc7a01704c27790de"],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"1d028314cced5858683a1bb4741423d0f934257b":["a199589f8815be5b2c960d0a591ca1ddad8b52b1","aba371508186796cc6151d8223a5b4e16d02e26e"],"aba371508186796cc6151d8223a5b4e16d02e26e":["8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","a199589f8815be5b2c960d0a591ca1ddad8b52b1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["1c6875fb4ae52dc6f55e9bc34eb8152666c41a6f","a199589f8815be5b2c960d0a591ca1ddad8b52b1"],"81b6049b4a65a8926e3a66746bfcff7a00a668c5":["71da933d30aea361ccc224d6544c451cbf49916d"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["3dcdf0501ecb64dde73646fe1f7533c1586deac1"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["6c576bf71df117a2003cac1787df5a9a5de44eb6"],"527cc14542789f47d75da436cb4287d1ab887e34":["ed7ba385535ed5109fa3082c791190945b382538"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["151e3584e1250f9d055922452701114a33302e49"],"6613659748fe4411a7dcf85266e55db1f95f7315":["527cc14542789f47d75da436cb4287d1ab887e34"],"86365ce8db75e42ebe10805e99e92c463fef63b6":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","b470f36a9372c97283360b1304eacbde22df6c0d"],"c55941686ce4a07d295ac4bf8993f170b6dff731":["6cfa34a40cb8b737559ddb09f664924b522aedb2"],"c265f9b8ca00c60bfea494ac277670b45827bcbd":["5a207d19eac354d649c3f0e2cce070017c78125e"],"6c576bf71df117a2003cac1787df5a9a5de44eb6":["6cfa34a40cb8b737559ddb09f664924b522aedb2"],"f4abec28b874149a7223e32cc7a01704c27790de":["cbc3688252d4a8045d69a164236b2cf87b721f17"],"d0d579490a72f2e6297eaa648940611234c57cf1":["e7810a02fa5c6b863ba7dbe926e1c790f80430a9"],"be04a7534a8a0679382542b62556ea5fba6cfb7f":["d0d579490a72f2e6297eaa648940611234c57cf1"],"ed7ba385535ed5109fa3082c791190945b382538":["1d028314cced5858683a1bb4741423d0f934257b"],"cbc3688252d4a8045d69a164236b2cf87b721f17":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["1c6875fb4ae52dc6f55e9bc34eb8152666c41a6f"],"1c6875fb4ae52dc6f55e9bc34eb8152666c41a6f":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"feb4029567b43f074ed7b6eb8fb126d355075dfd":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c":["be04a7534a8a0679382542b62556ea5fba6cfb7f"],"b470f36a9372c97283360b1304eacbde22df6c0d":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","6bfe104fc023fadc9e709f8d17403d2cc61133fe"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"6cfa34a40cb8b737559ddb09f664924b522aedb2":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["86365ce8db75e42ebe10805e99e92c463fef63b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e84628e1e00d06556b67af150a13dbfb1849a818"],"71da933d30aea361ccc224d6544c451cbf49916d":["feb4029567b43f074ed7b6eb8fb126d355075dfd"]},"commit2Childs":{"3dcdf0501ecb64dde73646fe1f7533c1586deac1":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4":["aba371508186796cc6151d8223a5b4e16d02e26e"],"e84628e1e00d06556b67af150a13dbfb1849a818":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"6b51e891605604cf911ab579fb28c49b26749f93":[],"e7810a02fa5c6b863ba7dbe926e1c790f80430a9":["96ea64d994d340044e0d57aeb6a5871539d10ca5","6b51e891605604cf911ab579fb28c49b26749f93","2a0f5bb79c600763ffe7b8141df59a3169d31e48","d0d579490a72f2e6297eaa648940611234c57cf1"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","86365ce8db75e42ebe10805e99e92c463fef63b6"],"a199589f8815be5b2c960d0a591ca1ddad8b52b1":["1d028314cced5858683a1bb4741423d0f934257b","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"5a207d19eac354d649c3f0e2cce070017c78125e":["c265f9b8ca00c60bfea494ac277670b45827bcbd"],"221076a44effb5561a3b799974ba1a35119fbcc0":["3dcdf0501ecb64dde73646fe1f7533c1586deac1"],"151e3584e1250f9d055922452701114a33302e49":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["6cfa34a40cb8b737559ddb09f664924b522aedb2"],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["1c6875fb4ae52dc6f55e9bc34eb8152666c41a6f"],"1d028314cced5858683a1bb4741423d0f934257b":["ed7ba385535ed5109fa3082c791190945b382538"],"aba371508186796cc6151d8223a5b4e16d02e26e":["1d028314cced5858683a1bb4741423d0f934257b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["b470f36a9372c97283360b1304eacbde22df6c0d"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["151e3584e1250f9d055922452701114a33302e49"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":[],"81b6049b4a65a8926e3a66746bfcff7a00a668c5":["e84628e1e00d06556b67af150a13dbfb1849a818"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["feb4029567b43f074ed7b6eb8fb126d355075dfd"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["5a207d19eac354d649c3f0e2cce070017c78125e","6bfe104fc023fadc9e709f8d17403d2cc61133fe","1e6acbaae7af722f17204ceccf0f7db5753eccf3","b470f36a9372c97283360b1304eacbde22df6c0d"],"527cc14542789f47d75da436cb4287d1ab887e34":["6613659748fe4411a7dcf85266e55db1f95f7315"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6613659748fe4411a7dcf85266e55db1f95f7315":["96ea64d994d340044e0d57aeb6a5871539d10ca5","6b51e891605604cf911ab579fb28c49b26749f93","e7810a02fa5c6b863ba7dbe926e1c790f80430a9"],"86365ce8db75e42ebe10805e99e92c463fef63b6":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":[],"c55941686ce4a07d295ac4bf8993f170b6dff731":[],"c265f9b8ca00c60bfea494ac277670b45827bcbd":["221076a44effb5561a3b799974ba1a35119fbcc0"],"6c576bf71df117a2003cac1787df5a9a5de44eb6":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"f4abec28b874149a7223e32cc7a01704c27790de":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"d0d579490a72f2e6297eaa648940611234c57cf1":["be04a7534a8a0679382542b62556ea5fba6cfb7f"],"be04a7534a8a0679382542b62556ea5fba6cfb7f":["a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c"],"ed7ba385535ed5109fa3082c791190945b382538":["527cc14542789f47d75da436cb4287d1ab887e34"],"cbc3688252d4a8045d69a164236b2cf87b721f17":["f4abec28b874149a7223e32cc7a01704c27790de"],"1c6875fb4ae52dc6f55e9bc34eb8152666c41a6f":["8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","d19974432be9aed28ee7dca73bdf01d139e763a9"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["a199589f8815be5b2c960d0a591ca1ddad8b52b1"],"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"b470f36a9372c97283360b1304eacbde22df6c0d":["5a207d19eac354d649c3f0e2cce070017c78125e","1e6acbaae7af722f17204ceccf0f7db5753eccf3"],"feb4029567b43f074ed7b6eb8fb126d355075dfd":["71da933d30aea361ccc224d6544c451cbf49916d"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["cbc3688252d4a8045d69a164236b2cf87b721f17"],"6cfa34a40cb8b737559ddb09f664924b522aedb2":["c55941686ce4a07d295ac4bf8993f170b6dff731","6c576bf71df117a2003cac1787df5a9a5de44eb6"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"71da933d30aea361ccc224d6544c451cbf49916d":["81b6049b4a65a8926e3a66746bfcff7a00a668c5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","6b51e891605604cf911ab579fb28c49b26749f93","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","1e6acbaae7af722f17204ceccf0f7db5753eccf3","c55941686ce4a07d295ac4bf8993f170b6dff731","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}