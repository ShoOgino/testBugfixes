{"path":"solr/contrib/solr-mr/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","commits":[{"id":"d6e604e9030fb0cabf0c5a85ae6039921a81419c","date":1386009743,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/solr-mr/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(\"ignoreTikaException\", false);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + ExternalPaths.SOURCE_HOME + \"/core/src/test-files/log4j.properties\",\n        \"--mappers=3\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      \n      res = ToolRunner.run(jobConf, tool, args);\n      \n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=6\",\n        \"--verbose\",\n        \"--go-live\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      results = server.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2126, results.getResults().getNumFound());\n    }    \n    \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    createCollection(replicatedCollection, 2, 3, 2);\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=6\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      results = cloudClient.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2104, results.getResults().getNumFound());\n      \n      checkConsistency(replicatedCollection);\n    }   \n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      results = cloudClient.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2104, results.getResults().getNumFound());\n    }  \n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","pathOld":"solr/contrib/solr-mr/src/test/org/apache/solr/hadoop/MorphlineGoLiveMiniMRTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(\"ignoreTikaException\", false);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + ExternalPaths.SOURCE_HOME + \"/core/src/test-files/log4j.properties\",\n        \"--mappers=3\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      \n      res = ToolRunner.run(jobConf, tool, args);\n      \n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=6\",\n        \"--verbose\",\n        \"--go-live\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      results = server.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2126, results.getResults().getNumFound());\n    }    \n    \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    createCollection(replicatedCollection, 2, 3, 2);\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=6\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      results = cloudClient.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2104, results.getResults().getNumFound());\n      \n      checkConsistency(replicatedCollection);\n    }   \n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      results = cloudClient.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2104, results.getResults().getNumFound());\n    }  \n    \n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    \n    waitForRecoveriesToFinish(false);\n    \n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\n        \"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n    \n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile1);\n    \n    JobConf jobConf = getJobConf();\n    // enable mapred.job.tracker = local to run in debugger and set breakpoints\n    // jobConf.set(\"mapred.job.tracker\", \"local\");\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(\"ignoreTikaException\", false);\n\n    MapReduceIndexerTool tool;\n    int res;\n    QueryResponse results;\n    HttpSolrServer server = new HttpSolrServer(cloudJettys.get(0).url);\n    String[] args = new String[]{};\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--log4j=\" + ExternalPaths.SOURCE_HOME + \"/core/src/test-files/log4j.properties\",\n        \"--mappers=3\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),\n        \"--verbose\",\n        \"--go-live\"\n    };\n    args = prependInitialArgs(args);\n    List<String> argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      \n      res = ToolRunner.run(jobConf, tool, args);\n      \n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      results = server.query(new SolrQuery(\"*:*\"));\n      assertEquals(20, results.getResults().getNumFound());\n    }    \n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true); \n    assertTrue(fs.mkdirs(inDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile2);\n\n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1)\n    };\n    args = prependInitialArgs(args);\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());      \n      results = server.query(new SolrQuery(\"*:*\"));\n      \n      assertEquals(22, results.getResults().getNumFound());\n    }    \n    \n    // try using zookeeper\n    String collection = \"collection1\";\n    if (random().nextBoolean()) {\n      // sometimes, use an alias\n      createAlias(\"updatealias\", \"collection1\");\n      collection = \"updatealias\";\n    }\n    \n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);    \n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=6\",\n        \"--verbose\",\n        \"--go-live\",\n        ++numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(), \n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", collection\n    };\n    args = prependInitialArgs(args);\n\n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      results = server.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2126, results.getResults().getNumFound());\n    }    \n    \n    server.shutdown();\n    \n    // try using zookeeper with replication\n    String replicatedCollection = \"replicated_collection\";\n    createCollection(replicatedCollection, 2, 3, 2);\n    waitForRecoveriesToFinish(false);\n    cloudClient.setDefaultCollection(replicatedCollection);\n    fs.delete(inDir, true);   \n    fs.delete(outDir, true);  \n    fs.delete(dataDir, true);  \n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--mappers=3\",\n        \"--reducers=6\",\n        \"--verbose\",\n        \"--go-live\",\n        \"--zk-host\", zkServer.getZkAddress(), \n        \"--collection\", replicatedCollection, dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      results = cloudClient.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2104, results.getResults().getNumFound());\n      \n      checkConsistency(replicatedCollection);\n    }   \n    \n    // try using solr_url with replication\n    cloudClient.deleteByQuery(\"*:*\");\n    cloudClient.commit();\n    fs.delete(inDir, true);    \n    fs.delete(dataDir, true);\n    assertTrue(fs.mkdirs(dataDir));\n    INPATH = upAvroFile(fs, inDir, DATADIR, dataDir, inputAvroFile3);\n    \n    args = new String[] {\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards\", \"2\",\n        \"--mappers=3\",\n        \"--verbose\",\n        \"--go-live\", \n        \"--go-live-threads\", Integer.toString(random().nextInt(15) + 1),  dataDir.toString()\n    };\n    args = prependInitialArgs(args);\n\n    argList = new ArrayList<String>();\n    getShardUrlArgs(argList, replicatedCollection);\n    args = concat(args, argList.toArray(new String[0]));\n    \n    if (true) {\n      tool = new MapReduceIndexerTool();\n      res = ToolRunner.run(jobConf, tool, args);\n      assertEquals(0, res);\n      assertTrue(tool.job.isComplete());\n      assertTrue(tool.job.isSuccessful());\n      \n      checkConsistency(replicatedCollection);\n      \n      results = cloudClient.query(new SolrQuery(\"*:*\"));      \n      assertEquals(2104, results.getResults().getNumFound());\n    }  \n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["d6e604e9030fb0cabf0c5a85ae6039921a81419c"],"d6e604e9030fb0cabf0c5a85ae6039921a81419c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"]},"commit2Childs":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d6e604e9030fb0cabf0c5a85ae6039921a81419c":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d6e604e9030fb0cabf0c5a85ae6039921a81419c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}