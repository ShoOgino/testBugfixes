{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d54abe263d1bd572b13eac370304417fbb8f8af","date":1343929167,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8fd5be977c105554c6a7b68afcdbc511439723ab","date":1344115570,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(1.0)\n                                           .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                           .setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(1.0)\n                                           .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                           .setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(1.0)\n                                           .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                           .setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(4.0)\n                                    .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(1.0)\n                                           .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                           .setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(4.0)\n                                    .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(1.0)\n                                           .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                           .setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(4.0)\n                                    .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(1.0)\n                                           .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n                                           .setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24f89e8a6aac05753cde4c83d62a74356098200d","date":1525768331,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(4.0)\n                                    .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(4.0)\n                                    .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f","date":1579652839,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  @Slow\n  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(4.0)\n                                    .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(1);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(4.0)\n                                    .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["71da933d30aea361ccc224d6544c451cbf49916d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"71da933d30aea361ccc224d6544c451cbf49916d","date":1579874339,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  // TODO: this test can hit pathological cases (IW settings?) where it runs for far too long\n  @Nightly\n  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(4.0)\n                                    .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(1);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Slow\n  public void testIndexingThenDeleting() throws Exception {\n    // TODO: move this test to its own class and just @SuppressCodecs?\n    // TODO: is it enough to just use newFSDirectory?\n    final String fieldFormat = TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    assumeFalse(\"This test cannot run with Direct codec\", fieldFormat.equals(\"Direct\"));\n    final Random r = random();\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new MockTokenizer(MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(analyzer)\n                                           .setRAMBufferSizeMB(4.0)\n                                    .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO));\n    int num = atLeast(1);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":["fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f":["24f89e8a6aac05753cde4c83d62a74356098200d"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["5d54abe263d1bd572b13eac370304417fbb8f8af"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"8fd5be977c105554c6a7b68afcdbc511439723ab":["04f07771a2a7dd3a395700665ed839c3dae2def2","5d54abe263d1bd572b13eac370304417fbb8f8af"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["d0ef034a4f10871667ae75181537775ddcf8ade4","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["d0ef034a4f10871667ae75181537775ddcf8ade4","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"5d54abe263d1bd572b13eac370304417fbb8f8af":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"24f89e8a6aac05753cde4c83d62a74356098200d":["28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["04f07771a2a7dd3a395700665ed839c3dae2def2","5d54abe263d1bd572b13eac370304417fbb8f8af"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71da933d30aea361ccc224d6544c451cbf49916d"],"71da933d30aea361ccc224d6544c451cbf49916d":["fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f"]},"commit2Childs":{"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f":["71da933d30aea361ccc224d6544c451cbf49916d"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["6613659748fe4411a7dcf85266e55db1f95f7315"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"8fd5be977c105554c6a7b68afcdbc511439723ab":[],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["24f89e8a6aac05753cde4c83d62a74356098200d"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["8fd5be977c105554c6a7b68afcdbc511439723ab","5d54abe263d1bd572b13eac370304417fbb8f8af","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"5d54abe263d1bd572b13eac370304417fbb8f8af":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","8fd5be977c105554c6a7b68afcdbc511439723ab","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"24f89e8a6aac05753cde4c83d62a74356098200d":["fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"71da933d30aea361ccc224d6544c451cbf49916d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["8fd5be977c105554c6a7b68afcdbc511439723ab","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","d6f074e73200c07d54f242d3880a8da5a35ff97b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}