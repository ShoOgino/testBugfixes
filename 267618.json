{"path":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","commits":[{"id":"91109046a59c58ee0ee5d0d2767b08d1f30d6702","date":1000830588,"type":0,"author":"Jason van Zyl","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"/dev/null","sourceNew":"  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n       throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      SegmentTermPositions postings = smi.postings;\n      int base = smi.base;\n      int[] docMap = smi.docMap;\n      smi.termEnum.termInfo(termInfo);\n      postings.seek(termInfo);\n      while (postings.next()) {\n\tint doc;\n\tif (docMap == null)\n\t  doc = base + postings.doc;\t\t  // no deletions\n\telse\n\t  doc = base + docMap[postings.doc];\t  // re-map around deletions\n\n\tif (doc < lastDoc)\n\t  throw new IllegalStateException(\"docs out of order\");\n\n\tint docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n\tlastDoc = doc;\n\n\tint freq = postings.freq;\n\tif (freq == 1) {\n\t  freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n\t} else {\n\t  freqOutput.writeVInt(docCode);\t  // write doc\n\t  freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n\t}\n\t  \n\tint lastPosition = 0;\t\t\t  // write position deltas\n\tfor (int j = 0; j < freq; j++) {\n\t  int position = postings.nextPosition();\n\t  proxOutput.writeVInt(position - lastPosition);\n\t  lastPosition = position;\n\t}\n\n\tdf++;\n      }\n    }\n    return df;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1","date":1064527311,"type":3,"author":"Dmitry Serebrennikov","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n       throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      SegmentTermPositions postings = smi.postings;\n      int base = smi.base;\n      int[] docMap = smi.docMap;\n      smi.termEnum.termInfo(termInfo);\n      postings.seek(termInfo);\n      while (postings.next()) {\n        int doc;\n        if (docMap == null)\n          doc = base + postings.doc;\t\t  // no deletions\n        else\n          doc = base + docMap[postings.doc];\t  // re-map around deletions\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order\");\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq;\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n          \n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n\n        df++;\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n       throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      SegmentTermPositions postings = smi.postings;\n      int base = smi.base;\n      int[] docMap = smi.docMap;\n      smi.termEnum.termInfo(termInfo);\n      postings.seek(termInfo);\n      while (postings.next()) {\n\tint doc;\n\tif (docMap == null)\n\t  doc = base + postings.doc;\t\t  // no deletions\n\telse\n\t  doc = base + docMap[postings.doc];\t  // re-map around deletions\n\n\tif (doc < lastDoc)\n\t  throw new IllegalStateException(\"docs out of order\");\n\n\tint docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n\tlastDoc = doc;\n\n\tint freq = postings.freq;\n\tif (freq == 1) {\n\t  freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n\t} else {\n\t  freqOutput.writeVInt(docCode);\t  // write doc\n\t  freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n\t}\n\t  \n\tint lastPosition = 0;\t\t\t  // write position deltas\n\tfor (int j = 0; j < freq; j++) {\n\t  int position = postings.nextPosition();\n\t  proxOutput.writeVInt(position - lastPosition);\n\t  lastPosition = position;\n\t}\n\n\tdf++;\n      }\n    }\n    return df;\n  }\n\n","bugFix":null,"bugIntro":["0e5554d2be0131fc1ce32526016ce4dcc90650e4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7fb6d70db034a5456ae560175dd1b821eea9ff4","date":1066759157,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n       throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.postings;\n      int base = smi.base;\n      int[] docMap = smi.docMap;\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order\");\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n        \n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\t  \n        int lastPosition = 0;\t\t\t  // write position deltas\n\tfor (int j = 0; j < freq; j++) {\n\t  int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n\n        df++;\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n       throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      SegmentTermPositions postings = smi.postings;\n      int base = smi.base;\n      int[] docMap = smi.docMap;\n      smi.termEnum.termInfo(termInfo);\n      postings.seek(termInfo);\n      while (postings.next()) {\n        int doc;\n        if (docMap == null)\n          doc = base + postings.doc;\t\t  // no deletions\n        else\n          doc = base + docMap[postings.doc];\t  // re-map around deletions\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order\");\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq;\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n          \n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n\n        df++;\n      }\n    }\n    return df;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"15c469602973ef1a33c9a07367a380d278ffab20","date":1074206555,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n       throws IOException {\n    final int skipInterval = termInfosWriter.skipInterval;\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.postings;\n      int base = smi.base;\n      int[] docMap = smi.docMap;\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n        \n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\t  \n        int lastPosition = 0;\t\t\t  // write position deltas\n\tfor (int j = 0; j < freq; j++) {\n\t  int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n       throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.postings;\n      int base = smi.base;\n      int[] docMap = smi.docMap;\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order\");\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n        \n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\t  \n        int lastPosition = 0;\t\t\t  // write position deltas\n\tfor (int j = 0; j < freq; j++) {\n\t  int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n\n        df++;\n      }\n    }\n    return df;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"770281b8a8459cafcdd2354b6a06078fea2d83c9","date":1077308096,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws IOException {\n    final int skipInterval = termInfosWriter.skipInterval;\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.postings;\n      int base = smi.base;\n      int[] docMap = smi.docMap;\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n       throws IOException {\n    final int skipInterval = termInfosWriter.skipInterval;\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.postings;\n      int base = smi.base;\n      int[] docMap = smi.docMap;\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n        \n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\t  \n        int lastPosition = 0;\t\t\t  // write position deltas\n\tfor (int j = 0; j < freq; j++) {\n\t  int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","bugFix":null,"bugIntro":["1b54a9bc667895a2095a886184bf69a3179e63df"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f5cf8ed8dfc080044944e2cabac618bc36199013","date":1082468878,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.postings;\n      int base = smi.base;\n      int[] docMap = smi.docMap;\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws IOException {\n    final int skipInterval = termInfosWriter.skipInterval;\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.postings;\n      int base = smi.base;\n      int[] docMap = smi.docMap;\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ea49b92b11aa9ad376db37e90a28844cc7c12566","date":1130471040,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.postings;\n      int base = smi.base;\n      int[] docMap = smi.docMap;\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fd205ba0d3dbba4ae2679d527c2c864d6a8293b8","date":1147467840,"type":3,"author":"Daniel Naber","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order (\" + doc +\n              \" < \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","bugFix":null,"bugIntro":["1b54a9bc667895a2095a886184bf69a3179e63df","0e5554d2be0131fc1ce32526016ce4dcc90650e4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0e5554d2be0131fc1ce32526016ce4dcc90650e4","date":1168279868,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (lastDoc != 0 && doc <= lastDoc)\n          throw new IllegalStateException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < lastDoc)\n          throw new IllegalStateException(\"docs out of order (\" + doc +\n              \" < \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","bugFix":["fd205ba0d3dbba4ae2679d527c2c864d6a8293b8","8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fcf637fee66c296142fb5989e338efc018320655","date":1168455996,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < 0 || (df > 0 && doc <= lastDoc))\n          throw new IllegalStateException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (lastDoc != 0 && doc <= lastDoc)\n          throw new IllegalStateException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1b54a9bc667895a2095a886184bf69a3179e63df","date":1172088096,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws CorruptIndexException, IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < 0 || (df > 0 && doc <= lastDoc))\n          throw new CorruptIndexException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < 0 || (df > 0 && doc <= lastDoc))\n          throw new IllegalStateException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","bugFix":["fd205ba0d3dbba4ae2679d527c2c864d6a8293b8","770281b8a8459cafcdd2354b6a06078fea2d83c9"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8522ae207a56c6db28ca06fe6cc33e70911c3600","date":1173935743,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws CorruptIndexException, IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    boolean storePayloads = fieldInfos.fieldInfo(smis[0].term.field).storePayloads;\n    int lastPayloadLength = -1;   // ensures that we write the first length\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < 0 || (df > 0 && doc <= lastDoc))\n          throw new CorruptIndexException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc, storePayloads, lastPayloadLength);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n        \n        /** See {@link DocumentWriter#writePostings(Posting[], String) for \n         *  documentation about the encoding of positions and payloads\n         */\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          int delta = position - lastPosition;\n          if (storePayloads) {\n            int payloadLength = postings.getPayloadLength();\n            if (payloadLength == lastPayloadLength) {\n              proxOutput.writeVInt(delta * 2);\n            } else {\n              proxOutput.writeVInt(delta * 2 + 1);\n              proxOutput.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            }\n            if (payloadLength > 0) {\n              if (payloadBuffer == null || payloadBuffer.length < payloadLength) {\n                payloadBuffer = new byte[payloadLength];\n              }\n              postings.getPayload(payloadBuffer, 0);\n              proxOutput.writeBytes(payloadBuffer, 0, payloadLength);\n            }\n          } else {\n            proxOutput.writeVInt(delta);\n          }\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws CorruptIndexException, IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < 0 || (df > 0 && doc <= lastDoc))\n          throw new CorruptIndexException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          proxOutput.writeVInt(position - lastPosition);\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"08d1ee9808e270c86ce5c9be08b8bd7a19d0a709","date":1180597733,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws CorruptIndexException, IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    skipListWriter.resetSkip();\n    boolean storePayloads = fieldInfos.fieldInfo(smis[0].term.field).storePayloads;\n    int lastPayloadLength = -1;   // ensures that we write the first length\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < 0 || (df > 0 && doc <= lastDoc))\n          throw new CorruptIndexException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          skipListWriter.setSkipData(lastDoc, storePayloads, lastPayloadLength);\n          skipListWriter.bufferSkip(df);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n        \n        /** See {@link DocumentWriter#writePostings(Posting[], String) for \n         *  documentation about the encoding of positions and payloads\n         */\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          int delta = position - lastPosition;\n          if (storePayloads) {\n            int payloadLength = postings.getPayloadLength();\n            if (payloadLength == lastPayloadLength) {\n              proxOutput.writeVInt(delta * 2);\n            } else {\n              proxOutput.writeVInt(delta * 2 + 1);\n              proxOutput.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            }\n            if (payloadLength > 0) {\n              if (payloadBuffer == null || payloadBuffer.length < payloadLength) {\n                payloadBuffer = new byte[payloadLength];\n              }\n              postings.getPayload(payloadBuffer, 0);\n              proxOutput.writeBytes(payloadBuffer, 0, payloadLength);\n            }\n          } else {\n            proxOutput.writeVInt(delta);\n          }\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws CorruptIndexException, IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    resetSkip();\n    boolean storePayloads = fieldInfos.fieldInfo(smis[0].term.field).storePayloads;\n    int lastPayloadLength = -1;   // ensures that we write the first length\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < 0 || (df > 0 && doc <= lastDoc))\n          throw new CorruptIndexException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          bufferSkip(lastDoc, storePayloads, lastPayloadLength);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n        \n        /** See {@link DocumentWriter#writePostings(Posting[], String) for \n         *  documentation about the encoding of positions and payloads\n         */\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          int delta = position - lastPosition;\n          if (storePayloads) {\n            int payloadLength = postings.getPayloadLength();\n            if (payloadLength == lastPayloadLength) {\n              proxOutput.writeVInt(delta * 2);\n            } else {\n              proxOutput.writeVInt(delta * 2 + 1);\n              proxOutput.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            }\n            if (payloadLength > 0) {\n              if (payloadBuffer == null || payloadBuffer.length < payloadLength) {\n                payloadBuffer = new byte[payloadLength];\n              }\n              postings.getPayload(payloadBuffer, 0);\n              proxOutput.writeBytes(payloadBuffer, 0, payloadLength);\n            }\n          } else {\n            proxOutput.writeVInt(delta);\n          }\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws CorruptIndexException, IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    skipListWriter.resetSkip();\n    boolean storePayloads = fieldInfos.fieldInfo(smis[0].term.field).storePayloads;\n    int lastPayloadLength = -1;   // ensures that we write the first length\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      assert postings != null;\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < 0 || (df > 0 && doc <= lastDoc))\n          throw new CorruptIndexException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          skipListWriter.setSkipData(lastDoc, storePayloads, lastPayloadLength);\n          skipListWriter.bufferSkip(df);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n        \n        /** See {@link DocumentWriter#writePostings(Posting[], String) for \n         *  documentation about the encoding of positions and payloads\n         */\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          int delta = position - lastPosition;\n          if (storePayloads) {\n            int payloadLength = postings.getPayloadLength();\n            if (payloadLength == lastPayloadLength) {\n              proxOutput.writeVInt(delta * 2);\n            } else {\n              proxOutput.writeVInt(delta * 2 + 1);\n              proxOutput.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            }\n            if (payloadLength > 0) {\n              if (payloadBuffer == null || payloadBuffer.length < payloadLength) {\n                payloadBuffer = new byte[payloadLength];\n              }\n              postings.getPayload(payloadBuffer, 0);\n              proxOutput.writeBytes(payloadBuffer, 0, payloadLength);\n            }\n          } else {\n            proxOutput.writeVInt(delta);\n          }\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws CorruptIndexException, IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    skipListWriter.resetSkip();\n    boolean storePayloads = fieldInfos.fieldInfo(smis[0].term.field).storePayloads;\n    int lastPayloadLength = -1;   // ensures that we write the first length\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < 0 || (df > 0 && doc <= lastDoc))\n          throw new CorruptIndexException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          skipListWriter.setSkipData(lastDoc, storePayloads, lastPayloadLength);\n          skipListWriter.bufferSkip(df);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n        \n        /** See {@link DocumentWriter#writePostings(Posting[], String) for \n         *  documentation about the encoding of positions and payloads\n         */\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          int delta = position - lastPosition;\n          if (storePayloads) {\n            int payloadLength = postings.getPayloadLength();\n            if (payloadLength == lastPayloadLength) {\n              proxOutput.writeVInt(delta * 2);\n            } else {\n              proxOutput.writeVInt(delta * 2 + 1);\n              proxOutput.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            }\n            if (payloadLength > 0) {\n              if (payloadBuffer == null || payloadBuffer.length < payloadLength) {\n                payloadBuffer = new byte[payloadLength];\n              }\n              postings.getPayload(payloadBuffer, 0);\n              proxOutput.writeBytes(payloadBuffer, 0, payloadLength);\n            }\n          } else {\n            proxOutput.writeVInt(delta);\n          }\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8b061ad70439d14d24218e2d9bac891078177c3e","date":1221835533,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws CorruptIndexException, IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    skipListWriter.resetSkip();\n    boolean storePayloads = fieldInfos.fieldInfo(smis[0].term.field).storePayloads;\n    int lastPayloadLength = -1;   // ensures that we write the first length\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      assert postings != null;\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < 0 || (df > 0 && doc <= lastDoc))\n          throw new CorruptIndexException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          skipListWriter.setSkipData(lastDoc, storePayloads, lastPayloadLength);\n          skipListWriter.bufferSkip(df);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n        \n        /** See {@link DocumentWriter#writePostings(Posting[], String)} for \n         *  documentation about the encoding of positions and payloads\n         */\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          int delta = position - lastPosition;\n          if (storePayloads) {\n            int payloadLength = postings.getPayloadLength();\n            if (payloadLength == lastPayloadLength) {\n              proxOutput.writeVInt(delta * 2);\n            } else {\n              proxOutput.writeVInt(delta * 2 + 1);\n              proxOutput.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            }\n            if (payloadLength > 0) {\n              if (payloadBuffer == null || payloadBuffer.length < payloadLength) {\n                payloadBuffer = new byte[payloadLength];\n              }\n              postings.getPayload(payloadBuffer, 0);\n              proxOutput.writeBytes(payloadBuffer, 0, payloadLength);\n            }\n          } else {\n            proxOutput.writeVInt(delta);\n          }\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","sourceOld":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws CorruptIndexException, IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    skipListWriter.resetSkip();\n    boolean storePayloads = fieldInfos.fieldInfo(smis[0].term.field).storePayloads;\n    int lastPayloadLength = -1;   // ensures that we write the first length\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      assert postings != null;\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < 0 || (df > 0 && doc <= lastDoc))\n          throw new CorruptIndexException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          skipListWriter.setSkipData(lastDoc, storePayloads, lastPayloadLength);\n          skipListWriter.bufferSkip(df);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n        \n        /** See {@link DocumentWriter#writePostings(Posting[], String) for \n         *  documentation about the encoding of positions and payloads\n         */\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          int delta = position - lastPosition;\n          if (storePayloads) {\n            int payloadLength = postings.getPayloadLength();\n            if (payloadLength == lastPayloadLength) {\n              proxOutput.writeVInt(delta * 2);\n            } else {\n              proxOutput.writeVInt(delta * 2 + 1);\n              proxOutput.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            }\n            if (payloadLength > 0) {\n              if (payloadBuffer == null || payloadBuffer.length < payloadLength) {\n                payloadBuffer = new byte[payloadLength];\n              }\n              postings.getPayload(payloadBuffer, 0);\n              proxOutput.writeBytes(payloadBuffer, 0, payloadLength);\n            }\n          } else {\n            proxOutput.writeVInt(delta);\n          }\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4d17492f26096e19670d947d1be5e9adc52b1d3d","date":1224931200,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#appendPostings(SegmentMergeInfo[],int).mjava","sourceNew":null,"sourceOld":"  /** Process postings from multiple segments all positioned on the\n   *  same term. Writes out merged entries into freqOutput and\n   *  the proxOutput streams.\n   *\n   * @param smis array of segments\n   * @param n number of cells in the array actually occupied\n   * @return number of documents across all segments where this term was found\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  private final int appendPostings(SegmentMergeInfo[] smis, int n)\n          throws CorruptIndexException, IOException {\n    int lastDoc = 0;\n    int df = 0;\t\t\t\t\t  // number of docs w/ term\n    skipListWriter.resetSkip();\n    boolean storePayloads = fieldInfos.fieldInfo(smis[0].term.field).storePayloads;\n    int lastPayloadLength = -1;   // ensures that we write the first length\n    for (int i = 0; i < n; i++) {\n      SegmentMergeInfo smi = smis[i];\n      TermPositions postings = smi.getPositions();\n      assert postings != null;\n      int base = smi.base;\n      int[] docMap = smi.getDocMap();\n      postings.seek(smi.termEnum);\n      while (postings.next()) {\n        int doc = postings.doc();\n        if (docMap != null)\n          doc = docMap[doc];                      // map around deletions\n        doc += base;                              // convert to merged space\n\n        if (doc < 0 || (df > 0 && doc <= lastDoc))\n          throw new CorruptIndexException(\"docs out of order (\" + doc +\n              \" <= \" + lastDoc + \" )\");\n\n        df++;\n\n        if ((df % skipInterval) == 0) {\n          skipListWriter.setSkipData(lastDoc, storePayloads, lastPayloadLength);\n          skipListWriter.bufferSkip(df);\n        }\n\n        int docCode = (doc - lastDoc) << 1;\t  // use low bit to flag freq=1\n        lastDoc = doc;\n\n        int freq = postings.freq();\n        if (freq == 1) {\n          freqOutput.writeVInt(docCode | 1);\t  // write doc & freq=1\n        } else {\n          freqOutput.writeVInt(docCode);\t  // write doc\n          freqOutput.writeVInt(freq);\t\t  // write frequency in doc\n        }\n        \n        /** See {@link DocumentWriter#writePostings(Posting[], String)} for \n         *  documentation about the encoding of positions and payloads\n         */\n        int lastPosition = 0;\t\t\t  // write position deltas\n        for (int j = 0; j < freq; j++) {\n          int position = postings.nextPosition();\n          int delta = position - lastPosition;\n          if (storePayloads) {\n            int payloadLength = postings.getPayloadLength();\n            if (payloadLength == lastPayloadLength) {\n              proxOutput.writeVInt(delta * 2);\n            } else {\n              proxOutput.writeVInt(delta * 2 + 1);\n              proxOutput.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            }\n            if (payloadLength > 0) {\n              if (payloadBuffer == null || payloadBuffer.length < payloadLength) {\n                payloadBuffer = new byte[payloadLength];\n              }\n              postings.getPayload(payloadBuffer, 0);\n              proxOutput.writeBytes(payloadBuffer, 0, payloadLength);\n            }\n          } else {\n            proxOutput.writeVInt(delta);\n          }\n          lastPosition = position;\n        }\n      }\n    }\n    return df;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"fcf637fee66c296142fb5989e338efc018320655":["0e5554d2be0131fc1ce32526016ce4dcc90650e4"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e7fb6d70db034a5456ae560175dd1b821eea9ff4":["8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1"],"8522ae207a56c6db28ca06fe6cc33e70911c3600":["1b54a9bc667895a2095a886184bf69a3179e63df"],"1b54a9bc667895a2095a886184bf69a3179e63df":["fcf637fee66c296142fb5989e338efc018320655"],"4d17492f26096e19670d947d1be5e9adc52b1d3d":["8b061ad70439d14d24218e2d9bac891078177c3e"],"8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"f5cf8ed8dfc080044944e2cabac618bc36199013":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"ea49b92b11aa9ad376db37e90a28844cc7c12566":["f5cf8ed8dfc080044944e2cabac618bc36199013"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["08d1ee9808e270c86ce5c9be08b8bd7a19d0a709"],"0e5554d2be0131fc1ce32526016ce4dcc90650e4":["fd205ba0d3dbba4ae2679d527c2c864d6a8293b8"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["15c469602973ef1a33c9a07367a380d278ffab20"],"fd205ba0d3dbba4ae2679d527c2c864d6a8293b8":["ea49b92b11aa9ad376db37e90a28844cc7c12566"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8b061ad70439d14d24218e2d9bac891078177c3e":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4d17492f26096e19670d947d1be5e9adc52b1d3d"],"08d1ee9808e270c86ce5c9be08b8bd7a19d0a709":["8522ae207a56c6db28ca06fe6cc33e70911c3600"],"15c469602973ef1a33c9a07367a380d278ffab20":["e7fb6d70db034a5456ae560175dd1b821eea9ff4"]},"commit2Childs":{"fcf637fee66c296142fb5989e338efc018320655":["1b54a9bc667895a2095a886184bf69a3179e63df"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1"],"e7fb6d70db034a5456ae560175dd1b821eea9ff4":["15c469602973ef1a33c9a07367a380d278ffab20"],"8522ae207a56c6db28ca06fe6cc33e70911c3600":["08d1ee9808e270c86ce5c9be08b8bd7a19d0a709"],"1b54a9bc667895a2095a886184bf69a3179e63df":["8522ae207a56c6db28ca06fe6cc33e70911c3600"],"8fb95844e4ba5160067c64c5eb1cd8a09f7a94f1":["e7fb6d70db034a5456ae560175dd1b821eea9ff4"],"4d17492f26096e19670d947d1be5e9adc52b1d3d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f5cf8ed8dfc080044944e2cabac618bc36199013":["ea49b92b11aa9ad376db37e90a28844cc7c12566"],"ea49b92b11aa9ad376db37e90a28844cc7c12566":["fd205ba0d3dbba4ae2679d527c2c864d6a8293b8"],"0e5554d2be0131fc1ce32526016ce4dcc90650e4":["fcf637fee66c296142fb5989e338efc018320655"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["8b061ad70439d14d24218e2d9bac891078177c3e"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["f5cf8ed8dfc080044944e2cabac618bc36199013"],"fd205ba0d3dbba4ae2679d527c2c864d6a8293b8":["0e5554d2be0131fc1ce32526016ce4dcc90650e4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"8b061ad70439d14d24218e2d9bac891078177c3e":["4d17492f26096e19670d947d1be5e9adc52b1d3d"],"08d1ee9808e270c86ce5c9be08b8bd7a19d0a709":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"15c469602973ef1a33c9a07367a380d278ffab20":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}