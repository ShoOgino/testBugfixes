{"path":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","commits":[{"id":"770281b8a8459cafcdd2354b6a06078fea2d83c9","date":1077308096,"type":0,"author":"Doug Cutting","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"/dev/null","sourceNew":"  public void testKnownSetOfDocuments() {\n    String [] termArray = {\"eating\", \"chocolate\", \"in\", \"a\", \"computer\", \"lab\", \"grows\", \"old\", \"colored\",\n                      \"with\", \"an\"};\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new RAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]) == true)\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      Hits hits = knownSearcher.search(query);\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length() == 3);\n      float score = hits.score(0);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(testDoc3.toString().equals(hits.doc(0).toString()));\n      assertTrue(testDoc4.toString().equals(hits.doc(1).toString()));\n      assertTrue(testDoc1.toString().equals(hits.doc(2).toString()));\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits.id(1), \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      } \n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n\n\n  } \n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b710dd422ed2d90f021391b466efb3bbc3eb4423","date":1092422324,"type":3,"author":"Daniel Naber","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new RAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]) == true)\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      Hits hits = knownSearcher.search(query);\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length() == 3);\n      float score = hits.score(0);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(testDoc3.toString().equals(hits.doc(0).toString()));\n      assertTrue(testDoc4.toString().equals(hits.doc(1).toString()));\n      assertTrue(testDoc1.toString().equals(hits.doc(2).toString()));\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits.id(1), \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      } \n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n\n\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String [] termArray = {\"eating\", \"chocolate\", \"in\", \"a\", \"computer\", \"lab\", \"grows\", \"old\", \"colored\",\n                      \"with\", \"an\"};\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new RAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]) == true)\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      Hits hits = knownSearcher.search(query);\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length() == 3);\n      float score = hits.score(0);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(testDoc3.toString().equals(hits.doc(0).toString()));\n      assertTrue(testDoc4.toString().equals(hits.doc(1).toString()));\n      assertTrue(testDoc1.toString().equals(hits.doc(2).toString()));\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits.id(1), \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      } \n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n\n\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0","date":1096997448,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new RAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      Hits hits = knownSearcher.search(query);\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length() == 3);\n      float score = hits.score(0);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits.id(0) == 2);\n      assertTrue(hits.id(1) == 3);\n      assertTrue(hits.id(2) == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits.id(1), \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      } \n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n\n\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new RAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]) == true)\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      Hits hits = knownSearcher.search(query);\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length() == 3);\n      float score = hits.score(0);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(testDoc3.toString().equals(hits.doc(0).toString()));\n      assertTrue(testDoc4.toString().equals(hits.doc(1).toString()));\n      assertTrue(testDoc1.toString().equals(hits.doc(2).toString()));\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits.id(1), \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      } \n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n\n\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"352bfe1fae83b92d1562f01c057bfbe6f5af3ddb","date":1185160645,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new RAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      Hits hits = knownSearcher.search(query);\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length() == 3);\n      float score = hits.score(0);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits.id(0) == 2);\n      assertTrue(hits.id(1) == 3);\n      assertTrue(hits.id(2) == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits.id(1), \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits.id(1), mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits.id(1), fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new RAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      Hits hits = knownSearcher.search(query);\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length() == 3);\n      float score = hits.score(0);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits.id(0) == 2);\n      assertTrue(hits.id(1) == 3);\n      assertTrue(hits.id(2) == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits.id(1), \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      } \n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n\n\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"235efcba838a273934c5dd0ef66bb07c7fb0d718","date":1201256475,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new RAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      Hits hits = knownSearcher.search(query);\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length() == 3);\n      float score = hits.score(0);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits.id(0) == 2);\n      assertTrue(hits.id(1) == 3);\n      assertTrue(hits.id(2) == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits.id(1), \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits.id(1), mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits.id(1), fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new RAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      Hits hits = knownSearcher.search(query);\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length() == 3);\n      float score = hits.score(0);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits.id(0) == 2);\n      assertTrue(hits.id(1) == 3);\n      assertTrue(hits.id(2) == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits.id(1), \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits.id(1), mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits.id(1), fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2684bcb2a921b6b5b76f64ba986564ab1ef0649d","date":1202988124,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      Hits hits = knownSearcher.search(query);\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length() == 3);\n      float score = hits.score(0);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits.id(0) == 2);\n      assertTrue(hits.id(1) == 3);\n      assertTrue(hits.id(2) == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits.id(1), \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits.id(1), mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits.id(1), fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new RAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      Hits hits = knownSearcher.search(query);\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length() == 3);\n      float score = hits.score(0);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits.id(0) == 2);\n      assertTrue(hits.id(1) == 3);\n      assertTrue(hits.id(2) == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits.id(1), \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits.id(1), mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits.id(1), fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5af07783dbc171e26a694c4f7d735e30c2769faa","date":1211569075,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      float score = hits[0].score;\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      Hits hits = knownSearcher.search(query);\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length() == 3);\n      float score = hits.score(0);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits.id(0) == 2);\n      assertTrue(hits.id(1) == 3);\n      assertTrue(hits.id(2) == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits.id(1), \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits.id(1), mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits.id(1), fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6bcde5e3f23911110baa101ed062b544162825b5","date":1254521804,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      float score = hits[0].score;\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      float score = hits[0].score;\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e8d1458a2543cbd30cbfe7929be4dcb5c5251659","date":1254582241,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      float score = hits[0].score;\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", new Integer(3));\n    test4Map.put(\"lab\", new Integer(2));\n    test4Map.put(\"eating\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"with\", new Integer(1));\n    test4Map.put(\"a\", new Integer(1));\n    test4Map.put(\"colored\", new Integer(1));\n    test4Map.put(\"in\", new Integer(1));\n    test4Map.put(\"an\", new Integer(1));\n    test4Map.put(\"computer\", new Integer(1));\n    test4Map.put(\"old\", new Integer(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      float score = hits[0].score;\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a046c0c310bc77931fc8441bd920053b607dd14","date":1254584734,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      float score = hits[0].score;\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      float score = hits[0].score;\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca7db7f1c27f53e2fbd8dc29c316e1ac3d808e58","date":1256297148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      float score = hits[0].score;\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(term, knownSearcher);\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      float score = hits[0].score;\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e450c7d50c2fc84c963d0d7ade9d3217d868064d","date":1259932067,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      float score = hits[0].score;\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe0932c1d340f83fb0a611e5829b3046a1cc1152","date":1264946739,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(Version.LUCENE_CURRENT), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6","date":1265808957,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(Version.LUCENE_CURRENT), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a","date":1267298041,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1cedb00d2dd44640194401179358a2e3ba6051bf","date":1268243626,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT).setAnalyzer(\n          new SimpleAnalyzer(TEST_VERSION_CURRENT))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e52fea2c4081a1e552b98506691990be59503168","date":1268250331,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT).setAnalyzer(\n          new SimpleAnalyzer(TEST_VERSION_CURRENT))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8","date":1268494368,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new SimpleAnalyzer(TEST_VERSION_CURRENT))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new SimpleAnalyzer(TEST_VERSION_CURRENT))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new SimpleAnalyzer(TEST_VERSION_CURRENT))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6bcde5e3f23911110baa101ed062b544162825b5":["5af07783dbc171e26a694c4f7d735e30c2769faa"],"235efcba838a273934c5dd0ef66bb07c7fb0d718":["352bfe1fae83b92d1562f01c057bfbe6f5af3ddb"],"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6":["fe0932c1d340f83fb0a611e5829b3046a1cc1152"],"0a046c0c310bc77931fc8441bd920053b607dd14":["6bcde5e3f23911110baa101ed062b544162825b5","e8d1458a2543cbd30cbfe7929be4dcb5c5251659"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["a9ac13b5f0ce5ef1b2ce168367d993a79594b23a"],"e52fea2c4081a1e552b98506691990be59503168":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0":["b710dd422ed2d90f021391b466efb3bbc3eb4423"],"e450c7d50c2fc84c963d0d7ade9d3217d868064d":["ca7db7f1c27f53e2fbd8dc29c316e1ac3d808e58"],"b710dd422ed2d90f021391b466efb3bbc3eb4423":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["e52fea2c4081a1e552b98506691990be59503168"],"2684bcb2a921b6b5b76f64ba986564ab1ef0649d":["235efcba838a273934c5dd0ef66bb07c7fb0d718"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["5af07783dbc171e26a694c4f7d735e30c2769faa"],"fe0932c1d340f83fb0a611e5829b3046a1cc1152":["e450c7d50c2fc84c963d0d7ade9d3217d868064d"],"ca7db7f1c27f53e2fbd8dc29c316e1ac3d808e58":["0a046c0c310bc77931fc8441bd920053b607dd14"],"352bfe1fae83b92d1562f01c057bfbe6f5af3ddb":["6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5af07783dbc171e26a694c4f7d735e30c2769faa":["2684bcb2a921b6b5b76f64ba986564ab1ef0649d"],"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a":["55f083e91bb056b57de136da1dfc3b9b6ecc4ef6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"]},"commit2Childs":{"6bcde5e3f23911110baa101ed062b544162825b5":["0a046c0c310bc77931fc8441bd920053b607dd14"],"235efcba838a273934c5dd0ef66bb07c7fb0d718":["2684bcb2a921b6b5b76f64ba986564ab1ef0649d"],"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6":["a9ac13b5f0ce5ef1b2ce168367d993a79594b23a"],"0a046c0c310bc77931fc8441bd920053b607dd14":["ca7db7f1c27f53e2fbd8dc29c316e1ac3d808e58"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["e52fea2c4081a1e552b98506691990be59503168"],"e52fea2c4081a1e552b98506691990be59503168":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"],"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0":["352bfe1fae83b92d1562f01c057bfbe6f5af3ddb"],"b710dd422ed2d90f021391b466efb3bbc3eb4423":["6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"e450c7d50c2fc84c963d0d7ade9d3217d868064d":["fe0932c1d340f83fb0a611e5829b3046a1cc1152"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["b710dd422ed2d90f021391b466efb3bbc3eb4423"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"2684bcb2a921b6b5b76f64ba986564ab1ef0649d":["5af07783dbc171e26a694c4f7d735e30c2769faa"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["0a046c0c310bc77931fc8441bd920053b607dd14"],"352bfe1fae83b92d1562f01c057bfbe6f5af3ddb":["235efcba838a273934c5dd0ef66bb07c7fb0d718"],"fe0932c1d340f83fb0a611e5829b3046a1cc1152":["55f083e91bb056b57de136da1dfc3b9b6ecc4ef6"],"ca7db7f1c27f53e2fbd8dc29c316e1ac3d808e58":["e450c7d50c2fc84c963d0d7ade9d3217d868064d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"5af07783dbc171e26a694c4f7d735e30c2769faa":["6bcde5e3f23911110baa101ed062b544162825b5","e8d1458a2543cbd30cbfe7929be4dcb5c5251659"],"a9ac13b5f0ce5ef1b2ce168367d993a79594b23a":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}