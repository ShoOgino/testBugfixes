{"path":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","commits":[{"id":"b1405362241b561f5590ff4a87d5d6e173bcd9cf","date":1190107634,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"/dev/null","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.UN_TOKENIZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        writer.close(false);\n        assertEquals(0, cms.getExceptions().size());\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*181, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    try {\n      directory.close();\n    } catch (RuntimeException ioe) {\n      // MockRAMDirectory will throw IOExceptions when there\n      // are still open files, which is OK since some merge\n      // threads may still be running at this point.\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["346d5897e4c4e77ed5dbd31f7730ff30973d5971","d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c","290c401c31db375e771805c3ba7ac5f64c7370dc","c507fbcc682939239c013568bca7ca40ea6bd844"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c507fbcc682939239c013568bca7ca40ea6bd844","date":1191683563,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.UN_TOKENIZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        writer.close(false);\n        assertEquals(0, cms.getExceptions().size());\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*181, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    try {\n      directory.close();\n    } catch (RuntimeException re) {\n      // MockRAMDirectory will throw RuntimeExceptions when there\n      // are still open files, which is OK since some merge\n      // threads may still be running at this point.\n    }\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.UN_TOKENIZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        writer.close(false);\n        assertEquals(0, cms.getExceptions().size());\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*181, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    try {\n      directory.close();\n    } catch (RuntimeException ioe) {\n      // MockRAMDirectory will throw IOExceptions when there\n      // are still open files, which is OK since some merge\n      // threads may still be running at this point.\n    }\n  }\n\n","bugFix":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"bugIntro":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"290c401c31db375e771805c3ba7ac5f64c7370dc","date":1192034795,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.UN_TOKENIZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*181, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    try {\n      directory.close();\n    } catch (RuntimeException re) {\n      // MockRAMDirectory will throw RuntimeExceptions when there\n      // are still open files, which is OK since some merge\n      // threads may still be running at this point.\n    }\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.UN_TOKENIZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        writer.close(false);\n        assertEquals(0, cms.getExceptions().size());\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*181, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    try {\n      directory.close();\n    } catch (RuntimeException re) {\n      // MockRAMDirectory will throw RuntimeExceptions when there\n      // are still open files, which is OK since some merge\n      // threads may still be running at this point.\n    }\n  }\n\n","bugFix":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c","date":1196806748,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.UN_TOKENIZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    try {\n      directory.close();\n    } catch (RuntimeException re) {\n      // MockRAMDirectory will throw RuntimeExceptions when there\n      // are still open files, which is OK since some merge\n      // threads may still be running at this point.\n    }\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.UN_TOKENIZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*181, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    try {\n      directory.close();\n    } catch (RuntimeException re) {\n      // MockRAMDirectory will throw RuntimeExceptions when there\n      // are still open files, which is OK since some merge\n      // threads may still be running at this point.\n    }\n  }\n\n","bugFix":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"346d5897e4c4e77ed5dbd31f7730ff30973d5971","date":1198317988,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.UN_TOKENIZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.UN_TOKENIZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    try {\n      directory.close();\n    } catch (RuntimeException re) {\n      // MockRAMDirectory will throw RuntimeExceptions when there\n      // are still open files, which is OK since some merge\n      // threads may still be running at this point.\n    }\n  }\n\n","bugFix":["b1405362241b561f5590ff4a87d5d6e173bcd9cf","c507fbcc682939239c013568bca7ca40ea6bd844"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0018e7a0579df5d3de71d0bd878322a7abef04d9","date":1202242049,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.UN_TOKENIZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true, IndexWriter.MaxFieldLength.LIMITED);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false, IndexWriter.MaxFieldLength.LIMITED);\n      }\n      writer.close();\n    }\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.UN_TOKENIZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a","date":1221082732,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true, IndexWriter.MaxFieldLength.LIMITED);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false, IndexWriter.MaxFieldLength.LIMITED);\n      }\n      writer.close();\n    }\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.UN_TOKENIZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true, IndexWriter.MaxFieldLength.LIMITED);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false, IndexWriter.MaxFieldLength.LIMITED);\n      }\n      writer.close();\n    }\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c4ae99f08f69aa3acba7cd75134e8447eb747559","date":1222344278,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true, IndexWriter.MaxFieldLength.LIMITED);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false, IndexWriter.MaxFieldLength.LIMITED);\n      }\n      writer.close();\n    }\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e8d1458a2543cbd30cbfe7929be4dcb5c5251659","date":1254582241,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory, true);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a046c0c310bc77931fc8441bd920053b607dd14","date":1254584734,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory, true);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"87c966e9308847938a7c905c2e46a56d8df788b8","date":1255035452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(directory, ANALYZER, true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    for(int iter=0;iter<10;iter++) {\n      ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n      writer.setMergeScheduler(cms);\n      writer.setMaxBufferedDocs(2);\n      writer.setMergeFactor(100);\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      writer.setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.flush();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory, true);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(directory, ANALYZER, false, IndexWriter.MaxFieldLength.UNLIMITED);\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    for(int pass=0;pass<2;pass++) {\n      boolean autoCommit = pass==0;\n      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);\n\n      for(int iter=0;iter<10;iter++) {\n        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n        writer.setMergeScheduler(cms);\n        writer.setMaxBufferedDocs(2);\n        writer.setMergeFactor(100);\n\n        for(int j=0;j<201;j++) {\n          idField.setValue(Integer.toString(iter*201+j));\n          writer.addDocument(doc);\n        }\n\n        int delID = iter*201;\n        for(int j=0;j<20;j++) {\n          writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n          delID += 5;\n        }\n\n        // Force a bunch of merge threads to kick off so we\n        // stress out aborting them on close:\n        writer.setMergeFactor(3);\n        writer.addDocument(doc);\n        writer.flush();\n\n        writer.close(false);\n\n        IndexReader reader = IndexReader.open(directory, true);\n        assertEquals((1+iter)*182, reader.numDocs());\n        reader.close();\n\n        // Reopen\n        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);\n      }\n      writer.close();\n    }\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1","date":1255502337,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(directory, ANALYZER, true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    for(int iter=0;iter<10;iter++) {\n      ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n      writer.setMergeScheduler(cms);\n      writer.setMaxBufferedDocs(2);\n      writer.setMergeFactor(100);\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      writer.setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory, true);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(directory, ANALYZER, false, IndexWriter.MaxFieldLength.UNLIMITED);\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(directory, ANALYZER, true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    for(int iter=0;iter<10;iter++) {\n      ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n      writer.setMergeScheduler(cms);\n      writer.setMaxBufferedDocs(2);\n      writer.setMergeFactor(100);\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      writer.setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.flush();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory, true);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(directory, ANALYZER, false, IndexWriter.MaxFieldLength.UNLIMITED);\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1cedb00d2dd44640194401179358a2e3ba6051bf","date":1268243626,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT).setMaxBufferedDocs(2));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(100);\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory, true);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT).setOpenMode(OpenMode.APPEND));\n      ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(100);\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(directory, ANALYZER, true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    for(int iter=0;iter<10;iter++) {\n      ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n      writer.setMergeScheduler(cms);\n      writer.setMaxBufferedDocs(2);\n      writer.setMergeFactor(100);\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      writer.setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory, true);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(directory, ANALYZER, false, IndexWriter.MaxFieldLength.UNLIMITED);\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e52fea2c4081a1e552b98506691990be59503168","date":1268250331,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(directory, ANALYZER, true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    for(int iter=0;iter<10;iter++) {\n      ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n      writer.setMergeScheduler(cms);\n      writer.setMaxBufferedDocs(2);\n      writer.setMergeFactor(100);\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      writer.setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory, true);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(directory, ANALYZER, false, IndexWriter.MaxFieldLength.UNLIMITED);\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT).setMaxBufferedDocs(2));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(100);\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory, true);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT).setOpenMode(OpenMode.APPEND));\n      ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(100);\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8","date":1268494368,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)).setMaxBufferedDocs(2));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(100);\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory, true);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n      ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(100);\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(directory, ANALYZER, true, IndexWriter.MaxFieldLength.UNLIMITED);\n\n    for(int iter=0;iter<10;iter++) {\n      ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();\n      writer.setMergeScheduler(cms);\n      writer.setMaxBufferedDocs(2);\n      writer.setMergeFactor(100);\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      writer.setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory, true);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(directory, ANALYZER, false, IndexWriter.MaxFieldLength.UNLIMITED);\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)).setMaxBufferedDocs(2));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(100);\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory, true);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n      ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(100);\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    RAMDirectory directory = new MockRAMDirectory();\n\n    Document doc = new Document();\n    Field idField = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)).setMaxBufferedDocs(2));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(100);\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory, true);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(directory, new IndexWriterConfig(TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n      ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(100);\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c4ae99f08f69aa3acba7cd75134e8447eb747559":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"0a046c0c310bc77931fc8441bd920053b607dd14":["c4ae99f08f69aa3acba7cd75134e8447eb747559","e8d1458a2543cbd30cbfe7929be4dcb5c5251659"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1"],"be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1":["87c966e9308847938a7c905c2e46a56d8df788b8"],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["0018e7a0579df5d3de71d0bd878322a7abef04d9"],"e52fea2c4081a1e552b98506691990be59503168":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"0018e7a0579df5d3de71d0bd878322a7abef04d9":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"c507fbcc682939239c013568bca7ca40ea6bd844":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["e52fea2c4081a1e552b98506691990be59503168"],"d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c":["290c401c31db375e771805c3ba7ac5f64c7370dc"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["c4ae99f08f69aa3acba7cd75134e8447eb747559"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"290c401c31db375e771805c3ba7ac5f64c7370dc":["c507fbcc682939239c013568bca7ca40ea6bd844"],"87c966e9308847938a7c905c2e46a56d8df788b8":["0a046c0c310bc77931fc8441bd920053b607dd14"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"]},"commit2Childs":{"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["0018e7a0579df5d3de71d0bd878322a7abef04d9"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["c507fbcc682939239c013568bca7ca40ea6bd844"],"c4ae99f08f69aa3acba7cd75134e8447eb747559":["0a046c0c310bc77931fc8441bd920053b607dd14","e8d1458a2543cbd30cbfe7929be4dcb5c5251659"],"0a046c0c310bc77931fc8441bd920053b607dd14":["87c966e9308847938a7c905c2e46a56d8df788b8"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["e52fea2c4081a1e552b98506691990be59503168"],"be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["c4ae99f08f69aa3acba7cd75134e8447eb747559"],"0018e7a0579df5d3de71d0bd878322a7abef04d9":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"e52fea2c4081a1e552b98506691990be59503168":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"],"c507fbcc682939239c013568bca7ca40ea6bd844":["290c401c31db375e771805c3ba7ac5f64c7370dc"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["0a046c0c310bc77931fc8441bd920053b607dd14"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"290c401c31db375e771805c3ba7ac5f64c7370dc":["d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c"],"87c966e9308847938a7c905c2e46a56d8df788b8":["be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}