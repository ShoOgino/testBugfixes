{"path":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","commits":[{"id":"c2042d3e27841c5b60112990fc33559e10ccf6dd","date":1424537395,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores)\n      throws IOException {\n    return new Weight(this) {\n\n      private float queryNorm;\n      private float queryWeight;\n\n      @Override\n      public float getValueForNormalization() throws IOException {\n        queryWeight = getBoost();\n        return queryWeight * queryWeight;\n      }\n\n      @Override\n      public void normalize(float norm, float topLevelBoost) {\n        queryNorm = norm * topLevelBoost;\n        queryWeight *= queryNorm;\n      }\n\n      @Override\n      public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n        final Scorer s = scorer(context, context.reader().getLiveDocs());\n        final boolean exists = (s != null && s.advance(doc) == doc);\n\n        final ComplexExplanation result = new ComplexExplanation();\n        if (exists) {\n          result.setDescription(TermsQuery.this.toString() + \", product of:\");\n          result.setValue(queryWeight);\n          result.setMatch(Boolean.TRUE);\n          result.addDetail(new Explanation(getBoost(), \"boost\"));\n          result.addDetail(new Explanation(queryNorm, \"queryNorm\"));\n        } else {\n          result.setDescription(TermsQuery.this.toString() + \" doesn't match id \" + doc);\n          result.setValue(0);\n          result.setMatch(Boolean.FALSE);\n        }\n        return result;\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        final BytesRef spare = new BytesRef(termsBytes);\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        for (TermsAndField termsAndField : termsAndFields) {\n          if ((terms = fields.terms(termsAndField.field)) != null) {\n            termsEnum = terms.iterator(termsEnum); // this won't return null\n            for (int i = termsAndField.start; i < termsAndField.end; i++) {\n              spare.offset = offsets[i];\n              spare.length = offsets[i+1] - offsets[i];\n              if (termsEnum.seekExact(spare)) {\n                docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE); // no freq since we don't need them\n                builder.or(docs);\n              }\n            }\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return queryWeight;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int nextPosition() throws IOException {\n            return -1;\n          }\n\n          @Override\n          public int startOffset() throws IOException {\n            return -1;\n          }\n\n          @Override\n          public int endOffset() throws IOException {\n            return -1;\n          }\n\n          @Override\n          public BytesRef getPayload() throws IOException {\n            return null;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"099eb73e9eafc9efce5df2893da98034fafeee81","date":1424537570,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores)\n      throws IOException {\n    return new Weight(this) {\n\n      private float queryNorm;\n      private float queryWeight;\n\n      @Override\n      public float getValueForNormalization() throws IOException {\n        queryWeight = getBoost();\n        return queryWeight * queryWeight;\n      }\n\n      @Override\n      public void normalize(float norm, float topLevelBoost) {\n        queryNorm = norm * topLevelBoost;\n        queryWeight *= queryNorm;\n      }\n\n      @Override\n      public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n        final Scorer s = scorer(context, context.reader().getLiveDocs());\n        final boolean exists = (s != null && s.advance(doc) == doc);\n\n        final ComplexExplanation result = new ComplexExplanation();\n        if (exists) {\n          result.setDescription(TermsQuery.this.toString() + \", product of:\");\n          result.setValue(queryWeight);\n          result.setMatch(Boolean.TRUE);\n          result.addDetail(new Explanation(getBoost(), \"boost\"));\n          result.addDetail(new Explanation(queryNorm, \"queryNorm\"));\n        } else {\n          result.setDescription(TermsQuery.this.toString() + \" doesn't match id \" + doc);\n          result.setValue(0);\n          result.setMatch(Boolean.FALSE);\n        }\n        return result;\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        final BytesRef spare = new BytesRef(termsBytes);\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        for (TermsAndField termsAndField : termsAndFields) {\n          if ((terms = fields.terms(termsAndField.field)) != null) {\n            termsEnum = terms.iterator(termsEnum); // this won't return null\n            for (int i = termsAndField.start; i < termsAndField.end; i++) {\n              spare.offset = offsets[i];\n              spare.length = offsets[i+1] - offsets[i];\n              if (termsEnum.seekExact(spare)) {\n                docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE); // no freq since we don't need them\n                builder.or(docs);\n              }\n            }\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return queryWeight;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores)\n      throws IOException {\n    return new Weight(this) {\n\n      private float queryNorm;\n      private float queryWeight;\n\n      @Override\n      public float getValueForNormalization() throws IOException {\n        queryWeight = getBoost();\n        return queryWeight * queryWeight;\n      }\n\n      @Override\n      public void normalize(float norm, float topLevelBoost) {\n        queryNorm = norm * topLevelBoost;\n        queryWeight *= queryNorm;\n      }\n\n      @Override\n      public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n        final Scorer s = scorer(context, context.reader().getLiveDocs());\n        final boolean exists = (s != null && s.advance(doc) == doc);\n\n        final ComplexExplanation result = new ComplexExplanation();\n        if (exists) {\n          result.setDescription(TermsQuery.this.toString() + \", product of:\");\n          result.setValue(queryWeight);\n          result.setMatch(Boolean.TRUE);\n          result.addDetail(new Explanation(getBoost(), \"boost\"));\n          result.addDetail(new Explanation(queryNorm, \"queryNorm\"));\n        } else {\n          result.setDescription(TermsQuery.this.toString() + \" doesn't match id \" + doc);\n          result.setValue(0);\n          result.setMatch(Boolean.FALSE);\n        }\n        return result;\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        final BytesRef spare = new BytesRef(termsBytes);\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        for (TermsAndField termsAndField : termsAndFields) {\n          if ((terms = fields.terms(termsAndField.field)) != null) {\n            termsEnum = terms.iterator(termsEnum); // this won't return null\n            for (int i = termsAndField.start; i < termsAndField.end; i++) {\n              spare.offset = offsets[i];\n              spare.length = offsets[i+1] - offsets[i];\n              if (termsEnum.seekExact(spare)) {\n                docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE); // no freq since we don't need them\n                builder.or(docs);\n              }\n            }\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return queryWeight;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int nextPosition() throws IOException {\n            return -1;\n          }\n\n          @Override\n          public int startOffset() throws IOException {\n            return -1;\n          }\n\n          @Override\n          public int endOffset() throws IOException {\n            return -1;\n          }\n\n          @Override\n          public BytesRef getPayload() throws IOException {\n            return null;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores)\n      throws IOException {\n    return new Weight(this) {\n\n      private float queryNorm;\n      private float queryWeight;\n\n      @Override\n      public float getValueForNormalization() throws IOException {\n        queryWeight = getBoost();\n        return queryWeight * queryWeight;\n      }\n\n      @Override\n      public void normalize(float norm, float topLevelBoost) {\n        queryNorm = norm * topLevelBoost;\n        queryWeight *= queryNorm;\n      }\n\n      @Override\n      public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n        final Scorer s = scorer(context, context.reader().getLiveDocs());\n        final boolean exists = (s != null && s.advance(doc) == doc);\n\n        final ComplexExplanation result = new ComplexExplanation();\n        if (exists) {\n          result.setDescription(TermsQuery.this.toString() + \", product of:\");\n          result.setValue(queryWeight);\n          result.setMatch(Boolean.TRUE);\n          result.addDetail(new Explanation(getBoost(), \"boost\"));\n          result.addDetail(new Explanation(queryNorm, \"queryNorm\"));\n        } else {\n          result.setDescription(TermsQuery.this.toString() + \" doesn't match id \" + doc);\n          result.setValue(0);\n          result.setMatch(Boolean.FALSE);\n        }\n        return result;\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        final BytesRef spare = new BytesRef(termsBytes);\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        for (TermsAndField termsAndField : termsAndFields) {\n          if ((terms = fields.terms(termsAndField.field)) != null) {\n            termsEnum = terms.iterator(); // this won't return null\n            for (int i = termsAndField.start; i < termsAndField.end; i++) {\n              spare.offset = offsets[i];\n              spare.length = offsets[i+1] - offsets[i];\n              if (termsEnum.seekExact(spare)) {\n                docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE); // no freq since we don't need them\n                builder.or(docs);\n              }\n            }\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return queryWeight;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores)\n      throws IOException {\n    return new Weight(this) {\n\n      private float queryNorm;\n      private float queryWeight;\n\n      @Override\n      public float getValueForNormalization() throws IOException {\n        queryWeight = getBoost();\n        return queryWeight * queryWeight;\n      }\n\n      @Override\n      public void normalize(float norm, float topLevelBoost) {\n        queryNorm = norm * topLevelBoost;\n        queryWeight *= queryNorm;\n      }\n\n      @Override\n      public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n        final Scorer s = scorer(context, context.reader().getLiveDocs());\n        final boolean exists = (s != null && s.advance(doc) == doc);\n\n        final ComplexExplanation result = new ComplexExplanation();\n        if (exists) {\n          result.setDescription(TermsQuery.this.toString() + \", product of:\");\n          result.setValue(queryWeight);\n          result.setMatch(Boolean.TRUE);\n          result.addDetail(new Explanation(getBoost(), \"boost\"));\n          result.addDetail(new Explanation(queryNorm, \"queryNorm\"));\n        } else {\n          result.setDescription(TermsQuery.this.toString() + \" doesn't match id \" + doc);\n          result.setValue(0);\n          result.setMatch(Boolean.FALSE);\n        }\n        return result;\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        final BytesRef spare = new BytesRef(termsBytes);\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        for (TermsAndField termsAndField : termsAndFields) {\n          if ((terms = fields.terms(termsAndField.field)) != null) {\n            termsEnum = terms.iterator(termsEnum); // this won't return null\n            for (int i = termsAndField.start; i < termsAndField.end; i++) {\n              spare.offset = offsets[i];\n              spare.length = offsets[i+1] - offsets[i];\n              if (termsEnum.seekExact(spare)) {\n                docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE); // no freq since we don't need them\n                builder.or(docs);\n              }\n            }\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return queryWeight;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05c52ac194342b760b830342ee8423fcf00e54d0","date":1429197275,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores)\n      throws IOException {\n    return new Weight(this) {\n\n      private float queryNorm;\n      private float queryWeight;\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public float getValueForNormalization() throws IOException {\n        queryWeight = getBoost();\n        return queryWeight * queryWeight;\n      }\n\n      @Override\n      public void normalize(float norm, float topLevelBoost) {\n        queryNorm = norm * topLevelBoost;\n        queryWeight *= queryNorm;\n      }\n\n      @Override\n      public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n        final Scorer s = scorer(context, context.reader().getLiveDocs());\n        final boolean exists = (s != null && s.advance(doc) == doc);\n\n        final ComplexExplanation result = new ComplexExplanation();\n        if (exists) {\n          result.setDescription(TermsQuery.this.toString() + \", product of:\");\n          result.setValue(queryWeight);\n          result.setMatch(Boolean.TRUE);\n          result.addDetail(new Explanation(getBoost(), \"boost\"));\n          result.addDetail(new Explanation(queryNorm, \"queryNorm\"));\n        } else {\n          result.setDescription(TermsQuery.this.toString() + \" doesn't match id \" + doc);\n          result.setValue(0);\n          result.setMatch(Boolean.FALSE);\n        }\n        return result;\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        final BytesRef spare = new BytesRef(termsBytes);\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        for (TermsAndField termsAndField : termsAndFields) {\n          if ((terms = fields.terms(termsAndField.field)) != null) {\n            termsEnum = terms.iterator(); // this won't return null\n            for (int i = termsAndField.start; i < termsAndField.end; i++) {\n              spare.offset = offsets[i];\n              spare.length = offsets[i+1] - offsets[i];\n              if (termsEnum.seekExact(spare)) {\n                docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE); // no freq since we don't need them\n                builder.or(docs);\n              }\n            }\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return queryWeight;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores)\n      throws IOException {\n    return new Weight(this) {\n\n      private float queryNorm;\n      private float queryWeight;\n\n      @Override\n      public float getValueForNormalization() throws IOException {\n        queryWeight = getBoost();\n        return queryWeight * queryWeight;\n      }\n\n      @Override\n      public void normalize(float norm, float topLevelBoost) {\n        queryNorm = norm * topLevelBoost;\n        queryWeight *= queryNorm;\n      }\n\n      @Override\n      public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n        final Scorer s = scorer(context, context.reader().getLiveDocs());\n        final boolean exists = (s != null && s.advance(doc) == doc);\n\n        final ComplexExplanation result = new ComplexExplanation();\n        if (exists) {\n          result.setDescription(TermsQuery.this.toString() + \", product of:\");\n          result.setValue(queryWeight);\n          result.setMatch(Boolean.TRUE);\n          result.addDetail(new Explanation(getBoost(), \"boost\"));\n          result.addDetail(new Explanation(queryNorm, \"queryNorm\"));\n        } else {\n          result.setDescription(TermsQuery.this.toString() + \" doesn't match id \" + doc);\n          result.setValue(0);\n          result.setMatch(Boolean.FALSE);\n        }\n        return result;\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        final BytesRef spare = new BytesRef(termsBytes);\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        for (TermsAndField termsAndField : termsAndFields) {\n          if ((terms = fields.terms(termsAndField.field)) != null) {\n            termsEnum = terms.iterator(); // this won't return null\n            for (int i = termsAndField.start; i < termsAndField.end; i++) {\n              spare.offset = offsets[i];\n              spare.length = offsets[i+1] - offsets[i];\n              if (termsEnum.seekExact(spare)) {\n                docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE); // no freq since we don't need them\n                builder.or(docs);\n              }\n            }\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return queryWeight;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff3285c7b2387faedef0ffb24db20c4cbbd9fd91","date":1429620941,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores)\n      throws IOException {\n    return new Weight(this) {\n\n      private float queryNorm;\n      private float queryWeight;\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public float getValueForNormalization() throws IOException {\n        queryWeight = getBoost();\n        return queryWeight * queryWeight;\n      }\n\n      @Override\n      public void normalize(float norm, float topLevelBoost) {\n        queryNorm = norm * topLevelBoost;\n        queryWeight *= queryNorm;\n      }\n\n      @Override\n      public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n        final Scorer s = scorer(context, context.reader().getLiveDocs());\n        final boolean exists = (s != null && s.advance(doc) == doc);\n\n        if (exists) {\n          return Explanation.match(queryWeight, TermsQuery.this.toString() + \", product of:\",\n              Explanation.match(getBoost(), \"boost\"), Explanation.match(queryNorm, \"queryNorm\"));\n        } else {\n          return Explanation.noMatch(TermsQuery.this.toString() + \" doesn't match id \" + doc);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        final BytesRef spare = new BytesRef(termsBytes);\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        for (TermsAndField termsAndField : termsAndFields) {\n          if ((terms = fields.terms(termsAndField.field)) != null) {\n            termsEnum = terms.iterator(); // this won't return null\n            for (int i = termsAndField.start; i < termsAndField.end; i++) {\n              spare.offset = offsets[i];\n              spare.length = offsets[i+1] - offsets[i];\n              if (termsEnum.seekExact(spare)) {\n                docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE); // no freq since we don't need them\n                builder.or(docs);\n              }\n            }\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return queryWeight;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores)\n      throws IOException {\n    return new Weight(this) {\n\n      private float queryNorm;\n      private float queryWeight;\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public float getValueForNormalization() throws IOException {\n        queryWeight = getBoost();\n        return queryWeight * queryWeight;\n      }\n\n      @Override\n      public void normalize(float norm, float topLevelBoost) {\n        queryNorm = norm * topLevelBoost;\n        queryWeight *= queryNorm;\n      }\n\n      @Override\n      public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n        final Scorer s = scorer(context, context.reader().getLiveDocs());\n        final boolean exists = (s != null && s.advance(doc) == doc);\n\n        final ComplexExplanation result = new ComplexExplanation();\n        if (exists) {\n          result.setDescription(TermsQuery.this.toString() + \", product of:\");\n          result.setValue(queryWeight);\n          result.setMatch(Boolean.TRUE);\n          result.addDetail(new Explanation(getBoost(), \"boost\"));\n          result.addDetail(new Explanation(queryNorm, \"queryNorm\"));\n        } else {\n          result.setDescription(TermsQuery.this.toString() + \" doesn't match id \" + doc);\n          result.setValue(0);\n          result.setMatch(Boolean.FALSE);\n        }\n        return result;\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        final BytesRef spare = new BytesRef(termsBytes);\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        for (TermsAndField termsAndField : termsAndFields) {\n          if ((terms = fields.terms(termsAndField.field)) != null) {\n            termsEnum = terms.iterator(); // this won't return null\n            for (int i = termsAndField.start; i < termsAndField.end; i++) {\n              spare.offset = offsets[i];\n              spare.length = offsets[i+1] - offsets[i];\n              if (termsEnum.seekExact(spare)) {\n                docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE); // no freq since we don't need them\n                builder.or(docs);\n              }\n            }\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return queryWeight;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a54410d37fe11baed59cc55dcad44db795f732c2","date":1430995912,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs, float score) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return score;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores)\n      throws IOException {\n    return new Weight(this) {\n\n      private float queryNorm;\n      private float queryWeight;\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public float getValueForNormalization() throws IOException {\n        queryWeight = getBoost();\n        return queryWeight * queryWeight;\n      }\n\n      @Override\n      public void normalize(float norm, float topLevelBoost) {\n        queryNorm = norm * topLevelBoost;\n        queryWeight *= queryNorm;\n      }\n\n      @Override\n      public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n        final Scorer s = scorer(context, context.reader().getLiveDocs());\n        final boolean exists = (s != null && s.advance(doc) == doc);\n\n        if (exists) {\n          return Explanation.match(queryWeight, TermsQuery.this.toString() + \", product of:\",\n              Explanation.match(getBoost(), \"boost\"), Explanation.match(queryNorm, \"queryNorm\"));\n        } else {\n          return Explanation.noMatch(TermsQuery.this.toString() + \" doesn't match id \" + doc);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        final BytesRef spare = new BytesRef(termsBytes);\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        for (TermsAndField termsAndField : termsAndFields) {\n          if ((terms = fields.terms(termsAndField.field)) != null) {\n            termsEnum = terms.iterator(); // this won't return null\n            for (int i = termsAndField.start; i < termsAndField.end; i++) {\n              spare.offset = offsets[i];\n              spare.length = offsets[i+1] - offsets[i];\n              if (termsEnum.seekExact(spare)) {\n                docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE); // no freq since we don't need them\n                builder.or(docs);\n              }\n            }\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return queryWeight;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a7f94ff172f40ff68a926d112e25b96bc38e5a27","date":1431002360,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs, float score) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        return new Scorer(this) {\n\n          @Override\n          public float score() throws IOException {\n            return score;\n          }\n\n          @Override\n          public int freq() throws IOException {\n            return 1;\n          }\n\n          @Override\n          public int docID() {\n            return disi.docID();\n          }\n\n          @Override\n          public int nextDoc() throws IOException {\n            return disi.nextDoc();\n          }\n\n          @Override\n          public int advance(int target) throws IOException {\n            return disi.advance(target);\n          }\n\n          @Override\n          public long cost() {\n            return disi.cost();\n          }\n\n        };\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e2b30bfb739689d33532e6b7d2d39582bd89a3a","date":1432237721,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrBitSet rewrite(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        BitDocIdSet.Builder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n              builder.or(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new BitDocIdSet.Builder(reader.maxDoc());\n              docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n              builder.or(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n                builder.or(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery bq = new BooleanQuery();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq);\n          q.setBoost(score());\n          return new WeightOrBitSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        } else {\n          assert builder != null;\n          return new WeightOrBitSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(BitDocIdSet set) {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context, acceptDocs);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.bitset);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context, acceptDocs);\n        } else {\n          return scorer(weightOrBitSet.bitset);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n        BitDocIdSet.Builder builder = new BitDocIdSet.Builder(reader.maxDoc());\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n            builder.or(docs);\n          }\n        }\n        BitDocIdSet result = builder.build();\n        if (result == null) {\n          return null;\n        }\n\n        final DocIdSetIterator disi = result.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f68d01cf19df971dcdcb05e30247f4ad7ec9747","date":1434611645,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrBitSet rewrite(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        BitDocIdSet.Builder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n              builder.or(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new BitDocIdSet.Builder(reader.maxDoc());\n              docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n              builder.or(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n                builder.or(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          q.setBoost(score());\n          return new WeightOrBitSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        } else {\n          assert builder != null;\n          return new WeightOrBitSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(BitDocIdSet set) {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context, acceptDocs);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.bitset);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context, acceptDocs);\n        } else {\n          return scorer(weightOrBitSet.bitset);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrBitSet rewrite(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        BitDocIdSet.Builder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n              builder.or(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new BitDocIdSet.Builder(reader.maxDoc());\n              docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n              builder.or(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n                builder.or(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery bq = new BooleanQuery();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq);\n          q.setBoost(score());\n          return new WeightOrBitSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        } else {\n          assert builder != null;\n          return new WeightOrBitSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(BitDocIdSet set) {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context, acceptDocs);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.bitset);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context, acceptDocs);\n        } else {\n          return scorer(weightOrBitSet.bitset);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrBitSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        BitDocIdSet.Builder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.or(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new BitDocIdSet.Builder(reader.maxDoc());\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.or(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.or(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          q.setBoost(score());\n          return new WeightOrBitSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        } else {\n          assert builder != null;\n          return new WeightOrBitSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(BitDocIdSet set) {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.bitset);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.bitset);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrBitSet rewrite(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        BitDocIdSet.Builder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n              builder.or(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new BitDocIdSet.Builder(reader.maxDoc());\n              docs = termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n              builder.or(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(acceptDocs, docs, PostingsEnum.NONE);\n                builder.or(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          q.setBoost(score());\n          return new WeightOrBitSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        } else {\n          assert builder != null;\n          return new WeightOrBitSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(BitDocIdSet set) {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context, acceptDocs);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.bitset);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context, acceptDocs);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context, acceptDocs);\n        } else {\n          return scorer(weightOrBitSet.bitset);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e91a2d9ed80172872da0f517870da6756289554","date":1436431140,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc());\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          q.setBoost(score());\n          return new WeightOrDocIdSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrBitSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        BitDocIdSet.Builder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.or(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new BitDocIdSet.Builder(reader.maxDoc());\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.or(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.or(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          q.setBoost(score());\n          return new WeightOrBitSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        } else {\n          assert builder != null;\n          return new WeightOrBitSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(BitDocIdSet set) {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.bitset);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrBitSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.bitset);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2dfdf766e55e943d942055d7de53c7ad6bc45283","date":1441632886,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc());\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc());\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          q.setBoost(score());\n          return new WeightOrDocIdSet(searcher.rewrite(q).createWeight(searcher, needsScores));\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9ee4c03e3ee986704eeeb45c571d001905a6430","date":1462194267,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (TermsQuery.this.fields.size() == 1) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc());\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30540ec27130887a9372c159e8fe971200f37727","date":1462223109,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (TermsQuery.this.fields.size() == 1) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc());\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dc891462bf24e8b4c8d0bdc862942e75015540c7","date":1462342542,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (singleField) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (TermsQuery.this.fields.size() == 1) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55b50463286869f584cf849d1587a0fcd54d1dfa","date":1462378517,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (singleField) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              builder = new DocIdSetBuilder(reader.maxDoc());\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"73450c0955930295d34703e7ddbfc6973b7a121a","date":1462431925,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (singleField) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (TermsQuery.this.fields.size() == 1) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"02e175abd2c4c1611c5a9647486ae8ba249a94c1","date":1468327116,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (singleField) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (singleField) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/TermsQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":null,"sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public void extractTerms(Set<Term> terms) {\n        // no-op\n        // This query is for abuse cases when the number of terms is too high to\n        // run efficiently as a BooleanQuery. So likewise we hide its terms in\n        // order to protect highlighters\n      }\n\n      /**\n       * On the given leaf context, try to either rewrite to a disjunction if\n       * there are few matching terms, or build a bitset containing matching docs.\n       */\n      private WeightOrDocIdSet rewrite(LeafReaderContext context) throws IOException {\n        final LeafReader reader = context.reader();\n\n        // We will first try to collect up to 'threshold' terms into 'matchingTerms'\n        // if there are two many terms, we will fall back to building the 'builder'\n        final int threshold = Math.min(BOOLEAN_REWRITE_TERM_COUNT_THRESHOLD, BooleanQuery.getMaxClauseCount());\n        assert termData.size() > threshold : \"Query should have been rewritten\";\n        List<TermAndState> matchingTerms = new ArrayList<>(threshold);\n        DocIdSetBuilder builder = null;\n\n        final Fields fields = reader.fields();\n        String lastField = null;\n        Terms terms = null;\n        TermsEnum termsEnum = null;\n        PostingsEnum docs = null;\n        TermIterator iterator = termData.iterator();\n        for (BytesRef term = iterator.next(); term != null; term = iterator.next()) {\n          String field = iterator.field();\n          // comparing references is fine here\n          if (field != lastField) {\n            terms = fields.terms(field);\n            if (terms == null) {\n              termsEnum = null;\n            } else {\n              termsEnum = terms.iterator();\n            }\n            lastField = field;\n          }\n          if (termsEnum != null && termsEnum.seekExact(term)) {\n            if (matchingTerms == null) {\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n            } else if (matchingTerms.size() < threshold) {\n              matchingTerms.add(new TermAndState(field, termsEnum));\n            } else {\n              assert matchingTerms.size() == threshold;\n              if (singleField) {\n                // common case: all terms are in the same field\n                // use an optimized builder that leverages terms stats to be more efficient\n                builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n              } else {\n                // corner case: different fields\n                // don't make assumptions about the docs we will get\n                builder = new DocIdSetBuilder(reader.maxDoc());\n              }\n              docs = termsEnum.postings(docs, PostingsEnum.NONE);\n              builder.add(docs);\n              for (TermAndState t : matchingTerms) {\n                t.termsEnum.seekExact(t.term, t.state);\n                docs = t.termsEnum.postings(docs, PostingsEnum.NONE);\n                builder.add(docs);\n              }\n              matchingTerms = null;\n            }\n          }\n        }\n        if (matchingTerms != null) {\n          assert builder == null;\n          BooleanQuery.Builder bq = new BooleanQuery.Builder();\n          for (TermAndState t : matchingTerms) {\n            final TermContext termContext = new TermContext(searcher.getTopReaderContext());\n            termContext.register(t.state, context.ord, t.docFreq, t.totalTermFreq);\n            bq.add(new TermQuery(new Term(t.field, t.term), termContext), Occur.SHOULD);\n          }\n          Query q = new ConstantScoreQuery(bq.build());\n          final Weight weight = searcher.rewrite(q).createWeight(searcher, needsScores);\n          weight.normalize(1f, score());\n          return new WeightOrDocIdSet(weight);\n        } else {\n          assert builder != null;\n          return new WeightOrDocIdSet(builder.build());\n        }\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.bulkScorer(context);\n        } else {\n          final Scorer scorer = scorer(weightOrBitSet.set);\n          if (scorer == null) {\n            return null;\n          }\n          return new DefaultBulkScorer(scorer);\n        }\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final WeightOrDocIdSet weightOrBitSet = rewrite(context);\n        if (weightOrBitSet.weight != null) {\n          return weightOrBitSet.weight.scorer(context);\n        } else {\n          return scorer(weightOrBitSet.set);\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3e2b30bfb739689d33532e6b7d2d39582bd89a3a":["a7f94ff172f40ff68a926d112e25b96bc38e5a27"],"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["099eb73e9eafc9efce5df2893da98034fafeee81"],"099eb73e9eafc9efce5df2893da98034fafeee81":["c2042d3e27841c5b60112990fc33559e10ccf6dd"],"a7f94ff172f40ff68a926d112e25b96bc38e5a27":["a54410d37fe11baed59cc55dcad44db795f732c2"],"73450c0955930295d34703e7ddbfc6973b7a121a":["30540ec27130887a9372c159e8fe971200f37727","dc891462bf24e8b4c8d0bdc862942e75015540c7"],"2dfdf766e55e943d942055d7de53c7ad6bc45283":["0e91a2d9ed80172872da0f517870da6756289554"],"30540ec27130887a9372c159e8fe971200f37727":["2dfdf766e55e943d942055d7de53c7ad6bc45283","c9ee4c03e3ee986704eeeb45c571d001905a6430"],"05c52ac194342b760b830342ee8423fcf00e54d0":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"c2042d3e27841c5b60112990fc33559e10ccf6dd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["55b50463286869f584cf849d1587a0fcd54d1dfa","02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"55b50463286869f584cf849d1587a0fcd54d1dfa":["2dfdf766e55e943d942055d7de53c7ad6bc45283","dc891462bf24e8b4c8d0bdc862942e75015540c7"],"3f68d01cf19df971dcdcb05e30247f4ad7ec9747":["3e2b30bfb739689d33532e6b7d2d39582bd89a3a"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["3f68d01cf19df971dcdcb05e30247f4ad7ec9747"],"dc891462bf24e8b4c8d0bdc862942e75015540c7":["30540ec27130887a9372c159e8fe971200f37727"],"c9ee4c03e3ee986704eeeb45c571d001905a6430":["2dfdf766e55e943d942055d7de53c7ad6bc45283"],"0e91a2d9ed80172872da0f517870da6756289554":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["73450c0955930295d34703e7ddbfc6973b7a121a"],"ff3285c7b2387faedef0ffb24db20c4cbbd9fd91":["05c52ac194342b760b830342ee8423fcf00e54d0"],"a54410d37fe11baed59cc55dcad44db795f732c2":["ff3285c7b2387faedef0ffb24db20c4cbbd9fd91"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["02e175abd2c4c1611c5a9647486ae8ba249a94c1"]},"commit2Childs":{"3e2b30bfb739689d33532e6b7d2d39582bd89a3a":["3f68d01cf19df971dcdcb05e30247f4ad7ec9747"],"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["05c52ac194342b760b830342ee8423fcf00e54d0"],"099eb73e9eafc9efce5df2893da98034fafeee81":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"a7f94ff172f40ff68a926d112e25b96bc38e5a27":["3e2b30bfb739689d33532e6b7d2d39582bd89a3a"],"73450c0955930295d34703e7ddbfc6973b7a121a":["02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"2dfdf766e55e943d942055d7de53c7ad6bc45283":["30540ec27130887a9372c159e8fe971200f37727","55b50463286869f584cf849d1587a0fcd54d1dfa","c9ee4c03e3ee986704eeeb45c571d001905a6430"],"30540ec27130887a9372c159e8fe971200f37727":["73450c0955930295d34703e7ddbfc6973b7a121a","dc891462bf24e8b4c8d0bdc862942e75015540c7"],"05c52ac194342b760b830342ee8423fcf00e54d0":["ff3285c7b2387faedef0ffb24db20c4cbbd9fd91"],"c2042d3e27841c5b60112990fc33559e10ccf6dd":["099eb73e9eafc9efce5df2893da98034fafeee81"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"55b50463286869f584cf849d1587a0fcd54d1dfa":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"3f68d01cf19df971dcdcb05e30247f4ad7ec9747":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"dc891462bf24e8b4c8d0bdc862942e75015540c7":["73450c0955930295d34703e7ddbfc6973b7a121a","55b50463286869f584cf849d1587a0fcd54d1dfa"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0e91a2d9ed80172872da0f517870da6756289554"],"c9ee4c03e3ee986704eeeb45c571d001905a6430":["30540ec27130887a9372c159e8fe971200f37727"],"0e91a2d9ed80172872da0f517870da6756289554":["2dfdf766e55e943d942055d7de53c7ad6bc45283"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c2042d3e27841c5b60112990fc33559e10ccf6dd"],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ff3285c7b2387faedef0ffb24db20c4cbbd9fd91":["a54410d37fe11baed59cc55dcad44db795f732c2"],"a54410d37fe11baed59cc55dcad44db795f732c2":["a7f94ff172f40ff68a926d112e25b96bc38e5a27"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}