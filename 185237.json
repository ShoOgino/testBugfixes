{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.RandomTokenStream#RandomTokenStream(int,String[],BytesRef[]).mjava","commits":[{"id":"f21ce13f410ee015e1ba14687ab4b8518ac52a11","date":1359713213,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.RandomTokenStream#RandomTokenStream(int,String[],BytesRef[]).mjava","pathOld":"/dev/null","sourceNew":"    protected RandomTokenStream(int len, String[] sampleTerms, BytesRef[] sampleTermBytes) {\n      this(len, sampleTerms, sampleTermBytes, rarely());\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0fa6955ed1b1007ded1349ab72cea4555640432f","date":1359721908,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.RandomTokenStream#RandomTokenStream(int,String[],BytesRef[]).mjava","pathOld":"/dev/null","sourceNew":"    protected RandomTokenStream(int len, String[] sampleTerms, BytesRef[] sampleTermBytes) {\n      this(len, sampleTerms, sampleTermBytes, rarely());\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae230518a1a68acc124bef8df61ef94bd7c1295e","date":1417181719,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.RandomTokenStream#RandomTokenStream(int,String[],BytesRef[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.RandomTokenStream#RandomTokenStream(int,String[],BytesRef[]).mjava","sourceNew":"    public RandomTokenStream(int len, String[] sampleTerms, BytesRef[] sampleTermBytes) {\n      this(len, sampleTerms, sampleTermBytes, rarely());\n    }\n\n","sourceOld":"    protected RandomTokenStream(int len, String[] sampleTerms, BytesRef[] sampleTermBytes) {\n      this(len, sampleTerms, sampleTermBytes, rarely());\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"762c80e29fe0c3bb83aabe2e64af6379273cec7b","date":1484347562,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.RandomTokenStream#RandomTokenStream(int,String[],BytesRef[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.RandomTokenStream#RandomTokenStream(int,String[],BytesRef[]).mjava","sourceNew":"    public RandomTokenStream(int len, String[] sampleTerms, BytesRef[] sampleTermBytes) {\n      terms = new String[len];\n      termBytes = new BytesRef[len];\n      positionsIncrements = new int[len];\n      positions = new int[len];\n      startOffsets = new int[len];\n      endOffsets = new int[len];\n      payloads = new BytesRef[len];\n      for (int i = 0; i < len; ++i) {\n        final int o = random().nextInt(sampleTerms.length);\n        terms[i] = sampleTerms[o];\n        termBytes[i] = sampleTermBytes[o];\n        positionsIncrements[i] = TestUtil.nextInt(random(), i == 0 ? 1 : 0, 10);\n        if (i == 0) {\n          startOffsets[i] = TestUtil.nextInt(random(), 0, 1 << 16);\n        } else {\n          startOffsets[i] = startOffsets[i-1] + TestUtil.nextInt(random(), 0, rarely() ? 1 << 16 : 20);\n        }\n        endOffsets[i] = startOffsets[i] + TestUtil.nextInt(random(), 0, rarely() ? 1 << 10 : 20);\n      }\n\n      for (int i = 0; i < len; ++i) {\n        if (i == 0) {\n          positions[i] = positionsIncrements[i] - 1;\n        } else {\n          positions[i] = positions[i - 1] + positionsIncrements[i];\n        }\n      }\n      if (rarely()) {\n        Arrays.fill(payloads, randomPayload());\n      } else {\n        for (int i = 0; i < len; ++i) {\n          payloads[i] = randomPayload();\n        }\n      }\n\n      positionToTerms = new HashMap<>(len);\n      startOffsetToTerms = new HashMap<>(len);\n      for (int i = 0; i < len; ++i) {\n        if (!positionToTerms.containsKey(positions[i])) {\n          positionToTerms.put(positions[i], new HashSet<Integer>(1));\n        }\n        positionToTerms.get(positions[i]).add(i);\n        if (!startOffsetToTerms.containsKey(startOffsets[i])) {\n          startOffsetToTerms.put(startOffsets[i], new HashSet<Integer>(1));\n        }\n        startOffsetToTerms.get(startOffsets[i]).add(i);\n      }\n\n      freqs = new HashMap<>();\n      for (String term : terms) {\n        if (freqs.containsKey(term)) {\n          freqs.put(term, freqs.get(term) + 1);\n        } else {\n          freqs.put(term, 1);\n        }\n      }\n\n      addAttributeImpl(new PermissiveOffsetAttributeImpl());\n\n      termAtt = addAttribute(CharTermAttribute.class);\n      piAtt = addAttribute(PositionIncrementAttribute.class);\n      oAtt = addAttribute(OffsetAttribute.class);\n      pAtt = addAttribute(PayloadAttribute.class);\n    }\n\n","sourceOld":"    public RandomTokenStream(int len, String[] sampleTerms, BytesRef[] sampleTermBytes) {\n      this(len, sampleTerms, sampleTermBytes, rarely());\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"507e7decdf00981d09a74632ea30299a4ce6ba72","date":1484600874,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.RandomTokenStream#RandomTokenStream(int,String[],BytesRef[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.RandomTokenStream#RandomTokenStream(int,String[],BytesRef[]).mjava","sourceNew":"    public RandomTokenStream(int len, String[] sampleTerms, BytesRef[] sampleTermBytes) {\n      terms = new String[len];\n      termBytes = new BytesRef[len];\n      positionsIncrements = new int[len];\n      positions = new int[len];\n      startOffsets = new int[len];\n      endOffsets = new int[len];\n      payloads = new BytesRef[len];\n      for (int i = 0; i < len; ++i) {\n        final int o = random().nextInt(sampleTerms.length);\n        terms[i] = sampleTerms[o];\n        termBytes[i] = sampleTermBytes[o];\n        positionsIncrements[i] = TestUtil.nextInt(random(), i == 0 ? 1 : 0, 10);\n        if (i == 0) {\n          startOffsets[i] = TestUtil.nextInt(random(), 0, 1 << 16);\n        } else {\n          startOffsets[i] = startOffsets[i-1] + TestUtil.nextInt(random(), 0, rarely() ? 1 << 16 : 20);\n        }\n        endOffsets[i] = startOffsets[i] + TestUtil.nextInt(random(), 0, rarely() ? 1 << 10 : 20);\n      }\n\n      for (int i = 0; i < len; ++i) {\n        if (i == 0) {\n          positions[i] = positionsIncrements[i] - 1;\n        } else {\n          positions[i] = positions[i - 1] + positionsIncrements[i];\n        }\n      }\n      if (rarely()) {\n        Arrays.fill(payloads, randomPayload());\n      } else {\n        for (int i = 0; i < len; ++i) {\n          payloads[i] = randomPayload();\n        }\n      }\n\n      positionToTerms = new HashMap<>(len);\n      startOffsetToTerms = new HashMap<>(len);\n      for (int i = 0; i < len; ++i) {\n        if (!positionToTerms.containsKey(positions[i])) {\n          positionToTerms.put(positions[i], new HashSet<Integer>(1));\n        }\n        positionToTerms.get(positions[i]).add(i);\n        if (!startOffsetToTerms.containsKey(startOffsets[i])) {\n          startOffsetToTerms.put(startOffsets[i], new HashSet<Integer>(1));\n        }\n        startOffsetToTerms.get(startOffsets[i]).add(i);\n      }\n\n      freqs = new HashMap<>();\n      for (String term : terms) {\n        if (freqs.containsKey(term)) {\n          freqs.put(term, freqs.get(term) + 1);\n        } else {\n          freqs.put(term, 1);\n        }\n      }\n\n      addAttributeImpl(new PermissiveOffsetAttributeImpl());\n\n      termAtt = addAttribute(CharTermAttribute.class);\n      piAtt = addAttribute(PositionIncrementAttribute.class);\n      oAtt = addAttribute(OffsetAttribute.class);\n      pAtt = addAttribute(PayloadAttribute.class);\n    }\n\n","sourceOld":"    public RandomTokenStream(int len, String[] sampleTerms, BytesRef[] sampleTermBytes) {\n      this(len, sampleTerms, sampleTermBytes, rarely());\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0fa6955ed1b1007ded1349ab72cea4555640432f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f21ce13f410ee015e1ba14687ab4b8518ac52a11"],"ae230518a1a68acc124bef8df61ef94bd7c1295e":["f21ce13f410ee015e1ba14687ab4b8518ac52a11"],"762c80e29fe0c3bb83aabe2e64af6379273cec7b":["ae230518a1a68acc124bef8df61ef94bd7c1295e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f21ce13f410ee015e1ba14687ab4b8518ac52a11":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"507e7decdf00981d09a74632ea30299a4ce6ba72":["ae230518a1a68acc124bef8df61ef94bd7c1295e","762c80e29fe0c3bb83aabe2e64af6379273cec7b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["762c80e29fe0c3bb83aabe2e64af6379273cec7b"]},"commit2Childs":{"0fa6955ed1b1007ded1349ab72cea4555640432f":[],"ae230518a1a68acc124bef8df61ef94bd7c1295e":["762c80e29fe0c3bb83aabe2e64af6379273cec7b","507e7decdf00981d09a74632ea30299a4ce6ba72"],"762c80e29fe0c3bb83aabe2e64af6379273cec7b":["507e7decdf00981d09a74632ea30299a4ce6ba72","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0fa6955ed1b1007ded1349ab72cea4555640432f","f21ce13f410ee015e1ba14687ab4b8518ac52a11"],"f21ce13f410ee015e1ba14687ab4b8518ac52a11":["0fa6955ed1b1007ded1349ab72cea4555640432f","ae230518a1a68acc124bef8df61ef94bd7c1295e"],"507e7decdf00981d09a74632ea30299a4ce6ba72":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["0fa6955ed1b1007ded1349ab72cea4555640432f","507e7decdf00981d09a74632ea30299a4ce6ba72","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}