{"path":"solr/core/src/java/org/apache/solr/util/FSHDFSUtils#recoverDFSFileLease(DistributedFileSystem,Path,Configuration,CallerInfo).mjava","commits":[{"id":"e0bbd91feb34c5f3efb419225ce6d207e7abb052","date":1426964933,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/FSHDFSUtils#recoverDFSFileLease(DistributedFileSystem,Path,Configuration,CallerInfo).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/FSHDFSUtils#recoverDFSFileLease(DistributedFileSystem,Path,Configuration).mjava","sourceNew":"  /*\n   * Run the dfs recover lease. recoverLease is asynchronous. It returns:\n   *    -false when it starts the lease recovery (i.e. lease recovery not *yet* done)\n   *    - true when the lease recovery has succeeded or the file is closed.\n   * But, we have to be careful.  Each time we call recoverLease, it starts the recover lease\n   * process over from the beginning.  We could put ourselves in a situation where we are\n   * doing nothing but starting a recovery, interrupting it to start again, and so on.\n   * The findings over in HBASE-8354 have it that the namenode will try to recover the lease\n   * on the file's primary node.  If all is well, it should return near immediately.  But,\n   * as is common, it is the very primary node that has crashed and so the namenode will be\n   * stuck waiting on a socket timeout before it will ask another datanode to start the\n   * recovery. It does not help if we call recoverLease in the meantime and in particular,\n   * subsequent to the socket timeout, a recoverLease invocation will cause us to start\n   * over from square one (possibly waiting on socket timeout against primary node).  So,\n   * in the below, we do the following:\n   * 1. Call recoverLease.\n   * 2. If it returns true, break.\n   * 3. If it returns false, wait a few seconds and then call it again.\n   * 4. If it returns true, break.\n   * 5. If it returns false, wait for what we think the datanode socket timeout is\n   * (configurable) and then try again.\n   * 6. If it returns true, break.\n   * 7. If it returns false, repeat starting at step 5. above.\n   *\n   * If HDFS-4525 is available, call it every second and we might be able to exit early.\n   */\n  static boolean recoverDFSFileLease(final DistributedFileSystem dfs, final Path p, final Configuration conf, CallerInfo callerInfo)\n  throws IOException {\n    log.info(\"Recovering lease on dfs file \" + p);\n    long startWaiting = System.nanoTime();\n    // Default is 15 minutes. It's huge, but the idea is that if we have a major issue, HDFS\n    // usually needs 10 minutes before marking the nodes as dead. So we're putting ourselves\n    // beyond that limit 'to be safe'.\n    long recoveryTimeout = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.timeout\", 900000), TimeUnit.MILLISECONDS) + startWaiting;\n    // This setting should be a little bit above what the cluster dfs heartbeat is set to.\n    long firstPause = conf.getInt(\"solr.hdfs.lease.recovery.first.pause\", 4000);\n    // This should be set to how long it'll take for us to timeout against primary datanode if it\n    // is dead.  We set it to 61 seconds, 1 second than the default READ_TIMEOUT in HDFS, the\n    // default value for DFS_CLIENT_SOCKET_TIMEOUT_KEY.\n    long subsequentPause = conf.getInt(\"solr.hdfs.lease.recovery.dfs.timeout\", 61 * 1000);\n    \n    Method isFileClosedMeth = null;\n    // whether we need to look for isFileClosed method\n    \n    try {\n      isFileClosedMeth = dfs.getClass().getMethod(\"isFileClosed\",\n          new Class[] {Path.class});\n    } catch (NoSuchMethodException nsme) {\n      log.debug(\"isFileClosed not available\");\n    }\n    \n    if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n      return true;\n    }\n    \n    boolean recovered = false;\n    // We break the loop if we succeed the lease recovery, timeout, or we throw an exception.\n    for (int nbAttempt = 0; !recovered; nbAttempt++) {\n      recovered = recoverLease(dfs, nbAttempt, p, startWaiting);\n      if (recovered) break;\n      if (checkIfTimedout(conf, recoveryTimeout, nbAttempt, p, startWaiting) || callerInfo.isCallerClosed()) break;\n      try {\n        // On the first time through wait the short 'firstPause'.\n        if (nbAttempt == 0) {\n          Thread.sleep(firstPause);\n        } else {\n          // Cycle here until subsequentPause elapses.  While spinning, check isFileClosed if\n          // available (should be in hadoop 2.0.5... not in hadoop 1 though.\n          long localStartWaiting = System.nanoTime();\n          while ((System.nanoTime() - localStartWaiting) < subsequentPause && !callerInfo.isCallerClosed()) {\n            Thread.sleep(conf.getInt(\"solr.hdfs.lease.recovery.pause\", 1000));\n\n            if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n              recovered = true;\n              break;\n            }\n          }\n        }\n      } catch (InterruptedException ie) {\n        InterruptedIOException iioe = new InterruptedIOException();\n        iioe.initCause(ie);\n        throw iioe;\n      }\n    }\n    return recovered;\n  }\n\n","sourceOld":"  /*\n   * Run the dfs recover lease. recoverLease is asynchronous. It returns:\n   *    -false when it starts the lease recovery (i.e. lease recovery not *yet* done)\n   *    - true when the lease recovery has succeeded or the file is closed.\n   * But, we have to be careful.  Each time we call recoverLease, it starts the recover lease\n   * process over from the beginning.  We could put ourselves in a situation where we are\n   * doing nothing but starting a recovery, interrupting it to start again, and so on.\n   * The findings over in HBASE-8354 have it that the namenode will try to recover the lease\n   * on the file's primary node.  If all is well, it should return near immediately.  But,\n   * as is common, it is the very primary node that has crashed and so the namenode will be\n   * stuck waiting on a socket timeout before it will ask another datanode to start the\n   * recovery. It does not help if we call recoverLease in the meantime and in particular,\n   * subsequent to the socket timeout, a recoverLease invocation will cause us to start\n   * over from square one (possibly waiting on socket timeout against primary node).  So,\n   * in the below, we do the following:\n   * 1. Call recoverLease.\n   * 2. If it returns true, break.\n   * 3. If it returns false, wait a few seconds and then call it again.\n   * 4. If it returns true, break.\n   * 5. If it returns false, wait for what we think the datanode socket timeout is\n   * (configurable) and then try again.\n   * 6. If it returns true, break.\n   * 7. If it returns false, repeat starting at step 5. above.\n   *\n   * If HDFS-4525 is available, call it every second and we might be able to exit early.\n   */\n  static boolean recoverDFSFileLease(final DistributedFileSystem dfs, final Path p, final Configuration conf)\n  throws IOException {\n    log.info(\"Recovering lease on dfs file \" + p);\n    long startWaiting = System.nanoTime();\n    // Default is 15 minutes. It's huge, but the idea is that if we have a major issue, HDFS\n    // usually needs 10 minutes before marking the nodes as dead. So we're putting ourselves\n    // beyond that limit 'to be safe'.\n    long recoveryTimeout = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.timeout\", 900000), TimeUnit.MILLISECONDS) + startWaiting;\n    // This setting should be a little bit above what the cluster dfs heartbeat is set to.\n    long firstPause = conf.getInt(\"solr.hdfs.lease.recovery.first.pause\", 4000);\n    // This should be set to how long it'll take for us to timeout against primary datanode if it\n    // is dead.  We set it to 61 seconds, 1 second than the default READ_TIMEOUT in HDFS, the\n    // default value for DFS_CLIENT_SOCKET_TIMEOUT_KEY.\n    long subsequentPause = conf.getInt(\"solr.hdfs.lease.recovery.dfs.timeout\", 61 * 1000);\n    \n    Method isFileClosedMeth = null;\n    // whether we need to look for isFileClosed method\n    boolean findIsFileClosedMeth = true;\n    boolean recovered = false;\n    // We break the loop if we succeed the lease recovery, timeout, or we throw an exception.\n    for (int nbAttempt = 0; !recovered; nbAttempt++) {\n      recovered = recoverLease(dfs, nbAttempt, p, startWaiting);\n      if (recovered) break;\n      if (checkIfTimedout(conf, recoveryTimeout, nbAttempt, p, startWaiting)) break;\n      try {\n        // On the first time through wait the short 'firstPause'.\n        if (nbAttempt == 0) {\n          Thread.sleep(firstPause);\n        } else {\n          // Cycle here until subsequentPause elapses.  While spinning, check isFileClosed if\n          // available (should be in hadoop 2.0.5... not in hadoop 1 though.\n          long localStartWaiting = System.nanoTime();\n          while ((System.nanoTime() - localStartWaiting) <\n              subsequentPause) {\n            Thread.sleep(conf.getInt(\"solr.hdfs.lease.recovery.pause\", 1000));\n            if (findIsFileClosedMeth) {\n              try {\n                isFileClosedMeth = dfs.getClass().getMethod(\"isFileClosed\",\n                  new Class[]{ Path.class });\n              } catch (NoSuchMethodException nsme) {\n                log.debug(\"isFileClosed not available\");\n              } finally {\n                findIsFileClosedMeth = false;\n              }\n            }\n            if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n              recovered = true;\n              break;\n            }\n          }\n        }\n      } catch (InterruptedException ie) {\n        InterruptedIOException iioe = new InterruptedIOException();\n        iioe.initCause(ie);\n        throw iioe;\n      }\n    }\n    return recovered;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":0,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/util/FSHDFSUtils#recoverDFSFileLease(DistributedFileSystem,Path,Configuration,CallerInfo).mjava","pathOld":"/dev/null","sourceNew":"  /*\n   * Run the dfs recover lease. recoverLease is asynchronous. It returns:\n   *    -false when it starts the lease recovery (i.e. lease recovery not *yet* done)\n   *    - true when the lease recovery has succeeded or the file is closed.\n   * But, we have to be careful.  Each time we call recoverLease, it starts the recover lease\n   * process over from the beginning.  We could put ourselves in a situation where we are\n   * doing nothing but starting a recovery, interrupting it to start again, and so on.\n   * The findings over in HBASE-8354 have it that the namenode will try to recover the lease\n   * on the file's primary node.  If all is well, it should return near immediately.  But,\n   * as is common, it is the very primary node that has crashed and so the namenode will be\n   * stuck waiting on a socket timeout before it will ask another datanode to start the\n   * recovery. It does not help if we call recoverLease in the meantime and in particular,\n   * subsequent to the socket timeout, a recoverLease invocation will cause us to start\n   * over from square one (possibly waiting on socket timeout against primary node).  So,\n   * in the below, we do the following:\n   * 1. Call recoverLease.\n   * 2. If it returns true, break.\n   * 3. If it returns false, wait a few seconds and then call it again.\n   * 4. If it returns true, break.\n   * 5. If it returns false, wait for what we think the datanode socket timeout is\n   * (configurable) and then try again.\n   * 6. If it returns true, break.\n   * 7. If it returns false, repeat starting at step 5. above.\n   *\n   * If HDFS-4525 is available, call it every second and we might be able to exit early.\n   */\n  static boolean recoverDFSFileLease(final DistributedFileSystem dfs, final Path p, final Configuration conf, CallerInfo callerInfo)\n  throws IOException {\n    log.info(\"Recovering lease on dfs file \" + p);\n    long startWaiting = System.nanoTime();\n    // Default is 15 minutes. It's huge, but the idea is that if we have a major issue, HDFS\n    // usually needs 10 minutes before marking the nodes as dead. So we're putting ourselves\n    // beyond that limit 'to be safe'.\n    long recoveryTimeout = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.timeout\", 900000), TimeUnit.MILLISECONDS) + startWaiting;\n    // This setting should be a little bit above what the cluster dfs heartbeat is set to.\n    long firstPause = conf.getInt(\"solr.hdfs.lease.recovery.first.pause\", 4000);\n    // This should be set to how long it'll take for us to timeout against primary datanode if it\n    // is dead.  We set it to 61 seconds, 1 second than the default READ_TIMEOUT in HDFS, the\n    // default value for DFS_CLIENT_SOCKET_TIMEOUT_KEY.\n    long subsequentPause = conf.getInt(\"solr.hdfs.lease.recovery.dfs.timeout\", 61 * 1000);\n    \n    Method isFileClosedMeth = null;\n    // whether we need to look for isFileClosed method\n    \n    try {\n      isFileClosedMeth = dfs.getClass().getMethod(\"isFileClosed\",\n          new Class[] {Path.class});\n    } catch (NoSuchMethodException nsme) {\n      log.debug(\"isFileClosed not available\");\n    }\n    \n    if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n      return true;\n    }\n    \n    boolean recovered = false;\n    // We break the loop if we succeed the lease recovery, timeout, or we throw an exception.\n    for (int nbAttempt = 0; !recovered; nbAttempt++) {\n      recovered = recoverLease(dfs, nbAttempt, p, startWaiting);\n      if (recovered) break;\n      if (checkIfTimedout(conf, recoveryTimeout, nbAttempt, p, startWaiting) || callerInfo.isCallerClosed()) break;\n      try {\n        // On the first time through wait the short 'firstPause'.\n        if (nbAttempt == 0) {\n          Thread.sleep(firstPause);\n        } else {\n          // Cycle here until subsequentPause elapses.  While spinning, check isFileClosed if\n          // available (should be in hadoop 2.0.5... not in hadoop 1 though.\n          long localStartWaiting = System.nanoTime();\n          while ((System.nanoTime() - localStartWaiting) < subsequentPause && !callerInfo.isCallerClosed()) {\n            Thread.sleep(conf.getInt(\"solr.hdfs.lease.recovery.pause\", 1000));\n\n            if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n              recovered = true;\n              break;\n            }\n          }\n        }\n      } catch (InterruptedException ie) {\n        InterruptedIOException iioe = new InterruptedIOException();\n        iioe.initCause(ie);\n        throw iioe;\n      }\n    }\n    return recovered;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efab926cbe25efbe747b226015cd9ae2ee3daa0","date":1428591386,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/FSHDFSUtils#recoverDFSFileLease(DistributedFileSystem,Path,Configuration,CallerInfo).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/FSHDFSUtils#recoverDFSFileLease(DistributedFileSystem,Path,Configuration,CallerInfo).mjava","sourceNew":"  /*\n   * Run the dfs recover lease. recoverLease is asynchronous. It returns:\n   *    -false when it starts the lease recovery (i.e. lease recovery not *yet* done)\n   *    - true when the lease recovery has succeeded or the file is closed.\n   * But, we have to be careful.  Each time we call recoverLease, it starts the recover lease\n   * process over from the beginning.  We could put ourselves in a situation where we are\n   * doing nothing but starting a recovery, interrupting it to start again, and so on.\n   * The findings over in HBASE-8354 have it that the namenode will try to recover the lease\n   * on the file's primary node.  If all is well, it should return near immediately.  But,\n   * as is common, it is the very primary node that has crashed and so the namenode will be\n   * stuck waiting on a socket timeout before it will ask another datanode to start the\n   * recovery. It does not help if we call recoverLease in the meantime and in particular,\n   * subsequent to the socket timeout, a recoverLease invocation will cause us to start\n   * over from square one (possibly waiting on socket timeout against primary node).  So,\n   * in the below, we do the following:\n   * 1. Call recoverLease.\n   * 2. If it returns true, break.\n   * 3. If it returns false, wait a few seconds and then call it again.\n   * 4. If it returns true, break.\n   * 5. If it returns false, wait for what we think the datanode socket timeout is\n   * (configurable) and then try again.\n   * 6. If it returns true, break.\n   * 7. If it returns false, repeat starting at step 5. above.\n   *\n   * If HDFS-4525 is available, call it every second and we might be able to exit early.\n   */\n  static boolean recoverDFSFileLease(final DistributedFileSystem dfs, final Path p, final Configuration conf, CallerInfo callerInfo)\n  throws IOException {\n    log.info(\"Recovering lease on dfs file \" + p);\n    long startWaiting = System.nanoTime();\n    // Default is 15 minutes. It's huge, but the idea is that if we have a major issue, HDFS\n    // usually needs 10 minutes before marking the nodes as dead. So we're putting ourselves\n    // beyond that limit 'to be safe'.\n    long recoveryTimeout = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.timeout\", 900000), TimeUnit.MILLISECONDS) + startWaiting;\n    // This setting should be a little bit above what the cluster dfs heartbeat is set to.\n    long firstPause = conf.getInt(\"solr.hdfs.lease.recovery.first.pause\", 4000);\n    // This should be set to how long it'll take for us to timeout against primary datanode if it\n    // is dead.  We set it to 61 seconds, 1 second than the default READ_TIMEOUT in HDFS, the\n    // default value for DFS_CLIENT_SOCKET_TIMEOUT_KEY.\n    long subsequentPause = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.dfs.timeout\", 61 * 1000), TimeUnit.MILLISECONDS);\n    \n    Method isFileClosedMeth = null;\n    // whether we need to look for isFileClosed method\n    \n    try {\n      isFileClosedMeth = dfs.getClass().getMethod(\"isFileClosed\",\n          new Class[] {Path.class});\n    } catch (NoSuchMethodException nsme) {\n      log.debug(\"isFileClosed not available\");\n    }\n    \n    if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n      return true;\n    }\n    \n    boolean recovered = false;\n    // We break the loop if we succeed the lease recovery, timeout, or we throw an exception.\n    for (int nbAttempt = 0; !recovered; nbAttempt++) {\n      recovered = recoverLease(dfs, nbAttempt, p, startWaiting);\n      if (recovered) break;\n      if (checkIfTimedout(conf, recoveryTimeout, nbAttempt, p, startWaiting) || callerInfo.isCallerClosed()) break;\n      try {\n        // On the first time through wait the short 'firstPause'.\n        if (nbAttempt == 0) {\n          Thread.sleep(firstPause);\n        } else {\n          // Cycle here until subsequentPause elapses.  While spinning, check isFileClosed if\n          // available (should be in hadoop 2.0.5... not in hadoop 1 though.\n          long localStartWaiting = System.nanoTime();\n          while ((System.nanoTime() - localStartWaiting) < subsequentPause && !callerInfo.isCallerClosed()) {\n            Thread.sleep(conf.getInt(\"solr.hdfs.lease.recovery.pause\", 1000));\n\n            if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n              recovered = true;\n              break;\n            }\n          }\n        }\n      } catch (InterruptedException ie) {\n        InterruptedIOException iioe = new InterruptedIOException();\n        iioe.initCause(ie);\n        throw iioe;\n      }\n    }\n    return recovered;\n  }\n\n","sourceOld":"  /*\n   * Run the dfs recover lease. recoverLease is asynchronous. It returns:\n   *    -false when it starts the lease recovery (i.e. lease recovery not *yet* done)\n   *    - true when the lease recovery has succeeded or the file is closed.\n   * But, we have to be careful.  Each time we call recoverLease, it starts the recover lease\n   * process over from the beginning.  We could put ourselves in a situation where we are\n   * doing nothing but starting a recovery, interrupting it to start again, and so on.\n   * The findings over in HBASE-8354 have it that the namenode will try to recover the lease\n   * on the file's primary node.  If all is well, it should return near immediately.  But,\n   * as is common, it is the very primary node that has crashed and so the namenode will be\n   * stuck waiting on a socket timeout before it will ask another datanode to start the\n   * recovery. It does not help if we call recoverLease in the meantime and in particular,\n   * subsequent to the socket timeout, a recoverLease invocation will cause us to start\n   * over from square one (possibly waiting on socket timeout against primary node).  So,\n   * in the below, we do the following:\n   * 1. Call recoverLease.\n   * 2. If it returns true, break.\n   * 3. If it returns false, wait a few seconds and then call it again.\n   * 4. If it returns true, break.\n   * 5. If it returns false, wait for what we think the datanode socket timeout is\n   * (configurable) and then try again.\n   * 6. If it returns true, break.\n   * 7. If it returns false, repeat starting at step 5. above.\n   *\n   * If HDFS-4525 is available, call it every second and we might be able to exit early.\n   */\n  static boolean recoverDFSFileLease(final DistributedFileSystem dfs, final Path p, final Configuration conf, CallerInfo callerInfo)\n  throws IOException {\n    log.info(\"Recovering lease on dfs file \" + p);\n    long startWaiting = System.nanoTime();\n    // Default is 15 minutes. It's huge, but the idea is that if we have a major issue, HDFS\n    // usually needs 10 minutes before marking the nodes as dead. So we're putting ourselves\n    // beyond that limit 'to be safe'.\n    long recoveryTimeout = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.timeout\", 900000), TimeUnit.MILLISECONDS) + startWaiting;\n    // This setting should be a little bit above what the cluster dfs heartbeat is set to.\n    long firstPause = conf.getInt(\"solr.hdfs.lease.recovery.first.pause\", 4000);\n    // This should be set to how long it'll take for us to timeout against primary datanode if it\n    // is dead.  We set it to 61 seconds, 1 second than the default READ_TIMEOUT in HDFS, the\n    // default value for DFS_CLIENT_SOCKET_TIMEOUT_KEY.\n    long subsequentPause = conf.getInt(\"solr.hdfs.lease.recovery.dfs.timeout\", 61 * 1000);\n    \n    Method isFileClosedMeth = null;\n    // whether we need to look for isFileClosed method\n    \n    try {\n      isFileClosedMeth = dfs.getClass().getMethod(\"isFileClosed\",\n          new Class[] {Path.class});\n    } catch (NoSuchMethodException nsme) {\n      log.debug(\"isFileClosed not available\");\n    }\n    \n    if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n      return true;\n    }\n    \n    boolean recovered = false;\n    // We break the loop if we succeed the lease recovery, timeout, or we throw an exception.\n    for (int nbAttempt = 0; !recovered; nbAttempt++) {\n      recovered = recoverLease(dfs, nbAttempt, p, startWaiting);\n      if (recovered) break;\n      if (checkIfTimedout(conf, recoveryTimeout, nbAttempt, p, startWaiting) || callerInfo.isCallerClosed()) break;\n      try {\n        // On the first time through wait the short 'firstPause'.\n        if (nbAttempt == 0) {\n          Thread.sleep(firstPause);\n        } else {\n          // Cycle here until subsequentPause elapses.  While spinning, check isFileClosed if\n          // available (should be in hadoop 2.0.5... not in hadoop 1 though.\n          long localStartWaiting = System.nanoTime();\n          while ((System.nanoTime() - localStartWaiting) < subsequentPause && !callerInfo.isCallerClosed()) {\n            Thread.sleep(conf.getInt(\"solr.hdfs.lease.recovery.pause\", 1000));\n\n            if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n              recovered = true;\n              break;\n            }\n          }\n        }\n      } catch (InterruptedException ie) {\n        InterruptedIOException iioe = new InterruptedIOException();\n        iioe.initCause(ie);\n        throw iioe;\n      }\n    }\n    return recovered;\n  }\n\n","bugFix":["c6d82c04c0bc088fae82f28ef47cb25a164f47fd"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e4bec8aad484912847547ca94038e1a3641c7bc7","date":1430228162,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/FSHDFSUtils#recoverDFSFileLease(DistributedFileSystem,Path,Configuration,CallerInfo).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/FSHDFSUtils#recoverDFSFileLease(DistributedFileSystem,Path,Configuration,CallerInfo).mjava","sourceNew":"  /*\n   * Run the dfs recover lease. recoverLease is asynchronous. It returns:\n   *    -false when it starts the lease recovery (i.e. lease recovery not *yet* done)\n   *    - true when the lease recovery has succeeded or the file is closed.\n   * But, we have to be careful.  Each time we call recoverLease, it starts the recover lease\n   * process over from the beginning.  We could put ourselves in a situation where we are\n   * doing nothing but starting a recovery, interrupting it to start again, and so on.\n   * The findings over in HBASE-8354 have it that the namenode will try to recover the lease\n   * on the file's primary node.  If all is well, it should return near immediately.  But,\n   * as is common, it is the very primary node that has crashed and so the namenode will be\n   * stuck waiting on a socket timeout before it will ask another datanode to start the\n   * recovery. It does not help if we call recoverLease in the meantime and in particular,\n   * subsequent to the socket timeout, a recoverLease invocation will cause us to start\n   * over from square one (possibly waiting on socket timeout against primary node).  So,\n   * in the below, we do the following:\n   * 1. Call recoverLease.\n   * 2. If it returns true, break.\n   * 3. If it returns false, wait a few seconds and then call it again.\n   * 4. If it returns true, break.\n   * 5. If it returns false, wait for what we think the datanode socket timeout is\n   * (configurable) and then try again.\n   * 6. If it returns true, break.\n   * 7. If it returns false, repeat starting at step 5. above.\n   *\n   * If HDFS-4525 is available, call it every second and we might be able to exit early.\n   */\n  static boolean recoverDFSFileLease(final DistributedFileSystem dfs, final Path p, final Configuration conf, CallerInfo callerInfo)\n  throws IOException {\n    log.info(\"Recovering lease on dfs file \" + p);\n    long startWaiting = System.nanoTime();\n    // Default is 15 minutes. It's huge, but the idea is that if we have a major issue, HDFS\n    // usually needs 10 minutes before marking the nodes as dead. So we're putting ourselves\n    // beyond that limit 'to be safe'.\n    long recoveryTimeout = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.timeout\", 900000), TimeUnit.MILLISECONDS) + startWaiting;\n    // This setting should be a little bit above what the cluster dfs heartbeat is set to.\n    long firstPause = conf.getInt(\"solr.hdfs.lease.recovery.first.pause\", 4000);\n    // This should be set to how long it'll take for us to timeout against primary datanode if it\n    // is dead.  We set it to 61 seconds, 1 second than the default READ_TIMEOUT in HDFS, the\n    // default value for DFS_CLIENT_SOCKET_TIMEOUT_KEY.\n    long subsequentPause = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.dfs.timeout\", 61 * 1000), TimeUnit.MILLISECONDS);\n    \n    Method isFileClosedMeth = null;\n    // whether we need to look for isFileClosed method\n    \n    try {\n      isFileClosedMeth = dfs.getClass().getMethod(\"isFileClosed\",\n          new Class[] {Path.class});\n    } catch (NoSuchMethodException nsme) {\n      log.debug(\"isFileClosed not available\");\n    }\n    \n    if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n      return true;\n    }\n    \n    boolean recovered = false;\n    // We break the loop if we succeed the lease recovery, timeout, or we throw an exception.\n    for (int nbAttempt = 0; !recovered; nbAttempt++) {\n      recovered = recoverLease(dfs, nbAttempt, p, startWaiting);\n      if (recovered) break;\n      if (checkIfTimedout(conf, recoveryTimeout, nbAttempt, p, startWaiting) || callerInfo.isCallerClosed()) break;\n      try {\n        // On the first time through wait the short 'firstPause'.\n        if (nbAttempt == 0) {\n          Thread.sleep(firstPause);\n        } else {\n          // Cycle here until subsequentPause elapses.  While spinning, check isFileClosed if\n          // available (should be in hadoop 2.0.5... not in hadoop 1 though.\n          long localStartWaiting = System.nanoTime();\n          while ((System.nanoTime() - localStartWaiting) < subsequentPause && !callerInfo.isCallerClosed()) {\n            Thread.sleep(conf.getInt(\"solr.hdfs.lease.recovery.pause\", 1000));\n\n            if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n              recovered = true;\n              break;\n            }\n          }\n        }\n      } catch (InterruptedException ie) {\n        InterruptedIOException iioe = new InterruptedIOException();\n        iioe.initCause(ie);\n        throw iioe;\n      }\n    }\n    if (recovered) {\n      RECOVER_LEASE_SUCCESS_COUNT.incrementAndGet();\n    }\n    return recovered;\n  }\n\n","sourceOld":"  /*\n   * Run the dfs recover lease. recoverLease is asynchronous. It returns:\n   *    -false when it starts the lease recovery (i.e. lease recovery not *yet* done)\n   *    - true when the lease recovery has succeeded or the file is closed.\n   * But, we have to be careful.  Each time we call recoverLease, it starts the recover lease\n   * process over from the beginning.  We could put ourselves in a situation where we are\n   * doing nothing but starting a recovery, interrupting it to start again, and so on.\n   * The findings over in HBASE-8354 have it that the namenode will try to recover the lease\n   * on the file's primary node.  If all is well, it should return near immediately.  But,\n   * as is common, it is the very primary node that has crashed and so the namenode will be\n   * stuck waiting on a socket timeout before it will ask another datanode to start the\n   * recovery. It does not help if we call recoverLease in the meantime and in particular,\n   * subsequent to the socket timeout, a recoverLease invocation will cause us to start\n   * over from square one (possibly waiting on socket timeout against primary node).  So,\n   * in the below, we do the following:\n   * 1. Call recoverLease.\n   * 2. If it returns true, break.\n   * 3. If it returns false, wait a few seconds and then call it again.\n   * 4. If it returns true, break.\n   * 5. If it returns false, wait for what we think the datanode socket timeout is\n   * (configurable) and then try again.\n   * 6. If it returns true, break.\n   * 7. If it returns false, repeat starting at step 5. above.\n   *\n   * If HDFS-4525 is available, call it every second and we might be able to exit early.\n   */\n  static boolean recoverDFSFileLease(final DistributedFileSystem dfs, final Path p, final Configuration conf, CallerInfo callerInfo)\n  throws IOException {\n    log.info(\"Recovering lease on dfs file \" + p);\n    long startWaiting = System.nanoTime();\n    // Default is 15 minutes. It's huge, but the idea is that if we have a major issue, HDFS\n    // usually needs 10 minutes before marking the nodes as dead. So we're putting ourselves\n    // beyond that limit 'to be safe'.\n    long recoveryTimeout = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.timeout\", 900000), TimeUnit.MILLISECONDS) + startWaiting;\n    // This setting should be a little bit above what the cluster dfs heartbeat is set to.\n    long firstPause = conf.getInt(\"solr.hdfs.lease.recovery.first.pause\", 4000);\n    // This should be set to how long it'll take for us to timeout against primary datanode if it\n    // is dead.  We set it to 61 seconds, 1 second than the default READ_TIMEOUT in HDFS, the\n    // default value for DFS_CLIENT_SOCKET_TIMEOUT_KEY.\n    long subsequentPause = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.dfs.timeout\", 61 * 1000), TimeUnit.MILLISECONDS);\n    \n    Method isFileClosedMeth = null;\n    // whether we need to look for isFileClosed method\n    \n    try {\n      isFileClosedMeth = dfs.getClass().getMethod(\"isFileClosed\",\n          new Class[] {Path.class});\n    } catch (NoSuchMethodException nsme) {\n      log.debug(\"isFileClosed not available\");\n    }\n    \n    if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n      return true;\n    }\n    \n    boolean recovered = false;\n    // We break the loop if we succeed the lease recovery, timeout, or we throw an exception.\n    for (int nbAttempt = 0; !recovered; nbAttempt++) {\n      recovered = recoverLease(dfs, nbAttempt, p, startWaiting);\n      if (recovered) break;\n      if (checkIfTimedout(conf, recoveryTimeout, nbAttempt, p, startWaiting) || callerInfo.isCallerClosed()) break;\n      try {\n        // On the first time through wait the short 'firstPause'.\n        if (nbAttempt == 0) {\n          Thread.sleep(firstPause);\n        } else {\n          // Cycle here until subsequentPause elapses.  While spinning, check isFileClosed if\n          // available (should be in hadoop 2.0.5... not in hadoop 1 though.\n          long localStartWaiting = System.nanoTime();\n          while ((System.nanoTime() - localStartWaiting) < subsequentPause && !callerInfo.isCallerClosed()) {\n            Thread.sleep(conf.getInt(\"solr.hdfs.lease.recovery.pause\", 1000));\n\n            if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n              recovered = true;\n              break;\n            }\n          }\n        }\n      } catch (InterruptedException ie) {\n        InterruptedIOException iioe = new InterruptedIOException();\n        iioe.initCause(ie);\n        throw iioe;\n      }\n    }\n    return recovered;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b93355434fdbfe132175a38a1946631387ad308a","date":1551293860,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/FSHDFSUtils#recoverDFSFileLease(DistributedFileSystem,Path,Configuration,CallerInfo).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/FSHDFSUtils#recoverDFSFileLease(DistributedFileSystem,Path,Configuration,CallerInfo).mjava","sourceNew":"  /*\n   * Run the dfs recover lease. recoverLease is asynchronous. It returns:\n   *    -false when it starts the lease recovery (i.e. lease recovery not *yet* done)\n   *    - true when the lease recovery has succeeded or the file is closed.\n   * But, we have to be careful.  Each time we call recoverLease, it starts the recover lease\n   * process over from the beginning.  We could put ourselves in a situation where we are\n   * doing nothing but starting a recovery, interrupting it to start again, and so on.\n   * The findings over in HBASE-8354 have it that the namenode will try to recover the lease\n   * on the file's primary node.  If all is well, it should return near immediately.  But,\n   * as is common, it is the very primary node that has crashed and so the namenode will be\n   * stuck waiting on a socket timeout before it will ask another datanode to start the\n   * recovery. It does not help if we call recoverLease in the meantime and in particular,\n   * subsequent to the socket timeout, a recoverLease invocation will cause us to start\n   * over from square one (possibly waiting on socket timeout against primary node).  So,\n   * in the below, we do the following:\n   * 1. Call recoverLease.\n   * 2. If it returns true, break.\n   * 3. If it returns false, wait a few seconds and then call it again.\n   * 4. If it returns true, break.\n   * 5. If it returns false, wait for what we think the datanode socket timeout is\n   * (configurable) and then try again.\n   * 6. If it returns true, break.\n   * 7. If it returns false, repeat starting at step 5. above.\n   *\n   * If HDFS-4525 is available, call it every second and we might be able to exit early.\n   */\n  static boolean recoverDFSFileLease(final DistributedFileSystem dfs, final Path p, final Configuration conf, CallerInfo callerInfo)\n  throws IOException {\n    log.info(\"Recovering lease on dfs file \" + p);\n    long startWaiting = System.nanoTime();\n    // Default is 15 minutes. It's huge, but the idea is that if we have a major issue, HDFS\n    // usually needs 10 minutes before marking the nodes as dead. So we're putting ourselves\n    // beyond that limit 'to be safe'.\n    long recoveryTimeout = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.timeout\", 900000), TimeUnit.MILLISECONDS) + startWaiting;\n    // This setting should be a little bit above what the cluster dfs heartbeat is set to.\n    long firstPause = conf.getInt(\"solr.hdfs.lease.recovery.first.pause\", 4000);\n    // This should be set to how long it'll take for us to timeout against primary datanode if it\n    // is dead.  We set it to 61 seconds, 1 second than the default READ_TIMEOUT in HDFS, the\n    // default value for DFS_CLIENT_SOCKET_TIMEOUT_KEY.\n    long subsequentPause = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.dfs.timeout\", 61 * 1000), TimeUnit.MILLISECONDS);\n    \n    if (dfs.isFileClosed(p)) {\n      return true;\n    }\n    \n    boolean recovered = false;\n    // We break the loop if we succeed the lease recovery, timeout, or we throw an exception.\n    for (int nbAttempt = 0; !recovered; nbAttempt++) {\n      recovered = recoverLease(dfs, nbAttempt, p, startWaiting);\n      if (recovered) break;\n      if (checkIfTimedout(conf, recoveryTimeout, nbAttempt, p, startWaiting) || callerInfo.isCallerClosed()) break;\n      try {\n        // On the first time through wait the short 'firstPause'.\n        if (nbAttempt == 0) {\n          Thread.sleep(firstPause);\n        } else {\n          // Cycle here until subsequentPause elapses.  While spinning, check isFileClosed\n          long localStartWaiting = System.nanoTime();\n          while ((System.nanoTime() - localStartWaiting) < subsequentPause && !callerInfo.isCallerClosed()) {\n            Thread.sleep(conf.getInt(\"solr.hdfs.lease.recovery.pause\", 1000));\n\n            if (dfs.isFileClosed(p)) {\n              recovered = true;\n              break;\n            }\n          }\n        }\n      } catch (InterruptedException ie) {\n        InterruptedIOException iioe = new InterruptedIOException();\n        iioe.initCause(ie);\n        throw iioe;\n      }\n    }\n    if (recovered) {\n      RECOVER_LEASE_SUCCESS_COUNT.incrementAndGet();\n    }\n    return recovered;\n  }\n\n","sourceOld":"  /*\n   * Run the dfs recover lease. recoverLease is asynchronous. It returns:\n   *    -false when it starts the lease recovery (i.e. lease recovery not *yet* done)\n   *    - true when the lease recovery has succeeded or the file is closed.\n   * But, we have to be careful.  Each time we call recoverLease, it starts the recover lease\n   * process over from the beginning.  We could put ourselves in a situation where we are\n   * doing nothing but starting a recovery, interrupting it to start again, and so on.\n   * The findings over in HBASE-8354 have it that the namenode will try to recover the lease\n   * on the file's primary node.  If all is well, it should return near immediately.  But,\n   * as is common, it is the very primary node that has crashed and so the namenode will be\n   * stuck waiting on a socket timeout before it will ask another datanode to start the\n   * recovery. It does not help if we call recoverLease in the meantime and in particular,\n   * subsequent to the socket timeout, a recoverLease invocation will cause us to start\n   * over from square one (possibly waiting on socket timeout against primary node).  So,\n   * in the below, we do the following:\n   * 1. Call recoverLease.\n   * 2. If it returns true, break.\n   * 3. If it returns false, wait a few seconds and then call it again.\n   * 4. If it returns true, break.\n   * 5. If it returns false, wait for what we think the datanode socket timeout is\n   * (configurable) and then try again.\n   * 6. If it returns true, break.\n   * 7. If it returns false, repeat starting at step 5. above.\n   *\n   * If HDFS-4525 is available, call it every second and we might be able to exit early.\n   */\n  static boolean recoverDFSFileLease(final DistributedFileSystem dfs, final Path p, final Configuration conf, CallerInfo callerInfo)\n  throws IOException {\n    log.info(\"Recovering lease on dfs file \" + p);\n    long startWaiting = System.nanoTime();\n    // Default is 15 minutes. It's huge, but the idea is that if we have a major issue, HDFS\n    // usually needs 10 minutes before marking the nodes as dead. So we're putting ourselves\n    // beyond that limit 'to be safe'.\n    long recoveryTimeout = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.timeout\", 900000), TimeUnit.MILLISECONDS) + startWaiting;\n    // This setting should be a little bit above what the cluster dfs heartbeat is set to.\n    long firstPause = conf.getInt(\"solr.hdfs.lease.recovery.first.pause\", 4000);\n    // This should be set to how long it'll take for us to timeout against primary datanode if it\n    // is dead.  We set it to 61 seconds, 1 second than the default READ_TIMEOUT in HDFS, the\n    // default value for DFS_CLIENT_SOCKET_TIMEOUT_KEY.\n    long subsequentPause = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.dfs.timeout\", 61 * 1000), TimeUnit.MILLISECONDS);\n    \n    Method isFileClosedMeth = null;\n    // whether we need to look for isFileClosed method\n    \n    try {\n      isFileClosedMeth = dfs.getClass().getMethod(\"isFileClosed\",\n          new Class[] {Path.class});\n    } catch (NoSuchMethodException nsme) {\n      log.debug(\"isFileClosed not available\");\n    }\n    \n    if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n      return true;\n    }\n    \n    boolean recovered = false;\n    // We break the loop if we succeed the lease recovery, timeout, or we throw an exception.\n    for (int nbAttempt = 0; !recovered; nbAttempt++) {\n      recovered = recoverLease(dfs, nbAttempt, p, startWaiting);\n      if (recovered) break;\n      if (checkIfTimedout(conf, recoveryTimeout, nbAttempt, p, startWaiting) || callerInfo.isCallerClosed()) break;\n      try {\n        // On the first time through wait the short 'firstPause'.\n        if (nbAttempt == 0) {\n          Thread.sleep(firstPause);\n        } else {\n          // Cycle here until subsequentPause elapses.  While spinning, check isFileClosed if\n          // available (should be in hadoop 2.0.5... not in hadoop 1 though.\n          long localStartWaiting = System.nanoTime();\n          while ((System.nanoTime() - localStartWaiting) < subsequentPause && !callerInfo.isCallerClosed()) {\n            Thread.sleep(conf.getInt(\"solr.hdfs.lease.recovery.pause\", 1000));\n\n            if (isFileClosedMeth != null && isFileClosed(dfs, isFileClosedMeth, p)) {\n              recovered = true;\n              break;\n            }\n          }\n        }\n      } catch (InterruptedException ie) {\n        InterruptedIOException iioe = new InterruptedIOException();\n        iioe.initCause(ie);\n        throw iioe;\n      }\n    }\n    if (recovered) {\n      RECOVER_LEASE_SUCCESS_COUNT.incrementAndGet();\n    }\n    return recovered;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54faedfb0e03479a38f5ee82f2dfaeea536e9404","date":1587251295,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/FSHDFSUtils#recoverDFSFileLease(DistributedFileSystem,Path,Configuration,CallerInfo).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/FSHDFSUtils#recoverDFSFileLease(DistributedFileSystem,Path,Configuration,CallerInfo).mjava","sourceNew":"  /*\n   * Run the dfs recover lease. recoverLease is asynchronous. It returns:\n   *    -false when it starts the lease recovery (i.e. lease recovery not *yet* done)\n   *    - true when the lease recovery has succeeded or the file is closed.\n   * But, we have to be careful.  Each time we call recoverLease, it starts the recover lease\n   * process over from the beginning.  We could put ourselves in a situation where we are\n   * doing nothing but starting a recovery, interrupting it to start again, and so on.\n   * The findings over in HBASE-8354 have it that the namenode will try to recover the lease\n   * on the file's primary node.  If all is well, it should return near immediately.  But,\n   * as is common, it is the very primary node that has crashed and so the namenode will be\n   * stuck waiting on a socket timeout before it will ask another datanode to start the\n   * recovery. It does not help if we call recoverLease in the meantime and in particular,\n   * subsequent to the socket timeout, a recoverLease invocation will cause us to start\n   * over from square one (possibly waiting on socket timeout against primary node).  So,\n   * in the below, we do the following:\n   * 1. Call recoverLease.\n   * 2. If it returns true, break.\n   * 3. If it returns false, wait a few seconds and then call it again.\n   * 4. If it returns true, break.\n   * 5. If it returns false, wait for what we think the datanode socket timeout is\n   * (configurable) and then try again.\n   * 6. If it returns true, break.\n   * 7. If it returns false, repeat starting at step 5. above.\n   *\n   * If HDFS-4525 is available, call it every second and we might be able to exit early.\n   */\n  static boolean recoverDFSFileLease(final DistributedFileSystem dfs, final Path p, final Configuration conf, CallerInfo callerInfo)\n  throws IOException {\n    log.info(\"Recovering lease on dfs file {}\", p);\n    long startWaiting = System.nanoTime();\n    // Default is 15 minutes. It's huge, but the idea is that if we have a major issue, HDFS\n    // usually needs 10 minutes before marking the nodes as dead. So we're putting ourselves\n    // beyond that limit 'to be safe'.\n    long recoveryTimeout = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.timeout\", 900000), TimeUnit.MILLISECONDS) + startWaiting;\n    // This setting should be a little bit above what the cluster dfs heartbeat is set to.\n    long firstPause = conf.getInt(\"solr.hdfs.lease.recovery.first.pause\", 4000);\n    // This should be set to how long it'll take for us to timeout against primary datanode if it\n    // is dead.  We set it to 61 seconds, 1 second than the default READ_TIMEOUT in HDFS, the\n    // default value for DFS_CLIENT_SOCKET_TIMEOUT_KEY.\n    long subsequentPause = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.dfs.timeout\", 61 * 1000), TimeUnit.MILLISECONDS);\n    \n    if (dfs.isFileClosed(p)) {\n      return true;\n    }\n    \n    boolean recovered = false;\n    // We break the loop if we succeed the lease recovery, timeout, or we throw an exception.\n    for (int nbAttempt = 0; !recovered; nbAttempt++) {\n      recovered = recoverLease(dfs, nbAttempt, p, startWaiting);\n      if (recovered) break;\n      if (checkIfTimedout(conf, recoveryTimeout, nbAttempt, p, startWaiting) || callerInfo.isCallerClosed()) break;\n      try {\n        // On the first time through wait the short 'firstPause'.\n        if (nbAttempt == 0) {\n          Thread.sleep(firstPause);\n        } else {\n          // Cycle here until subsequentPause elapses.  While spinning, check isFileClosed\n          long localStartWaiting = System.nanoTime();\n          while ((System.nanoTime() - localStartWaiting) < subsequentPause && !callerInfo.isCallerClosed()) {\n            Thread.sleep(conf.getInt(\"solr.hdfs.lease.recovery.pause\", 1000));\n\n            if (dfs.isFileClosed(p)) {\n              recovered = true;\n              break;\n            }\n          }\n        }\n      } catch (InterruptedException ie) {\n        InterruptedIOException iioe = new InterruptedIOException();\n        iioe.initCause(ie);\n        throw iioe;\n      }\n    }\n    if (recovered) {\n      RECOVER_LEASE_SUCCESS_COUNT.incrementAndGet();\n    }\n    return recovered;\n  }\n\n","sourceOld":"  /*\n   * Run the dfs recover lease. recoverLease is asynchronous. It returns:\n   *    -false when it starts the lease recovery (i.e. lease recovery not *yet* done)\n   *    - true when the lease recovery has succeeded or the file is closed.\n   * But, we have to be careful.  Each time we call recoverLease, it starts the recover lease\n   * process over from the beginning.  We could put ourselves in a situation where we are\n   * doing nothing but starting a recovery, interrupting it to start again, and so on.\n   * The findings over in HBASE-8354 have it that the namenode will try to recover the lease\n   * on the file's primary node.  If all is well, it should return near immediately.  But,\n   * as is common, it is the very primary node that has crashed and so the namenode will be\n   * stuck waiting on a socket timeout before it will ask another datanode to start the\n   * recovery. It does not help if we call recoverLease in the meantime and in particular,\n   * subsequent to the socket timeout, a recoverLease invocation will cause us to start\n   * over from square one (possibly waiting on socket timeout against primary node).  So,\n   * in the below, we do the following:\n   * 1. Call recoverLease.\n   * 2. If it returns true, break.\n   * 3. If it returns false, wait a few seconds and then call it again.\n   * 4. If it returns true, break.\n   * 5. If it returns false, wait for what we think the datanode socket timeout is\n   * (configurable) and then try again.\n   * 6. If it returns true, break.\n   * 7. If it returns false, repeat starting at step 5. above.\n   *\n   * If HDFS-4525 is available, call it every second and we might be able to exit early.\n   */\n  static boolean recoverDFSFileLease(final DistributedFileSystem dfs, final Path p, final Configuration conf, CallerInfo callerInfo)\n  throws IOException {\n    log.info(\"Recovering lease on dfs file \" + p);\n    long startWaiting = System.nanoTime();\n    // Default is 15 minutes. It's huge, but the idea is that if we have a major issue, HDFS\n    // usually needs 10 minutes before marking the nodes as dead. So we're putting ourselves\n    // beyond that limit 'to be safe'.\n    long recoveryTimeout = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.timeout\", 900000), TimeUnit.MILLISECONDS) + startWaiting;\n    // This setting should be a little bit above what the cluster dfs heartbeat is set to.\n    long firstPause = conf.getInt(\"solr.hdfs.lease.recovery.first.pause\", 4000);\n    // This should be set to how long it'll take for us to timeout against primary datanode if it\n    // is dead.  We set it to 61 seconds, 1 second than the default READ_TIMEOUT in HDFS, the\n    // default value for DFS_CLIENT_SOCKET_TIMEOUT_KEY.\n    long subsequentPause = TimeUnit.NANOSECONDS.convert(conf.getInt(\"solr.hdfs.lease.recovery.dfs.timeout\", 61 * 1000), TimeUnit.MILLISECONDS);\n    \n    if (dfs.isFileClosed(p)) {\n      return true;\n    }\n    \n    boolean recovered = false;\n    // We break the loop if we succeed the lease recovery, timeout, or we throw an exception.\n    for (int nbAttempt = 0; !recovered; nbAttempt++) {\n      recovered = recoverLease(dfs, nbAttempt, p, startWaiting);\n      if (recovered) break;\n      if (checkIfTimedout(conf, recoveryTimeout, nbAttempt, p, startWaiting) || callerInfo.isCallerClosed()) break;\n      try {\n        // On the first time through wait the short 'firstPause'.\n        if (nbAttempt == 0) {\n          Thread.sleep(firstPause);\n        } else {\n          // Cycle here until subsequentPause elapses.  While spinning, check isFileClosed\n          long localStartWaiting = System.nanoTime();\n          while ((System.nanoTime() - localStartWaiting) < subsequentPause && !callerInfo.isCallerClosed()) {\n            Thread.sleep(conf.getInt(\"solr.hdfs.lease.recovery.pause\", 1000));\n\n            if (dfs.isFileClosed(p)) {\n              recovered = true;\n              break;\n            }\n          }\n        }\n      } catch (InterruptedException ie) {\n        InterruptedIOException iioe = new InterruptedIOException();\n        iioe.initCause(ie);\n        throw iioe;\n      }\n    }\n    if (recovered) {\n      RECOVER_LEASE_SUCCESS_COUNT.incrementAndGet();\n    }\n    return recovered;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e0bbd91feb34c5f3efb419225ce6d207e7abb052":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e0bbd91feb34c5f3efb419225ce6d207e7abb052"],"54faedfb0e03479a38f5ee82f2dfaeea536e9404":["b93355434fdbfe132175a38a1946631387ad308a"],"0efab926cbe25efbe747b226015cd9ae2ee3daa0":["e0bbd91feb34c5f3efb419225ce6d207e7abb052"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b93355434fdbfe132175a38a1946631387ad308a":["e4bec8aad484912847547ca94038e1a3641c7bc7"],"e4bec8aad484912847547ca94038e1a3641c7bc7":["0efab926cbe25efbe747b226015cd9ae2ee3daa0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["54faedfb0e03479a38f5ee82f2dfaeea536e9404"]},"commit2Childs":{"e0bbd91feb34c5f3efb419225ce6d207e7abb052":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","0efab926cbe25efbe747b226015cd9ae2ee3daa0"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"54faedfb0e03479a38f5ee82f2dfaeea536e9404":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"0efab926cbe25efbe747b226015cd9ae2ee3daa0":["e4bec8aad484912847547ca94038e1a3641c7bc7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e0bbd91feb34c5f3efb419225ce6d207e7abb052","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"b93355434fdbfe132175a38a1946631387ad308a":["54faedfb0e03479a38f5ee82f2dfaeea536e9404"],"e4bec8aad484912847547ca94038e1a3641c7bc7":["b93355434fdbfe132175a38a1946631387ad308a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}