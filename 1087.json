{"path":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","commits":[{"id":"0f080986da691a3bba7b757f43ab72cdc82b57ce","date":1273069619,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)),\n    1,1,0,0,1,1,0);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)),\n    1,1,0,0,1,1,0);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)),\n    1,1,0,0,1,1,0);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)),\n    1,1,0,0,1,1,0);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4e8cc373c801e54cec75daf9f52792cb4b17f536","date":1291116159,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE, 1, 1, 0, 0, 1, 1, 0, 1, 1, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE, 1, 1, 0, 0, 1, 1, 0, 1, 1, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)),\n    1,1,0,0,1,1,0);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)),\n    1,1,0,0,1,1,0);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE, 1, 1, 0, 0, 1, 1, 0, 1, 1, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE, 1, 1, 0, 0, 1, 1, 0, 1, 1, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)),\n    1,1,0,0,1,1,0);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)),\n    1,1,0,0,1,1,0);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE, 1, 1, 0, 0, 1, 1, 0, 1, 1, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE, 1, 1, 0, 0, 1, 1, 0, 1, 1, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)),\n    1,1,0,0,1,1,0);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)),\n    1,1,0,0,1,1,0);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ec1acb945fb5751735f5c9482576c8760d97b6ab","date":1315370590,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE, 1, 1, 0, 0, 1, 1, 0, 1, 1, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE, 1, 1, 0, 0, 1, 1, 0, 1, 1, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","bugFix":null,"bugIntro":["c85fa43e6918808743daa7847ba0264373af687f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"888c2d6bca1edd8d9293631d6e1d188b036e0f05","date":1334076894,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 },\n        null, null, null, null, false);\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 },\n        null, null, null, null, false);\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","bugFix":null,"bugIntro":["c85fa43e6918808743daa7847ba0264373af687f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e","date":1334174049,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 },\n        null, null, null, null, false);\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 },\n        null, null, null, null, false);\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","bugFix":["2fd023a662cc25ae7e0ad0f33d71c476a16d0579"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 },\n        null, null, null, null, false);\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 },\n        null, null, null, null, false);\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 },\n        null, null, null, null, false);\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 },\n        null, null, null, null, false);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"888c2d6bca1edd8d9293631d6e1d188b036e0f05":["ec1acb945fb5751735f5c9482576c8760d97b6ab"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["ec1acb945fb5751735f5c9482576c8760d97b6ab","888c2d6bca1edd8d9293631d6e1d188b036e0f05"],"ec1acb945fb5751735f5c9482576c8760d97b6ab":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["0f080986da691a3bba7b757f43ab72cdc82b57ce","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"3bb13258feba31ab676502787ab2e1779f129b7a":["0f080986da691a3bba7b757f43ab72cdc82b57ce","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["0f080986da691a3bba7b757f43ab72cdc82b57ce"]},"commit2Childs":{"888c2d6bca1edd8d9293631d6e1d188b036e0f05":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"ec1acb945fb5751735f5c9482576c8760d97b6ab":["888c2d6bca1edd8d9293631d6e1d188b036e0f05","ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","3bb13258feba31ab676502787ab2e1779f129b7a","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"3bb13258feba31ab676502787ab2e1779f129b7a":[],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["ec1acb945fb5751735f5c9482576c8760d97b6ab","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","3bb13258feba31ab676502787ab2e1779f129b7a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","3bb13258feba31ab676502787ab2e1779f129b7a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}