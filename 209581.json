{"path":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"86365ce8db75e42ebe10805e99e92c463fef63b6","date":1330370408,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random,\n                                               defaultCodecSupportsDocValues());\n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random,\n                                               defaultCodecSupportsDocValues());\n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random);    \n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(),\n                                               defaultCodecSupportsDocValues());\n    final int numDocs = _TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    RandomIndexWriter w = new RandomIndexWriter(random, cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random,\n                                               defaultCodecSupportsDocValues());\n    final int numDocs = _TestUtil.nextInt(random, 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random.nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6550af40a4977993323e8fd3b3222a2402084800","date":1334548895,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(),\n                                               defaultCodecSupportsDocValues());\n    final int numDocs = _TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(),\n                                               defaultCodecSupportsDocValues());\n    final int numDocs = _TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57ae3024996ccdb3c36c42cb890e1efb37df4ce8","date":1338343651,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = _TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(),\n                                               defaultCodecSupportsDocValues());\n    final int numDocs = _TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = _TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = _TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = IndexReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = IndexReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6a0e3c1c21aac8ecf75706605133012833585c7","date":1347535263,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = _TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = _TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(\"docid\", id));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173","date":1365631993,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = _TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = _TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = new IndexSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = _TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e1151ecb4798f5c31137aec032c241638018ed20","date":1394284367,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a58bbbe1c866963764d3f15d3a26a6a85f6c6af4","date":1394564625,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<BytesRef>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.shutdown();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.shutdown();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.shutdown();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.shutdown();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a1862266772deb28cdcb7d996b64d2177022687","date":1453077824,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w, false);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"221076a44effb5561a3b799974ba1a35119fbcc0","date":1457468497,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory#testNRTAndCommit().mjava","sourceNew":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random());\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","sourceOld":"  public void testNRTAndCommit() throws Exception {\n    Directory dir = newDirectory();\n    NRTCachingDirectory cachedDir = new NRTCachingDirectory(dir, 2.0, 25.0);\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n    RandomIndexWriter w = new RandomIndexWriter(random(), cachedDir, conf);\n    final LineFileDocs docs = new LineFileDocs(random(), true);\n    final int numDocs = TestUtil.nextInt(random(), 100, 400);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: numDocs=\" + numDocs);\n    }\n\n    final List<BytesRef> ids = new ArrayList<>();\n    DirectoryReader r = null;\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      final Document doc = docs.nextDoc();\n      ids.add(new BytesRef(doc.get(\"docid\")));\n      w.addDocument(doc);\n      if (random().nextInt(20) == 17) {\n        if (r == null) {\n          r = DirectoryReader.open(w.w);\n        } else {\n          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n          if (r2 != null) {\n            r.close();\n            r = r2;\n          }\n        }\n        assertEquals(1+docCount, r.numDocs());\n        final IndexSearcher s = newSearcher(r);\n        // Just make sure search can run; we can't assert\n        // totHits since it could be 0\n        TopDocs hits = s.search(new TermQuery(new Term(\"body\", \"the\")), 10);\n        // System.out.println(\"tot hits \" + hits.totalHits);\n      }\n    }\n\n    if (r != null) {\n      r.close();\n    }\n\n    // Close should force cache to clear since all files are sync'd\n    w.close();\n\n    final String[] cachedFiles = cachedDir.listCachedFiles();\n    for(String file : cachedFiles) {\n      System.out.println(\"FAIL: cached file \" + file + \" remains after sync\");\n    }\n    assertEquals(0, cachedFiles.length);\n    \n    r = DirectoryReader.open(dir);\n    for(BytesRef id : ids) {\n      assertEquals(1, r.docFreq(new Term(\"docid\", id)));\n    }\n    r.close();\n    cachedDir.close();\n    docs.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["e1151ecb4798f5c31137aec032c241638018ed20"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","86365ce8db75e42ebe10805e99e92c463fef63b6"],"a58bbbe1c866963764d3f15d3a26a6a85f6c6af4":["6613659748fe4411a7dcf85266e55db1f95f7315","e1151ecb4798f5c31137aec032c241638018ed20"],"6613659748fe4411a7dcf85266e55db1f95f7315":["3b0e8c39ca08b5a02de6edcd33d6f3b90b865173"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"6550af40a4977993323e8fd3b3222a2402084800":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"86365ce8db75e42ebe10805e99e92c463fef63b6":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"221076a44effb5561a3b799974ba1a35119fbcc0":["2a1862266772deb28cdcb7d996b64d2177022687"],"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173":["b6a0e3c1c21aac8ecf75706605133012833585c7"],"e1151ecb4798f5c31137aec032c241638018ed20":["6613659748fe4411a7dcf85266e55db1f95f7315"],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["6550af40a4977993323e8fd3b3222a2402084800"],"2a1862266772deb28cdcb7d996b64d2177022687":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"b6a0e3c1c21aac8ecf75706605133012833585c7":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["86365ce8db75e42ebe10805e99e92c463fef63b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["221076a44effb5561a3b799974ba1a35119fbcc0"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"a58bbbe1c866963764d3f15d3a26a6a85f6c6af4":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","86365ce8db75e42ebe10805e99e92c463fef63b6"],"6613659748fe4411a7dcf85266e55db1f95f7315":["a58bbbe1c866963764d3f15d3a26a6a85f6c6af4","e1151ecb4798f5c31137aec032c241638018ed20"],"86365ce8db75e42ebe10805e99e92c463fef63b6":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"6550af40a4977993323e8fd3b3222a2402084800":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"221076a44effb5561a3b799974ba1a35119fbcc0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173":["6613659748fe4411a7dcf85266e55db1f95f7315"],"e1151ecb4798f5c31137aec032c241638018ed20":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","a58bbbe1c866963764d3f15d3a26a6a85f6c6af4"],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"2a1862266772deb28cdcb7d996b64d2177022687":["221076a44effb5561a3b799974ba1a35119fbcc0"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["b6a0e3c1c21aac8ecf75706605133012833585c7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"b6a0e3c1c21aac8ecf75706605133012833585c7":["3b0e8c39ca08b5a02de6edcd33d6f3b90b865173"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["2a1862266772deb28cdcb7d996b64d2177022687"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["6550af40a4977993323e8fd3b3222a2402084800"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","a58bbbe1c866963764d3f15d3a26a6a85f6c6af4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}