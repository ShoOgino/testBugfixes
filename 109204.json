{"path":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","commits":[{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"/dev/null","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"/dev/null","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"/dev/null","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0762b640e0d0d12b6edb96db68986e13145c3484","date":1307575932,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    for (int i = 0; i < 39; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", TextField.TYPE_STORED, \"d1 first field\"));\n        d.add(new Field(\"f2\", TextField.TYPE_STORED, \"d1 second field\"));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", TextField.TYPE_STORED, \"d2 first field\"));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", TextField.TYPE_STORED, \"d3 first field\"));\n        d.add(new Field(\"f2\", TextField.TYPE_STORED, \"d3 second field\"));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d1 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f2\", \"d3 second field\", Store.YES, Index.ANALYZED,\n            TermVector.NO));\n        d.add(new Field(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["fa0f44f887719e97183771e977cfc4bfb485b766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd","date":1317197236,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", TextField.TYPE_STORED, \"d1 first field\"));\n        d.add(new Field(\"f2\", TextField.TYPE_STORED, \"d1 second field\"));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", TextField.TYPE_STORED, \"d2 first field\"));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", TextField.TYPE_STORED, \"d3 first field\"));\n        d.add(new Field(\"f2\", TextField.TYPE_STORED, \"d3 second field\"));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"_1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n        assertFalse(dir.fileExists(\"_3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"_1.fnx\"));\n      assertTrue(dir.fileExists(\"_2.fnx\"));\n      assertFalse(dir.fileExists(\"_3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"1.fnx\"));\n        assertTrue(dir.fileExists(\"2.fnx\"));\n        assertFalse(dir.fileExists(\"3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"1.fnx\"));\n      assertTrue(dir.fileExists(\"2.fnx\"));\n      assertFalse(dir.fileExists(\"3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"_1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n        assertFalse(dir.fileExists(\"_3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.optimize();\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"_1.fnx\"));\n      assertTrue(dir.fileExists(\"_2.fnx\"));\n      assertFalse(dir.fileExists(\"_3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"_1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n        assertFalse(dir.fileExists(\"_3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()));\n      writer.optimize();\n      assertFalse(\" field numbers got mixed up\", writer.anyNonBulkMerges);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"_1.fnx\"));\n      assertTrue(dir.fileExists(\"_2.fnx\"));\n      assertFalse(dir.fileExists(\"_3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"_1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n        assertFalse(dir.fileExists(\"_3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"_1.fnx\"));\n      assertTrue(dir.fileExists(\"_2.fnx\"));\n      assertFalse(dir.fileExists(\"_3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"_1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n        assertFalse(dir.fileExists(\"_3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.optimize();\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"_1.fnx\"));\n      assertTrue(dir.fileExists(\"_2.fnx\"));\n      assertFalse(dir.fileExists(\"_3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"34ce7c842452c79b12c45a8feb64e4597c7110e8","date":1321637224,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"_1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n        assertFalse(dir.fileExists(\"_3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"_1.fnx\"));\n      assertTrue(dir.fileExists(\"_2.fnx\"));\n      assertFalse(dir.fileExists(\"_3.fnx\"));\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"_1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n        assertFalse(dir.fileExists(\"_3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.expungeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"_1.fnx\"));\n      assertTrue(dir.fileExists(\"_2.fnx\"));\n      assertFalse(dir.fileExists(\"_3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"319624eb66a10b717d3e66af448543e7dc5c479d","date":1322741556,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertTrue(dir.fileExists(\"_1.fnx\"));\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n        assertFalse(dir.fileExists(\"_1.fnx\"));\n        assertTrue(dir.fileExists(\"_2.fnx\"));\n        assertFalse(dir.fileExists(\"_3.fnx\"));\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      assertFalse(dir.fileExists(\"_1.fnx\"));\n      assertTrue(dir.fileExists(\"_2.fnx\"));\n      assertFalse(dir.fileExists(\"_3.fnx\"));\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fa0f44f887719e97183771e977cfc4bfb485b766","date":1326668713,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new BinaryField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConsistentFieldNumbers#testFieldNumberGaps().mjava","sourceNew":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testFieldNumberGaps() throws IOException {\n    int numIters = atLeast(13);\n    for (int i = 0; i < numIters; i++) {\n      Directory dir = newDirectory();\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            NoMergePolicy.NO_COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d1 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d1 second field\", TextField.TYPE_STORED));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(1, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      }\n      \n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d2 first field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(2, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        Document d = new Document();\n        d.add(new Field(\"f1\", \"d3 first field\", TextField.TYPE_STORED));\n        d.add(new Field(\"f2\", \"d3 second field\", TextField.TYPE_STORED));\n        d.add(new StoredField(\"f3\", new byte[] { 1, 2, 3, 4, 5 }));\n        writer.addDocument(d);\n        writer.close();\n        SegmentInfos sis = new SegmentInfos();\n        sis.read(dir);\n        assertEquals(3, sis.size());\n        FieldInfos fis1 = sis.info(0).getFieldInfos();\n        FieldInfos fis2 = sis.info(1).getFieldInfos();\n        FieldInfos fis3 = sis.info(2).getFieldInfos();\n        assertEquals(\"f1\", fis1.fieldInfo(0).name);\n        assertEquals(\"f2\", fis1.fieldInfo(1).name);\n        assertEquals(\"f1\", fis2.fieldInfo(0).name);\n        assertNull(fis2.fieldInfo(1));\n        assertEquals(\"f3\", fis2.fieldInfo(2).name);\n        assertEquals(\"f1\", fis3.fieldInfo(0).name);\n        assertEquals(\"f2\", fis3.fieldInfo(1).name);\n        assertEquals(\"f3\", fis3.fieldInfo(2).name);\n      }\n\n      {\n        IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n            TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n            random.nextBoolean() ? NoMergePolicy.NO_COMPOUND_FILES\n                : NoMergePolicy.COMPOUND_FILES));\n        writer.deleteDocuments(new Term(\"f1\", \"d1\"));\n        // nuke the first segment entirely so that the segment with gaps is\n        // loaded first!\n        writer.forceMergeDeletes();\n        writer.close();\n      }\n\n      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(\n          new LogByteSizeMergePolicy()).setInfoStream(new FailOnNonBulkMergesInfoStream()));\n      writer.forceMerge(1);\n      writer.close();\n\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      assertEquals(1, sis.size());\n      FieldInfos fis1 = sis.info(0).getFieldInfos();\n      assertEquals(\"f1\", fis1.fieldInfo(0).name);\n      assertEquals(\"f2\", fis1.fieldInfo(1).name);\n      assertEquals(\"f3\", fis1.fieldInfo(2).name);\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a3776dccca01c11e7046323cfad46a3b4a471233","0762b640e0d0d12b6edb96db68986e13145c3484"],"0762b640e0d0d12b6edb96db68986e13145c3484":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["fa0f44f887719e97183771e977cfc4bfb485b766"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["06584e6e98d592b34e1329b384182f368d2025e8"],"962d04139994fce5193143ef35615499a9a96d78":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"34ce7c842452c79b12c45a8feb64e4597c7110e8":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"7b91922b55d15444d554721b352861d028eb8278":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1224a4027481acce15495b03bce9b48b93b42722"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["1224a4027481acce15495b03bce9b48b93b42722"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1224a4027481acce15495b03bce9b48b93b42722"],"a3776dccca01c11e7046323cfad46a3b4a471233":["1224a4027481acce15495b03bce9b48b93b42722","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["135621f3a0670a9394eb563224a3b76cc4dddc0f","0762b640e0d0d12b6edb96db68986e13145c3484"],"fa0f44f887719e97183771e977cfc4bfb485b766":["319624eb66a10b717d3e66af448543e7dc5c479d"],"319624eb66a10b717d3e66af448543e7dc5c479d":["34ce7c842452c79b12c45a8feb64e4597c7110e8"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["0762b640e0d0d12b6edb96db68986e13145c3484"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"1224a4027481acce15495b03bce9b48b93b42722":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":[],"0762b640e0d0d12b6edb96db68986e13145c3484":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"06584e6e98d592b34e1329b384182f368d2025e8":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["34ce7c842452c79b12c45a8feb64e4597c7110e8"],"962d04139994fce5193143ef35615499a9a96d78":[],"34ce7c842452c79b12c45a8feb64e4597c7110e8":["319624eb66a10b717d3e66af448543e7dc5c479d"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["962d04139994fce5193143ef35615499a9a96d78"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["0762b640e0d0d12b6edb96db68986e13145c3484","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","1224a4027481acce15495b03bce9b48b93b42722"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"fa0f44f887719e97183771e977cfc4bfb485b766":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"319624eb66a10b717d3e66af448543e7dc5c479d":["fa0f44f887719e97183771e977cfc4bfb485b766"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd":["7b91922b55d15444d554721b352861d028eb8278"],"1224a4027481acce15495b03bce9b48b93b42722":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","f2c5f0cb44df114db4228c8f77861714b5cabaea","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","962d04139994fce5193143ef35615499a9a96d78","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}