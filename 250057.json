{"path":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms,int).mjava","commits":[{"id":"dd5cb61545fbf8394e449204e9780415b9f4c6fc","date":1429738516,"type":1,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms,int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms).mjava","sourceNew":"  /**\n   * Constructor. The uninversion doesn't happen here; it's delayed till the first call to\n   * {@link #incrementToken}.\n   *\n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and/or offsets.\n   * @param maxStartOffset if a token's start offset exceeds this then the token is not added. -1 disables the limit.\n   */\n  public TokenStreamFromTermVector(Terms vector, int maxStartOffset) throws IOException {\n    super(ATTRIBUTE_FACTORY);\n    this.maxStartOffset = maxStartOffset < 0 ? Integer.MAX_VALUE : maxStartOffset;\n    assert !hasAttribute(PayloadAttribute.class) : \"AttributeFactory shouldn't have payloads *yet*\";\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","sourceOld":"  /**\n   * Constructor.\n   * \n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and/or offsets.\n   */\n  public TokenStreamFromTermVector(Terms vector) throws IOException {\n    super(ATTRIBUTE_FACTORY);\n    assert !hasAttribute(PayloadAttribute.class) : \"AttributeFactory shouldn't have payloads *yet*\";\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1e2d88b79718ae66cfda8ad1dcbe7c20d8e690fd","date":1532104414,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms,int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms,int).mjava","sourceNew":"  /**\n   * Constructor. The uninversion doesn't happen here; it's delayed till the first call to\n   * {@link #incrementToken}.\n   *\n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and/or offsets.\n   * @param maxStartOffset if a token's start offset exceeds this then the token is not added. -1 disables the limit.\n   */\n  public TokenStreamFromTermVector(Terms vector, int maxStartOffset) throws IOException {\n    this.maxStartOffset = maxStartOffset < 0 ? Integer.MAX_VALUE : maxStartOffset;\n    assert !hasAttribute(PayloadAttribute.class) : \"AttributeFactory shouldn't have payloads *yet*\";\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","sourceOld":"  /**\n   * Constructor. The uninversion doesn't happen here; it's delayed till the first call to\n   * {@link #incrementToken}.\n   *\n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and/or offsets.\n   * @param maxStartOffset if a token's start offset exceeds this then the token is not added. -1 disables the limit.\n   */\n  public TokenStreamFromTermVector(Terms vector, int maxStartOffset) throws IOException {\n    super(ATTRIBUTE_FACTORY);\n    this.maxStartOffset = maxStartOffset < 0 ? Integer.MAX_VALUE : maxStartOffset;\n    assert !hasAttribute(PayloadAttribute.class) : \"AttributeFactory shouldn't have payloads *yet*\";\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"dd5cb61545fbf8394e449204e9780415b9f4c6fc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1e2d88b79718ae66cfda8ad1dcbe7c20d8e690fd":["dd5cb61545fbf8394e449204e9780415b9f4c6fc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1e2d88b79718ae66cfda8ad1dcbe7c20d8e690fd"]},"commit2Childs":{"dd5cb61545fbf8394e449204e9780415b9f4c6fc":["1e2d88b79718ae66cfda8ad1dcbe7c20d8e690fd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["dd5cb61545fbf8394e449204e9780415b9f4c6fc"],"1e2d88b79718ae66cfda8ad1dcbe7c20d8e690fd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}