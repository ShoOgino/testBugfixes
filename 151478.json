{"path":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.shutdown();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.shutdown();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.shutdown();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.shutdown();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3384e6013a93e4d11b7d75388693f8d0388602bf","date":1413951663,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db68c63cbfaa8698b9c4475f75ed2b9c9696d238","date":1414118621,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d77dafd89756a5161d244985903e3487ca109182","date":1548679743,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new ByteBuffersDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"253ccdb82bf7377339fd5b7bce8a35cd50f2742b","date":1601884698,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new ByteBuffersDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge(min / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new ByteBuffersDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = SegmentInfos.readLatestCommit(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"253ccdb82bf7377339fd5b7bce8a35cd50f2742b":["d77dafd89756a5161d244985903e3487ca109182"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"d77dafd89756a5161d244985903e3487ca109182":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["d0ef034a4f10871667ae75181537775ddcf8ade4","3384e6013a93e4d11b7d75388693f8d0388602bf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["253ccdb82bf7377339fd5b7bce8a35cd50f2742b"]},"commit2Childs":{"253ccdb82bf7377339fd5b7bce8a35cd50f2742b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["3384e6013a93e4d11b7d75388693f8d0388602bf","db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"d77dafd89756a5161d244985903e3487ca109182":["253ccdb82bf7377339fd5b7bce8a35cd50f2742b"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["d77dafd89756a5161d244985903e3487ca109182"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}