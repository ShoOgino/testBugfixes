{"path":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(Directory,List[String]).mjava","commits":[{"id":"867e3d9153fb761456b54a9dcce566e1545c5ef6","date":1444903098,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(Directory,List[String]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(List[Path],Path).mjava","sourceNew":"  /** Merge a list of sorted temporary files (partitions) into an output file.  Note that this closes the\n   *  incoming {@link IndexOutput}. */\n  void mergePartitions(Directory trackingDir, List<String> segments) throws IOException {\n    long start = System.currentTimeMillis();\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segments.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current.get(), b.current.get()) < 0;\n      }\n    };\n\n    ByteSequencesReader[] streams = new ByteSequencesReader[segments.size()];\n\n    String newSegmentName = null;\n\n    try (IndexOutput out = trackingDir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT)) {\n      newSegmentName = out.getName();\n      ByteSequencesWriter writer = new ByteSequencesWriter(out);\n\n      // Open streams and read the top for each file\n      for (int i = 0; i < segments.size(); i++) {\n        streams[i] = new ByteSequencesReader(dir.openInput(segments.get(i), IOContext.READONCE));\n        byte[] line = streams[i].read();\n        assert line != null;\n        queue.insertWithOverflow(new FileAndTop(i, line));\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        writer.write(top.current.bytes(), 0, top.current.length());\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      IOUtils.close(streams);\n    }\n\n    IOUtils.deleteFiles(trackingDir, segments);\n\n    segments.clear();\n    segments.add(newSegmentName);\n\n    sortInfo.tempMergeFiles++;\n  }\n\n","sourceOld":"  /** Merge a list of sorted temporary files (partitions) into an output file */\n  void mergePartitions(List<Path> merges, Path outputFile) throws IOException {\n    long start = System.currentTimeMillis();\n\n    ByteSequencesWriter out = new ByteSequencesWriter(outputFile);\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(merges.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current.get(), b.current.get()) < 0;\n      }\n    };\n\n    ByteSequencesReader [] streams = new ByteSequencesReader [merges.size()];\n    try {\n      // Open streams and read the top for each file\n      for (int i = 0; i < merges.size(); i++) {\n        streams[i] = new ByteSequencesReader(merges.get(i));\n        byte line[] = streams[i].read();\n        if (line != null) {\n          queue.insertWithOverflow(new FileAndTop(i, line));\n        }\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        out.write(top.current.bytes(), 0, top.current.length());\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      // The logic below is: if an exception occurs in closing out, it has a priority over exceptions\n      // happening in closing streams.\n      try {\n        IOUtils.close(streams);\n      } finally {\n        IOUtils.close(out);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e3ce1ef883d26aa73919aa2d53991726e96caa13","date":1445421402,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(Directory,List[String]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(Directory,List[String]).mjava","sourceNew":"  /** Merge a list of sorted temporary files (partitions) into an output file.  Note that this closes the\n   *  incoming {@link IndexOutput}. */\n  void mergePartitions(Directory trackingDir, List<String> segments) throws IOException {\n    long start = System.currentTimeMillis();\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segments.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current.get(), b.current.get()) < 0;\n      }\n    };\n\n    ByteSequencesReader[] streams = new ByteSequencesReader[segments.size()];\n\n    String newSegmentName = null;\n\n    try (IndexOutput out = trackingDir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT);\n         ByteSequencesWriter writer = getWriter(out);) {\n\n      newSegmentName = out.getName();\n      \n      // Open streams and read the top for each file\n      for (int i = 0; i < segments.size(); i++) {\n        streams[i] = getReader(dir.openInput(segments.get(i), IOContext.READONCE));\n        BytesRefBuilder bytes = new BytesRefBuilder();\n        boolean result = streams[i].read(bytes);\n        assert result;\n        queue.insertWithOverflow(new FileAndTop(i, bytes));\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        writer.write(top.current.bytes(), 0, top.current.length());\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      IOUtils.close(streams);\n    }\n\n    IOUtils.deleteFiles(trackingDir, segments);\n\n    segments.clear();\n    segments.add(newSegmentName);\n\n    sortInfo.tempMergeFiles++;\n  }\n\n","sourceOld":"  /** Merge a list of sorted temporary files (partitions) into an output file.  Note that this closes the\n   *  incoming {@link IndexOutput}. */\n  void mergePartitions(Directory trackingDir, List<String> segments) throws IOException {\n    long start = System.currentTimeMillis();\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segments.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current.get(), b.current.get()) < 0;\n      }\n    };\n\n    ByteSequencesReader[] streams = new ByteSequencesReader[segments.size()];\n\n    String newSegmentName = null;\n\n    try (IndexOutput out = trackingDir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT)) {\n      newSegmentName = out.getName();\n      ByteSequencesWriter writer = new ByteSequencesWriter(out);\n\n      // Open streams and read the top for each file\n      for (int i = 0; i < segments.size(); i++) {\n        streams[i] = new ByteSequencesReader(dir.openInput(segments.get(i), IOContext.READONCE));\n        byte[] line = streams[i].read();\n        assert line != null;\n        queue.insertWithOverflow(new FileAndTop(i, line));\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        writer.write(top.current.bytes(), 0, top.current.length());\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      IOUtils.close(streams);\n    }\n\n    IOUtils.deleteFiles(trackingDir, segments);\n\n    segments.clear();\n    segments.add(newSegmentName);\n\n    sortInfo.tempMergeFiles++;\n  }\n\n","bugFix":null,"bugIntro":["2b84d416bbd661ae4b2a28d103bdfccb851e00de"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2b84d416bbd661ae4b2a28d103bdfccb851e00de","date":1458041762,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(Directory,List[String]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(Directory,List[String]).mjava","sourceNew":"  /** Merge the most recent {@code maxTempFile} partitions into a new partition. */\n  void mergePartitions(Directory trackingDir, List<String> segments) throws IOException {\n    long start = System.currentTimeMillis();\n\n    List<String> segmentsToMerge;\n    if (segments.size() > maxTempFiles) {\n      segmentsToMerge = segments.subList(segments.size() - maxTempFiles, segments.size());\n    } else {\n      segmentsToMerge = segments;\n    }\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segmentsToMerge.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current.get(), b.current.get()) < 0;\n      }\n    };\n\n    ByteSequencesReader[] streams = new ByteSequencesReader[segmentsToMerge.size()];\n\n    String newSegmentName = null;\n\n    try (IndexOutput out = trackingDir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT);\n         ByteSequencesWriter writer = getWriter(out);) {\n\n      newSegmentName = out.getName();\n      \n      // Open streams and read the top for each file\n      for (int i = 0; i < segmentsToMerge.size(); i++) {\n        streams[i] = getReader(dir.openInput(segmentsToMerge.get(i), IOContext.READONCE));\n        BytesRefBuilder bytes = new BytesRefBuilder();\n        boolean result = streams[i].read(bytes);\n        assert result;\n        queue.insertWithOverflow(new FileAndTop(i, bytes));\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        writer.write(top.current.bytes(), 0, top.current.length());\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      IOUtils.close(streams);\n    }\n\n    IOUtils.deleteFiles(trackingDir, segmentsToMerge);\n\n    segmentsToMerge.clear();\n    segments.add(newSegmentName);\n\n    sortInfo.tempMergeFiles++;\n  }\n\n","sourceOld":"  /** Merge a list of sorted temporary files (partitions) into an output file.  Note that this closes the\n   *  incoming {@link IndexOutput}. */\n  void mergePartitions(Directory trackingDir, List<String> segments) throws IOException {\n    long start = System.currentTimeMillis();\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segments.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current.get(), b.current.get()) < 0;\n      }\n    };\n\n    ByteSequencesReader[] streams = new ByteSequencesReader[segments.size()];\n\n    String newSegmentName = null;\n\n    try (IndexOutput out = trackingDir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT);\n         ByteSequencesWriter writer = getWriter(out);) {\n\n      newSegmentName = out.getName();\n      \n      // Open streams and read the top for each file\n      for (int i = 0; i < segments.size(); i++) {\n        streams[i] = getReader(dir.openInput(segments.get(i), IOContext.READONCE));\n        BytesRefBuilder bytes = new BytesRefBuilder();\n        boolean result = streams[i].read(bytes);\n        assert result;\n        queue.insertWithOverflow(new FileAndTop(i, bytes));\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        writer.write(top.current.bytes(), 0, top.current.length());\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      IOUtils.close(streams);\n    }\n\n    IOUtils.deleteFiles(trackingDir, segments);\n\n    segments.clear();\n    segments.add(newSegmentName);\n\n    sortInfo.tempMergeFiles++;\n  }\n\n","bugFix":["e3ce1ef883d26aa73919aa2d53991726e96caa13","867e3d9153fb761456b54a9dcce566e1545c5ef6"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"950b7a6881d14da782b60444c11295e3ec50d41a","date":1458379095,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(Directory,List[String]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(Directory,List[String]).mjava","sourceNew":"  /** Merge the most recent {@code maxTempFile} partitions into a new partition. */\n  void mergePartitions(Directory trackingDir, List<String> segments) throws IOException {\n    long start = System.currentTimeMillis();\n\n    List<String> segmentsToMerge;\n    if (segments.size() > maxTempFiles) {\n      segmentsToMerge = segments.subList(segments.size() - maxTempFiles, segments.size());\n    } else {\n      segmentsToMerge = segments;\n    }\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segmentsToMerge.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current.get(), b.current.get()) < 0;\n      }\n    };\n\n    ByteSequencesReader[] streams = new ByteSequencesReader[segmentsToMerge.size()];\n\n    String newSegmentName = null;\n\n    try (ByteSequencesWriter writer = getWriter(trackingDir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT))) {\n\n      newSegmentName = writer.out.getName();\n      \n      // Open streams and read the top for each file\n      for (int i = 0; i < segmentsToMerge.size(); i++) {\n        streams[i] = getReader(dir.openChecksumInput(segmentsToMerge.get(i), IOContext.READONCE), segmentsToMerge.get(i));\n        BytesRefBuilder bytes = new BytesRefBuilder();\n        boolean result = false;\n        try {\n          result = streams[i].read(bytes);\n        } catch (Throwable t) {\n          verifyChecksum(t, streams[i]);\n        }\n        assert result;\n        queue.insertWithOverflow(new FileAndTop(i, bytes));\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        writer.write(top.current.bytes(), 0, top.current.length());\n        boolean result = false;\n        try {\n          result = streams[top.fd].read(top.current);\n        } catch (Throwable t) {\n          verifyChecksum(t, streams[top.fd]);\n        }\n\n        if (result) {\n          queue.updateTop();\n        } else {\n          queue.pop();\n        }\n      }\n\n      CodecUtil.writeFooter(writer.out);\n\n      for(ByteSequencesReader reader : streams) {\n        CodecUtil.checkFooter(reader.in);\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      IOUtils.close(streams);\n    }\n\n    IOUtils.deleteFiles(trackingDir, segmentsToMerge);\n\n    segmentsToMerge.clear();\n    segments.add(newSegmentName);\n\n    sortInfo.tempMergeFiles++;\n  }\n\n","sourceOld":"  /** Merge the most recent {@code maxTempFile} partitions into a new partition. */\n  void mergePartitions(Directory trackingDir, List<String> segments) throws IOException {\n    long start = System.currentTimeMillis();\n\n    List<String> segmentsToMerge;\n    if (segments.size() > maxTempFiles) {\n      segmentsToMerge = segments.subList(segments.size() - maxTempFiles, segments.size());\n    } else {\n      segmentsToMerge = segments;\n    }\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segmentsToMerge.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current.get(), b.current.get()) < 0;\n      }\n    };\n\n    ByteSequencesReader[] streams = new ByteSequencesReader[segmentsToMerge.size()];\n\n    String newSegmentName = null;\n\n    try (IndexOutput out = trackingDir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT);\n         ByteSequencesWriter writer = getWriter(out);) {\n\n      newSegmentName = out.getName();\n      \n      // Open streams and read the top for each file\n      for (int i = 0; i < segmentsToMerge.size(); i++) {\n        streams[i] = getReader(dir.openInput(segmentsToMerge.get(i), IOContext.READONCE));\n        BytesRefBuilder bytes = new BytesRefBuilder();\n        boolean result = streams[i].read(bytes);\n        assert result;\n        queue.insertWithOverflow(new FileAndTop(i, bytes));\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        writer.write(top.current.bytes(), 0, top.current.length());\n        if (!streams[top.fd].read(top.current)) {\n          queue.pop();\n        } else {\n          queue.updateTop();\n        }\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      IOUtils.close(streams);\n    }\n\n    IOUtils.deleteFiles(trackingDir, segmentsToMerge);\n\n    segmentsToMerge.clear();\n    segments.add(newSegmentName);\n\n    sortInfo.tempMergeFiles++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7849935cc625c020857f3b29be91b5d4323d19aa","date":1458978426,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(Directory,List[String]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(Directory,List[String]).mjava","sourceNew":"  /** Merge the most recent {@code maxTempFile} partitions into a new partition. */\n  void mergePartitions(Directory trackingDir, List<String> segments) throws IOException {\n    long start = System.currentTimeMillis();\n\n    List<String> segmentsToMerge;\n    if (segments.size() > maxTempFiles) {\n      segmentsToMerge = segments.subList(segments.size() - maxTempFiles, segments.size());\n    } else {\n      segmentsToMerge = segments;\n    }\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segmentsToMerge.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader[] streams = new ByteSequencesReader[segmentsToMerge.size()];\n\n    String newSegmentName = null;\n\n    try (ByteSequencesWriter writer = getWriter(trackingDir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT))) {\n\n      newSegmentName = writer.out.getName();\n      \n      // Open streams and read the top for each file\n      for (int i = 0; i < segmentsToMerge.size(); i++) {\n        streams[i] = getReader(dir.openChecksumInput(segmentsToMerge.get(i), IOContext.READONCE), segmentsToMerge.get(i));\n        BytesRef item = null;\n        try {\n          item = streams[i].next();\n        } catch (Throwable t) {\n          verifyChecksum(t, streams[i]);\n        }\n        assert item != null;\n        queue.insertWithOverflow(new FileAndTop(i, item));\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        writer.write(top.current);\n        try {\n          top.current = streams[top.fd].next();\n        } catch (Throwable t) {\n          verifyChecksum(t, streams[top.fd]);\n        }\n\n        if (top.current != null) {\n          queue.updateTop();\n        } else {\n          queue.pop();\n        }\n      }\n\n      CodecUtil.writeFooter(writer.out);\n\n      for(ByteSequencesReader reader : streams) {\n        CodecUtil.checkFooter(reader.in);\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      IOUtils.close(streams);\n    }\n\n    IOUtils.deleteFiles(trackingDir, segmentsToMerge);\n\n    segmentsToMerge.clear();\n    segments.add(newSegmentName);\n\n    sortInfo.tempMergeFiles++;\n  }\n\n","sourceOld":"  /** Merge the most recent {@code maxTempFile} partitions into a new partition. */\n  void mergePartitions(Directory trackingDir, List<String> segments) throws IOException {\n    long start = System.currentTimeMillis();\n\n    List<String> segmentsToMerge;\n    if (segments.size() > maxTempFiles) {\n      segmentsToMerge = segments.subList(segments.size() - maxTempFiles, segments.size());\n    } else {\n      segmentsToMerge = segments;\n    }\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segmentsToMerge.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current.get(), b.current.get()) < 0;\n      }\n    };\n\n    ByteSequencesReader[] streams = new ByteSequencesReader[segmentsToMerge.size()];\n\n    String newSegmentName = null;\n\n    try (ByteSequencesWriter writer = getWriter(trackingDir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT))) {\n\n      newSegmentName = writer.out.getName();\n      \n      // Open streams and read the top for each file\n      for (int i = 0; i < segmentsToMerge.size(); i++) {\n        streams[i] = getReader(dir.openChecksumInput(segmentsToMerge.get(i), IOContext.READONCE), segmentsToMerge.get(i));\n        BytesRefBuilder bytes = new BytesRefBuilder();\n        boolean result = false;\n        try {\n          result = streams[i].read(bytes);\n        } catch (Throwable t) {\n          verifyChecksum(t, streams[i]);\n        }\n        assert result;\n        queue.insertWithOverflow(new FileAndTop(i, bytes));\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        writer.write(top.current.bytes(), 0, top.current.length());\n        boolean result = false;\n        try {\n          result = streams[top.fd].read(top.current);\n        } catch (Throwable t) {\n          verifyChecksum(t, streams[top.fd]);\n        }\n\n        if (result) {\n          queue.updateTop();\n        } else {\n          queue.pop();\n        }\n      }\n\n      CodecUtil.writeFooter(writer.out);\n\n      for(ByteSequencesReader reader : streams) {\n        CodecUtil.checkFooter(reader.in);\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      IOUtils.close(streams);\n    }\n\n    IOUtils.deleteFiles(trackingDir, segmentsToMerge);\n\n    segmentsToMerge.clear();\n    segments.add(newSegmentName);\n\n    sortInfo.tempMergeFiles++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"62e7e8f89cb6b0283f3f5d6c0945453b73f09d45","date":1492172132,"type":5,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(Directory,List[PartitionAndCount]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(Directory,List[String]).mjava","sourceNew":"  /** Merge the most recent {@code maxTempFile} partitions into a new partition. */\n  void mergePartitions(Directory trackingDir, List<PartitionAndCount> segments) throws IOException {\n    long start = System.currentTimeMillis();\n\n    List<PartitionAndCount> segmentsToMerge;\n    if (segments.size() > maxTempFiles) {\n      segmentsToMerge = segments.subList(segments.size() - maxTempFiles, segments.size());\n    } else {\n      segmentsToMerge = segments;\n    }\n\n    long totalCount = 0;\n    for (PartitionAndCount segment : segmentsToMerge) {\n      totalCount += segment.count;\n    }\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segmentsToMerge.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader[] streams = new ByteSequencesReader[segmentsToMerge.size()];\n\n    String newSegmentName = null;\n\n    try (ByteSequencesWriter writer = getWriter(trackingDir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT), totalCount)) {\n\n      newSegmentName = writer.out.getName();\n      \n      // Open streams and read the top for each file\n      for (int i = 0; i < segmentsToMerge.size(); i++) {\n        streams[i] = getReader(dir.openChecksumInput(segmentsToMerge.get(i).fileName, IOContext.READONCE), segmentsToMerge.get(i).fileName);\n        BytesRef item = null;\n        try {\n          item = streams[i].next();\n        } catch (Throwable t) {\n          verifyChecksum(t, streams[i]);\n        }\n        assert item != null;\n        queue.insertWithOverflow(new FileAndTop(i, item));\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        writer.write(top.current);\n        try {\n          top.current = streams[top.fd].next();\n        } catch (Throwable t) {\n          verifyChecksum(t, streams[top.fd]);\n        }\n\n        if (top.current != null) {\n          queue.updateTop();\n        } else {\n          queue.pop();\n        }\n      }\n\n      CodecUtil.writeFooter(writer.out);\n\n      for(ByteSequencesReader reader : streams) {\n        CodecUtil.checkFooter(reader.in);\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      IOUtils.close(streams);\n    }\n\n    IOUtils.deleteFiles(trackingDir, segmentsToMerge.stream().map(segment -> segment.fileName).collect(Collectors.toList()));\n\n    segmentsToMerge.clear();\n    segments.add(new PartitionAndCount(totalCount, newSegmentName));\n\n    sortInfo.tempMergeFiles++;\n  }\n\n","sourceOld":"  /** Merge the most recent {@code maxTempFile} partitions into a new partition. */\n  void mergePartitions(Directory trackingDir, List<String> segments) throws IOException {\n    long start = System.currentTimeMillis();\n\n    List<String> segmentsToMerge;\n    if (segments.size() > maxTempFiles) {\n      segmentsToMerge = segments.subList(segments.size() - maxTempFiles, segments.size());\n    } else {\n      segmentsToMerge = segments;\n    }\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segmentsToMerge.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader[] streams = new ByteSequencesReader[segmentsToMerge.size()];\n\n    String newSegmentName = null;\n\n    try (ByteSequencesWriter writer = getWriter(trackingDir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT))) {\n\n      newSegmentName = writer.out.getName();\n      \n      // Open streams and read the top for each file\n      for (int i = 0; i < segmentsToMerge.size(); i++) {\n        streams[i] = getReader(dir.openChecksumInput(segmentsToMerge.get(i), IOContext.READONCE), segmentsToMerge.get(i));\n        BytesRef item = null;\n        try {\n          item = streams[i].next();\n        } catch (Throwable t) {\n          verifyChecksum(t, streams[i]);\n        }\n        assert item != null;\n        queue.insertWithOverflow(new FileAndTop(i, item));\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        writer.write(top.current);\n        try {\n          top.current = streams[top.fd].next();\n        } catch (Throwable t) {\n          verifyChecksum(t, streams[top.fd]);\n        }\n\n        if (top.current != null) {\n          queue.updateTop();\n        } else {\n          queue.pop();\n        }\n      }\n\n      CodecUtil.writeFooter(writer.out);\n\n      for(ByteSequencesReader reader : streams) {\n        CodecUtil.checkFooter(reader.in);\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      IOUtils.close(streams);\n    }\n\n    IOUtils.deleteFiles(trackingDir, segmentsToMerge);\n\n    segmentsToMerge.clear();\n    segments.add(newSegmentName);\n\n    sortInfo.tempMergeFiles++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54ca69905c5d9d1529286f06ab1d12c68f6c13cb","date":1492683554,"type":4,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/util/OfflineSorter#mergePartitions(Directory,List[String]).mjava","sourceNew":null,"sourceOld":"  /** Merge the most recent {@code maxTempFile} partitions into a new partition. */\n  void mergePartitions(Directory trackingDir, List<String> segments) throws IOException {\n    long start = System.currentTimeMillis();\n\n    List<String> segmentsToMerge;\n    if (segments.size() > maxTempFiles) {\n      segmentsToMerge = segments.subList(segments.size() - maxTempFiles, segments.size());\n    } else {\n      segmentsToMerge = segments;\n    }\n\n    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(segmentsToMerge.size()) {\n      @Override\n      protected boolean lessThan(FileAndTop a, FileAndTop b) {\n        return comparator.compare(a.current, b.current) < 0;\n      }\n    };\n\n    ByteSequencesReader[] streams = new ByteSequencesReader[segmentsToMerge.size()];\n\n    String newSegmentName = null;\n\n    try (ByteSequencesWriter writer = getWriter(trackingDir.createTempOutput(tempFileNamePrefix, \"sort\", IOContext.DEFAULT))) {\n\n      newSegmentName = writer.out.getName();\n      \n      // Open streams and read the top for each file\n      for (int i = 0; i < segmentsToMerge.size(); i++) {\n        streams[i] = getReader(dir.openChecksumInput(segmentsToMerge.get(i), IOContext.READONCE), segmentsToMerge.get(i));\n        BytesRef item = null;\n        try {\n          item = streams[i].next();\n        } catch (Throwable t) {\n          verifyChecksum(t, streams[i]);\n        }\n        assert item != null;\n        queue.insertWithOverflow(new FileAndTop(i, item));\n      }\n  \n      // Unix utility sort() uses ordered array of files to pick the next line from, updating\n      // it as it reads new lines. The PQ used here is a more elegant solution and has \n      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway\n      // so it shouldn't make much of a difference (didn't check).\n      FileAndTop top;\n      while ((top = queue.top()) != null) {\n        writer.write(top.current);\n        try {\n          top.current = streams[top.fd].next();\n        } catch (Throwable t) {\n          verifyChecksum(t, streams[top.fd]);\n        }\n\n        if (top.current != null) {\n          queue.updateTop();\n        } else {\n          queue.pop();\n        }\n      }\n\n      CodecUtil.writeFooter(writer.out);\n\n      for(ByteSequencesReader reader : streams) {\n        CodecUtil.checkFooter(reader.in);\n      }\n  \n      sortInfo.mergeTime += System.currentTimeMillis() - start;\n      sortInfo.mergeRounds++;\n    } finally {\n      IOUtils.close(streams);\n    }\n\n    IOUtils.deleteFiles(trackingDir, segmentsToMerge);\n\n    segmentsToMerge.clear();\n    segments.add(newSegmentName);\n\n    sortInfo.tempMergeFiles++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":["7849935cc625c020857f3b29be91b5d4323d19aa"],"7849935cc625c020857f3b29be91b5d4323d19aa":["950b7a6881d14da782b60444c11295e3ec50d41a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"950b7a6881d14da782b60444c11295e3ec50d41a":["2b84d416bbd661ae4b2a28d103bdfccb851e00de"],"e3ce1ef883d26aa73919aa2d53991726e96caa13":["867e3d9153fb761456b54a9dcce566e1545c5ef6"],"2b84d416bbd661ae4b2a28d103bdfccb851e00de":["e3ce1ef883d26aa73919aa2d53991726e96caa13"],"62e7e8f89cb6b0283f3f5d6c0945453b73f09d45":["7849935cc625c020857f3b29be91b5d4323d19aa"],"867e3d9153fb761456b54a9dcce566e1545c5ef6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["62e7e8f89cb6b0283f3f5d6c0945453b73f09d45"]},"commit2Childs":{"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":[],"7849935cc625c020857f3b29be91b5d4323d19aa":["54ca69905c5d9d1529286f06ab1d12c68f6c13cb","62e7e8f89cb6b0283f3f5d6c0945453b73f09d45"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["867e3d9153fb761456b54a9dcce566e1545c5ef6"],"950b7a6881d14da782b60444c11295e3ec50d41a":["7849935cc625c020857f3b29be91b5d4323d19aa"],"e3ce1ef883d26aa73919aa2d53991726e96caa13":["2b84d416bbd661ae4b2a28d103bdfccb851e00de"],"2b84d416bbd661ae4b2a28d103bdfccb851e00de":["950b7a6881d14da782b60444c11295e3ec50d41a"],"867e3d9153fb761456b54a9dcce566e1545c5ef6":["e3ce1ef883d26aa73919aa2d53991726e96caa13"],"62e7e8f89cb6b0283f3f5d6c0945453b73f09d45":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["54ca69905c5d9d1529286f06ab1d12c68f6c13cb","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}