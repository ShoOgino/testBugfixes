{"path":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","sourceNew":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<String,Document>();\n    IndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n                                                                                                  0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()));\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setUseCompoundFile(false);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    _TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","sourceOld":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<String,Document>();\n    IndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n                                                                                                  0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()));\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setUseCompoundFile(false);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    _TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","sourceNew":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<String,Document>();\n    IndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n                                                                                                  0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()));\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setUseCompoundFile(false);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    _TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","sourceOld":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<String,Document>();\n    IndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n                                                                                                  0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()));\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setUseCompoundFile(false);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    _TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"978de4e2d23054c6624dd5928ddeb734dca68eec","date":1370592803,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","sourceNew":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<String,Document>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n            0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setUseCompoundFile(false);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    _TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","sourceOld":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<String,Document>();\n    IndexWriter w = new MockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n                                                                                                  0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()));\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setUseCompoundFile(false);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    _TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"088a7ef694fd43d5d9a4d200c4005865f773d1e7","date":1371136274,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","sourceNew":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<String,Document>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n            0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setNoCFSRatio(0.0);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    _TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","sourceOld":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<String,Document>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n            0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setUseCompoundFile(false);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    _TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","sourceNew":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<String,Document>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n            0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setNoCFSRatio(0.0);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","sourceOld":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<String,Document>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n            0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setNoCFSRatio(0.0);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    _TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","sourceNew":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n            0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setNoCFSRatio(0.0);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","sourceOld":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<String,Document>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n            0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setNoCFSRatio(0.0);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","sourceNew":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n            0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setNoCFSRatio(0.0);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.shutdown();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","sourceOld":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n            0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setNoCFSRatio(0.0);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","sourceNew":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n            .setOpenMode(OpenMode.CREATE)\n            .setRAMBufferSizeMB(0.1)\n            .setMaxBufferedDocs(maxBufferedDocs)\n            .setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setNoCFSRatio(0.0);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.shutdown();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","sourceOld":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.CREATE).setRAMBufferSizeMB(\n            0.1).setMaxBufferedDocs(maxBufferedDocs).setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setNoCFSRatio(0.0);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.shutdown();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","sourceNew":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n            .setOpenMode(OpenMode.CREATE)\n            .setRAMBufferSizeMB(0.1)\n            .setMaxBufferedDocs(maxBufferedDocs)\n            .setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setNoCFSRatio(0.0);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();\n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","sourceOld":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n            .setOpenMode(OpenMode.CREATE)\n            .setRAMBufferSizeMB(0.1)\n            .setMaxBufferedDocs(maxBufferedDocs)\n            .setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setNoCFSRatio(0.0);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.shutdown();    \n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2165505a28a4f836f04ed5eb23bc64a6faae1b1e","date":1418824313,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2#indexRandomIWReader(int,int,int,Directory).mjava","sourceNew":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n            .setOpenMode(OpenMode.CREATE)\n            .setRAMBufferSizeMB(0.1)\n            .setMaxBufferedDocs(maxBufferedDocs)\n            .setMergePolicy(newLogMergePolicy()), random());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setNoCFSRatio(0.0);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();\n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","sourceOld":"  public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {\n    Map<String,Document> docs = new HashMap<>();\n    IndexWriter w = RandomIndexWriter.mockIndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n            .setOpenMode(OpenMode.CREATE)\n            .setRAMBufferSizeMB(0.1)\n            .setMaxBufferedDocs(maxBufferedDocs)\n            .setMergePolicy(newLogMergePolicy()), new YieldTestPoint());\n    w.commit();\n    LogMergePolicy lmp = (LogMergePolicy) w.getConfig().getMergePolicy();\n    lmp.setNoCFSRatio(0.0);\n    lmp.setMergeFactor(mergeFactor);\n    /***\n        w.setMaxMergeDocs(Integer.MAX_VALUE);\n        w.setMaxFieldLength(10000);\n        w.setRAMBufferSizeMB(1);\n        w.setMergeFactor(10);\n    ***/\n\n    threads = new IndexingThread[nThreads];\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = new IndexingThread();\n      th.w = w;\n      th.base = 1000000*i;\n      th.range = range;\n      th.iterations = iterations;\n      threads[i] = th;\n    }\n\n    for (int i=0; i<threads.length; i++) {\n      threads[i].start();\n    }\n    for (int i=0; i<threads.length; i++) {\n      threads[i].join();\n    }\n\n    // w.forceMerge(1);\n    //w.close();\n\n    for (int i=0; i<threads.length; i++) {\n      IndexingThread th = threads[i];\n      synchronized(th) {\n        docs.putAll(th.docs);\n      }\n    }\n\n    TestUtil.checkIndex(dir);\n    DocsAndWriter dw = new DocsAndWriter();\n    dw.docs = docs;\n    dw.writer = w;\n    return dw;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6613659748fe4411a7dcf85266e55db1f95f7315"],"978de4e2d23054c6624dd5928ddeb734dca68eec":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["978de4e2d23054c6624dd5928ddeb734dca68eec"],"6613659748fe4411a7dcf85266e55db1f95f7315":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["2165505a28a4f836f04ed5eb23bc64a6faae1b1e"],"2165505a28a4f836f04ed5eb23bc64a6faae1b1e":["d0ef034a4f10871667ae75181537775ddcf8ade4"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"978de4e2d23054c6624dd5928ddeb734dca68eec":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["6613659748fe4411a7dcf85266e55db1f95f7315"],"6613659748fe4411a7dcf85266e55db1f95f7315":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["2165505a28a4f836f04ed5eb23bc64a6faae1b1e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["978de4e2d23054c6624dd5928ddeb734dca68eec"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"2165505a28a4f836f04ed5eb23bc64a6faae1b1e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}