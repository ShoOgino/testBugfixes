{"path":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrCloudPartitioner#getPartition(Text,SolrInputDocumentWritable,int).mjava","commits":[{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrCloudPartitioner#getPartition(Text,SolrInputDocumentWritable,int).mjava","pathOld":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/SolrCloudPartitioner#getPartition(Text,SolrInputDocumentWritable,int).mjava","sourceNew":"  @Override\n  public int getPartition(Text key, SolrInputDocumentWritable value, int numPartitions) {\n    DocRouter docRouter = docCollection.getRouter();\n    SolrInputDocument doc = value.getSolrInputDocument();\n    String keyStr = key.toString();\n    \n    // TODO: scalability: replace linear search in HashBasedRouter.hashToSlice() with binary search on sorted hash ranges\n    Slice slice = docRouter.getTargetSlice(keyStr, doc, emptySolrParams, docCollection); \n    \n//    LOG.info(\"slice: {}\", slice);\n    if (slice == null) {\n      throw new IllegalStateException(\"No matching slice found! The slice seems unavailable. docRouterClass: \"\n          + docRouter.getClass().getName());\n    }\n    int rootShard = shardNumbers.get(slice.getName());\n    if (rootShard < 0 || rootShard >= shards) {\n      throw new IllegalStateException(\"Illegal shard number \" + rootShard + \" for slice: \" + slice + \", docCollection: \"\n          + docCollection);\n    }      \n\n    // map doc to micro shard aka leaf shard, akin to HashBasedRouter.sliceHash()\n    // taking into account mtree merge algorithm\n    assert numPartitions % shards == 0; // Also note that numPartitions is equal to the number of reducers\n    int hashCode = Hash.murmurhash3_x86_32(keyStr, 0, keyStr.length(), 0); \n    int offset = (hashCode & Integer.MAX_VALUE) % (numPartitions / shards);\n    int microShard = (rootShard * (numPartitions / shards)) + offset;\n//    LOG.info(\"Subpartitions rootShard: {}, offset: {}\", rootShard, offset);\n//    LOG.info(\"Partitioned to p: {} for numPartitions: {}, shards: {}, key: {}, value: {}\", microShard, numPartitions, shards, key, value);\n    \n    assert microShard >= 0 && microShard < numPartitions;\n    return microShard;\n  }\n\n","sourceOld":"  @Override\n  public int getPartition(Text key, SolrInputDocumentWritable value, int numPartitions) {\n    DocRouter docRouter = docCollection.getRouter();\n    SolrInputDocument doc = value.getSolrInputDocument();\n    String keyStr = key.toString();\n    \n    // TODO: scalability: replace linear search in HashBasedRouter.hashToSlice() with binary search on sorted hash ranges\n    Slice slice = docRouter.getTargetSlice(keyStr, doc, emptySolrParams, docCollection); \n    \n//    LOG.info(\"slice: {}\", slice);\n    if (slice == null) {\n      throw new IllegalStateException(\"No matching slice found! The slice seems unavailable. docRouterClass: \"\n          + docRouter.getClass().getName());\n    }\n    int rootShard = shardNumbers.get(slice.getName());\n    if (rootShard < 0 || rootShard >= shards) {\n      throw new IllegalStateException(\"Illegal shard number \" + rootShard + \" for slice: \" + slice + \", docCollection: \"\n          + docCollection);\n    }      \n\n    // map doc to micro shard aka leaf shard, akin to HashBasedRouter.sliceHash()\n    // taking into account mtree merge algorithm\n    assert numPartitions % shards == 0; // Also note that numPartitions is equal to the number of reducers\n    int hashCode = Hash.murmurhash3_x86_32(keyStr, 0, keyStr.length(), 0); \n    int offset = (hashCode & Integer.MAX_VALUE) % (numPartitions / shards);\n    int microShard = (rootShard * (numPartitions / shards)) + offset;\n//    LOG.info(\"Subpartitions rootShard: {}, offset: {}\", rootShard, offset);\n//    LOG.info(\"Partitioned to p: {} for numPartitions: {}, shards: {}, key: {}, value: {}\", microShard, numPartitions, shards, key, value);\n    \n    assert microShard >= 0 && microShard < numPartitions;\n    return microShard;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrCloudPartitioner#getPartition(Text,SolrInputDocumentWritable,int).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public int getPartition(Text key, SolrInputDocumentWritable value, int numPartitions) {\n    DocRouter docRouter = docCollection.getRouter();\n    SolrInputDocument doc = value.getSolrInputDocument();\n    String keyStr = key.toString();\n    \n    // TODO: scalability: replace linear search in HashBasedRouter.hashToSlice() with binary search on sorted hash ranges\n    Slice slice = docRouter.getTargetSlice(keyStr, doc, emptySolrParams, docCollection); \n    \n//    LOG.info(\"slice: {}\", slice);\n    if (slice == null) {\n      throw new IllegalStateException(\"No matching slice found! The slice seems unavailable. docRouterClass: \"\n          + docRouter.getClass().getName());\n    }\n    int rootShard = shardNumbers.get(slice.getName());\n    if (rootShard < 0 || rootShard >= shards) {\n      throw new IllegalStateException(\"Illegal shard number \" + rootShard + \" for slice: \" + slice + \", docCollection: \"\n          + docCollection);\n    }      \n\n    // map doc to micro shard aka leaf shard, akin to HashBasedRouter.sliceHash()\n    // taking into account mtree merge algorithm\n    assert numPartitions % shards == 0; // Also note that numPartitions is equal to the number of reducers\n    int hashCode = Hash.murmurhash3_x86_32(keyStr, 0, keyStr.length(), 0); \n    int offset = (hashCode & Integer.MAX_VALUE) % (numPartitions / shards);\n    int microShard = (rootShard * (numPartitions / shards)) + offset;\n//    LOG.info(\"Subpartitions rootShard: {}, offset: {}\", rootShard, offset);\n//    LOG.info(\"Partitioned to p: {} for numPartitions: {}, shards: {}, key: {}, value: {}\", microShard, numPartitions, shards, key, value);\n    \n    assert microShard >= 0 && microShard < numPartitions;\n    return microShard;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"747dd71fefcbc7142661c25334b74c518fef4d81","date":1423504825,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrCloudPartitioner#getPartition(Text,SolrInputDocumentWritable,int).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrCloudPartitioner#getPartition(Text,SolrInputDocumentWritable,int).mjava","sourceNew":"  @Override\n  public int getPartition(Text key, SolrInputDocumentWritable value, int numPartitions) {\n    DocRouter docRouter = docCollection.getRouter();\n    SolrInputDocument doc = value.getSolrInputDocument();\n    String keyStr = key.toString();\n    \n    // TODO: scalability: replace linear search in HashBasedRouter.hashToSlice() with binary search on sorted hash ranges\n    Slice slice = docRouter.getTargetSlice(keyStr, doc, null, emptySolrParams, docCollection); \n    \n//    LOG.info(\"slice: {}\", slice);\n    if (slice == null) {\n      throw new IllegalStateException(\"No matching slice found! The slice seems unavailable. docRouterClass: \"\n          + docRouter.getClass().getName());\n    }\n    int rootShard = shardNumbers.get(slice.getName());\n    if (rootShard < 0 || rootShard >= shards) {\n      throw new IllegalStateException(\"Illegal shard number \" + rootShard + \" for slice: \" + slice + \", docCollection: \"\n          + docCollection);\n    }      \n\n    // map doc to micro shard aka leaf shard, akin to HashBasedRouter.sliceHash()\n    // taking into account mtree merge algorithm\n    assert numPartitions % shards == 0; // Also note that numPartitions is equal to the number of reducers\n    int hashCode = Hash.murmurhash3_x86_32(keyStr, 0, keyStr.length(), 0); \n    int offset = (hashCode & Integer.MAX_VALUE) % (numPartitions / shards);\n    int microShard = (rootShard * (numPartitions / shards)) + offset;\n//    LOG.info(\"Subpartitions rootShard: {}, offset: {}\", rootShard, offset);\n//    LOG.info(\"Partitioned to p: {} for numPartitions: {}, shards: {}, key: {}, value: {}\", microShard, numPartitions, shards, key, value);\n    \n    assert microShard >= 0 && microShard < numPartitions;\n    return microShard;\n  }\n\n","sourceOld":"  @Override\n  public int getPartition(Text key, SolrInputDocumentWritable value, int numPartitions) {\n    DocRouter docRouter = docCollection.getRouter();\n    SolrInputDocument doc = value.getSolrInputDocument();\n    String keyStr = key.toString();\n    \n    // TODO: scalability: replace linear search in HashBasedRouter.hashToSlice() with binary search on sorted hash ranges\n    Slice slice = docRouter.getTargetSlice(keyStr, doc, emptySolrParams, docCollection); \n    \n//    LOG.info(\"slice: {}\", slice);\n    if (slice == null) {\n      throw new IllegalStateException(\"No matching slice found! The slice seems unavailable. docRouterClass: \"\n          + docRouter.getClass().getName());\n    }\n    int rootShard = shardNumbers.get(slice.getName());\n    if (rootShard < 0 || rootShard >= shards) {\n      throw new IllegalStateException(\"Illegal shard number \" + rootShard + \" for slice: \" + slice + \", docCollection: \"\n          + docCollection);\n    }      \n\n    // map doc to micro shard aka leaf shard, akin to HashBasedRouter.sliceHash()\n    // taking into account mtree merge algorithm\n    assert numPartitions % shards == 0; // Also note that numPartitions is equal to the number of reducers\n    int hashCode = Hash.murmurhash3_x86_32(keyStr, 0, keyStr.length(), 0); \n    int offset = (hashCode & Integer.MAX_VALUE) % (numPartitions / shards);\n    int microShard = (rootShard * (numPartitions / shards)) + offset;\n//    LOG.info(\"Subpartitions rootShard: {}, offset: {}\", rootShard, offset);\n//    LOG.info(\"Partitioned to p: {} for numPartitions: {}, shards: {}, key: {}, value: {}\", microShard, numPartitions, shards, key, value);\n    \n    assert microShard >= 0 && microShard < numPartitions;\n    return microShard;\n  }\n\n","bugFix":["d6e604e9030fb0cabf0c5a85ae6039921a81419c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"12109b652e9210b8d58fca47f6c4a725d058a58e","date":1490373076,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrCloudPartitioner#getPartition(Text,SolrInputDocumentWritable,int).mjava","sourceNew":null,"sourceOld":"  @Override\n  public int getPartition(Text key, SolrInputDocumentWritable value, int numPartitions) {\n    DocRouter docRouter = docCollection.getRouter();\n    SolrInputDocument doc = value.getSolrInputDocument();\n    String keyStr = key.toString();\n    \n    // TODO: scalability: replace linear search in HashBasedRouter.hashToSlice() with binary search on sorted hash ranges\n    Slice slice = docRouter.getTargetSlice(keyStr, doc, null, emptySolrParams, docCollection); \n    \n//    LOG.info(\"slice: {}\", slice);\n    if (slice == null) {\n      throw new IllegalStateException(\"No matching slice found! The slice seems unavailable. docRouterClass: \"\n          + docRouter.getClass().getName());\n    }\n    int rootShard = shardNumbers.get(slice.getName());\n    if (rootShard < 0 || rootShard >= shards) {\n      throw new IllegalStateException(\"Illegal shard number \" + rootShard + \" for slice: \" + slice + \", docCollection: \"\n          + docCollection);\n    }      \n\n    // map doc to micro shard aka leaf shard, akin to HashBasedRouter.sliceHash()\n    // taking into account mtree merge algorithm\n    assert numPartitions % shards == 0; // Also note that numPartitions is equal to the number of reducers\n    int hashCode = Hash.murmurhash3_x86_32(keyStr, 0, keyStr.length(), 0); \n    int offset = (hashCode & Integer.MAX_VALUE) % (numPartitions / shards);\n    int microShard = (rootShard * (numPartitions / shards)) + offset;\n//    LOG.info(\"Subpartitions rootShard: {}, offset: {}\", rootShard, offset);\n//    LOG.info(\"Partitioned to p: {} for numPartitions: {}, shards: {}, key: {}, value: {}\", microShard, numPartitions, shards, key, value);\n    \n    assert microShard >= 0 && microShard < numPartitions;\n    return microShard;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe1c4aa9af769a38e878f608070f672efbeac27f","date":1490594650,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrCloudPartitioner#getPartition(Text,SolrInputDocumentWritable,int).mjava","sourceNew":null,"sourceOld":"  @Override\n  public int getPartition(Text key, SolrInputDocumentWritable value, int numPartitions) {\n    DocRouter docRouter = docCollection.getRouter();\n    SolrInputDocument doc = value.getSolrInputDocument();\n    String keyStr = key.toString();\n    \n    // TODO: scalability: replace linear search in HashBasedRouter.hashToSlice() with binary search on sorted hash ranges\n    Slice slice = docRouter.getTargetSlice(keyStr, doc, null, emptySolrParams, docCollection); \n    \n//    LOG.info(\"slice: {}\", slice);\n    if (slice == null) {\n      throw new IllegalStateException(\"No matching slice found! The slice seems unavailable. docRouterClass: \"\n          + docRouter.getClass().getName());\n    }\n    int rootShard = shardNumbers.get(slice.getName());\n    if (rootShard < 0 || rootShard >= shards) {\n      throw new IllegalStateException(\"Illegal shard number \" + rootShard + \" for slice: \" + slice + \", docCollection: \"\n          + docCollection);\n    }      \n\n    // map doc to micro shard aka leaf shard, akin to HashBasedRouter.sliceHash()\n    // taking into account mtree merge algorithm\n    assert numPartitions % shards == 0; // Also note that numPartitions is equal to the number of reducers\n    int hashCode = Hash.murmurhash3_x86_32(keyStr, 0, keyStr.length(), 0); \n    int offset = (hashCode & Integer.MAX_VALUE) % (numPartitions / shards);\n    int microShard = (rootShard * (numPartitions / shards)) + offset;\n//    LOG.info(\"Subpartitions rootShard: {}, offset: {}\", rootShard, offset);\n//    LOG.info(\"Partitioned to p: {} for numPartitions: {}, shards: {}, key: {}, value: {}\", microShard, numPartitions, shards, key, value);\n    \n    assert microShard >= 0 && microShard < numPartitions;\n    return microShard;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["747dd71fefcbc7142661c25334b74c518fef4d81"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"747dd71fefcbc7142661c25334b74c518fef4d81":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"fe1c4aa9af769a38e878f608070f672efbeac27f":["747dd71fefcbc7142661c25334b74c518fef4d81"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["12109b652e9210b8d58fca47f6c4a725d058a58e"]},"commit2Childs":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","747dd71fefcbc7142661c25334b74c518fef4d81"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["70f91c8322fbffe3a3a897ef20ea19119cac10cd","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"747dd71fefcbc7142661c25334b74c518fef4d81":["12109b652e9210b8d58fca47f6c4a725d058a58e","fe1c4aa9af769a38e878f608070f672efbeac27f"],"fe1c4aa9af769a38e878f608070f672efbeac27f":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","fe1c4aa9af769a38e878f608070f672efbeac27f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}