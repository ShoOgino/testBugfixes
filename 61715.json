{"path":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","pathOld":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","sourceNew":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    final String[] files = benchmark.getRunData().getDirectory().listAll();\n    int cfsCount = 0;\n    for(int i=0;i<files.length;i++)\n      if (files[i].endsWith(\".cfs\"))\n        cfsCount++;\n    assertEquals(3, cfsCount);\n  }\n\n","sourceOld":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    final String[] files = benchmark.getRunData().getDirectory().listAll();\n    int cfsCount = 0;\n    for(int i=0;i<files.length;i++)\n      if (files[i].endsWith(\".cfs\"))\n        cfsCount++;\n    assertEquals(3, cfsCount);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56","date":1290598569,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","pathOld":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","sourceNew":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    SegmentInfos infos = new SegmentInfos();\n    infos.read(benchmark.getRunData().getDirectory());\n    assertEquals(3, infos.size());\n  }\n\n","sourceOld":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    final String[] files = benchmark.getRunData().getDirectory().listAll();\n    int cfsCount = 0;\n    for(int i=0;i<files.length;i++)\n      if (files[i].endsWith(\".cfs\"))\n        cfsCount++;\n    assertEquals(3, cfsCount);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","pathOld":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","sourceNew":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    SegmentInfos infos = new SegmentInfos();\n    infos.read(benchmark.getRunData().getDirectory());\n    assertEquals(3, infos.size());\n  }\n\n","sourceOld":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    final String[] files = benchmark.getRunData().getDirectory().listAll();\n    int cfsCount = 0;\n    for(int i=0;i<files.length;i++)\n      if (files[i].endsWith(\".cfs\"))\n        cfsCount++;\n    assertEquals(3, cfsCount);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","pathOld":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","sourceNew":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    SegmentInfos infos = new SegmentInfos();\n    infos.read(benchmark.getRunData().getDirectory());\n    assertEquals(3, infos.size());\n  }\n\n","sourceOld":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    final String[] files = benchmark.getRunData().getDirectory().listAll();\n    int cfsCount = 0;\n    for(int i=0;i<files.length;i++)\n      if (files[i].endsWith(\".cfs\"))\n        cfsCount++;\n    assertEquals(3, cfsCount);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ecc11368dc265bfdad90214f8bf5da99016ab1e2","date":1294144090,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","pathOld":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","sourceNew":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    SegmentInfos infos = new SegmentInfos();\n    infos.read(benchmark.getRunData().getDirectory());\n    assertEquals(3, infos.size());\n  }\n\n","sourceOld":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    SegmentInfos infos = new SegmentInfos();\n    infos.read(benchmark.getRunData().getDirectory());\n    assertEquals(3, infos.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":5,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","pathOld":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","sourceNew":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    SegmentInfos infos = new SegmentInfos();\n    infos.read(benchmark.getRunData().getDirectory());\n    assertEquals(3, infos.size());\n  }\n\n","sourceOld":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    SegmentInfos infos = new SegmentInfos();\n    infos.read(benchmark.getRunData().getDirectory());\n    assertEquals(3, infos.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":5,"author":"Michael Busch","isMerge":true,"pathNew":"modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","pathOld":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testOptimizeMaxNumSegments().mjava","sourceNew":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    SegmentInfos infos = new SegmentInfos();\n    infos.read(benchmark.getRunData().getDirectory());\n    assertEquals(3, infos.size());\n  }\n\n","sourceOld":"  /**\n   * Test that we can call optimize(maxNumSegments).\n   */\n  public void testOptimizeMaxNumSegments() throws Exception {\n    // 1. alg definition (required in every \"logic\" test)\n    String algLines[] = {\n        \"# ----- properties \",\n        \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n        \"docs.file=\" + getReuters20LinesFile(),\n        \"content.source.log.step=3\",\n        \"ram.flush.mb=-1\",\n        \"max.buffered=3\",\n        \"doc.term.vector=false\",\n        \"content.source.forever=false\",\n        \"directory=RAMDirectory\",\n        \"merge.policy=org.apache.lucene.index.LogDocMergePolicy\",\n        \"doc.stored=false\",\n        \"doc.tokenized=false\",\n        \"debug.level=1\",\n        \"# ----- alg \",\n        \"{ \\\"Rounds\\\"\",\n        \"  ResetSystemErase\",\n        \"  CreateIndex\",\n        \"  { \\\"AddDocs\\\"  AddDoc > : * \",\n        \"  Optimize(3)\",\n        \"  CloseIndex()\",\n        \"} : 2\",\n    };\n    \n    // 2. execute the algorithm  (required in every \"logic\" test)\n    Benchmark benchmark = execBenchmark(algLines);\n\n    // 3. test number of docs in the index\n    IndexReader ir = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    int ndocsExpected = 20; // first 20 reuters docs.\n    assertEquals(\"wrong number of docs in the index!\", ndocsExpected, ir.numDocs());\n    ir.close();\n\n    // Make sure we have 3 segments:\n    SegmentInfos infos = new SegmentInfos();\n    infos.read(benchmark.getRunData().getDirectory());\n    assertEquals(3, infos.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70ad682703b8585f5d0a637efec044d57ec05efb":["3bb13258feba31ab676502787ab2e1779f129b7a","ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"ecc11368dc265bfdad90214f8bf5da99016ab1e2":["5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["9454a6510e2db155fb01faa5c049b06ece95fab9","5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"3bb13258feba31ab676502787ab2e1779f129b7a":["9454a6510e2db155fb01faa5c049b06ece95fab9","5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"70ad682703b8585f5d0a637efec044d57ec05efb":[],"ecc11368dc265bfdad90214f8bf5da99016ab1e2":["70ad682703b8585f5d0a637efec044d57ec05efb","868da859b43505d9d2a023bfeae6dd0c795f5295","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56":["ecc11368dc265bfdad90214f8bf5da99016ab1e2","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","3bb13258feba31ab676502787ab2e1779f129b7a"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"868da859b43505d9d2a023bfeae6dd0c795f5295":[],"3bb13258feba31ab676502787ab2e1779f129b7a":["70ad682703b8585f5d0a637efec044d57ec05efb"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["5390d5f5bc8bf5d65eff4c1d596cf9547ead0c56","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","3bb13258feba31ab676502787ab2e1779f129b7a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["70ad682703b8585f5d0a637efec044d57ec05efb","868da859b43505d9d2a023bfeae6dd0c795f5295","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}