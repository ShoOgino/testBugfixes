{"path":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","commits":[{"id":"79759974460bc59933cd169acc94f5c6b16368d5","date":1471318443,"type":1,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorUIF,CountSlotAcc).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":"  private void getCounts(FacetFieldProcessorUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":1,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorUIF,CountSlotAcc).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":"  private void getCounts(FacetFieldProcessorUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorUIF,CountSlotAcc).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":"  private void getCounts(FacetFieldProcessorUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","pathOld":"/dev/null","sourceNew":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ac981db60ef979233b3438ec49ddae82e8cc4697","date":1503407558,"type":3,"author":"Toke Eskildsen","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a7809d1d753b67f48b1a706e17034bf8b624ea3","date":1504366927,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c841e27891873cab110ebeb89f124a8ec470176","date":1586527220,"type":3,"author":"Munendra S N","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - (int) counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","bugFix":["48a04370d92de1fba80afce42dd014d5a1e3aa52"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a56a9893014b284af4d1af451e6c02e7ffdf5b6e","date":1590065972,"type":5,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,SlotAcc.CountSlotAcc).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, SlotAcc.CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - (int) counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0x80000000)!=0) {\n          int pos = code & 0x7fffffff;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - (int) counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1c841e27891873cab110ebeb89f124a8ec470176":["ac981db60ef979233b3438ec49ddae82e8cc4697"],"a56a9893014b284af4d1af451e6c02e7ffdf5b6e":["1c841e27891873cab110ebeb89f124a8ec470176"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"3a7809d1d753b67f48b1a706e17034bf8b624ea3":["403d05f7f8d69b65659157eff1bc1d2717f04c66","ac981db60ef979233b3438ec49ddae82e8cc4697"],"ac981db60ef979233b3438ec49ddae82e8cc4697":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","79759974460bc59933cd169acc94f5c6b16368d5"],"79759974460bc59933cd169acc94f5c6b16368d5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","403d05f7f8d69b65659157eff1bc1d2717f04c66"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a56a9893014b284af4d1af451e6c02e7ffdf5b6e"]},"commit2Childs":{"1c841e27891873cab110ebeb89f124a8ec470176":["a56a9893014b284af4d1af451e6c02e7ffdf5b6e"],"a56a9893014b284af4d1af451e6c02e7ffdf5b6e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["3a7809d1d753b67f48b1a706e17034bf8b624ea3","ac981db60ef979233b3438ec49ddae82e8cc4697","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"3a7809d1d753b67f48b1a706e17034bf8b624ea3":[],"ac981db60ef979233b3438ec49ddae82e8cc4697":["1c841e27891873cab110ebeb89f124a8ec470176","3a7809d1d753b67f48b1a706e17034bf8b624ea3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["403d05f7f8d69b65659157eff1bc1d2717f04c66","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","79759974460bc59933cd169acc94f5c6b16368d5","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"79759974460bc59933cd169acc94f5c6b16368d5":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3a7809d1d753b67f48b1a706e17034bf8b624ea3","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}