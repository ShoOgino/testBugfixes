{"path":"lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator#accumulate(ScoredDocIDs).mjava","commits":[{"id":"d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6","date":1375108983,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          isUsingComplements = false;\n        } catch (IOException e) {\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b1c67b5eba853532b31132bf5aef70a3b2be63f","date":1375351298,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          isUsingComplements = false;\n        } catch (IOException e) {\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          isUsingComplements = false;\n        } catch (IOException e) {\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":1,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          isUsingComplements = false;\n        } catch (IOException e) {\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, \n              searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d33e19a97046248623a7591aeaa6547233fd15e2","date":1385424777,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":null,"sourceOld":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          isUsingComplements = false;\n        } catch (IOException e) {\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc728b07df73b197e6d940d27f9b08b63918f13","date":1388834348,"type":4,"author":"Michael McCandless","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":null,"sourceOld":"  // TODO: this should be removed once we clean the API\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, searchParams.indexingParams);\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          isUsingComplements = false;\n        } catch (IOException e) {\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          throw new IOException(\"PANIC: Got unexpected exception while trying to get/calculate total counts\", e);\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where the request maintains the merged result.\n          // In this implementation merges happen after each partition,\n          // but other impl could merge only at the end.\n          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();\n          for (FacetRequest fr : searchParams.facetRequests) {\n            // Handle and merge only facet requests which were not already handled.  \n            if (handledRequests.add(fr)) {\n              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));\n              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);\n              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n              if (oldRes != null) {\n                res4fr = frHndlr.mergeResults(oldRes, res4fr);\n              }\n              fr2tmpRes.put(fr, res4fr);\n            } \n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.facetRequests) {\n        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);\n        if (tmpResult == null) {\n          // Add empty FacetResult:\n          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));\n          continue;\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);\n        // final labeling if allowed (because labeling is a costly operation)\n        frHndlr.labelResult(facetRes);\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3b1c67b5eba853532b31132bf5aef70a3b2be63f":["d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6"],"d33e19a97046248623a7591aeaa6547233fd15e2":["3b1c67b5eba853532b31132bf5aef70a3b2be63f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3cc728b07df73b197e6d940d27f9b08b63918f13":["3b1c67b5eba853532b31132bf5aef70a3b2be63f","d33e19a97046248623a7591aeaa6547233fd15e2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cc728b07df73b197e6d940d27f9b08b63918f13"]},"commit2Childs":{"d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6":["3b1c67b5eba853532b31132bf5aef70a3b2be63f"],"3b1c67b5eba853532b31132bf5aef70a3b2be63f":["d33e19a97046248623a7591aeaa6547233fd15e2","3cc728b07df73b197e6d940d27f9b08b63918f13"],"d33e19a97046248623a7591aeaa6547233fd15e2":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"3cc728b07df73b197e6d940d27f9b08b63918f13":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}