{"path":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","commits":[{"id":"d6e604e9030fb0cabf0c5a85ae6039921a81419c","date":1386009743,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Merging into dstDir: \" + workDir + \", srcDirs: {}\", shards);\n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        IndexWriterConfig writerConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, null)\n            .setOpenMode(OpenMode.CREATE)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.currentTimeMillis();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.currentTimeMillis();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.currentTimeMillis();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }    \n\n","sourceOld":null,"bugFix":null,"bugIntro":["13734b36bfd631ed6a46b961df376f679e8a3f57"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Merging into dstDir: \" + workDir + \", srcDirs: {}\", shards);\n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        IndexWriterConfig writerConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, null)\n            .setOpenMode(OpenMode.CREATE)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.currentTimeMillis();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.currentTimeMillis();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.currentTimeMillis();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }    \n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Merging into dstDir: \" + workDir + \", srcDirs: {}\", shards);\n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        IndexWriterConfig writerConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, null)\n            .setOpenMode(OpenMode.CREATE)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.currentTimeMillis();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.currentTimeMillis();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.currentTimeMillis();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["d6e604e9030fb0cabf0c5a85ae6039921a81419c"],"d6e604e9030fb0cabf0c5a85ae6039921a81419c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"]},"commit2Childs":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d6e604e9030fb0cabf0c5a85ae6039921a81419c":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d6e604e9030fb0cabf0c5a85ae6039921a81419c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}