{"path":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(DocListAndSet,Query,List[Query],DocSet,Sort,int,int,int).mjava","commits":[{"id":"0c3e228bf650e96f3002a8fb73dd0c13d55af077","date":1138253849,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(DocListAndSet,Query,List[Query],DocSet,Sort,int,int,int).mjava","pathOld":"/dev/null","sourceNew":"  protected void getDocListC(DocListAndSet out, Query query, List<Query> filterList, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    QueryResultKey key=null;\n    int maxDoc = offset + len;\n    int supersetMaxDoc=maxDoc;\n    DocList superset;\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && filter==null) {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(query, filterList, lsort, flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = (DocList)queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              out.docList = superset.subset(offset,len);\n            }\n          }\n          if (out.docList != null) return;\n        }\n\n        // If we are going to generate the result, bump up to the\n        // next resultWindowSize for better caching.\n\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDoc < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDoc-1)/queryResultWindowSize + 1)*queryResultWindowSize;\n        }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && lsort != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = lsort.getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(query,filter);\n        DocSet bigFilt = getDocSet(filterList);\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,lsort,supersetMaxDoc);\n      out.docList = superset.subset(offset,len);\n    } else {\n      // do it the normal way...\n      DocSet theFilt = filter!=null ? filter : getDocSet(filterList);\n      superset = getDocListNC(query,theFilt,lsort,0,supersetMaxDoc,flags);\n      // OPT... if getDocListNC can get the set at the same time (later version)\n      // then set it as out.docSet.\n      out.docList = superset.subset(offset,len);\n    }\n\n    // lastly, put the superset in the cache\n    if (key != null) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["30c238dad8c4234f556cd28cd22ff426247e70c4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"18d06c030cb0a920c116ec6e16933a4590a70bb9","date":1144087994,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(DocListAndSet,Query,List[Query],DocSet,Sort,int,int,int).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(DocListAndSet,Query,List[Query],DocSet,Sort,int,int,int).mjava","sourceNew":"  private void getDocListC(DocListAndSet out, Query query, List<Query> filterList, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    QueryResultKey key=null;\n    int maxDoc = offset + len;\n    int supersetMaxDoc=maxDoc;\n    DocList superset;\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && filter==null) {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(query, filterList, lsort, flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = (DocList)queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              out.docList = superset.subset(offset,len);\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (filterList==null) {\n                out.docSet = getDocSet(query);\n              } else {\n                List<Query> newList = new ArrayList<Query>(filterList.size()+1);\n                newList.add(query);\n                newList.addAll(filterList);\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n        // If we are going to generate the result, bump up to the\n        // next resultWindowSize for better caching.\n\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDoc < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDoc-1)/queryResultWindowSize + 1)*queryResultWindowSize;\n        }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && lsort != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = lsort.getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(query,filter);\n        DocSet bigFilt = getDocSet(filterList);\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,lsort,supersetMaxDoc);\n      out.docList = superset.subset(offset,len);\n    } else {\n      // do it the normal way...\n      DocSet theFilt = filter!=null ? filter : getDocSet(filterList);\n\n      if ((flags & GET_DOCSET)!=0) {\n        DocSet qDocSet = getDocListAndSetNC(out,query,theFilt,lsort,0,supersetMaxDoc,flags);\n        // cache the docSet matching the query w/o filtering\n        if (filterCache!=null) filterCache.put(query,qDocSet);\n      } else {\n        out.docList = getDocListNC(query,theFilt,lsort,0,supersetMaxDoc,flags);\n      }\n      superset = out.docList;\n      out.docList = superset.subset(offset,len);\n    }\n\n    // lastly, put the superset in the cache\n    if (key != null) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  protected void getDocListC(DocListAndSet out, Query query, List<Query> filterList, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    QueryResultKey key=null;\n    int maxDoc = offset + len;\n    int supersetMaxDoc=maxDoc;\n    DocList superset;\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && filter==null) {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(query, filterList, lsort, flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = (DocList)queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              out.docList = superset.subset(offset,len);\n            }\n          }\n          if (out.docList != null) return;\n        }\n\n        // If we are going to generate the result, bump up to the\n        // next resultWindowSize for better caching.\n\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDoc < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDoc-1)/queryResultWindowSize + 1)*queryResultWindowSize;\n        }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && lsort != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = lsort.getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(query,filter);\n        DocSet bigFilt = getDocSet(filterList);\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,lsort,supersetMaxDoc);\n      out.docList = superset.subset(offset,len);\n    } else {\n      // do it the normal way...\n      DocSet theFilt = filter!=null ? filter : getDocSet(filterList);\n      superset = getDocListNC(query,theFilt,lsort,0,supersetMaxDoc,flags);\n      // OPT... if getDocListNC can get the set at the same time (later version)\n      // then set it as out.docSet.\n      out.docList = superset.subset(offset,len);\n    }\n\n    // lastly, put the superset in the cache\n    if (key != null) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"660013db716c87722fe5d4684d75e893596f49d0","date":1183995930,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(DocListAndSet,Query,List[Query],DocSet,Sort,int,int,int).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(DocListAndSet,Query,List[Query],DocSet,Sort,int,int,int).mjava","sourceNew":"  private void getDocListC(DocListAndSet out, Query query, List<Query> filterList, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    QueryResultKey key=null;\n    int maxDoc = offset + len;\n    int supersetMaxDoc=maxDoc;\n    DocList superset;\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && filter==null) {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(query, filterList, lsort, flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = (DocList)queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              out.docList = superset.subset(offset,len);\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (filterList==null) {\n                out.docSet = getDocSet(query);\n              } else {\n                List<Query> newList = new ArrayList<Query>(filterList.size()+1);\n                newList.add(query);\n                newList.addAll(filterList);\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n        // If we are going to generate the result, bump up to the\n        // next resultWindowSize for better caching.\n\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDoc < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDoc-1)/queryResultWindowSize + 1)*queryResultWindowSize;\n        }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && lsort != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = lsort.getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(query,filter);\n        DocSet bigFilt = getDocSet(filterList);\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,lsort,supersetMaxDoc);\n      out.docList = superset.subset(offset,len);\n    } else {\n      // do it the normal way...\n      DocSet theFilt = filter!=null ? filter : getDocSet(filterList);\n\n      if ((flags & GET_DOCSET)!=0) {\n        DocSet qDocSet = getDocListAndSetNC(out,query,theFilt,lsort,0,supersetMaxDoc,flags);\n        // cache the docSet matching the query w/o filtering\n        if (filterCache!=null) filterCache.put(query,qDocSet);\n      } else {\n        out.docList = getDocListNC(query,theFilt,lsort,0,supersetMaxDoc,flags);\n      }\n      superset = out.docList;\n      out.docList = superset.subset(offset,len);\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  private void getDocListC(DocListAndSet out, Query query, List<Query> filterList, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    QueryResultKey key=null;\n    int maxDoc = offset + len;\n    int supersetMaxDoc=maxDoc;\n    DocList superset;\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && filter==null) {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(query, filterList, lsort, flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = (DocList)queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              out.docList = superset.subset(offset,len);\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (filterList==null) {\n                out.docSet = getDocSet(query);\n              } else {\n                List<Query> newList = new ArrayList<Query>(filterList.size()+1);\n                newList.add(query);\n                newList.addAll(filterList);\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n        // If we are going to generate the result, bump up to the\n        // next resultWindowSize for better caching.\n\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDoc < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDoc-1)/queryResultWindowSize + 1)*queryResultWindowSize;\n        }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && lsort != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = lsort.getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(query,filter);\n        DocSet bigFilt = getDocSet(filterList);\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,lsort,supersetMaxDoc);\n      out.docList = superset.subset(offset,len);\n    } else {\n      // do it the normal way...\n      DocSet theFilt = filter!=null ? filter : getDocSet(filterList);\n\n      if ((flags & GET_DOCSET)!=0) {\n        DocSet qDocSet = getDocListAndSetNC(out,query,theFilt,lsort,0,supersetMaxDoc,flags);\n        // cache the docSet matching the query w/o filtering\n        if (filterCache!=null) filterCache.put(query,qDocSet);\n      } else {\n        out.docList = getDocListNC(query,theFilt,lsort,0,supersetMaxDoc,flags);\n      }\n      superset = out.docList;\n      out.docList = superset.subset(offset,len);\n    }\n\n    // lastly, put the superset in the cache\n    if (key != null) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5df952ccf58ae0134bf9774c7b0d769f1645f317","date":1191994194,"type":3,"author":"Mike Klaas","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(DocListAndSet,Query,List[Query],DocSet,Sort,int,int,int).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(DocListAndSet,Query,List[Query],DocSet,Sort,int,int,int).mjava","sourceNew":"  private void getDocListC(DocListAndSet out, Query query, List<Query> filterList, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    QueryResultKey key=null;\n    int maxDoc = offset + len;\n    int supersetMaxDoc=maxDoc;\n    DocList superset;\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && filter==null) {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(query, filterList, lsort, flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = (DocList)queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(offset,len);\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (filterList==null) {\n                out.docSet = getDocSet(query);\n              } else {\n                List<Query> newList = new ArrayList<Query>(filterList.size()+1);\n                newList.add(query);\n                newList.addAll(filterList);\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n        // If we are going to generate the result, bump up to the\n        // next resultWindowSize for better caching.\n\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDoc < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDoc-1)/queryResultWindowSize + 1)*queryResultWindowSize;\n        }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && lsort != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = lsort.getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(query,filter);\n        DocSet bigFilt = getDocSet(filterList);\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,lsort,supersetMaxDoc);\n      out.docList = superset.subset(offset,len);\n    } else {\n      // do it the normal way...\n      DocSet theFilt = filter!=null ? filter : getDocSet(filterList);\n\n      if ((flags & GET_DOCSET)!=0) {\n        DocSet qDocSet = getDocListAndSetNC(out,query,theFilt,lsort,0,supersetMaxDoc,flags);\n        // cache the docSet matching the query w/o filtering\n        if (filterCache!=null) filterCache.put(query,qDocSet);\n      } else {\n        out.docList = getDocListNC(query,theFilt,lsort,0,supersetMaxDoc,flags);\n      }\n      superset = out.docList;\n      out.docList = superset.subset(offset,len);\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  private void getDocListC(DocListAndSet out, Query query, List<Query> filterList, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    QueryResultKey key=null;\n    int maxDoc = offset + len;\n    int supersetMaxDoc=maxDoc;\n    DocList superset;\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && filter==null) {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(query, filterList, lsort, flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = (DocList)queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              out.docList = superset.subset(offset,len);\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (filterList==null) {\n                out.docSet = getDocSet(query);\n              } else {\n                List<Query> newList = new ArrayList<Query>(filterList.size()+1);\n                newList.add(query);\n                newList.addAll(filterList);\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n        // If we are going to generate the result, bump up to the\n        // next resultWindowSize for better caching.\n\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDoc < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDoc-1)/queryResultWindowSize + 1)*queryResultWindowSize;\n        }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && lsort != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = lsort.getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(query,filter);\n        DocSet bigFilt = getDocSet(filterList);\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,lsort,supersetMaxDoc);\n      out.docList = superset.subset(offset,len);\n    } else {\n      // do it the normal way...\n      DocSet theFilt = filter!=null ? filter : getDocSet(filterList);\n\n      if ((flags & GET_DOCSET)!=0) {\n        DocSet qDocSet = getDocListAndSetNC(out,query,theFilt,lsort,0,supersetMaxDoc,flags);\n        // cache the docSet matching the query w/o filtering\n        if (filterCache!=null) filterCache.put(query,qDocSet);\n      } else {\n        out.docList = getDocListNC(query,theFilt,lsort,0,supersetMaxDoc,flags);\n      }\n      superset = out.docList;\n      out.docList = superset.subset(offset,len);\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba4ca82eb5aaa1d2278e921a71257bc9eff71ecc","date":1193326796,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(DocListAndSet,Query,List[Query],DocSet,Sort,int,int,int).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(DocListAndSet,Query,List[Query],DocSet,Sort,int,int,int).mjava","sourceNew":"  /** getDocList version that uses+populates query and filter caches.\n   * This should only be called using either filterList or filter, but not both.\n   */\n  private void getDocListC(DocListAndSet out, Query query, List<Query> filterList, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    QueryResultKey key=null;\n    int maxDoc = offset + len;\n    int supersetMaxDoc=maxDoc;\n    DocList superset;\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && filter==null) {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(query, filterList, lsort, flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = (DocList)queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(offset,len);\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (filterList==null) {\n                out.docSet = getDocSet(query);\n              } else {\n                List<Query> newList = new ArrayList<Query>(filterList.size()+1);\n                newList.add(query);\n                newList.addAll(filterList);\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n        // If we are going to generate the result, bump up to the\n        // next resultWindowSize for better caching.\n\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDoc < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDoc-1)/queryResultWindowSize + 1)*queryResultWindowSize;\n        }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && lsort != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = lsort.getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(query,filter);\n        DocSet bigFilt = getDocSet(filterList);\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,lsort,supersetMaxDoc);\n      out.docList = superset.subset(offset,len);\n    } else {\n      // do it the normal way...\n      DocSet theFilt = filter!=null ? filter : getDocSet(filterList);\n\n      if ((flags & GET_DOCSET)!=0) {\n        DocSet qDocSet = getDocListAndSetNC(out,query,theFilt,lsort,0,supersetMaxDoc,flags);\n        // cache the docSet matching the query w/o filtering\n        if (filterCache!=null) filterCache.put(query,qDocSet);\n      } else {\n        out.docList = getDocListNC(query,theFilt,lsort,0,supersetMaxDoc,flags);\n      }\n      superset = out.docList;\n      out.docList = superset.subset(offset,len);\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  private void getDocListC(DocListAndSet out, Query query, List<Query> filterList, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    QueryResultKey key=null;\n    int maxDoc = offset + len;\n    int supersetMaxDoc=maxDoc;\n    DocList superset;\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && filter==null) {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(query, filterList, lsort, flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = (DocList)queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(offset,len);\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (filterList==null) {\n                out.docSet = getDocSet(query);\n              } else {\n                List<Query> newList = new ArrayList<Query>(filterList.size()+1);\n                newList.add(query);\n                newList.addAll(filterList);\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n        // If we are going to generate the result, bump up to the\n        // next resultWindowSize for better caching.\n\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDoc < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDoc-1)/queryResultWindowSize + 1)*queryResultWindowSize;\n        }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && lsort != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = lsort.getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(query,filter);\n        DocSet bigFilt = getDocSet(filterList);\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,lsort,supersetMaxDoc);\n      out.docList = superset.subset(offset,len);\n    } else {\n      // do it the normal way...\n      DocSet theFilt = filter!=null ? filter : getDocSet(filterList);\n\n      if ((flags & GET_DOCSET)!=0) {\n        DocSet qDocSet = getDocListAndSetNC(out,query,theFilt,lsort,0,supersetMaxDoc,flags);\n        // cache the docSet matching the query w/o filtering\n        if (filterCache!=null) filterCache.put(query,qDocSet);\n      } else {\n        out.docList = getDocListNC(query,theFilt,lsort,0,supersetMaxDoc,flags);\n      }\n      superset = out.docList;\n      out.docList = superset.subset(offset,len);\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"30c238dad8c4234f556cd28cd22ff426247e70c4","date":1195490330,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(DocListAndSet,Query,List[Query],DocSet,Sort,int,int,int).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(DocListAndSet,Query,List[Query],DocSet,Sort,int,int,int).mjava","sourceNew":"  /** getDocList version that uses+populates query and filter caches.\n   * This should only be called using either filterList or filter, but not both.\n   */\n  private void getDocListC(DocListAndSet out, Query query, List<Query> filterList, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    QueryResultKey key=null;\n    int maxDocRequested = offset + len;\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset;\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && filter==null) {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(query, filterList, lsort, flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = (DocList)queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(offset,len);\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (filterList==null) {\n                out.docSet = getDocSet(query);\n              } else {\n                List<Query> newList = new ArrayList<Query>(filterList.size()+1);\n                newList.add(query);\n                newList.addAll(filterList);\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n        // If we are going to generate the result, bump up to the\n        // next resultWindowSize for better caching.\n\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && lsort != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = lsort.getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(query,filter);\n        DocSet bigFilt = getDocSet(filterList);\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,lsort,supersetMaxDoc);\n      out.docList = superset.subset(offset,len);\n    } else {\n      // do it the normal way...\n      DocSet theFilt = filter!=null ? filter : getDocSet(filterList);\n\n      if ((flags & GET_DOCSET)!=0) {\n        DocSet qDocSet = getDocListAndSetNC(out,query,theFilt,lsort,0,supersetMaxDoc,flags);\n        // cache the docSet matching the query w/o filtering\n        if (filterCache!=null) filterCache.put(query,qDocSet);\n      } else {\n        out.docList = getDocListNC(query,theFilt,lsort,0,supersetMaxDoc,flags);\n      }\n      superset = out.docList;\n      out.docList = superset.subset(offset,len);\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  /** getDocList version that uses+populates query and filter caches.\n   * This should only be called using either filterList or filter, but not both.\n   */\n  private void getDocListC(DocListAndSet out, Query query, List<Query> filterList, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    QueryResultKey key=null;\n    int maxDoc = offset + len;\n    int supersetMaxDoc=maxDoc;\n    DocList superset;\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && filter==null) {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(query, filterList, lsort, flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = (DocList)queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(offset,len);\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (filterList==null) {\n                out.docSet = getDocSet(query);\n              } else {\n                List<Query> newList = new ArrayList<Query>(filterList.size()+1);\n                newList.add(query);\n                newList.addAll(filterList);\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n        // If we are going to generate the result, bump up to the\n        // next resultWindowSize for better caching.\n\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDoc < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDoc-1)/queryResultWindowSize + 1)*queryResultWindowSize;\n        }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && lsort != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = lsort.getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(query,filter);\n        DocSet bigFilt = getDocSet(filterList);\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,lsort,supersetMaxDoc);\n      out.docList = superset.subset(offset,len);\n    } else {\n      // do it the normal way...\n      DocSet theFilt = filter!=null ? filter : getDocSet(filterList);\n\n      if ((flags & GET_DOCSET)!=0) {\n        DocSet qDocSet = getDocListAndSetNC(out,query,theFilt,lsort,0,supersetMaxDoc,flags);\n        // cache the docSet matching the query w/o filtering\n        if (filterCache!=null) filterCache.put(query,qDocSet);\n      } else {\n        out.docList = getDocListNC(query,theFilt,lsort,0,supersetMaxDoc,flags);\n      }\n      superset = out.docList;\n      out.docList = superset.subset(offset,len);\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":["0c3e228bf650e96f3002a8fb73dd0c13d55af077"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db25c1f61b5ae826f10777da6551a832703967d5","date":1215306972,"type":5,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListC(DocListAndSet,Query,List[Query],DocSet,Sort,int,int,int).mjava","sourceNew":"  /**\n   * getDocList version that uses+populates query and filter caches.\n   * In the event of a timeout, the cache is not populated.\n   */\n  private void getDocListC(QueryResult qr, QueryCommand cmd) throws IOException {\n    // old parameters: DocListAndSet out, Query query, List<Query> filterList, DocSet filter, Sort lsort, int offset, int len, int flags, long timeAllowed, NamedList<Object> responseHeader\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n    QueryResultKey key=null;\n    int maxDocRequested = cmd.getOffset() + cmd.getLen();\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset;\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && cmd.getFilter()==null) {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(cmd.getQuery(), cmd.getFilterList(), cmd.getSort(), cmd.getFlags());\n        if ((cmd.getFlags() & NO_CHECK_QCACHE)==0) {\n          superset = (DocList)queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((cmd.getFlags() & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((cmd.getFlags() & GET_DOCSET)!=0) ) {\n              if (cmd.getFilterList()==null) {\n                out.docSet = getDocSet(cmd.getQuery());\n              } else {\n                List<Query> newList = new ArrayList<Query>(cmd.getFilterList()\n.size()+1);\n                newList.add(cmd.getQuery());\n                newList.addAll(cmd.getFilterList());\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n        // If we are going to generate the result, bump up to the\n        // next resultWindowSize for better caching.\n\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((cmd.getFlags() & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && cmd.getSort() != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = cmd.getSort().getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(cmd.getQuery(),cmd.getFilter());\n        DocSet bigFilt = getDocSet(cmd.getFilterList());\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,cmd.getSort(),supersetMaxDoc);\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    } else {\n      // do it the normal way...\n      cmd.setSupersetMaxDoc(supersetMaxDoc);\n      if ((cmd.getFlags() & GET_DOCSET)!=0) {\n        DocSet qDocSet = getDocListAndSetNC(qr,cmd);\n        // cache the docSet matching the query w/o filtering\n        if (filterCache!=null && !qr.isPartialResults()) filterCache.put(cmd.getQuery(),qDocSet);\n      } else {\n        getDocListNC(qr,cmd);\n        //Parameters: cmd.getQuery(),theFilt,cmd.getSort(),0,supersetMaxDoc,cmd.getFlags(),cmd.getTimeAllowed(),responseHeader);\n      }\n      superset = out.docList;\n      out.docList = superset.subset(cmd.getOffset(),cmd.getLen());\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached && !qr.isPartialResults()) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","sourceOld":"  /** getDocList version that uses+populates query and filter caches.\n   * This should only be called using either filterList or filter, but not both.\n   */\n  private void getDocListC(DocListAndSet out, Query query, List<Query> filterList, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    QueryResultKey key=null;\n    int maxDocRequested = offset + len;\n    // check for overflow, and check for # docs in index\n    if (maxDocRequested < 0 || maxDocRequested > maxDoc()) maxDocRequested = maxDoc();\n    int supersetMaxDoc= maxDocRequested;\n    DocList superset;\n\n\n    // we can try and look up the complete query in the cache.\n    // we can't do that if filter!=null though (we don't want to\n    // do hashCode() and equals() for a big DocSet).\n    if (queryResultCache != null && filter==null) {\n        // all of the current flags can be reused during warming,\n        // so set all of them on the cache key.\n        key = new QueryResultKey(query, filterList, lsort, flags);\n        if ((flags & NO_CHECK_QCACHE)==0) {\n          superset = (DocList)queryResultCache.get(key);\n\n          if (superset != null) {\n            // check that the cache entry has scores recorded if we need them\n            if ((flags & GET_SCORES)==0 || superset.hasScores()) {\n              // NOTE: subset() returns null if the DocList has fewer docs than\n              // requested\n              out.docList = superset.subset(offset,len);\n            }\n          }\n          if (out.docList != null) {\n            // found the docList in the cache... now check if we need the docset too.\n            // OPT: possible future optimization - if the doclist contains all the matches,\n            // use it to make the docset instead of rerunning the query.\n            if (out.docSet==null && ((flags & GET_DOCSET)!=0) ) {\n              if (filterList==null) {\n                out.docSet = getDocSet(query);\n              } else {\n                List<Query> newList = new ArrayList<Query>(filterList.size()+1);\n                newList.add(query);\n                newList.addAll(filterList);\n                out.docSet = getDocSet(newList);\n              }\n            }\n            return;\n          }\n        }\n\n        // If we are going to generate the result, bump up to the\n        // next resultWindowSize for better caching.\n\n        // handle 0 special case as well as avoid idiv in the common case.\n        if (maxDocRequested < queryResultWindowSize) {\n          supersetMaxDoc=queryResultWindowSize;\n        } else {\n          supersetMaxDoc = ((maxDocRequested -1)/queryResultWindowSize + 1)*queryResultWindowSize;\n          if (supersetMaxDoc < 0) supersetMaxDoc=maxDocRequested;\n        }\n    }\n\n\n    // OK, so now we need to generate an answer.\n    // One way to do that would be to check if we have an unordered list\n    // of results for the base query.  If so, we can apply the filters and then\n    // sort by the resulting set.  This can only be used if:\n    // - the sort doesn't contain score\n    // - we don't want score returned.\n\n    // check if we should try and use the filter cache\n    boolean useFilterCache=false;\n    if ((flags & (GET_SCORES|NO_CHECK_FILTERCACHE))==0 && useFilterForSortedQuery && lsort != null && filterCache != null) {\n      useFilterCache=true;\n      SortField[] sfields = lsort.getSort();\n      for (SortField sf : sfields) {\n        if (sf.getType() == SortField.SCORE) {\n          useFilterCache=false;\n          break;\n        }\n      }\n    }\n\n    if (useFilterCache) {\n      // now actually use the filter cache.\n      // for large filters that match few documents, this may be\n      // slower than simply re-executing the query.\n      if (out.docSet == null) {\n        out.docSet = getDocSet(query,filter);\n        DocSet bigFilt = getDocSet(filterList);\n        if (bigFilt != null) out.docSet = out.docSet.intersection(bigFilt);\n      }\n      // todo: there could be a sortDocSet that could take a list of\n      // the filters instead of anding them first...\n      // perhaps there should be a multi-docset-iterator\n      superset = sortDocSet(out.docSet,lsort,supersetMaxDoc);\n      out.docList = superset.subset(offset,len);\n    } else {\n      // do it the normal way...\n      DocSet theFilt = filter!=null ? filter : getDocSet(filterList);\n\n      if ((flags & GET_DOCSET)!=0) {\n        DocSet qDocSet = getDocListAndSetNC(out,query,theFilt,lsort,0,supersetMaxDoc,flags);\n        // cache the docSet matching the query w/o filtering\n        if (filterCache!=null) filterCache.put(query,qDocSet);\n      } else {\n        out.docList = getDocListNC(query,theFilt,lsort,0,supersetMaxDoc,flags);\n      }\n      superset = out.docList;\n      out.docList = superset.subset(offset,len);\n    }\n\n    // lastly, put the superset in the cache if the size is less than or equal\n    // to queryResultMaxDocsCached\n    if (key != null && superset.size() <= queryResultMaxDocsCached) {\n      queryResultCache.put(key, superset);\n    }\n  }\n\n","bugFix":null,"bugIntro":["0d6197aa6dfead2636cc42b887d9ceef82f4491b","0d6197aa6dfead2636cc42b887d9ceef82f4491b","0d6197aa6dfead2636cc42b887d9ceef82f4491b"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"660013db716c87722fe5d4684d75e893596f49d0":["18d06c030cb0a920c116ec6e16933a4590a70bb9"],"30c238dad8c4234f556cd28cd22ff426247e70c4":["ba4ca82eb5aaa1d2278e921a71257bc9eff71ecc"],"18d06c030cb0a920c116ec6e16933a4590a70bb9":["0c3e228bf650e96f3002a8fb73dd0c13d55af077"],"0c3e228bf650e96f3002a8fb73dd0c13d55af077":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5df952ccf58ae0134bf9774c7b0d769f1645f317":["660013db716c87722fe5d4684d75e893596f49d0"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"db25c1f61b5ae826f10777da6551a832703967d5":["30c238dad8c4234f556cd28cd22ff426247e70c4"],"ba4ca82eb5aaa1d2278e921a71257bc9eff71ecc":["5df952ccf58ae0134bf9774c7b0d769f1645f317"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"660013db716c87722fe5d4684d75e893596f49d0":["5df952ccf58ae0134bf9774c7b0d769f1645f317"],"18d06c030cb0a920c116ec6e16933a4590a70bb9":["660013db716c87722fe5d4684d75e893596f49d0"],"30c238dad8c4234f556cd28cd22ff426247e70c4":["db25c1f61b5ae826f10777da6551a832703967d5"],"0c3e228bf650e96f3002a8fb73dd0c13d55af077":["18d06c030cb0a920c116ec6e16933a4590a70bb9"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["0c3e228bf650e96f3002a8fb73dd0c13d55af077"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5df952ccf58ae0134bf9774c7b0d769f1645f317":["ba4ca82eb5aaa1d2278e921a71257bc9eff71ecc"],"db25c1f61b5ae826f10777da6551a832703967d5":[],"ba4ca82eb5aaa1d2278e921a71257bc9eff71ecc":["30c238dad8c4234f556cd28cd22ff426247e70c4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["db25c1f61b5ae826f10777da6551a832703967d5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}