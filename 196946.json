{"path":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","commits":[{"id":"98d45c1ff2c99694b6de2201175f9b8b8b27b597","date":1332757908,"type":1,"author":"Christian Moen","isMerge":false,"pathNew":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","pathOld":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/kuromoji/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        // FIXME: Should probably deal with this differently.  Exception?\n        System.out.println(\"This entry is not properly formatted : \" + line);\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        // FIXME: Should probably deal with this differently.  Exception?\n        System.out.println(\"This entry is not properly formatted : \" + line);\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2337069d9afb809f117538f07c3e20ee830125f","date":1332955248,"type":3,"author":"Christian Moen","isMerge":false,"pathNew":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","pathOld":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        // FIXME: Should probably deal with this differently.  Exception?\n        System.out.println(\"This entry is not properly formatted : \" + line);\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","pathOld":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["d2337069d9afb809f117538f07c3e20ee830125f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d2337069d9afb809f117538f07c3e20ee830125f":["98d45c1ff2c99694b6de2201175f9b8b8b27b597"],"98d45c1ff2c99694b6de2201175f9b8b8b27b597":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["98d45c1ff2c99694b6de2201175f9b8b8b27b597"],"d2337069d9afb809f117538f07c3e20ee830125f":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"98d45c1ff2c99694b6de2201175f9b8b8b27b597":["d2337069d9afb809f117538f07c3e20ee830125f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}