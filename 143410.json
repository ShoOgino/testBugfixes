{"path":"backwards/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","commits":[{"id":"480d01e5b0ef8efb136d51670fec297ae5ae2c9c","date":1268821447,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"backwards/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","pathOld":"/dev/null","sourceNew":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\n                \"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\n                \"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = null;\n        try {\n            os = directory.createOutput(fileName);\n\n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(fe.file);\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            byte buffer[] = new byte[16384];\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os, buffer);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n\n        } finally {\n            if (os != null) try { os.close(); } catch (IOException e) { }\n        }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","pathOld":"backwards/src/java/org/apache/lucene/index/CompoundFileWriter#close().mjava","sourceNew":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\n                \"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\n                \"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = null;\n        try {\n            os = directory.createOutput(fileName);\n\n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(fe.file);\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            byte buffer[] = new byte[16384];\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os, buffer);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n\n        } finally {\n            if (os != null) try { os.close(); } catch (IOException e) { }\n        }\n    }\n\n","sourceOld":"    /** Merge files with the extensions added up to now.\n     *  All files with these extensions are combined sequentially into the\n     *  compound stream. After successful merge, the source files\n     *  are deleted.\n     *  @throws IllegalStateException if close() had been called before or\n     *   if no file has been added to this object\n     */\n    public void close() throws IOException {\n        if (merged)\n            throw new IllegalStateException(\n                \"Merge already performed\");\n\n        if (entries.isEmpty())\n            throw new IllegalStateException(\n                \"No entries to merge have been defined\");\n\n        merged = true;\n\n        // open the compound stream\n        IndexOutput os = null;\n        try {\n            os = directory.createOutput(fileName);\n\n            // Write the number of entries\n            os.writeVInt(entries.size());\n\n            // Write the directory with all offsets at 0.\n            // Remember the positions of directory entries so that we can\n            // adjust the offsets later\n            long totalSize = 0;\n            for (FileEntry fe : entries) {\n                fe.directoryOffset = os.getFilePointer();\n                os.writeLong(0);    // for now\n                os.writeString(fe.file);\n                totalSize += directory.fileLength(fe.file);\n            }\n\n            // Pre-allocate size of file as optimization --\n            // this can potentially help IO performance as\n            // we write the file and also later during\n            // searching.  It also uncovers a disk-full\n            // situation earlier and hopefully without\n            // actually filling disk to 100%:\n            final long finalLength = totalSize+os.getFilePointer();\n            os.setLength(finalLength);\n\n            // Open the files and copy their data into the stream.\n            // Remember the locations of each file's data section.\n            byte buffer[] = new byte[16384];\n            for (FileEntry fe : entries) {\n                fe.dataOffset = os.getFilePointer();\n                copyFile(fe, os, buffer);\n            }\n\n            // Write the data offsets into the directory of the compound stream\n            for (FileEntry fe : entries) {\n                os.seek(fe.directoryOffset);\n                os.writeLong(fe.dataOffset);\n            }\n\n            assert finalLength == os.length();\n\n            // Close the output stream. Set the os to null before trying to\n            // close so that if an exception occurs during the close, the\n            // finally clause below will not attempt to close the stream\n            // the second time.\n            IndexOutput tmp = os;\n            os = null;\n            tmp.close();\n\n        } finally {\n            if (os != null) try { os.close(); } catch (IOException e) { }\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"]},"commit2Childs":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}