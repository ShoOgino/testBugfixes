{"path":"lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","commits":[{"id":"519ac3b8f2711b5bfeb1c90c77bb007032270a41","date":1384456090,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"/dev/null","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        return;\n      }\n\n      final int maxDoc = reader.maxDoc();\n      assert maxDoc == hits.bits.length();\n\n      // nocommit, yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (dv instanceof MultiSortedSetDocValues) {\n        MultiDocValues.OrdinalMap ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          // Remap every ord to global ord as we iterate:\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n        } else {\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n\n        int doc = 0;\n        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n          ++doc;\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"19f5022544a8fc895776356d1b35a4b46d05945c","date":1385063323,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // nocommit not quite right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        // nocommit in trunk this was a \"return\" which is\n        // wrong; make a failing test\n        continue;\n      }\n\n      final int maxDoc = reader.maxDoc();\n      assert maxDoc == hits.bits.length();\n      //System.out.println(\"  dv=\" + dv);\n\n      // nocommit, yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n\n        int doc = 0;\n        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n          ++doc;\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        return;\n      }\n\n      final int maxDoc = reader.maxDoc();\n      assert maxDoc == hits.bits.length();\n\n      // nocommit, yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (dv instanceof MultiSortedSetDocValues) {\n        MultiDocValues.OrdinalMap ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          // Remap every ord to global ord as we iterate:\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n        } else {\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n\n        int doc = 0;\n        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n          ++doc;\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"21d36d0db865f7b84026b447bec653469a6e66df","date":1385495602,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts#count(List[MatchingDocs]).mjava","sourceNew":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // nocommit not quite right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        // nocommit in trunk this was a \"return\" which is\n        // wrong; make a failing test\n        continue;\n      }\n\n      final int maxDoc = reader.maxDoc();\n      assert maxDoc == hits.bits.length();\n      //System.out.println(\"  dv=\" + dv);\n\n      // nocommit, yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n\n        int doc = 0;\n        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n          ++doc;\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Does all the \"real work\" of tallying up the counts. */\n  private final void count(List<MatchingDocs> matchingDocs) throws IOException {\n    //System.out.println(\"ssdv count\");\n\n    MultiDocValues.OrdinalMap ordinalMap;\n\n    // nocommit not quite right?  really, we need a way to\n    // verify that this ordinalMap \"matches\" the leaves in\n    // matchingDocs...\n    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {\n      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;\n    } else {\n      ordinalMap = null;\n    }\n\n    for(MatchingDocs hits : matchingDocs) {\n\n      AtomicReader reader = hits.context.reader();\n      //System.out.println(\"  reader=\" + reader);\n      // LUCENE-5090: make sure the provided reader context \"matches\"\n      // the top-level reader passed to the\n      // SortedSetDocValuesReaderState, else cryptic\n      // AIOOBE can happen:\n      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {\n        throw new IllegalStateException(\"the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader\");\n      }\n      \n      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);\n      if (segValues == null) {\n        // nocommit in trunk this was a \"return\" which is\n        // wrong; make a failing test\n        continue;\n      }\n\n      final int maxDoc = reader.maxDoc();\n      assert maxDoc == hits.bits.length();\n      //System.out.println(\"  dv=\" + dv);\n\n      // nocommit, yet another option is to count all segs\n      // first, only in seg-ord space, and then do a\n      // merge-sort-PQ in the end to only \"resolve to\n      // global\" those seg ords that can compete, if we know\n      // we just want top K?  ie, this is the same algo\n      // that'd be used for merging facets across shards\n      // (distributed faceting).  but this has much higher\n      // temp ram req'ts (sum of number of ords across all\n      // segs)\n      if (ordinalMap != null) {\n        int segOrd = hits.context.ord;\n\n        int numSegOrds = (int) segValues.getValueCount();\n\n        if (hits.totalHits < numSegOrds/10) {\n          //System.out.println(\"    remap as-we-go\");\n          // Remap every ord to global ord as we iterate:\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      segOrd=\" + segOrd + \" ord=\" + term + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, term));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n        } else {\n          //System.out.println(\"    count in seg ord first\");\n\n          // First count in seg-ord space:\n          final int[] segCounts = new int[numSegOrds];\n          int doc = 0;\n          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n            //System.out.println(\"    doc=\" + doc);\n            segValues.setDocument(doc);\n            int term = (int) segValues.nextOrd();\n            while (term != SortedSetDocValues.NO_MORE_ORDS) {\n              //System.out.println(\"      ord=\" + term);\n              segCounts[term]++;\n              term = (int) segValues.nextOrd();\n            }\n            ++doc;\n          }\n\n          // Then, migrate to global ords:\n          for(int ord=0;ord<numSegOrds;ord++) {\n            int count = segCounts[ord];\n            if (count != 0) {\n              //System.out.println(\"    migrate segOrd=\" + segOrd + \" ord=\" + ord + \" globalOrd=\" + ordinalMap.getGlobalOrd(segOrd, ord));\n              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;\n            }\n          }\n        }\n      } else {\n        // No ord mapping (e.g., single segment index):\n        // just aggregate directly into counts:\n\n        int doc = 0;\n        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {\n          segValues.setDocument(doc);\n          int term = (int) segValues.nextOrd();\n          while (term != SortedSetDocValues.NO_MORE_ORDS) {\n            counts[term]++;\n            term = (int) segValues.nextOrd();\n          }\n          ++doc;\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"19f5022544a8fc895776356d1b35a4b46d05945c":["519ac3b8f2711b5bfeb1c90c77bb007032270a41"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"21d36d0db865f7b84026b447bec653469a6e66df":["19f5022544a8fc895776356d1b35a4b46d05945c"],"519ac3b8f2711b5bfeb1c90c77bb007032270a41":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"19f5022544a8fc895776356d1b35a4b46d05945c":["21d36d0db865f7b84026b447bec653469a6e66df"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["519ac3b8f2711b5bfeb1c90c77bb007032270a41","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"519ac3b8f2711b5bfeb1c90c77bb007032270a41":["19f5022544a8fc895776356d1b35a4b46d05945c"],"21d36d0db865f7b84026b447bec653469a6e66df":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["21d36d0db865f7b84026b447bec653469a6e66df","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}