{"path":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(int,IndexableField,boolean).mjava","commits":[{"id":"f97270426d92300e08ac1bd1a4ef499ae02e88b7","date":1592503330,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(int,IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(int docID, IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add(invertState.termAttribute.getBytesRef(), docID);\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            docWriter.onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && docState.analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add(invertState.termAttribute.getBytesRef(), docState.docID);\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (docState.infoStream.isEnabled(\"IW\")) {\n              docState.infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            docWriter.onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && docState.infoStream.isEnabled(\"DW\")) {\n          docState.infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"49f1924bd448393fbdfef8b5ebed799f938169d3","date":1600069616,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(int,IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(int,IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(int docID, IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add(invertState.termAttribute.getBytesRef(), docID);\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + IndexWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(int docID, IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add(invertState.termAttribute.getBytesRef(), docID);\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            docWriter.onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0dcf8f79417865e5028d753e669fae06457e8369","date":1600073240,"type":3,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(int,IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(int,IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(int docID, IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add(invertState.termAttribute.getBytesRef(), docID);\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + IndexWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(int docID, IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add(invertState.termAttribute.getBytesRef(), docID);\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8 + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            docWriter.onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a6f8af01d9b3067b143bbdc0a492720e2af97cf","date":1600157724,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexingChain.PerField#invert(int,IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(int,IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(int docID, IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add(invertState.termAttribute.getBytesRef(), docID);\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + IndexWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(int docID, IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add(invertState.termAttribute.getBytesRef(), docID);\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + IndexWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"680b6449f09827f58fe987aff279e014c311d966","date":1600247985,"type":5,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexingChain.PerField#invert(int,IndexableField,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.PerField#invert(int,IndexableField,boolean).mjava","sourceNew":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(int docID, IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add(invertState.termAttribute.getBytesRef(), docID);\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + IndexWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","sourceOld":"    /** Inverts one field for one document; first is true\n     *  if this is the first time we are seeing this field\n     *  name in this document. */\n    public void invert(int docID, IndexableField field, boolean first) throws IOException {\n      if (first) {\n        // First time we're seeing this field (indexed) in\n        // this document:\n        invertState.reset();\n      }\n\n      IndexableFieldType fieldType = field.fieldType();\n\n      IndexOptions indexOptions = fieldType.indexOptions();\n      fieldInfo.setIndexOptions(indexOptions);\n\n      if (fieldType.omitNorms()) {\n        fieldInfo.setOmitsNorms();\n      }\n\n      final boolean analyzed = fieldType.tokenized() && analyzer != null;\n        \n      /*\n       * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n       * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n       * but rather a finally that takes note of the problem.\n       */\n      boolean succeededInProcessingField = false;\n      try (TokenStream stream = tokenStream = field.tokenStream(analyzer, tokenStream)) {\n        // reset the TokenStream to the first token\n        stream.reset();\n        invertState.setAttributeSource(stream);\n        termsHashPerField.start(field, first);\n\n        while (stream.incrementToken()) {\n\n          // If we hit an exception in stream.next below\n          // (which is fairly common, e.g. if analyzer\n          // chokes on a given document), then it's\n          // non-aborting and (above) this one document\n          // will be marked as deleted, but still\n          // consume a docID\n\n          int posIncr = invertState.posIncrAttribute.getPositionIncrement();\n          invertState.position += posIncr;\n          if (invertState.position < invertState.lastPosition) {\n            if (posIncr == 0) {\n              throw new IllegalArgumentException(\"first position increment must be > 0 (got 0) for field '\" + field.name() + \"'\");\n            } else if (posIncr < 0) {\n              throw new IllegalArgumentException(\"position increment must be >= 0 (got \" + posIncr + \") for field '\" + field.name() + \"'\");\n            } else {\n              throw new IllegalArgumentException(\"position overflowed Integer.MAX_VALUE (got posIncr=\" + posIncr + \" lastPosition=\" + invertState.lastPosition + \" position=\" + invertState.position + \") for field '\" + field.name() + \"'\");\n            }\n          } else if (invertState.position > IndexWriter.MAX_POSITION) {\n            throw new IllegalArgumentException(\"position \" + invertState.position + \" is too large for field '\" + field.name() + \"': max allowed position is \" + IndexWriter.MAX_POSITION);\n          }\n          invertState.lastPosition = invertState.position;\n          if (posIncr == 0) {\n            invertState.numOverlap++;\n          }\n              \n          int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();\n          int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();\n          if (startOffset < invertState.lastStartOffset || endOffset < startOffset) {\n            throw new IllegalArgumentException(\"startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards \"\n                                               + \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",lastStartOffset=\" + invertState.lastStartOffset + \" for field '\" + field.name() + \"'\");\n          }\n          invertState.lastStartOffset = startOffset;\n\n          try {\n            invertState.length = Math.addExact(invertState.length, invertState.termFreqAttribute.getTermFrequency());\n          } catch (ArithmeticException ae) {\n            throw new IllegalArgumentException(\"too many tokens for field \\\"\" + field.name() + \"\\\"\");\n          }\n          \n          //System.out.println(\"  term=\" + invertState.termAttribute);\n\n          // If we hit an exception in here, we abort\n          // all buffered documents since the last\n          // flush, on the likelihood that the\n          // internal state of the terms hash is now\n          // corrupt and should not be flushed to a\n          // new segment:\n          try {\n            termsHashPerField.add(invertState.termAttribute.getBytesRef(), docID);\n          } catch (MaxBytesLengthExceededException e) {\n            byte[] prefix = new byte[30];\n            BytesRef bigTerm = invertState.termAttribute.getBytesRef();\n            System.arraycopy(bigTerm.bytes, bigTerm.offset, prefix, 0, 30);\n            String msg = \"Document contains at least one immense term in field=\\\"\" + fieldInfo.name + \"\\\" (whose UTF8 encoding is longer than the max length \" + IndexWriter.MAX_TERM_LENGTH + \"), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '\" + Arrays.toString(prefix) + \"...', original message: \" + e.getMessage();\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"ERROR: \" + msg);\n            }\n            // Document will be deleted above:\n            throw new IllegalArgumentException(msg, e);\n          } catch (Throwable th) {\n            onAbortingException(th);\n            throw th;\n          }\n        }\n\n        // trigger streams to perform end-of-stream operations\n        stream.end();\n\n        // TODO: maybe add some safety? then again, it's already checked \n        // when we come back around to the field...\n        invertState.position += invertState.posIncrAttribute.getPositionIncrement();\n        invertState.offset += invertState.offsetAttribute.endOffset();\n\n        /* if there is an exception coming through, we won't set this to true here:*/\n        succeededInProcessingField = true;\n      } finally {\n        if (!succeededInProcessingField && infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", \"An exception was thrown while processing field \" + fieldInfo.name);\n        }\n      }\n\n      if (analyzed) {\n        invertState.position += analyzer.getPositionIncrementGap(fieldInfo.name);\n        invertState.offset += analyzer.getOffsetGap(fieldInfo.name);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"49f1924bd448393fbdfef8b5ebed799f938169d3":["f97270426d92300e08ac1bd1a4ef499ae02e88b7"],"680b6449f09827f58fe987aff279e014c311d966":["0dcf8f79417865e5028d753e669fae06457e8369","7a6f8af01d9b3067b143bbdc0a492720e2af97cf"],"7a6f8af01d9b3067b143bbdc0a492720e2af97cf":["49f1924bd448393fbdfef8b5ebed799f938169d3"],"0dcf8f79417865e5028d753e669fae06457e8369":["f97270426d92300e08ac1bd1a4ef499ae02e88b7","49f1924bd448393fbdfef8b5ebed799f938169d3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f97270426d92300e08ac1bd1a4ef499ae02e88b7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["680b6449f09827f58fe987aff279e014c311d966"]},"commit2Childs":{"49f1924bd448393fbdfef8b5ebed799f938169d3":["7a6f8af01d9b3067b143bbdc0a492720e2af97cf","0dcf8f79417865e5028d753e669fae06457e8369"],"680b6449f09827f58fe987aff279e014c311d966":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7a6f8af01d9b3067b143bbdc0a492720e2af97cf":["680b6449f09827f58fe987aff279e014c311d966"],"0dcf8f79417865e5028d753e669fae06457e8369":["680b6449f09827f58fe987aff279e014c311d966"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f97270426d92300e08ac1bd1a4ef499ae02e88b7"],"f97270426d92300e08ac1bd1a4ef499ae02e88b7":["49f1924bd448393fbdfef8b5ebed799f938169d3","0dcf8f79417865e5028d753e669fae06457e8369"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}