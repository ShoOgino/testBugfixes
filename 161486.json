{"path":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector#init().mjava","commits":[{"id":"ae230518a1a68acc124bef8df61ef94bd7c1295e","date":1417181719,"type":0,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector#init().mjava","pathOld":"/dev/null","sourceNew":"  //We initialize in reset() because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef termBytesRef;\n    DocsAndPositionsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          // Must make a deep copy of the returned payload,\n          // since D&PEnum API is allowed to re-use on every\n          // call:\n          final BytesRef payload = dpEnum.getPayload();\n          if (payload != null) {\n            token.payload = BytesRef.deepCopyOf(payload);//TODO share a ByteBlockPool & re-use BytesRef\n          }\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"714aa8d007eef87d7203cfc6e0fe4dab8dd8a497","date":1417181893,"type":5,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#init().mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector#init().mjava","sourceNew":"  //We initialize in reset() because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef termBytesRef;\n    DocsAndPositionsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          // Must make a deep copy of the returned payload,\n          // since D&PEnum API is allowed to re-use on every\n          // call:\n          final BytesRef payload = dpEnum.getPayload();\n          if (payload != null) {\n            token.payload = BytesRef.deepCopyOf(payload);//TODO share a ByteBlockPool & re-use BytesRef\n          }\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n  }\n\n","sourceOld":"  //We initialize in reset() because we can see which attributes the consumer wants, particularly payloads\n  private void init() throws IOException {\n    if (vector.hasOffsets()) {\n      offsetAttribute = addAttribute(OffsetAttribute.class);\n    }\n    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {\n      payloadAttribute = getAttribute(PayloadAttribute.class);\n    }\n\n    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position\n\n    TokenLL[] positionedTokens = initTokensArray();\n\n    int lastPosition = -1;\n\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef termBytesRef;\n    DocsAndPositionsEnum dpEnum = null;\n    //int sumFreq = 0;\n    while ((termBytesRef = termsEnum.next()) != null) {\n      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)\n      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand\n      final char[] termChars = new char[termBytesRef.length];\n      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      //sumFreq += freq;\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        TokenLL token = new TokenLL();\n        token.termChars = termChars;\n        token.termCharsLen = termCharsLen;\n        if (offsetAttribute != null) {\n          token.startOffset = dpEnum.startOffset();\n          token.endOffset = dpEnum.endOffset();\n          if (pos == -1) {\n            pos = token.startOffset >> 3;//divide by 8\n          }\n        }\n\n        if (payloadAttribute != null) {\n          // Must make a deep copy of the returned payload,\n          // since D&PEnum API is allowed to re-use on every\n          // call:\n          final BytesRef payload = dpEnum.getPayload();\n          if (payload != null) {\n            token.payload = BytesRef.deepCopyOf(payload);//TODO share a ByteBlockPool & re-use BytesRef\n          }\n        }\n\n        //Add token to an array indexed by position\n        if (positionedTokens.length <= pos) {\n          //grow, but not 2x since we think our original length estimate is close\n          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];\n          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);\n          positionedTokens = newPositionedTokens;\n        }\n        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);\n\n        lastPosition = Math.max(lastPosition, pos);\n      }\n    }\n\n//    System.out.println(String.format(\n//        \"SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f\",\n//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,\n//        (originalPositionEstimate/(lastPosition + 1.0f))));\n\n    // Step 2:  Link all Tokens into a linked-list and set position increments as we go\n\n    int prevTokenPos = -1;\n    TokenLL prevToken = null;\n    for (int pos = 0; pos <= lastPosition; pos++) {\n      TokenLL token = positionedTokens[pos];\n      if (token == null) {\n        continue;\n      }\n      //link\n      if (prevToken != null) {\n        assert prevToken.next == null;\n        prevToken.next = token; //concatenate linked-list\n      } else {\n        assert firstToken == null;\n        firstToken = token;\n      }\n      //set increments\n      if (vector.hasPositions()) {\n        token.positionIncrement = pos - prevTokenPos;\n        while (token.next != null) {\n          token = token.next;\n          token.positionIncrement = 0;\n        }\n      } else {\n        token.positionIncrement = 1;\n        while (token.next != null) {\n          prevToken = token;\n          token = token.next;\n          if (prevToken.startOffset == token.startOffset) {\n            token.positionIncrement = 0;\n          } else {\n            token.positionIncrement = 1;\n          }\n        }\n      }\n      prevTokenPos = pos;\n      prevToken = token;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"714aa8d007eef87d7203cfc6e0fe4dab8dd8a497":["ae230518a1a68acc124bef8df61ef94bd7c1295e"],"ae230518a1a68acc124bef8df61ef94bd7c1295e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["714aa8d007eef87d7203cfc6e0fe4dab8dd8a497"]},"commit2Childs":{"714aa8d007eef87d7203cfc6e0fe4dab8dd8a497":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ae230518a1a68acc124bef8df61ef94bd7c1295e":["714aa8d007eef87d7203cfc6e0fe4dab8dd8a497"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ae230518a1a68acc124bef8df61ef94bd7c1295e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}