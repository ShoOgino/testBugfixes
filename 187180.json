{"path":"lucene/core/src/test/org/apache/lucene/index/TestTermsHashPerField#testAddAndUpdateRandom().mjava","commits":[{"id":"d3cc3fa1ecad75b99ec55169e44628808f9866ad","date":1592311545,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsHashPerField#testAddAndUpdateRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testAddAndUpdateRandom() throws IOException {\n    AtomicInteger newCalled = new AtomicInteger(0);\n    AtomicInteger addCalled = new AtomicInteger(0);\n    TermsHashPerField hash = createNewHash(newCalled, addCalled);\n    hash.start(null, true);\n    class Posting {\n      int termId = -1;\n      final TreeMap<Integer, Integer> docAndFreq = new TreeMap<>();\n    }\n    Map<BytesRef, Posting> postingMap = new HashMap<>();\n    int numStrings = 1 + random().nextInt(200);\n    for (int i = 0; i < numStrings; i++) {\n      String randomString = RandomStrings.randomRealisticUnicodeOfCodepointLengthBetween(random(), 1, 10);\n      postingMap.putIfAbsent(new BytesRef(randomString), new Posting());\n    }\n    List<BytesRef> bytesRefs = Arrays.asList(postingMap.keySet().toArray(new BytesRef[0]));\n    Collections.sort(bytesRefs);\n    int numDocs = 1 + random().nextInt(200);\n    int termOrd = 0;\n    for (int i = 0; i < numDocs; i++) {\n      int numTerms = 1 + random().nextInt(200);\n      int doc = i;\n      for (int j = 0; i < numTerms; i++) {\n        BytesRef ref = RandomPicks.randomFrom(random(), bytesRefs);\n        Posting posting = postingMap.get(ref);\n        if (posting.termId == -1) {\n          posting.termId = termOrd++;\n        }\n        posting.docAndFreq.putIfAbsent(doc, 0);\n        posting.docAndFreq.compute(doc, (key, oldVal) -> oldVal+1);\n        hash.add(ref, doc);\n      }\n      hash.finish();\n    }\n    List<Posting> values = postingMap.values().stream().filter( x -> x.termId != -1)\n        .collect(Collectors.toList());\n    Collections.shuffle(values, random()); // term order doesn't matter\n    final ByteSliceReader reader = new ByteSliceReader();\n    for (Posting p : values) {\n      hash.initReader(reader, p.termId, 0);\n      boolean eof = false;\n      int prefDoc = 0;\n      for (Map.Entry<Integer, Integer> entry : p.docAndFreq.entrySet()) {\n        assertFalse(\"the reader must not be EOF here\", eof);\n        eof = assertDocAndFreq(reader, (FreqProxTermsWriterPerField.FreqProxPostingsArray) hash.postingsArray,\n            prefDoc, p.termId, entry.getKey(), entry.getValue());\n        prefDoc = entry.getKey();\n      }\n      assertTrue(\"the last posting must be EOF on the reader\", eof);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d3cc3fa1ecad75b99ec55169e44628808f9866ad":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d3cc3fa1ecad75b99ec55169e44628808f9866ad"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d3cc3fa1ecad75b99ec55169e44628808f9866ad"],"d3cc3fa1ecad75b99ec55169e44628808f9866ad":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}