{"path":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","commits":[{"id":"b5fa1c8367f821057f943ece929329485ec708ba","date":1475186606,"type":0,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testClassifyStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTION;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\",\"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testClassifyStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTION;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\",\"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testClassifyStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTION;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\",\"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c9480bf8e52feb02af9b7fa13bf50929da7900b","date":1477527580,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","sourceNew":"  @Test\n  public void testClassifyStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTION;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  public void testClassifyStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTION;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\",\"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"80d0e6d59ae23f4a6f30eaf40bfb40742300287f","date":1477598926,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","sourceNew":"  @Test\n  public void testClassifyStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTION;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  public void testClassifyStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTION;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\",\"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8c969f15cd04d31e520319c619a445ae21f02d72","date":1479263638,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","sourceNew":"  @Test\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  public void testClassifyStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTION;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":["d7ced979f39d7651addfc7d805e1d9bfac215822"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a1ef55e1fff7ff44354432770ad8bc19be1fcc75","date":1479266056,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","sourceNew":"  @Test\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  public void testClassifyStream() throws Exception {\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTION;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTION);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf","date":1522951207,"type":5,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testClassifyStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","sourceNew":"  @Test\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"b5fa1c8367f821057f943ece929329485ec708ba":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b5fa1c8367f821057f943ece929329485ec708ba"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","5c9480bf8e52feb02af9b7fa13bf50929da7900b"],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","8c969f15cd04d31e520319c619a445ae21f02d72"],"8c969f15cd04d31e520319c619a445ae21f02d72":["5c9480bf8e52feb02af9b7fa13bf50929da7900b"],"5c9480bf8e52feb02af9b7fa13bf50929da7900b":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf"],"8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf":["8c969f15cd04d31e520319c619a445ae21f02d72"]},"commit2Childs":{"b5fa1c8367f821057f943ece929329485ec708ba":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b5fa1c8367f821057f943ece929329485ec708ba","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["5c9480bf8e52feb02af9b7fa13bf50929da7900b","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75"],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":[],"8c969f15cd04d31e520319c619a445ae21f02d72":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f"],"5c9480bf8e52feb02af9b7fa13bf50929da7900b":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","8c969f15cd04d31e520319c619a445ae21f02d72"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}