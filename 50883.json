{"path":"sandbox/contributions/hilighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestDocFragments(TokenStream,String,StringBuffer,int).mjava","commits":[{"id":"e3847974308b7ee4bf1bd64ab343a6758eb9ccb1","date":1081469776,"type":0,"author":"Erik Hatcher","isMerge":false,"pathNew":"sandbox/contributions/hilighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestDocFragments(TokenStream,String,StringBuffer,int).mjava","pathOld":"/dev/null","sourceNew":"\t/**\n\t * Low level api to get the most relevant sections of the document\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @return \n\t * @throws IOException\n\t */\n\tprivate final TextFragment[] getBestDocFragments(\n\t\tTokenStream tokenStream,\t\n\t\tString text,\n\t\tStringBuffer newText,\n\t\tint maxNumFragments)\n\t\tthrows IOException\n\t{\n\t\tArrayList docFrags = new ArrayList();\n\n\t\tTextFragment currentFrag =\tnew TextFragment(newText.length(), docFrags.size());\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\t\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\t\t\torg.apache.lucene.analysis.Token token;\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text);\n\n\t\t\twhile ((token = tokenStream.next()) != null)\n\t\t\t{\n\t\t\t\t\n\t\t\t\tstartOffset = token.startOffset();\n\t\t\t\tendOffset = token.endOffset();\t\t\n\t\t\t\t//FIXME an issue was reported with CJKTokenizer that I couldnt reproduce\n\t\t\t\t// where the analyzer was producing overlapping tokens.\n\t\t\t\t// I suspect the fix is to make startOffset=Math.max(startOffset,lastEndOffset+1)\n\t\t\t\t// but cant be sure so I'll just leave this comment in for now\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\n\n\t\t\t\t// append text between end of last token (or beginning of text) and start of current token\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(text.substring(lastEndOffset, startOffset));\n\n\t\t\t\t// does query contain current token?\n\t\t\t\tfloat score=fragmentScorer.getTokenScore(token);\t\t\t\n\t\t\t\tnewText.append(formatter.highlightTerm(tokenText, token.termText(), score, startOffset));\n\t\t\t\t\n\n\t\t\t\tif(textFragmenter.isNewFragment(token))\n\t\t\t\t{\n\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\tcurrentFrag =new TextFragment(newText.length(), docFrags.size());\n\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t}\n\n\t\t\t\tlastEndOffset = endOffset;\n\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\n\n\t\t\t// append text after end of last token\n\t\t\tif (lastEndOffset < text.length())\n\t\t\t\tnewText.append(text.substring(lastEndOffset));\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tint minScore = 0;\n\t\t\tfor (Iterator i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = (TextFragment) i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\t\t\t\t\t\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insert(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = (TextFragment) fragQueue.pop();\n\t\t\t}\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e36e216d6ddce8d71e813a8b0d29d42d4c5025b9","date":1081470820,"type":4,"author":"Erik Hatcher","isMerge":false,"pathNew":"/dev/null","pathOld":"sandbox/contributions/hilighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestDocFragments(TokenStream,String,StringBuffer,int).mjava","sourceNew":null,"sourceOld":"\t/**\n\t * Low level api to get the most relevant sections of the document\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @return \n\t * @throws IOException\n\t */\n\tprivate final TextFragment[] getBestDocFragments(\n\t\tTokenStream tokenStream,\t\n\t\tString text,\n\t\tStringBuffer newText,\n\t\tint maxNumFragments)\n\t\tthrows IOException\n\t{\n\t\tArrayList docFrags = new ArrayList();\n\n\t\tTextFragment currentFrag =\tnew TextFragment(newText.length(), docFrags.size());\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\t\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\t\t\torg.apache.lucene.analysis.Token token;\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text);\n\n\t\t\twhile ((token = tokenStream.next()) != null)\n\t\t\t{\n\t\t\t\t\n\t\t\t\tstartOffset = token.startOffset();\n\t\t\t\tendOffset = token.endOffset();\t\t\n\t\t\t\t//FIXME an issue was reported with CJKTokenizer that I couldnt reproduce\n\t\t\t\t// where the analyzer was producing overlapping tokens.\n\t\t\t\t// I suspect the fix is to make startOffset=Math.max(startOffset,lastEndOffset+1)\n\t\t\t\t// but cant be sure so I'll just leave this comment in for now\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\n\n\t\t\t\t// append text between end of last token (or beginning of text) and start of current token\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(text.substring(lastEndOffset, startOffset));\n\n\t\t\t\t// does query contain current token?\n\t\t\t\tfloat score=fragmentScorer.getTokenScore(token);\t\t\t\n\t\t\t\tnewText.append(formatter.highlightTerm(tokenText, token.termText(), score, startOffset));\n\t\t\t\t\n\n\t\t\t\tif(textFragmenter.isNewFragment(token))\n\t\t\t\t{\n\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\tcurrentFrag =new TextFragment(newText.length(), docFrags.size());\n\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t}\n\n\t\t\t\tlastEndOffset = endOffset;\n\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\n\n\t\t\t// append text after end of last token\n\t\t\tif (lastEndOffset < text.length())\n\t\t\t\tnewText.append(text.substring(lastEndOffset));\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tint minScore = 0;\n\t\t\tfor (Iterator i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = (TextFragment) i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\t\t\t\t\t\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insert(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = (TextFragment) fragQueue.pop();\n\t\t\t}\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e36e216d6ddce8d71e813a8b0d29d42d4c5025b9":["e3847974308b7ee4bf1bd64ab343a6758eb9ccb1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e36e216d6ddce8d71e813a8b0d29d42d4c5025b9"],"e3847974308b7ee4bf1bd64ab343a6758eb9ccb1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e3847974308b7ee4bf1bd64ab343a6758eb9ccb1"],"e36e216d6ddce8d71e813a8b0d29d42d4c5025b9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e3847974308b7ee4bf1bd64ab343a6758eb9ccb1":["e36e216d6ddce8d71e813a8b0d29d42d4c5025b9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}