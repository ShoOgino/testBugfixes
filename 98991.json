{"path":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","commits":[{"id":"d6a3823714ed5de938fb4f3fc814824fe0f95e1a","date":1413422458,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","sourceNew":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if its a \"fixed-length\" terms dict\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      numValues++;\n    }\n    if (minLength == maxLength) {\n      // no index needed: direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene410DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but its not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","sourceOld":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if its a \"fixed-length\" terms dict\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      numValues++;\n    }\n    if (minLength == maxLength) {\n      // no index needed: direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene410DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but its not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db68c63cbfaa8698b9c4475f75ed2b9c9696d238","date":1414118621,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","sourceNew":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if its a \"fixed-length\" terms dict\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      numValues++;\n    }\n    if (minLength == maxLength) {\n      // no index needed: direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene410DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but its not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","sourceOld":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if its a \"fixed-length\" terms dict\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      numValues++;\n    }\n    if (minLength == maxLength) {\n      // no index needed: direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene410DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but its not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71387d8cb6923eb831b17a8b734608ba2e21c653","date":1414126093,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","sourceNew":null,"sourceOld":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if its a \"fixed-length\" terms dict\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      numValues++;\n    }\n    if (minLength == maxLength) {\n      // no index needed: direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene410DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but its not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d6a3823714ed5de938fb4f3fc814824fe0f95e1a"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71387d8cb6923eb831b17a8b734608ba2e21c653"],"d6a3823714ed5de938fb4f3fc814824fe0f95e1a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238","d6a3823714ed5de938fb4f3fc814824fe0f95e1a"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["71387d8cb6923eb831b17a8b734608ba2e21c653"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d6a3823714ed5de938fb4f3fc814824fe0f95e1a":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}