{"path":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","commits":[{"id":"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a","date":1341524239,"type":1,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    try {\n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":1,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    try {\n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ac7fa87956e618e2e88572544ae87078647f6351","date":1355482487,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    try {\n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    try {\n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"978de4e2d23054c6624dd5928ddeb734dca68eec","date":1370592803,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7af110b00ea8df9429309d83e38e0533d82e144f","date":1376924768,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort(filesToDelete);\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31d4861802ca404d78ca1d15f4550eec415b9199","date":1376947894,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort(filesToDelete);\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort(filesToDelete);\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segmentInfo == null) {\n      initSegmentInfo();\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort(filesToDelete);\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort(filesToDelete);\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52c7e49be259508735752fba88085255014a6ecf","date":1398706273,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort(filesToDelete);\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort(filesToDelete);\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort(filesToDelete);\n          }\n        }\n\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6b7c6630218ed9693cdb8643276513f9f0043f4","date":1406648084,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4","date":1414017220,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9299079153fd7895bf3cf6835cf7019af2ba89b3","date":1417813477,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n            if (!aborting) {\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort(filesToDelete);\n            }\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborting) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2","date":1424979404,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98a04f56464afdffd4c430d6c47a0c868a38354e","date":1424985833,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":["d6b7c6630218ed9693cdb8643276513f9f0043f4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-IndexDocument],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(Iterable<? extends IndexableField> doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends IndexDocument> docs, Analyzer analyzer, Term delTerm) throws IOException, AbortingException {\n    testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segmentInfo.name);\n    }\n    int docCount = 0;\n    boolean allDocsIndexed = false;\n    try {\n      \n      for(IndexDocument doc : docs) {\n        // Even on exception, the document is still added (but marked\n        // deleted), so we don't need to un-reserve at that point.\n        // Aborting exceptions will actually \"lose\" more than one\n        // document, so the counter will be \"wrong\" in that case, but\n        // it's very hard to fix (we can't easily distinguish aborting\n        // vs non-aborting exceptions):\n        reserveOneDoc();\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            // Incr here because finishDocument will not\n            // be called (because an exc is being thrown):\n            numDocsInRAM++;\n          }\n        }\n        finishDocument(null);\n      }\n      allDocsIndexed = true;\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingUpdates, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      if (!allDocsIndexed && !aborted) {\n        // the iterator threw an exception that is not aborting \n        // go and mark all docs from this block as deleted\n        int docID = numDocsInRAM-1;\n        final int endDocID = docID - docCount;\n        while (docID > endDocID) {\n          deleteDocID(docID);\n          docID--;\n        }\n      }\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["1d028314cced5858683a1bb4741423d0f934257b","ac7fa87956e618e2e88572544ae87078647f6351"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["7af110b00ea8df9429309d83e38e0533d82e144f"],"7af110b00ea8df9429309d83e38e0533d82e144f":["978de4e2d23054c6624dd5928ddeb734dca68eec"],"978de4e2d23054c6624dd5928ddeb734dca68eec":["ac7fa87956e618e2e88572544ae87078647f6351"],"98a04f56464afdffd4c430d6c47a0c868a38354e":["9299079153fd7895bf3cf6835cf7019af2ba89b3","acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2"],"9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4":["d6b7c6630218ed9693cdb8643276513f9f0043f4"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","3394716f52b34ab259ad5247e7595d9f9db6e935"],"ac7fa87956e618e2e88572544ae87078647f6351":["1d028314cced5858683a1bb4741423d0f934257b"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["978de4e2d23054c6624dd5928ddeb734dca68eec","7af110b00ea8df9429309d83e38e0533d82e144f"],"31d4861802ca404d78ca1d15f4550eec415b9199":["978de4e2d23054c6624dd5928ddeb734dca68eec","7af110b00ea8df9429309d83e38e0533d82e144f"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","52c7e49be259508735752fba88085255014a6ecf"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["9299079153fd7895bf3cf6835cf7019af2ba89b3","98a04f56464afdffd4c430d6c47a0c868a38354e"],"1d028314cced5858683a1bb4741423d0f934257b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a"],"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2":["9299079153fd7895bf3cf6835cf7019af2ba89b3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d6b7c6630218ed9693cdb8643276513f9f0043f4":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"9299079153fd7895bf3cf6835cf7019af2ba89b3":["9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4"],"52c7e49be259508735752fba88085255014a6ecf":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["98a04f56464afdffd4c430d6c47a0c868a38354e"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3394716f52b34ab259ad5247e7595d9f9db6e935","52c7e49be259508735752fba88085255014a6ecf"],"7af110b00ea8df9429309d83e38e0533d82e144f":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199"],"978de4e2d23054c6624dd5928ddeb734dca68eec":["7af110b00ea8df9429309d83e38e0533d82e144f","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199"],"98a04f56464afdffd4c430d6c47a0c868a38354e":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4":["9299079153fd7895bf3cf6835cf7019af2ba89b3"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"ac7fa87956e618e2e88572544ae87078647f6351":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","978de4e2d23054c6624dd5928ddeb734dca68eec"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"31d4861802ca404d78ca1d15f4550eec415b9199":[],"3394716f52b34ab259ad5247e7595d9f9db6e935":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","d6b7c6630218ed9693cdb8643276513f9f0043f4"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"1d028314cced5858683a1bb4741423d0f934257b":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","ac7fa87956e618e2e88572544ae87078647f6351"],"33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a":["1d028314cced5858683a1bb4741423d0f934257b"],"acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1d028314cced5858683a1bb4741423d0f934257b","33e9fa3b49f4a365a04fdfc8a32dbcd0df798f5a"],"d6b7c6630218ed9693cdb8643276513f9f0043f4":["9cdbc2cadeaf282528fe4d1c06e9f8bee38ccec4"],"9299079153fd7895bf3cf6835cf7019af2ba89b3":["98a04f56464afdffd4c430d6c47a0c868a38354e","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2"],"52c7e49be259508735752fba88085255014a6ecf":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}