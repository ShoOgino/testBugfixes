{"path":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorUIF,CountSlotAcc).mjava","commits":[{"id":"48a04370d92de1fba80afce42dd014d5a1e3aa52","date":1431204655,"type":1,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorUIF,CountSlotAcc).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCountsInArray(FacetFieldProcessorUIF,int[]).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n  }\n\n","sourceOld":"  private void getCountsInArray(FacetFieldProcessorUIF processor, int[] counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts[tt.termNum] = searcher.numDocs(tt.termQuery, docs);\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n        counts[i] = maxTermCounts[i] - counts[i];\n      }\n    }\n\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts[i];\n      }\n      counts[processor.allBucketsSlot] = all;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9e13d0d4d8b6dc352cb304974502b9a36c153f78","date":1436492687,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorUIF,CountSlotAcc).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorUIF,CountSlotAcc).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":"  private void getCounts(FacetFieldProcessorUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"79759974460bc59933cd169acc94f5c6b16368d5","date":1471318443,"type":5,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorUIF,CountSlotAcc).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":"  private void getCounts(FacetFieldProcessorUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorUIF,CountSlotAcc).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":"  private void getCounts(FacetFieldProcessorUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorByArrayUIF,CountSlotAcc).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorUIF,CountSlotAcc).mjava","sourceNew":"  private void getCounts(FacetFieldProcessorByArrayUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","sourceOld":"  private void getCounts(FacetFieldProcessorUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#getCounts(FacetFieldProcessorUIF,CountSlotAcc).mjava","sourceNew":null,"sourceOld":"  private void getCounts(FacetFieldProcessorUIF processor, CountSlotAcc counts) throws IOException {\n    DocSet docs = processor.fcontext.base;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    // what about allBuckets?\n    if (baseSize < processor.effectiveMincount) {\n      return;\n    }\n\n    final int[] index = this.index;\n\n    boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0 && docs instanceof BitDocSet;\n\n    if (doNegative) {\n      FixedBitSet bs = ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorting by index order\n      counts.incrementCount(tt.termNum, searcher.numDocs(tt.termQuery, docs));\n    }\n\n    // TODO: we could short-circuit counting altogether for sorted faceting\n    // where we already have enough terms from the bigTerms\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ; ) {\n            int delta = 0;\n            for (; ; ) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts.incrementCount(tnum,1);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ; ) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts.incrementCount(tnum,1);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    if (doNegative) {\n      for (int i=0; i<numTermsInField; i++) {\n //       counts[i] = maxTermCounts[i] - counts[i];\n        counts.incrementCount(i, maxTermCounts[i] - counts.getCount(i)*2);\n      }\n    }\n\n    /*** TODO - future optimization to handle allBuckets\n    if (processor.allBucketsSlot >= 0) {\n      int all = 0;  // overflow potential\n      for (int i=0; i<numTermsInField; i++) {\n        all += counts.getCount(i);\n      }\n      counts.incrementCount(processor.allBucketsSlot, all);\n    }\n     ***/\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"403d05f7f8d69b65659157eff1bc1d2717f04c66":["9e13d0d4d8b6dc352cb304974502b9a36c153f78","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"9e13d0d4d8b6dc352cb304974502b9a36c153f78":["48a04370d92de1fba80afce42dd014d5a1e3aa52"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["9e13d0d4d8b6dc352cb304974502b9a36c153f78","79759974460bc59933cd169acc94f5c6b16368d5"],"48a04370d92de1fba80afce42dd014d5a1e3aa52":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"79759974460bc59933cd169acc94f5c6b16368d5":["9e13d0d4d8b6dc352cb304974502b9a36c153f78"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["9e13d0d4d8b6dc352cb304974502b9a36c153f78","403d05f7f8d69b65659157eff1bc1d2717f04c66"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["403d05f7f8d69b65659157eff1bc1d2717f04c66"]},"commit2Childs":{"403d05f7f8d69b65659157eff1bc1d2717f04c66":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9e13d0d4d8b6dc352cb304974502b9a36c153f78":["403d05f7f8d69b65659157eff1bc1d2717f04c66","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","79759974460bc59933cd169acc94f5c6b16368d5","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["48a04370d92de1fba80afce42dd014d5a1e3aa52"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"48a04370d92de1fba80afce42dd014d5a1e3aa52":["9e13d0d4d8b6dc352cb304974502b9a36c153f78"],"79759974460bc59933cd169acc94f5c6b16368d5":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}