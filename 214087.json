{"path":"lucene/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat#testChangeCodecAndMerge().mjava","commits":[{"id":"a0ae5e3ed1232483b7b8a014f175a5fe43595982","date":1324062192,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat#testChangeCodecAndMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/codecs/perfield/TestPerFieldPostingsFormat#testChangeCodecAndMerge().mjava","sourceNew":"  /*\n   * Test that heterogeneous index segments are merged sucessfully\n   */\n  // TODO: not sure this test is that great, we should probably peek inside PerFieldPostingsFormat or something?!\n  @Test\n  public void testChangeCodecAndMerge() throws IOException {\n    Directory dir = newDirectory();\n    if (VERBOSE) {\n      System.out.println(\"TEST: make new index\");\n    }\n    IndexWriterConfig iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT,\n             new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE).setCodec(new MockCodec());\n    iwconf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);\n    //((LogMergePolicy) iwconf.getMergePolicy()).setMergeFactor(10);\n    IndexWriter writer = newWriter(dir, iwconf);\n\n    addDocs(writer, 10);\n    writer.commit();\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n    if (VERBOSE) {\n      System.out.println(\"TEST: addDocs3\");\n    }\n    addDocs3(writer, 10);\n    writer.commit();\n    writer.close();\n\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n    Lucene40Codec codec = (Lucene40Codec)iwconf.getCodec();\n\n    iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setOpenMode(OpenMode.APPEND).setCodec(codec);\n    //((LogMergePolicy) iwconf.getMergePolicy()).setUseCompoundFile(false);\n    //((LogMergePolicy) iwconf.getMergePolicy()).setMergeFactor(10);\n    iwconf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);\n\n    iwconf.setCodec(new MockCodec2()); // uses standard for field content\n    writer = newWriter(dir, iwconf);\n    // swap in new codec for currently written segments\n    if (VERBOSE) {\n      System.out.println(\"TEST: add docs w/ Standard codec for content field\");\n    }\n    addDocs2(writer, 10);\n    writer.commit();\n    codec = (Lucene40Codec)iwconf.getCodec();\n    assertEquals(30, writer.maxDoc());\n    assertQuery(new Term(\"content\", \"bbb\"), dir, 10);\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);   ////\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: add more docs w/ new codec\");\n    }\n    addDocs2(writer, 10);\n    writer.commit();\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);\n    assertQuery(new Term(\"content\", \"bbb\"), dir, 20);\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n    assertEquals(40, writer.maxDoc());\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: now optimize\");\n    }\n    writer.forceMerge(1);\n    assertEquals(40, writer.maxDoc());\n    writer.close();\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);\n    assertQuery(new Term(\"content\", \"bbb\"), dir, 20);\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Test that heterogeneous index segments are merged sucessfully\n   */\n  // TODO: not sure this test is that great, we should probably peek inside PerFieldPostingsFormat or something?!\n  @Test\n  public void testChangeCodecAndMerge() throws IOException {\n    Directory dir = newDirectory();\n    if (VERBOSE) {\n      System.out.println(\"TEST: make new index\");\n    }\n    IndexWriterConfig iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT,\n             new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE).setCodec(new MockCodec());\n    iwconf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);\n    //((LogMergePolicy) iwconf.getMergePolicy()).setMergeFactor(10);\n    IndexWriter writer = newWriter(dir, iwconf);\n\n    addDocs(writer, 10);\n    writer.commit();\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n    if (VERBOSE) {\n      System.out.println(\"TEST: addDocs3\");\n    }\n    addDocs3(writer, 10);\n    writer.commit();\n    writer.close();\n\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n    Lucene40Codec codec = (Lucene40Codec)iwconf.getCodec();\n\n    iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setOpenMode(OpenMode.APPEND).setCodec(codec);\n    //((LogMergePolicy) iwconf.getMergePolicy()).setUseCompoundFile(false);\n    //((LogMergePolicy) iwconf.getMergePolicy()).setMergeFactor(10);\n    iwconf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);\n\n    iwconf.setCodec(new MockCodec2()); // uses standard for field content\n    writer = newWriter(dir, iwconf);\n    // swap in new codec for currently written segments\n    if (VERBOSE) {\n      System.out.println(\"TEST: add docs w/ Standard codec for content field\");\n    }\n    addDocs2(writer, 10);\n    writer.commit();\n    codec = (Lucene40Codec)iwconf.getCodec();\n    assertEquals(30, writer.maxDoc());\n    assertQuery(new Term(\"content\", \"bbb\"), dir, 10);\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);   ////\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: add more docs w/ new codec\");\n    }\n    addDocs2(writer, 10);\n    writer.commit();\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);\n    assertQuery(new Term(\"content\", \"bbb\"), dir, 20);\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n    assertEquals(40, writer.maxDoc());\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: now optimize\");\n    }\n    writer.forceMerge(1);\n    assertEquals(40, writer.maxDoc());\n    writer.close();\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);\n    assertQuery(new Term(\"content\", \"bbb\"), dir, 20);\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat#testChangeCodecAndMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat#testChangeCodecAndMerge().mjava","sourceNew":"  /*\n   * Test that heterogeneous index segments are merged sucessfully\n   */\n  // TODO: not sure this test is that great, we should probably peek inside PerFieldPostingsFormat or something?!\n  @Test\n  public void testChangeCodecAndMerge() throws IOException {\n    Directory dir = newDirectory();\n    if (VERBOSE) {\n      System.out.println(\"TEST: make new index\");\n    }\n    IndexWriterConfig iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT,\n             new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE).setCodec(new MockCodec());\n    iwconf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);\n    //((LogMergePolicy) iwconf.getMergePolicy()).setMergeFactor(10);\n    IndexWriter writer = newWriter(dir, iwconf);\n\n    addDocs(writer, 10);\n    writer.commit();\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n    if (VERBOSE) {\n      System.out.println(\"TEST: addDocs3\");\n    }\n    addDocs3(writer, 10);\n    writer.commit();\n    writer.close();\n\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n    Lucene40Codec codec = (Lucene40Codec)iwconf.getCodec();\n\n    iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setOpenMode(OpenMode.APPEND).setCodec(codec);\n    //((LogMergePolicy) iwconf.getMergePolicy()).setUseCompoundFile(false);\n    //((LogMergePolicy) iwconf.getMergePolicy()).setMergeFactor(10);\n    iwconf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);\n\n    iwconf.setCodec(new MockCodec2()); // uses standard for field content\n    writer = newWriter(dir, iwconf);\n    // swap in new codec for currently written segments\n    if (VERBOSE) {\n      System.out.println(\"TEST: add docs w/ Standard codec for content field\");\n    }\n    addDocs2(writer, 10);\n    writer.commit();\n    codec = (Lucene40Codec)iwconf.getCodec();\n    assertEquals(30, writer.maxDoc());\n    assertQuery(new Term(\"content\", \"bbb\"), dir, 10);\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);   ////\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: add more docs w/ new codec\");\n    }\n    addDocs2(writer, 10);\n    writer.commit();\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);\n    assertQuery(new Term(\"content\", \"bbb\"), dir, 20);\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n    assertEquals(40, writer.maxDoc());\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: now optimize\");\n    }\n    writer.forceMerge(1);\n    assertEquals(40, writer.maxDoc());\n    writer.close();\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);\n    assertQuery(new Term(\"content\", \"bbb\"), dir, 20);\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Test that heterogeneous index segments are merged sucessfully\n   */\n  // TODO: not sure this test is that great, we should probably peek inside PerFieldPostingsFormat or something?!\n  @Test\n  public void testChangeCodecAndMerge() throws IOException {\n    Directory dir = newDirectory();\n    if (VERBOSE) {\n      System.out.println(\"TEST: make new index\");\n    }\n    IndexWriterConfig iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT,\n             new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE).setCodec(new MockCodec());\n    iwconf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);\n    //((LogMergePolicy) iwconf.getMergePolicy()).setMergeFactor(10);\n    IndexWriter writer = newWriter(dir, iwconf);\n\n    addDocs(writer, 10);\n    writer.commit();\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n    if (VERBOSE) {\n      System.out.println(\"TEST: addDocs3\");\n    }\n    addDocs3(writer, 10);\n    writer.commit();\n    writer.close();\n\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n    Lucene40Codec codec = (Lucene40Codec)iwconf.getCodec();\n\n    iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setOpenMode(OpenMode.APPEND).setCodec(codec);\n    //((LogMergePolicy) iwconf.getMergePolicy()).setUseCompoundFile(false);\n    //((LogMergePolicy) iwconf.getMergePolicy()).setMergeFactor(10);\n    iwconf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);\n\n    iwconf.setCodec(new MockCodec2()); // uses standard for field content\n    writer = newWriter(dir, iwconf);\n    // swap in new codec for currently written segments\n    if (VERBOSE) {\n      System.out.println(\"TEST: add docs w/ Standard codec for content field\");\n    }\n    addDocs2(writer, 10);\n    writer.commit();\n    codec = (Lucene40Codec)iwconf.getCodec();\n    assertEquals(30, writer.maxDoc());\n    assertQuery(new Term(\"content\", \"bbb\"), dir, 10);\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);   ////\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: add more docs w/ new codec\");\n    }\n    addDocs2(writer, 10);\n    writer.commit();\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);\n    assertQuery(new Term(\"content\", \"bbb\"), dir, 20);\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n    assertEquals(40, writer.maxDoc());\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: now optimize\");\n    }\n    writer.forceMerge(1);\n    assertEquals(40, writer.maxDoc());\n    writer.close();\n    assertQuery(new Term(\"content\", \"ccc\"), dir, 10);\n    assertQuery(new Term(\"content\", \"bbb\"), dir, 20);\n    assertQuery(new Term(\"content\", \"aaa\"), dir, 10);\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}