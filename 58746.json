{"path":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized()) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized()) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"af0757679472670141514cb791eafeed05abc4e5","date":1292883496,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized()) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized()) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ecc11368dc265bfdad90214f8bf5da99016ab1e2","date":1294144090,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":null,"sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized()) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":5,"author":"Michael Busch","isMerge":true,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","pathOld":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTokensTask#doLogic().mjava","sourceNew":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","sourceOld":"  @Override\n  public int doLogic() throws Exception {\n    List<Fieldable> fields = doc.getFields();\n    Analyzer analyzer = getRunData().getAnalyzer();\n    int tokenCount = 0;\n    for(final Fieldable field : fields) {\n      if (!field.isTokenized() || field instanceof NumericField) continue;\n      \n      final TokenStream stream;\n      final TokenStream streamValue = field.tokenStreamValue();\n\n      if (streamValue != null) \n        stream = streamValue;\n      else {\n        // the field does not have a TokenStream,\n        // so we have to obtain one from the analyzer\n        final Reader reader;\t\t\t  // find or make Reader\n        final Reader readerValue = field.readerValue();\n\n        if (readerValue != null)\n          reader = readerValue;\n        else {\n          String stringValue = field.stringValue();\n          if (stringValue == null)\n            throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n          stringReader.init(stringValue);\n          reader = stringReader;\n        }\n        \n        // Tokenize field\n        stream = analyzer.reusableTokenStream(field.name(), reader);\n      }\n\n      // reset the TokenStream to the first token\n      stream.reset();\n\n      while(stream.incrementToken())\n        tokenCount++;\n    }\n    totalTokenCount += tokenCount;\n    return tokenCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70ad682703b8585f5d0a637efec044d57ec05efb":["9454a6510e2db155fb01faa5c049b06ece95fab9","ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"ecc11368dc265bfdad90214f8bf5da99016ab1e2":["af0757679472670141514cb791eafeed05abc4e5"],"af0757679472670141514cb791eafeed05abc4e5":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["9454a6510e2db155fb01faa5c049b06ece95fab9","af0757679472670141514cb791eafeed05abc4e5"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"70ad682703b8585f5d0a637efec044d57ec05efb":[],"ecc11368dc265bfdad90214f8bf5da99016ab1e2":["70ad682703b8585f5d0a637efec044d57ec05efb","868da859b43505d9d2a023bfeae6dd0c795f5295","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"af0757679472670141514cb791eafeed05abc4e5":["ecc11368dc265bfdad90214f8bf5da99016ab1e2","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"868da859b43505d9d2a023bfeae6dd0c795f5295":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["70ad682703b8585f5d0a637efec044d57ec05efb","af0757679472670141514cb791eafeed05abc4e5","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["70ad682703b8585f5d0a637efec044d57ec05efb","868da859b43505d9d2a023bfeae6dd0c795f5295","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}