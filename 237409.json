{"path":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogram(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","commits":[{"id":"a05f3f5161c62339ec5560b8f6958f3df8483618","date":1563550501,"type":0,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogram(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","pathOld":"/dev/null","sourceNew":"  // Returns a list of range counts sorted by the range lower bound\n  static Collection<RangeCount> getHashHistogram(SolrIndexSearcher searcher, String prefixField, DocRouter router, DocCollection collection) throws IOException {\n    TreeMap<DocRouter.Range,RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), prefixField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    TermsEnum termsEnum = terms.iterator();\n    for (;;) {\n      BytesRef term = termsEnum.next();\n      if (term == null) break;\n\n      String termStr = term.utf8ToString();\n      int firstSep = termStr.indexOf(CompositeIdRouter.SEPARATOR);\n      // truncate to first separator since we don't support multiple levels currently\n      if (firstSep != termStr.length()-1 && firstSep > 0) {\n        termStr = termStr.substring(0, firstSep+1);\n      }\n      DocRouter.Range range = router.getSearchRangeSingle(termStr, null, collection);\n      int numDocs = termsEnum.docFreq();\n\n      RangeCount rangeCount = new RangeCount(range, numDocs);\n\n      RangeCount prev = counts.put(rangeCount.range, rangeCount);\n      if (prev != null) {\n        // we hit a hash collision or truncated a prefix to first level, so add the buckets together.\n        rangeCount.count += prev.count;\n      }\n    }\n\n    return counts.values();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff2097f3a1298209d513f906d65f86bc0ab8d9c0","date":1564851155,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogram(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogram(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","sourceNew":"  // Returns a list of range counts sorted by the range lower bound\n  static Collection<RangeCount> getHashHistogram(SolrIndexSearcher searcher, String prefixField, DocRouter router, DocCollection collection) throws IOException {\n    RTimer timer = new RTimer();\n    TreeMap<DocRouter.Range,RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), prefixField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    int numPrefixes = 0;\n    int numTriLevel = 0;\n    int numCollisions = 0;\n    long sumBuckets = 0;\n\n    TermsEnum termsEnum = terms.iterator();\n    for (;;) {\n      BytesRef term = termsEnum.next();\n      if (term == null) break;\n      numPrefixes++;\n\n      String termStr = term.utf8ToString();\n      int firstSep = termStr.indexOf(CompositeIdRouter.SEPARATOR);\n      // truncate to first separator since we don't support multiple levels currently\n      // NOTE: this does not currently work for tri-level composite ids since the number of bits allocated to the first ID is 16 for a 2 part id\n      // and 8 for a 3 part id!\n      if (firstSep != termStr.length()-1 && firstSep > 0) {\n        numTriLevel++;\n        termStr = termStr.substring(0, firstSep+1);\n      }\n\n      DocRouter.Range range = router.getSearchRangeSingle(termStr, null, collection);\n      int numDocs = termsEnum.docFreq();\n      sumBuckets += numDocs;\n\n      RangeCount rangeCount = new RangeCount(range, numDocs);\n\n      RangeCount prev = counts.put(rangeCount.range, rangeCount);\n      if (prev != null) {\n        // we hit a hash collision or truncated a prefix to first level, so add the buckets together.\n        rangeCount.count += prev.count;\n        numCollisions++;\n      }\n    }\n\n    log.info(\"Split histogram: ms={}, numBuckets={} sumBuckets={} numPrefixes={} numTriLevel={} numCollisions={}\", timer.getTime(), counts.size(), sumBuckets, numPrefixes, numTriLevel, numCollisions);\n\n    return counts.values();\n  }\n\n","sourceOld":"  // Returns a list of range counts sorted by the range lower bound\n  static Collection<RangeCount> getHashHistogram(SolrIndexSearcher searcher, String prefixField, DocRouter router, DocCollection collection) throws IOException {\n    TreeMap<DocRouter.Range,RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), prefixField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    TermsEnum termsEnum = terms.iterator();\n    for (;;) {\n      BytesRef term = termsEnum.next();\n      if (term == null) break;\n\n      String termStr = term.utf8ToString();\n      int firstSep = termStr.indexOf(CompositeIdRouter.SEPARATOR);\n      // truncate to first separator since we don't support multiple levels currently\n      if (firstSep != termStr.length()-1 && firstSep > 0) {\n        termStr = termStr.substring(0, firstSep+1);\n      }\n      DocRouter.Range range = router.getSearchRangeSingle(termStr, null, collection);\n      int numDocs = termsEnum.docFreq();\n\n      RangeCount rangeCount = new RangeCount(range, numDocs);\n\n      RangeCount prev = counts.put(rangeCount.range, rangeCount);\n      if (prev != null) {\n        // we hit a hash collision or truncated a prefix to first level, so add the buckets together.\n        rangeCount.count += prev.count;\n      }\n    }\n\n    return counts.values();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8061ddd97f3352007d927dae445884a6f3d857b","date":1564988276,"type":3,"author":"Atri Sharma","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogram(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogram(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","sourceNew":"  // Returns a list of range counts sorted by the range lower bound\n  static Collection<RangeCount> getHashHistogram(SolrIndexSearcher searcher, String prefixField, DocRouter router, DocCollection collection) throws IOException {\n    RTimer timer = new RTimer();\n    TreeMap<DocRouter.Range,RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), prefixField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    int numPrefixes = 0;\n    int numTriLevel = 0;\n    int numCollisions = 0;\n    long sumBuckets = 0;\n\n    TermsEnum termsEnum = terms.iterator();\n    for (;;) {\n      BytesRef term = termsEnum.next();\n      if (term == null) break;\n      numPrefixes++;\n\n      String termStr = term.utf8ToString();\n      int firstSep = termStr.indexOf(CompositeIdRouter.SEPARATOR);\n      // truncate to first separator since we don't support multiple levels currently\n      // NOTE: this does not currently work for tri-level composite ids since the number of bits allocated to the first ID is 16 for a 2 part id\n      // and 8 for a 3 part id!\n      if (firstSep != termStr.length()-1 && firstSep > 0) {\n        numTriLevel++;\n        termStr = termStr.substring(0, firstSep+1);\n      }\n\n      DocRouter.Range range = router.getSearchRangeSingle(termStr, null, collection);\n      int numDocs = termsEnum.docFreq();\n      sumBuckets += numDocs;\n\n      RangeCount rangeCount = new RangeCount(range, numDocs);\n\n      RangeCount prev = counts.put(rangeCount.range, rangeCount);\n      if (prev != null) {\n        // we hit a hash collision or truncated a prefix to first level, so add the buckets together.\n        rangeCount.count += prev.count;\n        numCollisions++;\n      }\n    }\n\n    log.info(\"Split histogram: ms={}, numBuckets={} sumBuckets={} numPrefixes={} numTriLevel={} numCollisions={}\", timer.getTime(), counts.size(), sumBuckets, numPrefixes, numTriLevel, numCollisions);\n\n    return counts.values();\n  }\n\n","sourceOld":"  // Returns a list of range counts sorted by the range lower bound\n  static Collection<RangeCount> getHashHistogram(SolrIndexSearcher searcher, String prefixField, DocRouter router, DocCollection collection) throws IOException {\n    TreeMap<DocRouter.Range,RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), prefixField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    TermsEnum termsEnum = terms.iterator();\n    for (;;) {\n      BytesRef term = termsEnum.next();\n      if (term == null) break;\n\n      String termStr = term.utf8ToString();\n      int firstSep = termStr.indexOf(CompositeIdRouter.SEPARATOR);\n      // truncate to first separator since we don't support multiple levels currently\n      if (firstSep != termStr.length()-1 && firstSep > 0) {\n        termStr = termStr.substring(0, firstSep+1);\n      }\n      DocRouter.Range range = router.getSearchRangeSingle(termStr, null, collection);\n      int numDocs = termsEnum.docFreq();\n\n      RangeCount rangeCount = new RangeCount(range, numDocs);\n\n      RangeCount prev = counts.put(rangeCount.range, rangeCount);\n      if (prev != null) {\n        // we hit a hash collision or truncated a prefix to first level, so add the buckets together.\n        rangeCount.count += prev.count;\n      }\n    }\n\n    return counts.values();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d8daa7a1d5d0c033d73962d5ca3bf3f9c9687693","date":1565097295,"type":3,"author":"Jan HÃ¸ydahl","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogram(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogram(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","sourceNew":"  // Returns a list of range counts sorted by the range lower bound\n  static Collection<RangeCount> getHashHistogram(SolrIndexSearcher searcher, String prefixField, DocRouter router, DocCollection collection) throws IOException {\n    RTimer timer = new RTimer();\n    TreeMap<DocRouter.Range,RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), prefixField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    int numPrefixes = 0;\n    int numTriLevel = 0;\n    int numCollisions = 0;\n    long sumBuckets = 0;\n\n    TermsEnum termsEnum = terms.iterator();\n    for (;;) {\n      BytesRef term = termsEnum.next();\n      if (term == null) break;\n      numPrefixes++;\n\n      String termStr = term.utf8ToString();\n      int firstSep = termStr.indexOf(CompositeIdRouter.SEPARATOR);\n      // truncate to first separator since we don't support multiple levels currently\n      // NOTE: this does not currently work for tri-level composite ids since the number of bits allocated to the first ID is 16 for a 2 part id\n      // and 8 for a 3 part id!\n      if (firstSep != termStr.length()-1 && firstSep > 0) {\n        numTriLevel++;\n        termStr = termStr.substring(0, firstSep+1);\n      }\n\n      DocRouter.Range range = router.getSearchRangeSingle(termStr, null, collection);\n      int numDocs = termsEnum.docFreq();\n      sumBuckets += numDocs;\n\n      RangeCount rangeCount = new RangeCount(range, numDocs);\n\n      RangeCount prev = counts.put(rangeCount.range, rangeCount);\n      if (prev != null) {\n        // we hit a hash collision or truncated a prefix to first level, so add the buckets together.\n        rangeCount.count += prev.count;\n        numCollisions++;\n      }\n    }\n\n    log.info(\"Split histogram: ms={}, numBuckets={} sumBuckets={} numPrefixes={} numTriLevel={} numCollisions={}\", timer.getTime(), counts.size(), sumBuckets, numPrefixes, numTriLevel, numCollisions);\n\n    return counts.values();\n  }\n\n","sourceOld":"  // Returns a list of range counts sorted by the range lower bound\n  static Collection<RangeCount> getHashHistogram(SolrIndexSearcher searcher, String prefixField, DocRouter router, DocCollection collection) throws IOException {\n    TreeMap<DocRouter.Range,RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), prefixField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    TermsEnum termsEnum = terms.iterator();\n    for (;;) {\n      BytesRef term = termsEnum.next();\n      if (term == null) break;\n\n      String termStr = term.utf8ToString();\n      int firstSep = termStr.indexOf(CompositeIdRouter.SEPARATOR);\n      // truncate to first separator since we don't support multiple levels currently\n      if (firstSep != termStr.length()-1 && firstSep > 0) {\n        termStr = termStr.substring(0, firstSep+1);\n      }\n      DocRouter.Range range = router.getSearchRangeSingle(termStr, null, collection);\n      int numDocs = termsEnum.docFreq();\n\n      RangeCount rangeCount = new RangeCount(range, numDocs);\n\n      RangeCount prev = counts.put(rangeCount.range, rangeCount);\n      if (prev != null) {\n        // we hit a hash collision or truncated a prefix to first level, so add the buckets together.\n        rangeCount.count += prev.count;\n      }\n    }\n\n    return counts.values();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"36bdabc04743acfe0e82c9cf8208b1111b2b193a","date":1565115020,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogram(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogram(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","sourceNew":"  /*\n   * Returns a list of range counts sorted by the range lower bound\n   */\n  static Collection<RangeCount> getHashHistogram(SolrIndexSearcher searcher, String prefixField, DocRouter router, DocCollection collection) throws IOException {\n    RTimer timer = new RTimer();\n    TreeMap<DocRouter.Range,RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), prefixField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    int numPrefixes = 0;\n    int numTriLevel = 0;\n    int numCollisions = 0;\n    long sumBuckets = 0;\n\n    TermsEnum termsEnum = terms.iterator();\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      numPrefixes++;\n\n      String termStr = term.utf8ToString();\n      int firstSep = termStr.indexOf(CompositeIdRouter.SEPARATOR);\n      // truncate to first separator since we don't support multiple levels currently\n      // NOTE: this does not currently work for tri-level composite ids since the number of bits allocated to the first ID is 16 for a 2 part id\n      // and 8 for a 3 part id!\n      if (firstSep != termStr.length()-1 && firstSep > 0) {\n        numTriLevel++;\n        termStr = termStr.substring(0, firstSep+1);\n      }\n\n      DocRouter.Range range = router.getSearchRangeSingle(termStr, null, collection);\n      int numDocs = termsEnum.docFreq();\n      sumBuckets += numDocs;\n\n      RangeCount rangeCount = new RangeCount(range, numDocs);\n\n      RangeCount prev = counts.put(rangeCount.range, rangeCount);\n      if (prev != null) {\n        // we hit a hash collision or truncated a prefix to first level, so add the buckets together.\n        rangeCount.count += prev.count;\n        numCollisions++;\n      }\n    }\n\n    log.info(\"Split histogram: ms={}, numBuckets={} sumBuckets={} numPrefixes={} numTriLevel={} numCollisions={}\", timer.getTime(), counts.size(), sumBuckets, numPrefixes, numTriLevel, numCollisions);\n\n    return counts.values();\n  }\n\n","sourceOld":"  // Returns a list of range counts sorted by the range lower bound\n  static Collection<RangeCount> getHashHistogram(SolrIndexSearcher searcher, String prefixField, DocRouter router, DocCollection collection) throws IOException {\n    RTimer timer = new RTimer();\n    TreeMap<DocRouter.Range,RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), prefixField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    int numPrefixes = 0;\n    int numTriLevel = 0;\n    int numCollisions = 0;\n    long sumBuckets = 0;\n\n    TermsEnum termsEnum = terms.iterator();\n    for (;;) {\n      BytesRef term = termsEnum.next();\n      if (term == null) break;\n      numPrefixes++;\n\n      String termStr = term.utf8ToString();\n      int firstSep = termStr.indexOf(CompositeIdRouter.SEPARATOR);\n      // truncate to first separator since we don't support multiple levels currently\n      // NOTE: this does not currently work for tri-level composite ids since the number of bits allocated to the first ID is 16 for a 2 part id\n      // and 8 for a 3 part id!\n      if (firstSep != termStr.length()-1 && firstSep > 0) {\n        numTriLevel++;\n        termStr = termStr.substring(0, firstSep+1);\n      }\n\n      DocRouter.Range range = router.getSearchRangeSingle(termStr, null, collection);\n      int numDocs = termsEnum.docFreq();\n      sumBuckets += numDocs;\n\n      RangeCount rangeCount = new RangeCount(range, numDocs);\n\n      RangeCount prev = counts.put(rangeCount.range, rangeCount);\n      if (prev != null) {\n        // we hit a hash collision or truncated a prefix to first level, so add the buckets together.\n        rangeCount.count += prev.count;\n        numCollisions++;\n      }\n    }\n\n    log.info(\"Split histogram: ms={}, numBuckets={} sumBuckets={} numPrefixes={} numTriLevel={} numCollisions={}\", timer.getTime(), counts.size(), sumBuckets, numPrefixes, numTriLevel, numCollisions);\n\n    return counts.values();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"575e66bd4b2349209027f6801184da7fc3cba13f","date":1587609169,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogram(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/SplitOp#getHashHistogram(SolrIndexSearcher,String,DocRouter,DocCollection).mjava","sourceNew":"  /*\n   * Returns a list of range counts sorted by the range lower bound\n   */\n  static Collection<RangeCount> getHashHistogram(SolrIndexSearcher searcher, String prefixField, DocRouter router, DocCollection collection) throws IOException {\n    RTimer timer = new RTimer();\n    TreeMap<DocRouter.Range,RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), prefixField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    int numPrefixes = 0;\n    int numTriLevel = 0;\n    int numCollisions = 0;\n    long sumBuckets = 0;\n\n    TermsEnum termsEnum = terms.iterator();\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      numPrefixes++;\n\n      String termStr = term.utf8ToString();\n      int firstSep = termStr.indexOf(CompositeIdRouter.SEPARATOR);\n      // truncate to first separator since we don't support multiple levels currently\n      // NOTE: this does not currently work for tri-level composite ids since the number of bits allocated to the first ID is 16 for a 2 part id\n      // and 8 for a 3 part id!\n      if (firstSep != termStr.length()-1 && firstSep > 0) {\n        numTriLevel++;\n        termStr = termStr.substring(0, firstSep+1);\n      }\n\n      DocRouter.Range range = router.getSearchRangeSingle(termStr, null, collection);\n      int numDocs = termsEnum.docFreq();\n      sumBuckets += numDocs;\n\n      RangeCount rangeCount = new RangeCount(range, numDocs);\n\n      RangeCount prev = counts.put(rangeCount.range, rangeCount);\n      if (prev != null) {\n        // we hit a hash collision or truncated a prefix to first level, so add the buckets together.\n        rangeCount.count += prev.count;\n        numCollisions++;\n      }\n    }\n\n    if (log.isInfoEnabled()) {\n      log.info(\"Split histogram: ms={}, numBuckets={} sumBuckets={} numPrefixes={} numTriLevel={} numCollisions={}\"\n          , timer.getTime(), counts.size(), sumBuckets, numPrefixes, numTriLevel, numCollisions);\n    }\n\n    return counts.values();\n  }\n\n","sourceOld":"  /*\n   * Returns a list of range counts sorted by the range lower bound\n   */\n  static Collection<RangeCount> getHashHistogram(SolrIndexSearcher searcher, String prefixField, DocRouter router, DocCollection collection) throws IOException {\n    RTimer timer = new RTimer();\n    TreeMap<DocRouter.Range,RangeCount> counts = new TreeMap<>();\n\n    Terms terms = MultiTerms.getTerms(searcher.getIndexReader(), prefixField);\n    if (terms == null) {\n      return counts.values();\n    }\n\n    int numPrefixes = 0;\n    int numTriLevel = 0;\n    int numCollisions = 0;\n    long sumBuckets = 0;\n\n    TermsEnum termsEnum = terms.iterator();\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      numPrefixes++;\n\n      String termStr = term.utf8ToString();\n      int firstSep = termStr.indexOf(CompositeIdRouter.SEPARATOR);\n      // truncate to first separator since we don't support multiple levels currently\n      // NOTE: this does not currently work for tri-level composite ids since the number of bits allocated to the first ID is 16 for a 2 part id\n      // and 8 for a 3 part id!\n      if (firstSep != termStr.length()-1 && firstSep > 0) {\n        numTriLevel++;\n        termStr = termStr.substring(0, firstSep+1);\n      }\n\n      DocRouter.Range range = router.getSearchRangeSingle(termStr, null, collection);\n      int numDocs = termsEnum.docFreq();\n      sumBuckets += numDocs;\n\n      RangeCount rangeCount = new RangeCount(range, numDocs);\n\n      RangeCount prev = counts.put(rangeCount.range, rangeCount);\n      if (prev != null) {\n        // we hit a hash collision or truncated a prefix to first level, so add the buckets together.\n        rangeCount.count += prev.count;\n        numCollisions++;\n      }\n    }\n\n    log.info(\"Split histogram: ms={}, numBuckets={} sumBuckets={} numPrefixes={} numTriLevel={} numCollisions={}\", timer.getTime(), counts.size(), sumBuckets, numPrefixes, numTriLevel, numCollisions);\n\n    return counts.values();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ff2097f3a1298209d513f906d65f86bc0ab8d9c0":["a05f3f5161c62339ec5560b8f6958f3df8483618"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"575e66bd4b2349209027f6801184da7fc3cba13f":["36bdabc04743acfe0e82c9cf8208b1111b2b193a"],"a05f3f5161c62339ec5560b8f6958f3df8483618":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d8daa7a1d5d0c033d73962d5ca3bf3f9c9687693":["a05f3f5161c62339ec5560b8f6958f3df8483618","ff2097f3a1298209d513f906d65f86bc0ab8d9c0"],"36bdabc04743acfe0e82c9cf8208b1111b2b193a":["ff2097f3a1298209d513f906d65f86bc0ab8d9c0"],"f8061ddd97f3352007d927dae445884a6f3d857b":["a05f3f5161c62339ec5560b8f6958f3df8483618","ff2097f3a1298209d513f906d65f86bc0ab8d9c0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["575e66bd4b2349209027f6801184da7fc3cba13f"]},"commit2Childs":{"ff2097f3a1298209d513f906d65f86bc0ab8d9c0":["d8daa7a1d5d0c033d73962d5ca3bf3f9c9687693","36bdabc04743acfe0e82c9cf8208b1111b2b193a","f8061ddd97f3352007d927dae445884a6f3d857b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a05f3f5161c62339ec5560b8f6958f3df8483618"],"575e66bd4b2349209027f6801184da7fc3cba13f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a05f3f5161c62339ec5560b8f6958f3df8483618":["ff2097f3a1298209d513f906d65f86bc0ab8d9c0","d8daa7a1d5d0c033d73962d5ca3bf3f9c9687693","f8061ddd97f3352007d927dae445884a6f3d857b"],"d8daa7a1d5d0c033d73962d5ca3bf3f9c9687693":[],"36bdabc04743acfe0e82c9cf8208b1111b2b193a":["575e66bd4b2349209027f6801184da7fc3cba13f"],"f8061ddd97f3352007d927dae445884a6f3d857b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d8daa7a1d5d0c033d73962d5ca3bf3f9c9687693","f8061ddd97f3352007d927dae445884a6f3d857b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}