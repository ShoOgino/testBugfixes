{"path":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(_TestUtil.nextInt(random, 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newField(\"field\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(_TestUtil.nextInt(random, 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newField(\"field\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(_TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newField(\"field\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(_TestUtil.nextInt(random, 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newField(\"field\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(_TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(_TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newField(\"field\", English.intToEnglish(i), TextField.TYPE_STORED));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6e2893fd5349134af382d33ccc3d84840394c6c1","date":1353682567,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(_TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(_TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(_TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(_TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(_TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(_TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(_TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(simplePayloadAnalyzer)\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)\n                                                     .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(simplePayloadAnalyzer)\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(simplePayloadAnalyzer)\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c4f600f812447b5512daeaf8e5c9df5dbcc4a254","date":1428874774,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/spans/TestBasics#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(simplePayloadAnalyzer)\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"6e2893fd5349134af382d33ccc3d84840394c6c1":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["04f07771a2a7dd3a395700665ed839c3dae2def2","6e2893fd5349134af382d33ccc3d84840394c6c1"],"c4f600f812447b5512daeaf8e5c9df5dbcc4a254":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c4f600f812447b5512daeaf8e5c9df5dbcc4a254"]},"commit2Childs":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["6613659748fe4411a7dcf85266e55db1f95f7315"],"6e2893fd5349134af382d33ccc3d84840394c6c1":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["6e2893fd5349134af382d33ccc3d84840394c6c1","d4d69c535930b5cce125cff868d40f6373dc27d4"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"c4f600f812447b5512daeaf8e5c9df5dbcc4a254":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["c4f600f812447b5512daeaf8e5c9df5dbcc4a254"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}