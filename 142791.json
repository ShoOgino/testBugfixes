{"path":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","commits":[{"id":"d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626","date":1339522233,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"/dev/null","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<String>();\n    List<String> types = new ArrayList<String>();\n    List<Integer> positions = new ArrayList<Integer>();\n    List<Integer> positionLengths = new ArrayList<Integer>();\n    List<Integer> startOffsets = new ArrayList<Integer>();\n    List<Integer> endOffsets = new ArrayList<Integer>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["a7c7a5405c388fd86e5962126be8ad09283eb5cc","995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a7c7a5405c388fd86e5962126be8ad09283eb5cc","date":1357256120,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.hasAttribute(CharTermAttribute.class) ? ts.getAttribute(CharTermAttribute.class) : null;\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<String>();\n    List<String> types = new ArrayList<String>();\n    List<Integer> positions = new ArrayList<Integer>();\n    List<Integer> positionLengths = new ArrayList<Integer>();\n    List<Integer> startOffsets = new ArrayList<Integer>();\n    List<Integer> endOffsets = new ArrayList<Integer>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<String>();\n    List<String> types = new ArrayList<String>();\n    List<Integer> positions = new ArrayList<Integer>();\n    List<Integer> positionLengths = new ArrayList<Integer>();\n    List<Integer> startOffsets = new ArrayList<Integer>();\n    List<Integer> endOffsets = new ArrayList<Integer>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":["d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.hasAttribute(CharTermAttribute.class) ? ts.getAttribute(CharTermAttribute.class) : null;\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<String>();\n    List<String> types = new ArrayList<String>();\n    List<Integer> positions = new ArrayList<Integer>();\n    List<Integer> positionLengths = new ArrayList<Integer>();\n    List<Integer> startOffsets = new ArrayList<Integer>();\n    List<Integer> endOffsets = new ArrayList<Integer>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<String>();\n    List<String> types = new ArrayList<String>();\n    List<Integer> positions = new ArrayList<Integer>();\n    List<Integer> positionLengths = new ArrayList<Integer>();\n    List<Integer> startOffsets = new ArrayList<Integer>();\n    List<Integer> endOffsets = new ArrayList<Integer>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.hasAttribute(CharTermAttribute.class) ? ts.getAttribute(CharTermAttribute.class) : null;\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.hasAttribute(CharTermAttribute.class) ? ts.getAttribute(CharTermAttribute.class) : null;\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<String>();\n    List<String> types = new ArrayList<String>();\n    List<Integer> positions = new ArrayList<Integer>();\n    List<Integer> positionLengths = new ArrayList<Integer>();\n    List<Integer> startOffsets = new ArrayList<Integer>();\n    List<Integer> endOffsets = new ArrayList<Integer>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"63241596de245e96a0a3c36c7b03eb92130b81db","date":1398708795,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.hasAttribute(CharTermAttribute.class) ? ts.getAttribute(CharTermAttribute.class) : null;\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.hasAttribute(CharTermAttribute.class) ? ts.getAttribute(CharTermAttribute.class) : null;\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.hasAttribute(CharTermAttribute.class) ? ts.getAttribute(CharTermAttribute.class) : null;\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"50d5b588b112eeb3d6b2a3fcc43a40ef0615a529","date":1419024596,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5ca09cfd0198baeab4d54dc2d866c1d96cf869c6","date":1433805608,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ea4107f60b9f95623c16025c9c247412ff809092","date":1468333987,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n\n    a.normalize(\"dummy\", text);\n    // TODO: what can we do besides testing that the above method does not throw?\n\n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n\n    a.normalize(\"dummy\", text);\n    // TODO: what can we do besides testing that the above method does not throw?\n\n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n    \n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24a98f5fdd23e04f85819dbc63b47a12f7c44311","date":1482439157,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (ise.getMessage().contains(\"end() called in wrong state=\")) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (ise.getMessage().contains(\"end() called in wrong state=\")) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n\n    a.normalize(\"dummy\", text);\n    // TODO: what can we do besides testing that the above method does not throw?\n\n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n\n    a.normalize(\"dummy\", text);\n    // TODO: what can we do besides testing that the above method does not throw?\n\n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (ise.getMessage().contains(\"end() called in wrong state=\")) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (ise.getMessage().contains(\"end() called in wrong state=\")) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n\n    a.normalize(\"dummy\", text);\n    // TODO: what can we do besides testing that the above method does not throw?\n\n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ise.getMessage())) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n\n    a.normalize(\"dummy\", text);\n    // TODO: what can we do besides testing that the above method does not throw?\n\n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"995993f24c9f6feb42b49b71e1982cda8fa0b37c","date":1522116154,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean graphOffsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (ise.getMessage().contains(\"end() called in wrong state=\")) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (ise.getMessage().contains(\"end() called in wrong state=\")) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                graphOffsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                graphOffsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                graphOffsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                graphOffsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                graphOffsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n\n    a.normalize(\"dummy\", text);\n    // TODO: what can we do besides testing that the above method does not throw?\n\n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (ise.getMessage().contains(\"end() called in wrong state=\")) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (ise.getMessage().contains(\"end() called in wrong state=\")) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n\n    a.normalize(\"dummy\", text);\n    // TODO: what can we do besides testing that the above method does not throw?\n\n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":["d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7","date":1522191940,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String,boolean,Field).mjava","sourceNew":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean graphOffsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (ise.getMessage().contains(\"end() called in wrong state=\")) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (ise.getMessage().contains(\"end() called in wrong state=\")) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                graphOffsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                graphOffsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                graphOffsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                graphOffsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                graphOffsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n\n    a.normalize(\"dummy\", text);\n    // TODO: what can we do besides testing that the above method does not throw?\n\n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","sourceOld":"  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute posLengthAtt = ts.getAttribute(PositionLengthAttribute.class);\n    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);\n    List<String> tokens = new ArrayList<>();\n    List<String> types = new ArrayList<>();\n    List<Integer> positions = new ArrayList<>();\n    List<Integer> positionLengths = new ArrayList<>();\n    List<Integer> startOffsets = new ArrayList<>();\n    List<Integer> endOffsets = new ArrayList<>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      assertNotNull(\"has no CharTermAttribute\", termAtt);\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (ise.getMessage().contains(\"end() called in wrong state=\")) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (IllegalStateException ise) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (ise.getMessage().contains(\"end() called in wrong state=\")) {\n              // OK\n            } else {\n              throw ise;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    long seed = random.nextLong();\n    random = new Random(seed);\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length(),\n                                offsetsAreCorrect);\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n\n    a.normalize(\"dummy\", text);\n    // TODO: what can we do besides testing that the above method does not throw?\n\n    if (field != null) {\n      reader = new StringReader(text);\n      random = new Random(seed);\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: indexing using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      field.setReaderValue(useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626","a7c7a5405c388fd86e5962126be8ad09283eb5cc"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["a7c7a5405c388fd86e5962126be8ad09283eb5cc"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","3394716f52b34ab259ad5247e7595d9f9db6e935"],"d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7":["24a98f5fdd23e04f85819dbc63b47a12f7c44311","995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["ea4107f60b9f95623c16025c9c247412ff809092"],"5ca09cfd0198baeab4d54dc2d866c1d96cf869c6":["50d5b588b112eeb3d6b2a3fcc43a40ef0615a529"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["5ca09cfd0198baeab4d54dc2d866c1d96cf869c6","ea4107f60b9f95623c16025c9c247412ff809092"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","24a98f5fdd23e04f85819dbc63b47a12f7c44311"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","63241596de245e96a0a3c36c7b03eb92130b81db"],"a7c7a5405c388fd86e5962126be8ad09283eb5cc":["d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"63241596de245e96a0a3c36c7b03eb92130b81db":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"995993f24c9f6feb42b49b71e1982cda8fa0b37c":["24a98f5fdd23e04f85819dbc63b47a12f7c44311"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"50d5b588b112eeb3d6b2a3fcc43a40ef0615a529":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"ea4107f60b9f95623c16025c9c247412ff809092":["5ca09cfd0198baeab4d54dc2d866c1d96cf869c6"],"d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3394716f52b34ab259ad5247e7595d9f9db6e935","63241596de245e96a0a3c36c7b03eb92130b81db"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7","f03e4bed5023ec3ef93a771b8888cae991cf448d","995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"5ca09cfd0198baeab4d54dc2d866c1d96cf869c6":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","ea4107f60b9f95623c16025c9c247412ff809092"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["f03e4bed5023ec3ef93a771b8888cae991cf448d"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":[],"3394716f52b34ab259ad5247e7595d9f9db6e935":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","50d5b588b112eeb3d6b2a3fcc43a40ef0615a529"],"a7c7a5405c388fd86e5962126be8ad09283eb5cc":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"63241596de245e96a0a3c36c7b03eb92130b81db":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"995993f24c9f6feb42b49b71e1982cda8fa0b37c":["d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"50d5b588b112eeb3d6b2a3fcc43a40ef0615a529":["5ca09cfd0198baeab4d54dc2d866c1d96cf869c6"],"d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","a7c7a5405c388fd86e5962126be8ad09283eb5cc"],"ea4107f60b9f95623c16025c9c247412ff809092":["24a98f5fdd23e04f85819dbc63b47a12f7c44311","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","f03e4bed5023ec3ef93a771b8888cae991cf448d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}