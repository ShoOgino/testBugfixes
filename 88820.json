{"path":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","commits":[{"id":"db628922a5eb84f4c7e097a23b99c6fcfc46e084","date":1117731958,"type":0,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","pathOld":"/dev/null","sourceNew":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    int mergedDocCount = merger.merge();\n\n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i >= minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.addElement(new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            deleteSegments(segmentsToDelete);  // delete now-unused segments\n            return null;\n          }\n        }.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["6eb6723414c7578e3be2fa28b281a224547cdf93","6eb6723414c7578e3be2fa28b281a224547cdf93","1b54a9bc667895a2095a886184bf69a3179e63df","6b772434f9f358356f8eedc2e017b9ddfb2615ec"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"14e0bcefcbabf34d28e4fd7060dabc9a9fe37cea","date":1142635892,"type":3,"author":"Daniel Naber","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","sourceNew":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    int mergedDocCount = merger.merge();\n\n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i >= minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.addElement(new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            deleteSegments(segmentsToDelete);  // delete now-unused segments\n            return null;\n          }\n        }.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    int mergedDocCount = merger.merge();\n\n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i >= minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.addElement(new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            deleteSegments(segmentsToDelete);  // delete now-unused segments\n            return null;\n          }\n        }.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6eb6723414c7578e3be2fa28b281a224547cdf93","date":1144287642,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","sourceNew":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    int mergedDocCount = merger.merge();\n\n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.set(minSegment, new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            deleteSegments(segmentsToDelete);  // delete now-unused segments\n            return null;\n          }\n        }.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    int mergedDocCount = merger.merge();\n\n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i >= minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.addElement(new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            deleteSegments(segmentsToDelete);  // delete now-unused segments\n            return null;\n          }\n        }.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","bugFix":["db628922a5eb84f4c7e097a23b99c6fcfc46e084"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8c3ae2a9b6bdc01b2b6665e79fdfc71ae4a8b1ca","date":1148660052,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","sourceNew":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    int mergedDocCount = merger.merge();\n\n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.set(minSegment, new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            return null;\n          }\n        }.run();\n    }\n    \n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment \n      deleteFiles(filesToDelete);   \n    }\n  }\n\n","sourceOld":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    int mergedDocCount = merger.merge();\n\n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.set(minSegment, new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            deleteSegments(segmentsToDelete);  // delete now-unused segments\n            return null;\n          }\n        }.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1a34f23ad863179f1a3d973f561d76331e5dce92","date":1155457213,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","sourceNew":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    int mergedDocCount = merger.merge();\n\n    bufferedDocCount -= mergedDocCount; // update bookkeeping about how many docs we have buffered\n    \n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.set(minSegment, new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            return null;\n          }\n        }.run();\n    }\n    \n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment \n      deleteFiles(filesToDelete);   \n    }\n  }\n\n","sourceOld":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    int mergedDocCount = merger.merge();\n\n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.set(minSegment, new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            return null;\n          }\n        }.run();\n    }\n    \n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment \n      deleteFiles(filesToDelete);   \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7cf920bc70a57dc75d59acb55a7d893cc2924ad","date":1155538722,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","sourceNew":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    int mergedDocCount = merger.merge();\n\n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.set(minSegment, new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            return null;\n          }\n        }.run();\n    }\n    \n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment \n      deleteFiles(filesToDelete);   \n    }\n  }\n\n","sourceOld":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    int mergedDocCount = merger.merge();\n\n    bufferedDocCount -= mergedDocCount; // update bookkeeping about how many docs we have buffered\n    \n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.set(minSegment, new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            return null;\n          }\n        }.run();\n    }\n    \n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment \n      deleteFiles(filesToDelete);   \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"92bc0e1d83efa4bc1167ba6f8498b065d3d8ce37","date":1155907996,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","sourceNew":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    // update 1-doc segments counter accordin to range of merged segments\n    if (singleDocSegmentsCount>0) {\n      singleDocSegmentsCount = Math.min(singleDocSegmentsCount, segmentInfos.size()-end);\n    }\n\n    int mergedDocCount = merger.merge();\n\n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.set(minSegment, new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            return null;\n          }\n        }.run();\n    }\n    \n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment \n      deleteFiles(filesToDelete);   \n    }\n  }\n\n","sourceOld":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    int mergedDocCount = merger.merge();\n\n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.set(minSegment, new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            return null;\n          }\n        }.run();\n    }\n    \n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment \n      deleteFiles(filesToDelete);   \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d7052f725a053aa55424f966831826f61b798bf1","date":1158258681,"type":5,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(SegmentInfos,int,int).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","sourceNew":"  /**\n   * Merges the named range of segments, replacing them in the stack with a\n   * single segment.\n   */\n  private final int mergeSegments(SegmentInfos sourceSegments, int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    boolean fromRAM = false;\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n      if (!fromRAM && (reader.directory() == this.ramDirectory)) {\n        fromRAM = true;\n      }\n    }\n\n    int mergedDocCount = merger.merge();\n\n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    SegmentInfo newSegment = new SegmentInfo(mergedName, mergedDocCount,\n        directory);\n    if (fromRAM) {\n      sourceSegments.removeAllElements();\n      if (mergedDocCount > 0)\n        segmentInfos.addElement(newSegment);\n    } else {\n      for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n        sourceSegments.remove(i);\n      if (mergedDocCount > 0)\n        segmentInfos.set(minSegment, newSegment);\n      else\n        sourceSegments.remove(minSegment);\n    }\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            return null;\n          }\n        }.run();\n    }\n    \n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment \n      deleteFiles(filesToDelete);   \n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Merges the named range of segments, replacing them in the stack with a\n   * single segment. */\n  private final void mergeSegments(int minSegment, int end)\n    throws IOException {\n    final String mergedName = newSegmentName();\n    if (infoStream != null) infoStream.print(\"merging segments\");\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    for (int i = minSegment; i < end; i++) {\n      SegmentInfo si = segmentInfos.info(i);\n      if (infoStream != null)\n        infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n      IndexReader reader = SegmentReader.get(si);\n      merger.add(reader);\n      if ((reader.directory() == this.directory) || // if we own the directory\n          (reader.directory() == this.ramDirectory))\n        segmentsToDelete.addElement(reader);   // queue segment for deletion\n    }\n\n    // update 1-doc segments counter accordin to range of merged segments\n    if (singleDocSegmentsCount>0) {\n      singleDocSegmentsCount = Math.min(singleDocSegmentsCount, segmentInfos.size()-end);\n    }\n\n    int mergedDocCount = merger.merge();\n\n    if (infoStream != null) {\n      infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n    }\n\n    for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n      segmentInfos.remove(i);\n    segmentInfos.set(minSegment, new SegmentInfo(mergedName, mergedDocCount,\n                                            directory));\n\n    // close readers before we attempt to delete now-obsolete segments\n    merger.closeReaders();\n\n    synchronized (directory) {                 // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            segmentInfos.write(directory);     // commit before deleting\n            return null;\n          }\n        }.run();\n    }\n    \n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment \n      deleteFiles(filesToDelete);   \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(SegmentInfos,int,int).mjava","sourceNew":"  /**\n   * Merges the named range of segments, replacing them in the stack with a\n   * single segment.\n   */\n\n  private final int mergeSegments(int minSegment, int end)\n    throws CorruptIndexException, IOException {\n\n    final String mergedName = newSegmentName();\n    \n    SegmentMerger merger = null;\n    SegmentInfo newSegment = null;\n\n    int mergedDocCount = 0;\n\n    // This is try/finally to make sure merger's readers are closed:\n    try {\n\n      if (infoStream != null) infoStream.print(\"merging segments\");\n\n      // Check whether this merge will allow us to skip\n      // merging the doc stores (stored field & vectors).\n      // This is a very substantial optimization (saves tons\n      // of IO) that can only be applied with\n      // autoCommit=false.\n\n      Directory lastDir = directory;\n      String lastDocStoreSegment = null;\n      boolean mergeDocStores = false;\n      boolean doFlushDocStore = false;\n      int next = -1;\n\n      // Test each segment to be merged\n      for (int i = minSegment; i < end; i++) {\n        SegmentInfo si = segmentInfos.info(i);\n\n        // If it has deletions we must merge the doc stores\n        if (si.hasDeletions())\n          mergeDocStores = true;\n\n        // If it has its own (private) doc stores we must\n        // merge the doc stores\n        if (-1 == si.getDocStoreOffset())\n          mergeDocStores = true;\n\n        // If it has a different doc store segment than\n        // previous segments, we must merge the doc stores\n        String docStoreSegment = si.getDocStoreSegment();\n        if (docStoreSegment == null)\n          mergeDocStores = true;\n        else if (lastDocStoreSegment == null)\n          lastDocStoreSegment = docStoreSegment;\n        else if (!lastDocStoreSegment.equals(docStoreSegment))\n          mergeDocStores = true;\n\n        // Segments' docScoreOffsets must be in-order,\n        // contiguous.  For the default merge policy now\n        // this will always be the case but for an arbitrary\n        // merge policy this may not be the case\n        if (-1 == next)\n          next = si.getDocStoreOffset() + si.docCount;\n        else if (next != si.getDocStoreOffset())\n          mergeDocStores = true;\n        else\n          next = si.getDocStoreOffset() + si.docCount;\n      \n        // If the segment comes from a different directory\n        // we must merge\n        if (lastDir != si.dir)\n          mergeDocStores = true;\n\n        // If the segment is referencing the current \"live\"\n        // doc store outputs then we must merge\n        if (si.getDocStoreOffset() != -1 && si.getDocStoreSegment().equals(docWriter.getDocStoreSegment()))\n          doFlushDocStore = true;\n      }\n\n      final int docStoreOffset;\n      final String docStoreSegment;\n      final boolean docStoreIsCompoundFile;\n      if (mergeDocStores) {\n        docStoreOffset = -1;\n        docStoreSegment = null;\n        docStoreIsCompoundFile = false;\n      } else {\n        SegmentInfo si = segmentInfos.info(minSegment);        \n        docStoreOffset = si.getDocStoreOffset();\n        docStoreSegment = si.getDocStoreSegment();\n        docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n      }\n\n      if (mergeDocStores && doFlushDocStore)\n        // SegmentMerger intends to merge the doc stores\n        // (stored fields, vectors), and at least one of the\n        // segments to be merged refers to the currently\n        // live doc stores.\n        flushDocStores();\n\n      merger = new SegmentMerger(this, mergedName);\n\n      for (int i = minSegment; i < end; i++) {\n        SegmentInfo si = segmentInfos.info(i);\n        if (infoStream != null)\n          infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n      }\n\n      SegmentInfos rollback = null;\n      boolean success = false;\n\n      // This is try/finally to rollback our internal state\n      // if we hit exception when doing the merge:\n      try {\n\n        mergedDocCount = merger.merge(mergeDocStores);\n\n        if (infoStream != null) {\n          infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n        }\n\n        newSegment = new SegmentInfo(mergedName, mergedDocCount,\n                                     directory, false, true,\n                                     docStoreOffset,\n                                     docStoreSegment,\n                                     docStoreIsCompoundFile);\n        \n        rollback = (SegmentInfos) segmentInfos.clone();\n\n        for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n          segmentInfos.remove(i);\n\n        segmentInfos.set(minSegment, newSegment);\n\n        checkpoint();\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          if (rollback != null) {\n            // Rollback the individual SegmentInfo\n            // instances, but keep original SegmentInfos\n            // instance (so we don't try to write again the\n            // same segments_N file -- write once):\n            segmentInfos.clear();\n            segmentInfos.addAll(rollback);\n          }\n\n          // Delete any partially created and now unreferenced files:\n          deleter.refresh();\n        }\n      }\n    } finally {\n      // close readers before we attempt to delete now-obsolete segments\n      merger.closeReaders();\n    }\n\n    // Give deleter a chance to remove files now.\n    deleter.checkpoint(segmentInfos, autoCommit);\n\n    if (useCompoundFile) {\n\n      boolean success = false;\n\n      try {\n\n        merger.createCompoundFile(mergedName + \".cfs\");\n        newSegment.setUseCompoundFile(true);\n        checkpoint();\n        success = true;\n\n      } finally {\n        if (!success) {  \n          // Must rollback:\n          newSegment.setUseCompoundFile(false);\n          deleter.refresh();\n        }\n      }\n      \n      // Give deleter a chance to remove files now.\n      deleter.checkpoint(segmentInfos, autoCommit);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /**\n   * Merges the named range of segments, replacing them in the stack with a\n   * single segment.\n   */\n  private final int mergeSegments(SegmentInfos sourceSegments, int minSegment, int end)\n    throws CorruptIndexException, IOException {\n\n    // We may be called solely because there are deletes\n    // pending, in which case doMerge is false:\n    boolean doMerge = end > 0;\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = null;\n\n    final List ramSegmentsToDelete = new ArrayList();\n\n    SegmentInfo newSegment = null;\n\n    int mergedDocCount = 0;\n    boolean anyDeletes = (bufferedDeleteTerms.size() != 0);\n\n    // This is try/finally to make sure merger's readers are closed:\n    try {\n\n      if (doMerge) {\n        if (infoStream != null) infoStream.print(\"merging segments\");\n        merger = new SegmentMerger(this, mergedName);\n\n        for (int i = minSegment; i < end; i++) {\n          SegmentInfo si = sourceSegments.info(i);\n          if (infoStream != null)\n            infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n          IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE); // no need to set deleter (yet)\n          merger.add(reader);\n          if (reader.directory() == this.ramDirectory) {\n            ramSegmentsToDelete.add(si);\n          }\n        }\n      }\n\n      SegmentInfos rollback = null;\n      boolean success = false;\n\n      // This is try/finally to rollback our internal state\n      // if we hit exception when doing the merge:\n      try {\n\n        if (doMerge) {\n          mergedDocCount = merger.merge();\n\n          if (infoStream != null) {\n            infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n          }\n\n          newSegment = new SegmentInfo(mergedName, mergedDocCount,\n                                       directory, false, true);\n        }\n        \n        if (sourceSegments != ramSegmentInfos || anyDeletes) {\n          // Now save the SegmentInfo instances that\n          // we are replacing:\n          rollback = (SegmentInfos) segmentInfos.clone();\n        }\n\n        if (doMerge) {\n          if (sourceSegments == ramSegmentInfos) {\n            segmentInfos.addElement(newSegment);\n          } else {\n            for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n              sourceSegments.remove(i);\n\n            segmentInfos.set(minSegment, newSegment);\n          }\n        }\n\n        if (sourceSegments == ramSegmentInfos) {\n          maybeApplyDeletes(doMerge);\n          doAfterFlush();\n        }\n        \n        checkpoint();\n\n        success = true;\n\n      } finally {\n\n        if (success) {\n          // The non-ram-segments case is already committed\n          // (above), so all the remains for ram segments case\n          // is to clear the ram segments:\n          if (sourceSegments == ramSegmentInfos) {\n            ramSegmentInfos.removeAllElements();\n          }\n        } else {\n\n          // Must rollback so our state matches index:\n          if (sourceSegments == ramSegmentInfos && !anyDeletes) {\n            // Simple case: newSegment may or may not have\n            // been added to the end of our segment infos,\n            // so just check & remove if so:\n            if (newSegment != null && \n                segmentInfos.size() > 0 && \n                segmentInfos.info(segmentInfos.size()-1) == newSegment) {\n              segmentInfos.remove(segmentInfos.size()-1);\n            }\n          } else if (rollback != null) {\n            // Rollback the individual SegmentInfo\n            // instances, but keep original SegmentInfos\n            // instance (so we don't try to write again the\n            // same segments_N file -- write once):\n            segmentInfos.clear();\n            segmentInfos.addAll(rollback);\n          }\n\n          // Delete any partially created and now unreferenced files:\n          deleter.refresh();\n        }\n      }\n    } finally {\n      // close readers before we attempt to delete now-obsolete segments\n      if (doMerge) merger.closeReaders();\n    }\n\n    // Delete the RAM segments\n    deleter.deleteDirect(ramDirectory, ramSegmentsToDelete);\n\n    // Give deleter a chance to remove files now.\n    deleter.checkpoint(segmentInfos, autoCommit);\n\n    if (useCompoundFile && doMerge) {\n\n      boolean success = false;\n\n      try {\n\n        merger.createCompoundFile(mergedName + \".cfs\");\n        newSegment.setUseCompoundFile(true);\n        checkpoint();\n        success = true;\n\n      } finally {\n        if (!success) {  \n          // Must rollback:\n          newSegment.setUseCompoundFile(false);\n          deleter.refresh();\n        }\n      }\n      \n      // Give deleter a chance to remove files now.\n      deleter.checkpoint(segmentInfos, autoCommit);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6c67f8a7c6b111c0379a29c74df57b7c09418344","date":1184967535,"type":3,"author":"Daniel Naber","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","sourceNew":"  /**\n   * Merges the named range of segments, replacing them in the stack with a\n   * single segment.\n   */\n\n  private final int mergeSegments(int minSegment, int end)\n    throws CorruptIndexException, IOException {\n\n    final String mergedName = newSegmentName();\n    \n    SegmentMerger merger = null;\n    SegmentInfo newSegment = null;\n\n    int mergedDocCount = 0;\n\n    // This is try/finally to make sure merger's readers are closed:\n    try {\n\n      if (infoStream != null) infoStream.print(\"merging segments\");\n\n      // Check whether this merge will allow us to skip\n      // merging the doc stores (stored field & vectors).\n      // This is a very substantial optimization (saves tons\n      // of IO) that can only be applied with\n      // autoCommit=false.\n\n      Directory lastDir = directory;\n      String lastDocStoreSegment = null;\n      boolean mergeDocStores = false;\n      boolean doFlushDocStore = false;\n      int next = -1;\n\n      // Test each segment to be merged\n      for (int i = minSegment; i < end; i++) {\n        SegmentInfo si = segmentInfos.info(i);\n\n        // If it has deletions we must merge the doc stores\n        if (si.hasDeletions())\n          mergeDocStores = true;\n\n        // If it has its own (private) doc stores we must\n        // merge the doc stores\n        if (-1 == si.getDocStoreOffset())\n          mergeDocStores = true;\n\n        // If it has a different doc store segment than\n        // previous segments, we must merge the doc stores\n        String docStoreSegment = si.getDocStoreSegment();\n        if (docStoreSegment == null)\n          mergeDocStores = true;\n        else if (lastDocStoreSegment == null)\n          lastDocStoreSegment = docStoreSegment;\n        else if (!lastDocStoreSegment.equals(docStoreSegment))\n          mergeDocStores = true;\n\n        // Segments' docScoreOffsets must be in-order,\n        // contiguous.  For the default merge policy now\n        // this will always be the case but for an arbitrary\n        // merge policy this may not be the case\n        if (-1 == next)\n          next = si.getDocStoreOffset() + si.docCount;\n        else if (next != si.getDocStoreOffset())\n          mergeDocStores = true;\n        else\n          next = si.getDocStoreOffset() + si.docCount;\n      \n        // If the segment comes from a different directory\n        // we must merge\n        if (lastDir != si.dir)\n          mergeDocStores = true;\n\n        // If the segment is referencing the current \"live\"\n        // doc store outputs then we must merge\n        if (si.getDocStoreOffset() != -1 && si.getDocStoreSegment().equals(docWriter.getDocStoreSegment()))\n          doFlushDocStore = true;\n      }\n\n      final int docStoreOffset;\n      final String docStoreSegment;\n      final boolean docStoreIsCompoundFile;\n      if (mergeDocStores) {\n        docStoreOffset = -1;\n        docStoreSegment = null;\n        docStoreIsCompoundFile = false;\n      } else {\n        SegmentInfo si = segmentInfos.info(minSegment);        \n        docStoreOffset = si.getDocStoreOffset();\n        docStoreSegment = si.getDocStoreSegment();\n        docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n      }\n\n      if (mergeDocStores && doFlushDocStore)\n        // SegmentMerger intends to merge the doc stores\n        // (stored fields, vectors), and at least one of the\n        // segments to be merged refers to the currently\n        // live doc stores.\n        flushDocStores();\n\n      merger = new SegmentMerger(this, mergedName);\n\n      for (int i = minSegment; i < end; i++) {\n        SegmentInfo si = segmentInfos.info(i);\n        if (infoStream != null)\n          infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n      }\n\n      SegmentInfos rollback = null;\n      boolean success = false;\n\n      // This is try/finally to rollback our internal state\n      // if we hit exception when doing the merge:\n      try {\n\n        mergedDocCount = merger.merge(mergeDocStores);\n\n        if (infoStream != null) {\n          infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n        }\n\n        newSegment = new SegmentInfo(mergedName, mergedDocCount,\n                                     directory, false, true,\n                                     docStoreOffset,\n                                     docStoreSegment,\n                                     docStoreIsCompoundFile);\n        \n        rollback = (SegmentInfos) segmentInfos.clone();\n\n        for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n          segmentInfos.remove(i);\n\n        segmentInfos.set(minSegment, newSegment);\n\n        checkpoint();\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          if (rollback != null) {\n            // Rollback the individual SegmentInfo\n            // instances, but keep original SegmentInfos\n            // instance (so we don't try to write again the\n            // same segments_N file -- write once):\n            segmentInfos.clear();\n            segmentInfos.addAll(rollback);\n          }\n\n          // Delete any partially created and now unreferenced files:\n          deleter.refresh();\n        }\n      }\n    } finally {\n      // close readers before we attempt to delete now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    // Give deleter a chance to remove files now.\n    deleter.checkpoint(segmentInfos, autoCommit);\n\n    if (useCompoundFile) {\n\n      boolean success = false;\n\n      try {\n\n        merger.createCompoundFile(mergedName + \".cfs\");\n        newSegment.setUseCompoundFile(true);\n        checkpoint();\n        success = true;\n\n      } finally {\n        if (!success) {  \n          // Must rollback:\n          newSegment.setUseCompoundFile(false);\n          deleter.refresh();\n        }\n      }\n      \n      // Give deleter a chance to remove files now.\n      deleter.checkpoint(segmentInfos, autoCommit);\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /**\n   * Merges the named range of segments, replacing them in the stack with a\n   * single segment.\n   */\n\n  private final int mergeSegments(int minSegment, int end)\n    throws CorruptIndexException, IOException {\n\n    final String mergedName = newSegmentName();\n    \n    SegmentMerger merger = null;\n    SegmentInfo newSegment = null;\n\n    int mergedDocCount = 0;\n\n    // This is try/finally to make sure merger's readers are closed:\n    try {\n\n      if (infoStream != null) infoStream.print(\"merging segments\");\n\n      // Check whether this merge will allow us to skip\n      // merging the doc stores (stored field & vectors).\n      // This is a very substantial optimization (saves tons\n      // of IO) that can only be applied with\n      // autoCommit=false.\n\n      Directory lastDir = directory;\n      String lastDocStoreSegment = null;\n      boolean mergeDocStores = false;\n      boolean doFlushDocStore = false;\n      int next = -1;\n\n      // Test each segment to be merged\n      for (int i = minSegment; i < end; i++) {\n        SegmentInfo si = segmentInfos.info(i);\n\n        // If it has deletions we must merge the doc stores\n        if (si.hasDeletions())\n          mergeDocStores = true;\n\n        // If it has its own (private) doc stores we must\n        // merge the doc stores\n        if (-1 == si.getDocStoreOffset())\n          mergeDocStores = true;\n\n        // If it has a different doc store segment than\n        // previous segments, we must merge the doc stores\n        String docStoreSegment = si.getDocStoreSegment();\n        if (docStoreSegment == null)\n          mergeDocStores = true;\n        else if (lastDocStoreSegment == null)\n          lastDocStoreSegment = docStoreSegment;\n        else if (!lastDocStoreSegment.equals(docStoreSegment))\n          mergeDocStores = true;\n\n        // Segments' docScoreOffsets must be in-order,\n        // contiguous.  For the default merge policy now\n        // this will always be the case but for an arbitrary\n        // merge policy this may not be the case\n        if (-1 == next)\n          next = si.getDocStoreOffset() + si.docCount;\n        else if (next != si.getDocStoreOffset())\n          mergeDocStores = true;\n        else\n          next = si.getDocStoreOffset() + si.docCount;\n      \n        // If the segment comes from a different directory\n        // we must merge\n        if (lastDir != si.dir)\n          mergeDocStores = true;\n\n        // If the segment is referencing the current \"live\"\n        // doc store outputs then we must merge\n        if (si.getDocStoreOffset() != -1 && si.getDocStoreSegment().equals(docWriter.getDocStoreSegment()))\n          doFlushDocStore = true;\n      }\n\n      final int docStoreOffset;\n      final String docStoreSegment;\n      final boolean docStoreIsCompoundFile;\n      if (mergeDocStores) {\n        docStoreOffset = -1;\n        docStoreSegment = null;\n        docStoreIsCompoundFile = false;\n      } else {\n        SegmentInfo si = segmentInfos.info(minSegment);        \n        docStoreOffset = si.getDocStoreOffset();\n        docStoreSegment = si.getDocStoreSegment();\n        docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n      }\n\n      if (mergeDocStores && doFlushDocStore)\n        // SegmentMerger intends to merge the doc stores\n        // (stored fields, vectors), and at least one of the\n        // segments to be merged refers to the currently\n        // live doc stores.\n        flushDocStores();\n\n      merger = new SegmentMerger(this, mergedName);\n\n      for (int i = minSegment; i < end; i++) {\n        SegmentInfo si = segmentInfos.info(i);\n        if (infoStream != null)\n          infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n      }\n\n      SegmentInfos rollback = null;\n      boolean success = false;\n\n      // This is try/finally to rollback our internal state\n      // if we hit exception when doing the merge:\n      try {\n\n        mergedDocCount = merger.merge(mergeDocStores);\n\n        if (infoStream != null) {\n          infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n        }\n\n        newSegment = new SegmentInfo(mergedName, mergedDocCount,\n                                     directory, false, true,\n                                     docStoreOffset,\n                                     docStoreSegment,\n                                     docStoreIsCompoundFile);\n        \n        rollback = (SegmentInfos) segmentInfos.clone();\n\n        for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n          segmentInfos.remove(i);\n\n        segmentInfos.set(minSegment, newSegment);\n\n        checkpoint();\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          if (rollback != null) {\n            // Rollback the individual SegmentInfo\n            // instances, but keep original SegmentInfos\n            // instance (so we don't try to write again the\n            // same segments_N file -- write once):\n            segmentInfos.clear();\n            segmentInfos.addAll(rollback);\n          }\n\n          // Delete any partially created and now unreferenced files:\n          deleter.refresh();\n        }\n      }\n    } finally {\n      // close readers before we attempt to delete now-obsolete segments\n      merger.closeReaders();\n    }\n\n    // Give deleter a chance to remove files now.\n    deleter.checkpoint(segmentInfos, autoCommit);\n\n    if (useCompoundFile) {\n\n      boolean success = false;\n\n      try {\n\n        merger.createCompoundFile(mergedName + \".cfs\");\n        newSegment.setUseCompoundFile(true);\n        checkpoint();\n        success = true;\n\n      } finally {\n        if (!success) {  \n          // Must rollback:\n          newSegment.setUseCompoundFile(false);\n          deleter.refresh();\n        }\n      }\n      \n      // Give deleter a chance to remove files now.\n      deleter.checkpoint(segmentInfos, autoCommit);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b1405362241b561f5590ff4a87d5d6e173bcd9cf","date":1190107634,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeSegments(int,int).mjava","sourceNew":null,"sourceOld":"  /**\n   * Merges the named range of segments, replacing them in the stack with a\n   * single segment.\n   */\n\n  private final int mergeSegments(int minSegment, int end)\n    throws CorruptIndexException, IOException {\n\n    final String mergedName = newSegmentName();\n    \n    SegmentMerger merger = null;\n    SegmentInfo newSegment = null;\n\n    int mergedDocCount = 0;\n\n    // This is try/finally to make sure merger's readers are closed:\n    try {\n\n      if (infoStream != null) infoStream.print(\"merging segments\");\n\n      // Check whether this merge will allow us to skip\n      // merging the doc stores (stored field & vectors).\n      // This is a very substantial optimization (saves tons\n      // of IO) that can only be applied with\n      // autoCommit=false.\n\n      Directory lastDir = directory;\n      String lastDocStoreSegment = null;\n      boolean mergeDocStores = false;\n      boolean doFlushDocStore = false;\n      int next = -1;\n\n      // Test each segment to be merged\n      for (int i = minSegment; i < end; i++) {\n        SegmentInfo si = segmentInfos.info(i);\n\n        // If it has deletions we must merge the doc stores\n        if (si.hasDeletions())\n          mergeDocStores = true;\n\n        // If it has its own (private) doc stores we must\n        // merge the doc stores\n        if (-1 == si.getDocStoreOffset())\n          mergeDocStores = true;\n\n        // If it has a different doc store segment than\n        // previous segments, we must merge the doc stores\n        String docStoreSegment = si.getDocStoreSegment();\n        if (docStoreSegment == null)\n          mergeDocStores = true;\n        else if (lastDocStoreSegment == null)\n          lastDocStoreSegment = docStoreSegment;\n        else if (!lastDocStoreSegment.equals(docStoreSegment))\n          mergeDocStores = true;\n\n        // Segments' docScoreOffsets must be in-order,\n        // contiguous.  For the default merge policy now\n        // this will always be the case but for an arbitrary\n        // merge policy this may not be the case\n        if (-1 == next)\n          next = si.getDocStoreOffset() + si.docCount;\n        else if (next != si.getDocStoreOffset())\n          mergeDocStores = true;\n        else\n          next = si.getDocStoreOffset() + si.docCount;\n      \n        // If the segment comes from a different directory\n        // we must merge\n        if (lastDir != si.dir)\n          mergeDocStores = true;\n\n        // If the segment is referencing the current \"live\"\n        // doc store outputs then we must merge\n        if (si.getDocStoreOffset() != -1 && si.getDocStoreSegment().equals(docWriter.getDocStoreSegment()))\n          doFlushDocStore = true;\n      }\n\n      final int docStoreOffset;\n      final String docStoreSegment;\n      final boolean docStoreIsCompoundFile;\n      if (mergeDocStores) {\n        docStoreOffset = -1;\n        docStoreSegment = null;\n        docStoreIsCompoundFile = false;\n      } else {\n        SegmentInfo si = segmentInfos.info(minSegment);        \n        docStoreOffset = si.getDocStoreOffset();\n        docStoreSegment = si.getDocStoreSegment();\n        docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n      }\n\n      if (mergeDocStores && doFlushDocStore)\n        // SegmentMerger intends to merge the doc stores\n        // (stored fields, vectors), and at least one of the\n        // segments to be merged refers to the currently\n        // live doc stores.\n        flushDocStores();\n\n      merger = new SegmentMerger(this, mergedName);\n\n      for (int i = minSegment; i < end; i++) {\n        SegmentInfo si = segmentInfos.info(i);\n        if (infoStream != null)\n          infoStream.print(\" \" + si.name + \" (\" + si.docCount + \" docs)\");\n        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, mergeDocStores); // no need to set deleter (yet)\n        merger.add(reader);\n      }\n\n      SegmentInfos rollback = null;\n      boolean success = false;\n\n      // This is try/finally to rollback our internal state\n      // if we hit exception when doing the merge:\n      try {\n\n        mergedDocCount = merger.merge(mergeDocStores);\n\n        if (infoStream != null) {\n          infoStream.println(\" into \"+mergedName+\" (\"+mergedDocCount+\" docs)\");\n        }\n\n        newSegment = new SegmentInfo(mergedName, mergedDocCount,\n                                     directory, false, true,\n                                     docStoreOffset,\n                                     docStoreSegment,\n                                     docStoreIsCompoundFile);\n        \n        rollback = (SegmentInfos) segmentInfos.clone();\n\n        for (int i = end-1; i > minSegment; i--)     // remove old infos & add new\n          segmentInfos.remove(i);\n\n        segmentInfos.set(minSegment, newSegment);\n\n        checkpoint();\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          if (rollback != null) {\n            // Rollback the individual SegmentInfo\n            // instances, but keep original SegmentInfos\n            // instance (so we don't try to write again the\n            // same segments_N file -- write once):\n            segmentInfos.clear();\n            segmentInfos.addAll(rollback);\n          }\n\n          // Delete any partially created and now unreferenced files:\n          deleter.refresh();\n        }\n      }\n    } finally {\n      // close readers before we attempt to delete now-obsolete segments\n      if (merger != null) {\n        merger.closeReaders();\n      }\n    }\n\n    // Give deleter a chance to remove files now.\n    deleter.checkpoint(segmentInfos, autoCommit);\n\n    if (useCompoundFile) {\n\n      boolean success = false;\n\n      try {\n\n        merger.createCompoundFile(mergedName + \".cfs\");\n        newSegment.setUseCompoundFile(true);\n        checkpoint();\n        success = true;\n\n      } finally {\n        if (!success) {  \n          // Must rollback:\n          newSegment.setUseCompoundFile(false);\n          deleter.refresh();\n        }\n      }\n      \n      // Give deleter a chance to remove files now.\n      deleter.checkpoint(segmentInfos, autoCommit);\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d7052f725a053aa55424f966831826f61b798bf1":["92bc0e1d83efa4bc1167ba6f8498b065d3d8ce37"],"14e0bcefcbabf34d28e4fd7060dabc9a9fe37cea":["db628922a5eb84f4c7e097a23b99c6fcfc46e084"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["6c67f8a7c6b111c0379a29c74df57b7c09418344"],"6c67f8a7c6b111c0379a29c74df57b7c09418344":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"db628922a5eb84f4c7e097a23b99c6fcfc46e084":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1a34f23ad863179f1a3d973f561d76331e5dce92":["8c3ae2a9b6bdc01b2b6665e79fdfc71ae4a8b1ca"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["d7052f725a053aa55424f966831826f61b798bf1"],"6eb6723414c7578e3be2fa28b281a224547cdf93":["14e0bcefcbabf34d28e4fd7060dabc9a9fe37cea"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8c3ae2a9b6bdc01b2b6665e79fdfc71ae4a8b1ca":["6eb6723414c7578e3be2fa28b281a224547cdf93"],"92bc0e1d83efa4bc1167ba6f8498b065d3d8ce37":["c7cf920bc70a57dc75d59acb55a7d893cc2924ad"],"c7cf920bc70a57dc75d59acb55a7d893cc2924ad":["1a34f23ad863179f1a3d973f561d76331e5dce92"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"]},"commit2Childs":{"d7052f725a053aa55424f966831826f61b798bf1":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"14e0bcefcbabf34d28e4fd7060dabc9a9fe37cea":["6eb6723414c7578e3be2fa28b281a224547cdf93"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6c67f8a7c6b111c0379a29c74df57b7c09418344":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"db628922a5eb84f4c7e097a23b99c6fcfc46e084":["14e0bcefcbabf34d28e4fd7060dabc9a9fe37cea"],"1a34f23ad863179f1a3d973f561d76331e5dce92":["c7cf920bc70a57dc75d59acb55a7d893cc2924ad"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["6c67f8a7c6b111c0379a29c74df57b7c09418344"],"6eb6723414c7578e3be2fa28b281a224547cdf93":["8c3ae2a9b6bdc01b2b6665e79fdfc71ae4a8b1ca"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["db628922a5eb84f4c7e097a23b99c6fcfc46e084"],"8c3ae2a9b6bdc01b2b6665e79fdfc71ae4a8b1ca":["1a34f23ad863179f1a3d973f561d76331e5dce92"],"92bc0e1d83efa4bc1167ba6f8498b065d3d8ce37":["d7052f725a053aa55424f966831826f61b798bf1"],"c7cf920bc70a57dc75d59acb55a7d893cc2924ad":["92bc0e1d83efa4bc1167ba6f8498b065d3d8ce37"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}