{"path":"lucene/core/src/java/org/apache/lucene/search/ConstantScoreAutoRewrite#rewrite(IndexReader,MultiTermQuery).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/ConstantScoreAutoRewrite#rewrite(IndexReader,MultiTermQuery).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/ConstantScoreAutoRewrite#rewrite(IndexReader,MultiTermQuery).mjava","sourceNew":"  @Override\n  public Query rewrite(final IndexReader reader, final MultiTermQuery query) throws IOException {\n\n    // Get the enum and start visiting terms.  If we\n    // exhaust the enum before hitting either of the\n    // cutoffs, we use ConstantBooleanQueryRewrite; else,\n    // ConstantFilterRewrite:\n    final int docCountCutoff = (int) ((docCountPercent / 100.) * reader.maxDoc());\n    final int termCountLimit = Math.min(BooleanQuery.getMaxClauseCount(), termCountCutoff);\n\n    final CutOffTermCollector col = new CutOffTermCollector(docCountCutoff, termCountLimit);\n    collectTerms(reader, query, col);\n    final int size = col.pendingTerms.size();\n    if (col.hasCutOff) {\n      return MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE.rewrite(reader, query);\n    } else if (size == 0) {\n      return getTopLevelQuery();\n    } else {\n      final BooleanQuery bq = getTopLevelQuery();\n      final BytesRefHash pendingTerms = col.pendingTerms;\n      final int sort[] = pendingTerms.sort(col.termsEnum.getComparator());\n      for(int i = 0; i < size; i++) {\n        final int pos = sort[i];\n        // docFreq is not used for constant score here, we pass 1\n        // to explicitely set a fake value, so it's not calculated\n        addClause(bq, new Term(query.field, pendingTerms.get(pos, new BytesRef())), 1, 1.0f, col.array.termState[pos]);\n      }\n      // Strip scores\n      final Query result = new ConstantScoreQuery(bq);\n      result.setBoost(query.getBoost());\n      return result;\n    }\n  }\n\n","sourceOld":"  @Override\n  public Query rewrite(final IndexReader reader, final MultiTermQuery query) throws IOException {\n\n    // Get the enum and start visiting terms.  If we\n    // exhaust the enum before hitting either of the\n    // cutoffs, we use ConstantBooleanQueryRewrite; else,\n    // ConstantFilterRewrite:\n    final int docCountCutoff = (int) ((docCountPercent / 100.) * reader.maxDoc());\n    final int termCountLimit = Math.min(BooleanQuery.getMaxClauseCount(), termCountCutoff);\n\n    final CutOffTermCollector col = new CutOffTermCollector(docCountCutoff, termCountLimit);\n    collectTerms(reader, query, col);\n    final int size = col.pendingTerms.size();\n    if (col.hasCutOff) {\n      return MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE.rewrite(reader, query);\n    } else if (size == 0) {\n      return getTopLevelQuery();\n    } else {\n      final BooleanQuery bq = getTopLevelQuery();\n      final BytesRefHash pendingTerms = col.pendingTerms;\n      final int sort[] = pendingTerms.sort(col.termsEnum.getComparator());\n      for(int i = 0; i < size; i++) {\n        final int pos = sort[i];\n        // docFreq is not used for constant score here, we pass 1\n        // to explicitely set a fake value, so it's not calculated\n        addClause(bq, new Term(query.field, pendingTerms.get(pos, new BytesRef())), 1, 1.0f, col.array.termState[pos]);\n      }\n      // Strip scores\n      final Query result = new ConstantScoreQuery(bq);\n      result.setBoost(query.getBoost());\n      return result;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","date":1379624229,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/ConstantScoreAutoRewrite#rewrite(IndexReader,MultiTermQuery).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/ConstantScoreAutoRewrite#rewrite(IndexReader,MultiTermQuery).mjava","sourceNew":"  @Override\n  public Query rewrite(final IndexReader reader, final MultiTermQuery query) throws IOException {\n\n    // Get the enum and start visiting terms.  If we\n    // exhaust the enum before hitting either of the\n    // cutoffs, we use ConstantBooleanQueryRewrite; else,\n    // ConstantFilterRewrite:\n    final int docCountCutoff = (int) ((docCountPercent / 100.) * reader.maxDoc());\n    final int termCountLimit = Math.min(BooleanQuery.getMaxClauseCount(), termCountCutoff);\n\n    final CutOffTermCollector col = new CutOffTermCollector(docCountCutoff, termCountLimit);\n    collectTerms(reader, query, col);\n    final int size = col.pendingTerms.size();\n    if (col.hasCutOff) {\n      return MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE.rewrite(reader, query);\n    } else if (size == 0) {\n      return getTopLevelQuery();\n    } else {\n      final BooleanQuery bq = getTopLevelQuery();\n      final BytesRefHash pendingTerms = col.pendingTerms;\n      final int sort[] = pendingTerms.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n      for(int i = 0; i < size; i++) {\n        final int pos = sort[i];\n        // docFreq is not used for constant score here, we pass 1\n        // to explicitely set a fake value, so it's not calculated\n        addClause(bq, new Term(query.field, pendingTerms.get(pos, new BytesRef())), 1, 1.0f, col.array.termState[pos]);\n      }\n      // Strip scores\n      final Query result = new ConstantScoreQuery(bq);\n      result.setBoost(query.getBoost());\n      return result;\n    }\n  }\n\n","sourceOld":"  @Override\n  public Query rewrite(final IndexReader reader, final MultiTermQuery query) throws IOException {\n\n    // Get the enum and start visiting terms.  If we\n    // exhaust the enum before hitting either of the\n    // cutoffs, we use ConstantBooleanQueryRewrite; else,\n    // ConstantFilterRewrite:\n    final int docCountCutoff = (int) ((docCountPercent / 100.) * reader.maxDoc());\n    final int termCountLimit = Math.min(BooleanQuery.getMaxClauseCount(), termCountCutoff);\n\n    final CutOffTermCollector col = new CutOffTermCollector(docCountCutoff, termCountLimit);\n    collectTerms(reader, query, col);\n    final int size = col.pendingTerms.size();\n    if (col.hasCutOff) {\n      return MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE.rewrite(reader, query);\n    } else if (size == 0) {\n      return getTopLevelQuery();\n    } else {\n      final BooleanQuery bq = getTopLevelQuery();\n      final BytesRefHash pendingTerms = col.pendingTerms;\n      final int sort[] = pendingTerms.sort(col.termsEnum.getComparator());\n      for(int i = 0; i < size; i++) {\n        final int pos = sort[i];\n        // docFreq is not used for constant score here, we pass 1\n        // to explicitely set a fake value, so it's not calculated\n        addClause(bq, new Term(query.field, pendingTerms.get(pos, new BytesRef())), 1, 1.0f, col.array.termState[pos]);\n      }\n      // Strip scores\n      final Query result = new ConstantScoreQuery(bq);\n      result.setBoost(query.getBoost());\n      return result;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"73c3f3499afb7895a585b82e3273508381608269","date":1380177979,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/ConstantScoreAutoRewrite#rewrite(IndexReader,MultiTermQuery).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/ConstantScoreAutoRewrite#rewrite(IndexReader,MultiTermQuery).mjava","sourceNew":"  @Override\n  public Query rewrite(final IndexReader reader, final MultiTermQuery query) throws IOException {\n\n    // Get the enum and start visiting terms.  If we\n    // exhaust the enum before hitting either of the\n    // cutoffs, we use ConstantBooleanQueryRewrite; else,\n    // ConstantFilterRewrite:\n    final int docCountCutoff = (int) ((docCountPercent / 100.) * reader.maxDoc());\n    final int termCountLimit = Math.min(BooleanQuery.getMaxClauseCount(), termCountCutoff);\n\n    final CutOffTermCollector col = new CutOffTermCollector(docCountCutoff, termCountLimit);\n    collectTerms(reader, query, col);\n    final int size = col.pendingTerms.size();\n    if (col.hasCutOff) {\n      return MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE.rewrite(reader, query);\n    } else {\n      final BooleanQuery bq = getTopLevelQuery();\n      if (size > 0) {\n        final BytesRefHash pendingTerms = col.pendingTerms;\n        final int sort[] = pendingTerms.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n        for(int i = 0; i < size; i++) {\n          final int pos = sort[i];\n          // docFreq is not used for constant score here, we pass 1\n          // to explicitely set a fake value, so it's not calculated\n          addClause(bq, new Term(query.field, pendingTerms.get(pos, new BytesRef())), 1, 1.0f, col.array.termState[pos]);\n        }\n      }\n      // Strip scores\n      final Query result = new ConstantScoreQuery(bq);\n      result.setBoost(query.getBoost());\n      return result;\n    }\n  }\n\n","sourceOld":"  @Override\n  public Query rewrite(final IndexReader reader, final MultiTermQuery query) throws IOException {\n\n    // Get the enum and start visiting terms.  If we\n    // exhaust the enum before hitting either of the\n    // cutoffs, we use ConstantBooleanQueryRewrite; else,\n    // ConstantFilterRewrite:\n    final int docCountCutoff = (int) ((docCountPercent / 100.) * reader.maxDoc());\n    final int termCountLimit = Math.min(BooleanQuery.getMaxClauseCount(), termCountCutoff);\n\n    final CutOffTermCollector col = new CutOffTermCollector(docCountCutoff, termCountLimit);\n    collectTerms(reader, query, col);\n    final int size = col.pendingTerms.size();\n    if (col.hasCutOff) {\n      return MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE.rewrite(reader, query);\n    } else if (size == 0) {\n      return getTopLevelQuery();\n    } else {\n      final BooleanQuery bq = getTopLevelQuery();\n      final BytesRefHash pendingTerms = col.pendingTerms;\n      final int sort[] = pendingTerms.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n      for(int i = 0; i < size; i++) {\n        final int pos = sort[i];\n        // docFreq is not used for constant score here, we pass 1\n        // to explicitely set a fake value, so it's not calculated\n        addClause(bq, new Term(query.field, pendingTerms.get(pos, new BytesRef())), 1, 1.0f, col.array.termState[pos]);\n      }\n      // Strip scores\n      final Query result = new ConstantScoreQuery(bq);\n      result.setBoost(query.getBoost());\n      return result;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"59ce67ef5584d0d65a576a6bbe06322cc84eb9b0","date":1412077943,"type":4,"author":"Adrien Grand","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/search/ConstantScoreAutoRewrite#rewrite(IndexReader,MultiTermQuery).mjava","sourceNew":null,"sourceOld":"  @Override\n  public Query rewrite(final IndexReader reader, final MultiTermQuery query) throws IOException {\n\n    // Get the enum and start visiting terms.  If we\n    // exhaust the enum before hitting either of the\n    // cutoffs, we use ConstantBooleanQueryRewrite; else,\n    // ConstantFilterRewrite:\n    final int docCountCutoff = (int) ((docCountPercent / 100.) * reader.maxDoc());\n    final int termCountLimit = Math.min(BooleanQuery.getMaxClauseCount(), termCountCutoff);\n\n    final CutOffTermCollector col = new CutOffTermCollector(docCountCutoff, termCountLimit);\n    collectTerms(reader, query, col);\n    final int size = col.pendingTerms.size();\n    if (col.hasCutOff) {\n      return MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE.rewrite(reader, query);\n    } else {\n      final BooleanQuery bq = getTopLevelQuery();\n      if (size > 0) {\n        final BytesRefHash pendingTerms = col.pendingTerms;\n        final int sort[] = pendingTerms.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n        for(int i = 0; i < size; i++) {\n          final int pos = sort[i];\n          // docFreq is not used for constant score here, we pass 1\n          // to explicitely set a fake value, so it's not calculated\n          addClause(bq, new Term(query.field, pendingTerms.get(pos, new BytesRef())), 1, 1.0f, col.array.termState[pos]);\n        }\n      }\n      // Strip scores\n      final Query result = new ConstantScoreQuery(bq);\n      result.setBoost(query.getBoost());\n      return result;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d9a47902d6207303f5ed3e7aaca62ca33433af66","date":1412435312,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/search/ConstantScoreAutoRewrite#rewrite(IndexReader,MultiTermQuery).mjava","sourceNew":null,"sourceOld":"  @Override\n  public Query rewrite(final IndexReader reader, final MultiTermQuery query) throws IOException {\n\n    // Get the enum and start visiting terms.  If we\n    // exhaust the enum before hitting either of the\n    // cutoffs, we use ConstantBooleanQueryRewrite; else,\n    // ConstantFilterRewrite:\n    final int docCountCutoff = (int) ((docCountPercent / 100.) * reader.maxDoc());\n    final int termCountLimit = Math.min(BooleanQuery.getMaxClauseCount(), termCountCutoff);\n\n    final CutOffTermCollector col = new CutOffTermCollector(docCountCutoff, termCountLimit);\n    collectTerms(reader, query, col);\n    final int size = col.pendingTerms.size();\n    if (col.hasCutOff) {\n      return MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE.rewrite(reader, query);\n    } else {\n      final BooleanQuery bq = getTopLevelQuery();\n      if (size > 0) {\n        final BytesRefHash pendingTerms = col.pendingTerms;\n        final int sort[] = pendingTerms.sort(BytesRef.getUTF8SortedAsUnicodeComparator());\n        for(int i = 0; i < size; i++) {\n          final int pos = sort[i];\n          // docFreq is not used for constant score here, we pass 1\n          // to explicitely set a fake value, so it's not calculated\n          addClause(bq, new Term(query.field, pendingTerms.get(pos, new BytesRef())), 1, 1.0f, col.array.termState[pos]);\n        }\n      }\n      // Strip scores\n      final Query result = new ConstantScoreQuery(bq);\n      result.setBoost(query.getBoost());\n      return result;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"73c3f3499afb7895a585b82e3273508381608269":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d9a47902d6207303f5ed3e7aaca62ca33433af66":["73c3f3499afb7895a585b82e3273508381608269","59ce67ef5584d0d65a576a6bbe06322cc84eb9b0"],"59ce67ef5584d0d65a576a6bbe06322cc84eb9b0":["73c3f3499afb7895a585b82e3273508381608269"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["59ce67ef5584d0d65a576a6bbe06322cc84eb9b0"]},"commit2Childs":{"73c3f3499afb7895a585b82e3273508381608269":["d9a47902d6207303f5ed3e7aaca62ca33433af66","59ce67ef5584d0d65a576a6bbe06322cc84eb9b0"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["73c3f3499afb7895a585b82e3273508381608269"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d9a47902d6207303f5ed3e7aaca62ca33433af66":[],"59ce67ef5584d0d65a576a6bbe06322cc84eb9b0":["d9a47902d6207303f5ed3e7aaca62ca33433af66","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d9a47902d6207303f5ed3e7aaca62ca33433af66","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}