{"path":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrRecordWriter#write(K,V).mjava","commits":[{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrRecordWriter#write(K,V).mjava","pathOld":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/SolrRecordWriter#write(K,V).mjava","sourceNew":"  /**\n   * Write a record. This method accumulates records in to a batch, and when\n   * {@link #batchSize} items are present flushes it to the indexer. The writes\n   * can take a substantial amount of time, depending on {@link #batchSize}. If\n   * there is heavy disk contention the writes may take more than the 600 second\n   * default timeout.\n   */\n  @Override\n  public void write(K key, V value) throws IOException {\n    heartBeater.needHeartBeat();\n    try {\n      try {\n        SolrInputDocumentWritable sidw = (SolrInputDocumentWritable) value;\n        batch.add(sidw.getSolrInputDocument());\n        if (batch.size() >= batchSize) {\n          batchWriter.queueBatch(batch);\n          numDocsWritten += batch.size();\n          if (System.currentTimeMillis() >= nextLogTime) {\n            LOG.info(\"docsWritten: {}\", numDocsWritten);\n            nextLogTime += 10000;\n          }\n          batch.clear();\n        }\n      } catch (SolrServerException e) {\n        throw new IOException(e);\n      }\n    } finally {\n      heartBeater.cancelHeartBeat();\n    }\n\n  }\n\n","sourceOld":"  /**\n   * Write a record. This method accumulates records in to a batch, and when\n   * {@link #batchSize} items are present flushes it to the indexer. The writes\n   * can take a substantial amount of time, depending on {@link #batchSize}. If\n   * there is heavy disk contention the writes may take more than the 600 second\n   * default timeout.\n   */\n  @Override\n  public void write(K key, V value) throws IOException {\n    heartBeater.needHeartBeat();\n    try {\n      try {\n        SolrInputDocumentWritable sidw = (SolrInputDocumentWritable) value;\n        batch.add(sidw.getSolrInputDocument());\n        if (batch.size() >= batchSize) {\n          batchWriter.queueBatch(batch);\n          numDocsWritten += batch.size();\n          if (System.currentTimeMillis() >= nextLogTime) {\n            LOG.info(\"docsWritten: {}\", numDocsWritten);\n            nextLogTime += 10000;\n          }\n          batch.clear();\n        }\n      } catch (SolrServerException e) {\n        throw new IOException(e);\n      }\n    } finally {\n      heartBeater.cancelHeartBeat();\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrRecordWriter#write(K,V).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Write a record. This method accumulates records in to a batch, and when\n   * {@link #batchSize} items are present flushes it to the indexer. The writes\n   * can take a substantial amount of time, depending on {@link #batchSize}. If\n   * there is heavy disk contention the writes may take more than the 600 second\n   * default timeout.\n   */\n  @Override\n  public void write(K key, V value) throws IOException {\n    heartBeater.needHeartBeat();\n    try {\n      try {\n        SolrInputDocumentWritable sidw = (SolrInputDocumentWritable) value;\n        batch.add(sidw.getSolrInputDocument());\n        if (batch.size() >= batchSize) {\n          batchWriter.queueBatch(batch);\n          numDocsWritten += batch.size();\n          if (System.currentTimeMillis() >= nextLogTime) {\n            LOG.info(\"docsWritten: {}\", numDocsWritten);\n            nextLogTime += 10000;\n          }\n          batch.clear();\n        }\n      } catch (SolrServerException e) {\n        throw new IOException(e);\n      }\n    } finally {\n      heartBeater.cancelHeartBeat();\n    }\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd5bc858b8426d40bbe90b94120ead37c77d7954","date":1393812525,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrRecordWriter#write(K,V).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrRecordWriter#write(K,V).mjava","sourceNew":"  /**\n   * Write a record. This method accumulates records in to a batch, and when\n   * {@link #batchSize} items are present flushes it to the indexer. The writes\n   * can take a substantial amount of time, depending on {@link #batchSize}. If\n   * there is heavy disk contention the writes may take more than the 600 second\n   * default timeout.\n   */\n  @Override\n  public void write(K key, V value) throws IOException {\n    heartBeater.needHeartBeat();\n    try {\n      try {\n        SolrInputDocumentWritable sidw = (SolrInputDocumentWritable) value;\n        batch.add(sidw.getSolrInputDocument());\n        if (batch.size() >= batchSize) {\n          batchWriter.queueBatch(batch);\n          numDocsWritten += batch.size();\n          if (System.nanoTime() >= nextLogTime) {\n            LOG.info(\"docsWritten: {}\", numDocsWritten);\n            nextLogTime += TimeUnit.NANOSECONDS.convert(10, TimeUnit.SECONDS);\n          }\n          batch.clear();\n        }\n      } catch (SolrServerException e) {\n        throw new IOException(e);\n      }\n    } finally {\n      heartBeater.cancelHeartBeat();\n    }\n\n  }\n\n","sourceOld":"  /**\n   * Write a record. This method accumulates records in to a batch, and when\n   * {@link #batchSize} items are present flushes it to the indexer. The writes\n   * can take a substantial amount of time, depending on {@link #batchSize}. If\n   * there is heavy disk contention the writes may take more than the 600 second\n   * default timeout.\n   */\n  @Override\n  public void write(K key, V value) throws IOException {\n    heartBeater.needHeartBeat();\n    try {\n      try {\n        SolrInputDocumentWritable sidw = (SolrInputDocumentWritable) value;\n        batch.add(sidw.getSolrInputDocument());\n        if (batch.size() >= batchSize) {\n          batchWriter.queueBatch(batch);\n          numDocsWritten += batch.size();\n          if (System.currentTimeMillis() >= nextLogTime) {\n            LOG.info(\"docsWritten: {}\", numDocsWritten);\n            nextLogTime += 10000;\n          }\n          batch.clear();\n        }\n      } catch (SolrServerException e) {\n        throw new IOException(e);\n      }\n    } finally {\n      heartBeater.cancelHeartBeat();\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"12109b652e9210b8d58fca47f6c4a725d058a58e","date":1490373076,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrRecordWriter#write(K,V).mjava","sourceNew":null,"sourceOld":"  /**\n   * Write a record. This method accumulates records in to a batch, and when\n   * {@link #batchSize} items are present flushes it to the indexer. The writes\n   * can take a substantial amount of time, depending on {@link #batchSize}. If\n   * there is heavy disk contention the writes may take more than the 600 second\n   * default timeout.\n   */\n  @Override\n  public void write(K key, V value) throws IOException {\n    heartBeater.needHeartBeat();\n    try {\n      try {\n        SolrInputDocumentWritable sidw = (SolrInputDocumentWritable) value;\n        batch.add(sidw.getSolrInputDocument());\n        if (batch.size() >= batchSize) {\n          batchWriter.queueBatch(batch);\n          numDocsWritten += batch.size();\n          if (System.nanoTime() >= nextLogTime) {\n            LOG.info(\"docsWritten: {}\", numDocsWritten);\n            nextLogTime += TimeUnit.NANOSECONDS.convert(10, TimeUnit.SECONDS);\n          }\n          batch.clear();\n        }\n      } catch (SolrServerException e) {\n        throw new IOException(e);\n      }\n    } finally {\n      heartBeater.cancelHeartBeat();\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe1c4aa9af769a38e878f608070f672efbeac27f","date":1490594650,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrRecordWriter#write(K,V).mjava","sourceNew":null,"sourceOld":"  /**\n   * Write a record. This method accumulates records in to a batch, and when\n   * {@link #batchSize} items are present flushes it to the indexer. The writes\n   * can take a substantial amount of time, depending on {@link #batchSize}. If\n   * there is heavy disk contention the writes may take more than the 600 second\n   * default timeout.\n   */\n  @Override\n  public void write(K key, V value) throws IOException {\n    heartBeater.needHeartBeat();\n    try {\n      try {\n        SolrInputDocumentWritable sidw = (SolrInputDocumentWritable) value;\n        batch.add(sidw.getSolrInputDocument());\n        if (batch.size() >= batchSize) {\n          batchWriter.queueBatch(batch);\n          numDocsWritten += batch.size();\n          if (System.nanoTime() >= nextLogTime) {\n            LOG.info(\"docsWritten: {}\", numDocsWritten);\n            nextLogTime += TimeUnit.NANOSECONDS.convert(10, TimeUnit.SECONDS);\n          }\n          batch.clear();\n        }\n      } catch (SolrServerException e) {\n        throw new IOException(e);\n      }\n    } finally {\n      heartBeater.cancelHeartBeat();\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["fd5bc858b8426d40bbe90b94120ead37c77d7954"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"fe1c4aa9af769a38e878f608070f672efbeac27f":["fd5bc858b8426d40bbe90b94120ead37c77d7954"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["12109b652e9210b8d58fca47f6c4a725d058a58e"],"fd5bc858b8426d40bbe90b94120ead37c77d7954":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"]},"commit2Childs":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","fd5bc858b8426d40bbe90b94120ead37c77d7954"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["70f91c8322fbffe3a3a897ef20ea19119cac10cd","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"fe1c4aa9af769a38e878f608070f672efbeac27f":[],"fd5bc858b8426d40bbe90b94120ead37c77d7954":["12109b652e9210b8d58fca47f6c4a725d058a58e","fe1c4aa9af769a38e878f608070f672efbeac27f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","fe1c4aa9af769a38e878f608070f672efbeac27f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}