{"path":"backwards/src/test/org/apache/lucene/index/TestBackwardsCompatibility#assertCompressedFields29(Directory,boolean).mjava","commits":[{"id":"480d01e5b0ef8efb136d51670fec297ae5ae2c9c","date":1268821447,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"backwards/src/test/org/apache/lucene/index/TestBackwardsCompatibility#assertCompressedFields29(Directory,boolean).mjava","pathOld":"/dev/null","sourceNew":"  private void assertCompressedFields29(Directory dir, boolean shouldStillBeCompressed) throws IOException {\n    int count = 0;\n    final int TEXT_PLAIN_LENGTH = TEXT_TO_COMPRESS.length() * 2;\n    // FieldSelectorResult.SIZE returns 2*number_of_chars for String fields:\n    final int BINARY_PLAIN_LENGTH = BINARY_TO_COMPRESS.length;\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    try {\n      // look into sub readers and check if raw merge is on/off\n      List<IndexReader> readers = new ArrayList<IndexReader>();\n      ReaderUtil.gatherSubReaders(readers, reader);\n      for (IndexReader ir : readers) {\n        final FieldsReader fr = ((SegmentReader) ir).getFieldsReader();\n        assertTrue(\"for a 2.9 index, FieldsReader.canReadRawDocs() must be false and other way round for a trunk index\",\n          shouldStillBeCompressed != fr.canReadRawDocs());\n      }\n    \n      // test that decompression works correctly\n      for(int i=0; i<reader.maxDoc(); i++) {\n        if (!reader.isDeleted(i)) {\n          Document d = reader.document(i);\n          if (d.get(\"content3\") != null) continue;\n          count++;\n          Fieldable compressed = d.getFieldable(\"compressed\");\n          if (Integer.parseInt(d.get(\"id\")) % 2 == 0) {\n            assertFalse(compressed.isBinary());\n            assertEquals(\"incorrectly decompressed string\", TEXT_TO_COMPRESS, compressed.stringValue());\n          } else {\n            assertTrue(compressed.isBinary());\n            assertTrue(\"incorrectly decompressed binary\", Arrays.equals(BINARY_TO_COMPRESS, compressed.getBinaryValue()));\n          }\n        }\n      }\n      \n      // check if field was decompressed after optimize\n      for(int i=0; i<reader.maxDoc(); i++) {\n        if (!reader.isDeleted(i)) {\n          Document d = reader.document(i, new FieldSelector() {\n            public FieldSelectorResult accept(String fieldName) {\n              return (\"compressed\".equals(fieldName)) ? FieldSelectorResult.SIZE : FieldSelectorResult.LOAD;\n            }\n          });\n          if (d.get(\"content3\") != null) continue;\n          count++;\n          // read the size from the binary value using DataInputStream (this prevents us from doing the shift ops ourselves):\n          final DataInputStream ds = new DataInputStream(new ByteArrayInputStream(d.getFieldable(\"compressed\").getBinaryValue()));\n          final int actualSize = ds.readInt();\n          ds.close();\n          final int compressedSize = Integer.parseInt(d.get(\"compressedSize\"));\n          final boolean binary = Integer.parseInt(d.get(\"id\")) % 2 > 0;\n          final int shouldSize = shouldStillBeCompressed ?\n            compressedSize :\n            (binary ? BINARY_PLAIN_LENGTH : TEXT_PLAIN_LENGTH);\n          assertEquals(\"size incorrect\", shouldSize, actualSize);\n          if (!shouldStillBeCompressed) {\n            assertFalse(\"uncompressed field should have another size than recorded in index\", compressedSize == actualSize);\n          }\n        }\n      }\n      assertEquals(\"correct number of tests\", 34 * 2, count);\n    } finally {\n      reader.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/test/org/apache/lucene/index/TestBackwardsCompatibility#assertCompressedFields29(Directory,boolean).mjava","pathOld":"backwards/src/test/org/apache/lucene/index/TestBackwardsCompatibility#assertCompressedFields29(Directory,boolean).mjava","sourceNew":"  private void assertCompressedFields29(Directory dir, boolean shouldStillBeCompressed) throws IOException {\n    int count = 0;\n    final int TEXT_PLAIN_LENGTH = TEXT_TO_COMPRESS.length() * 2;\n    // FieldSelectorResult.SIZE returns 2*number_of_chars for String fields:\n    final int BINARY_PLAIN_LENGTH = BINARY_TO_COMPRESS.length;\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    try {\n      // look into sub readers and check if raw merge is on/off\n      List<IndexReader> readers = new ArrayList<IndexReader>();\n      ReaderUtil.gatherSubReaders(readers, reader);\n      for (IndexReader ir : readers) {\n        final FieldsReader fr = ((SegmentReader) ir).getFieldsReader();\n        assertTrue(\"for a 2.9 index, FieldsReader.canReadRawDocs() must be false and other way round for a trunk index\",\n          shouldStillBeCompressed != fr.canReadRawDocs());\n      }\n    \n      // test that decompression works correctly\n      for(int i=0; i<reader.maxDoc(); i++) {\n        if (!reader.isDeleted(i)) {\n          Document d = reader.document(i);\n          if (d.get(\"content3\") != null) continue;\n          count++;\n          Fieldable compressed = d.getFieldable(\"compressed\");\n          if (Integer.parseInt(d.get(\"id\")) % 2 == 0) {\n            assertFalse(compressed.isBinary());\n            assertEquals(\"incorrectly decompressed string\", TEXT_TO_COMPRESS, compressed.stringValue());\n          } else {\n            assertTrue(compressed.isBinary());\n            assertTrue(\"incorrectly decompressed binary\", Arrays.equals(BINARY_TO_COMPRESS, compressed.getBinaryValue()));\n          }\n        }\n      }\n      \n      // check if field was decompressed after optimize\n      for(int i=0; i<reader.maxDoc(); i++) {\n        if (!reader.isDeleted(i)) {\n          Document d = reader.document(i, new FieldSelector() {\n            public FieldSelectorResult accept(String fieldName) {\n              return (\"compressed\".equals(fieldName)) ? FieldSelectorResult.SIZE : FieldSelectorResult.LOAD;\n            }\n          });\n          if (d.get(\"content3\") != null) continue;\n          count++;\n          // read the size from the binary value using DataInputStream (this prevents us from doing the shift ops ourselves):\n          final DataInputStream ds = new DataInputStream(new ByteArrayInputStream(d.getFieldable(\"compressed\").getBinaryValue()));\n          final int actualSize = ds.readInt();\n          ds.close();\n          final int compressedSize = Integer.parseInt(d.get(\"compressedSize\"));\n          final boolean binary = Integer.parseInt(d.get(\"id\")) % 2 > 0;\n          final int shouldSize = shouldStillBeCompressed ?\n            compressedSize :\n            (binary ? BINARY_PLAIN_LENGTH : TEXT_PLAIN_LENGTH);\n          assertEquals(\"size incorrect\", shouldSize, actualSize);\n          if (!shouldStillBeCompressed) {\n            assertFalse(\"uncompressed field should have another size than recorded in index\", compressedSize == actualSize);\n          }\n        }\n      }\n      assertEquals(\"correct number of tests\", 34 * 2, count);\n    } finally {\n      reader.close();\n    }\n  }\n\n","sourceOld":"  private void assertCompressedFields29(Directory dir, boolean shouldStillBeCompressed) throws IOException {\n    int count = 0;\n    final int TEXT_PLAIN_LENGTH = TEXT_TO_COMPRESS.length() * 2;\n    // FieldSelectorResult.SIZE returns 2*number_of_chars for String fields:\n    final int BINARY_PLAIN_LENGTH = BINARY_TO_COMPRESS.length;\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    try {\n      // look into sub readers and check if raw merge is on/off\n      List<IndexReader> readers = new ArrayList<IndexReader>();\n      ReaderUtil.gatherSubReaders(readers, reader);\n      for (IndexReader ir : readers) {\n        final FieldsReader fr = ((SegmentReader) ir).getFieldsReader();\n        assertTrue(\"for a 2.9 index, FieldsReader.canReadRawDocs() must be false and other way round for a trunk index\",\n          shouldStillBeCompressed != fr.canReadRawDocs());\n      }\n    \n      // test that decompression works correctly\n      for(int i=0; i<reader.maxDoc(); i++) {\n        if (!reader.isDeleted(i)) {\n          Document d = reader.document(i);\n          if (d.get(\"content3\") != null) continue;\n          count++;\n          Fieldable compressed = d.getFieldable(\"compressed\");\n          if (Integer.parseInt(d.get(\"id\")) % 2 == 0) {\n            assertFalse(compressed.isBinary());\n            assertEquals(\"incorrectly decompressed string\", TEXT_TO_COMPRESS, compressed.stringValue());\n          } else {\n            assertTrue(compressed.isBinary());\n            assertTrue(\"incorrectly decompressed binary\", Arrays.equals(BINARY_TO_COMPRESS, compressed.getBinaryValue()));\n          }\n        }\n      }\n      \n      // check if field was decompressed after optimize\n      for(int i=0; i<reader.maxDoc(); i++) {\n        if (!reader.isDeleted(i)) {\n          Document d = reader.document(i, new FieldSelector() {\n            public FieldSelectorResult accept(String fieldName) {\n              return (\"compressed\".equals(fieldName)) ? FieldSelectorResult.SIZE : FieldSelectorResult.LOAD;\n            }\n          });\n          if (d.get(\"content3\") != null) continue;\n          count++;\n          // read the size from the binary value using DataInputStream (this prevents us from doing the shift ops ourselves):\n          final DataInputStream ds = new DataInputStream(new ByteArrayInputStream(d.getFieldable(\"compressed\").getBinaryValue()));\n          final int actualSize = ds.readInt();\n          ds.close();\n          final int compressedSize = Integer.parseInt(d.get(\"compressedSize\"));\n          final boolean binary = Integer.parseInt(d.get(\"id\")) % 2 > 0;\n          final int shouldSize = shouldStillBeCompressed ?\n            compressedSize :\n            (binary ? BINARY_PLAIN_LENGTH : TEXT_PLAIN_LENGTH);\n          assertEquals(\"size incorrect\", shouldSize, actualSize);\n          if (!shouldStillBeCompressed) {\n            assertFalse(\"uncompressed field should have another size than recorded in index\", compressedSize == actualSize);\n          }\n        }\n      }\n      assertEquals(\"correct number of tests\", 34 * 2, count);\n    } finally {\n      reader.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"]},"commit2Childs":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}