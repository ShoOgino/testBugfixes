{"path":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","commits":[{"id":"2dd9934a49477c83301120ba51827d91eb3606d5","date":1353767072,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        final BinaryDocValues ramInstance = valuesIn.newRAMInstance();\n        return new DocTerms() {\n\n          @Override\n          public BytesRef getTerm(int docID, BytesRef ret) {\n            ramInstance.get(docID, ret);\n            return ret;\n          }\n\n          @Override\n          public boolean exists(int docID) {\n            // nocommit lying ...?\n            return true;\n          }\n\n          @Override\n          public int size() {\n            return ramInstance.size();\n          }     \n        };\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f7178a82d1134111f4511f28bb9ad57573a57d93","date":1354112608,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        int sameLength = -2;\n        int maxLength = -1;\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (sameLength == -2) {\n              sameLength = term.length;\n            } else if (sameLength != term.length) {\n              sameLength = -1;\n            }\n            maxLength = Math.max(maxLength, term.length);\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable(), maxLength, sameLength >= 0);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"127981e5a1e1d1425c5fdc816ceacf753ca70ee4","date":1354205321,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn.newRAMInstance();\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        int sameLength = -2;\n        int maxLength = -1;\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (sameLength == -2) {\n              sameLength = term.length;\n            } else if (sameLength != term.length) {\n              sameLength = -1;\n            }\n            maxLength = Math.max(maxLength, term.length);\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable(), maxLength, sameLength >= 0);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        int sameLength = -2;\n        int maxLength = -1;\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (sameLength == -2) {\n              sameLength = term.length;\n            } else if (sameLength != term.length) {\n              sameLength = -1;\n            }\n            maxLength = Math.max(maxLength, term.length);\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable(), maxLength, sameLength >= 0);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4bc9c79bf95b1262e0a6908ffbd895de19e33dc2","date":1354573582,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        int sameLength = -2;\n        int maxLength = -1;\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (sameLength == -2) {\n              sameLength = term.length;\n            } else if (sameLength != term.length) {\n              sameLength = -1;\n            }\n            maxLength = Math.max(maxLength, term.length);\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable(), maxLength, sameLength >= 0);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn.newRAMInstance();\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        int sameLength = -2;\n        int maxLength = -1;\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (sameLength == -2) {\n              sameLength = term.length;\n            } else if (sameLength != term.length) {\n              sameLength = -1;\n            }\n            maxLength = Math.max(maxLength, term.length);\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable(), maxLength, sameLength >= 0);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3e29774db46ad98ca4a8d7fcbfab633ebc01f358","date":1355170812,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n\n      // TODO: would be nice to fallback to SortedDV if it's\n      // available but BinaryDV isn't?\n\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        int sameLength = -2;\n        int maxLength = -1;\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (sameLength == -2) {\n              sameLength = term.length;\n            } else if (sameLength != term.length) {\n              sameLength = -1;\n            }\n            maxLength = Math.max(maxLength, term.length);\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable(), maxLength, sameLength >= 0);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        int sameLength = -2;\n        int maxLength = -1;\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (sameLength == -2) {\n              sameLength = term.length;\n            } else if (sameLength != term.length) {\n              sameLength = -1;\n            }\n            maxLength = Math.max(maxLength, term.length);\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable(), maxLength, sameLength >= 0);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1ed9002c5afac843c7f2d04d88e74b40d627e1af","date":1357602069,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n\n      // TODO: would be nice to fallback to SortedDV if it's\n      // available but BinaryDV isn't?\n\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n\n      // TODO: would be nice to fallback to SortedDV if it's\n      // available but BinaryDV isn't?\n\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        int sameLength = -2;\n        int maxLength = -1;\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (sameLength == -2) {\n              sameLength = term.length;\n            } else if (sameLength != term.length) {\n              sameLength = -1;\n            }\n            maxLength = Math.max(maxLength, term.length);\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable(), maxLength, sameLength >= 0);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n\n      // TODO: would be nice to fallback to SortedDV if it's\n      // available but BinaryDV isn't?\n\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n\n      // TODO: would be nice to fallback to SortedDV if it's\n      // available but BinaryDV isn't?\n\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"59b9b9ac7f92816370e9f792b67516f47bea224c","date":1358294103,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn == null) {\n        // nocommit is this auto-fallback ... OK?\n        valuesIn = reader.getSortedDocValues(key.field);\n      }\n\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n\n      // TODO: would be nice to fallback to SortedDV if it's\n      // available but BinaryDV isn't?\n\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6312aec6ba581f919d406ceff362bef430382c31","date":1358775555,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn == null) {\n        // nocommit is this auto-fallback ... OK?\n        valuesIn = reader.getSortedDocValues(key.field);\n      }\n\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","bugFix":null,"bugIntro":["87d6f9603307ae2ad642fb01deedf031320fd0c3"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dfc9a1c0f8c40dae949281597c97e61defe903fc","date":1359040461,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"87d6f9603307ae2ad642fb01deedf031320fd0c3","date":1377877563,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n    }\n\n","bugFix":["6312aec6ba581f919d406ceff362bef430382c31","a4d374b2bebd0d52acaa61038fbf23068620fba7"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","date":1399816179,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":5,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)\n        throws IOException {\n\n      // TODO: would be nice to first check if DocTermsIndex\n      // was already cached for this field and then return\n      // that instead, to avoid insanity\n\n      final int maxDoc = reader.maxDoc();\n      Terms terms = reader.terms(key.field);\n\n      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n      final int termCountHardLimit = maxDoc;\n\n      // Holds the actual term data, expanded.\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBPV;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = terms.size();\n        if (numUniqueTerms != -1L) {\n          if (numUniqueTerms > termCountHardLimit) {\n            numUniqueTerms = termCountHardLimit;\n          }\n          startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n        } else {\n          startBPV = 1;\n        }\n      } else {\n        startBPV = 1;\n      }\n\n      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n      // pointer==0 means not set\n      bytes.copyUsingLengthPrefix(new BytesRef());\n\n      if (terms != null) {\n        int termCount = 0;\n        final TermsEnum termsEnum = terms.iterator(null);\n        DocsEnum docs = null;\n        while(true) {\n          if (termCount++ == termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            break;\n          }\n\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          final long pointer = bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n              break;\n            }\n            docToOffset.set(docID, pointer);\n          }\n        }\n      }\n\n      final PackedInts.Reader offsetReader = docToOffset.getMutable();\n      if (setDocsWithField) {\n        wrapper.setDocsWithField(reader, key.field, new Bits() {\n          @Override\n          public boolean get(int index) {\n            return offsetReader.get(index) != 0;\n          }\n\n          @Override\n          public int length() {\n            return maxDoc;\n          }\n        });\n      }\n      // maybe an int-only impl?\n      return new BinaryDocValuesImpl(bytes.freeze(true), offsetReader);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["1ed9002c5afac843c7f2d04d88e74b40d627e1af","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"59b9b9ac7f92816370e9f792b67516f47bea224c":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064"],"2dd9934a49477c83301120ba51827d91eb3606d5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f7178a82d1134111f4511f28bb9ad57573a57d93":["2dd9934a49477c83301120ba51827d91eb3606d5"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["87d6f9603307ae2ad642fb01deedf031320fd0c3"],"87d6f9603307ae2ad642fb01deedf031320fd0c3":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"56572ec06f1407c066d6b7399413178b33176cd8":["87d6f9603307ae2ad642fb01deedf031320fd0c3","93dd449115a9247533e44bab47e8429e5dccbc6d"],"3e29774db46ad98ca4a8d7fcbfab633ebc01f358":["4bc9c79bf95b1262e0a6908ffbd895de19e33dc2"],"dfc9a1c0f8c40dae949281597c97e61defe903fc":["6312aec6ba581f919d406ceff362bef430382c31"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","dfc9a1c0f8c40dae949281597c97e61defe903fc"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["87d6f9603307ae2ad642fb01deedf031320fd0c3","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"127981e5a1e1d1425c5fdc816ceacf753ca70ee4":["f7178a82d1134111f4511f28bb9ad57573a57d93"],"6312aec6ba581f919d406ceff362bef430382c31":["59b9b9ac7f92816370e9f792b67516f47bea224c"],"4bc9c79bf95b1262e0a6908ffbd895de19e33dc2":["127981e5a1e1d1425c5fdc816ceacf753ca70ee4"],"1ed9002c5afac843c7f2d04d88e74b40d627e1af":["3e29774db46ad98ca4a8d7fcbfab633ebc01f358"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["93dd449115a9247533e44bab47e8429e5dccbc6d"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["59b9b9ac7f92816370e9f792b67516f47bea224c"],"59b9b9ac7f92816370e9f792b67516f47bea224c":["6312aec6ba581f919d406ceff362bef430382c31"],"2dd9934a49477c83301120ba51827d91eb3606d5":["f7178a82d1134111f4511f28bb9ad57573a57d93"],"f7178a82d1134111f4511f28bb9ad57573a57d93":["127981e5a1e1d1425c5fdc816ceacf753ca70ee4"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"87d6f9603307ae2ad642fb01deedf031320fd0c3":["b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","56572ec06f1407c066d6b7399413178b33176cd8","93dd449115a9247533e44bab47e8429e5dccbc6d"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"3e29774db46ad98ca4a8d7fcbfab633ebc01f358":["1ed9002c5afac843c7f2d04d88e74b40d627e1af"],"dfc9a1c0f8c40dae949281597c97e61defe903fc":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["87d6f9603307ae2ad642fb01deedf031320fd0c3"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["56572ec06f1407c066d6b7399413178b33176cd8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","2dd9934a49477c83301120ba51827d91eb3606d5","d4d69c535930b5cce125cff868d40f6373dc27d4"],"127981e5a1e1d1425c5fdc816ceacf753ca70ee4":["4bc9c79bf95b1262e0a6908ffbd895de19e33dc2"],"6312aec6ba581f919d406ceff362bef430382c31":["dfc9a1c0f8c40dae949281597c97e61defe903fc"],"4bc9c79bf95b1262e0a6908ffbd895de19e33dc2":["3e29774db46ad98ca4a8d7fcbfab633ebc01f358"],"1ed9002c5afac843c7f2d04d88e74b40d627e1af":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["56572ec06f1407c066d6b7399413178b33176cd8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}