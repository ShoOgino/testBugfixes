{"path":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      Term term = terms.term();\n      /* not-tokenized, but indexed field */\n      if (term != null && term.field() != DocMaker.ID_FIELD) { \n        termDocs.seek(terms.term());\n        while (termDocs.next())\n          totalTokenCount2 += termDocs.freq();\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      Term term = terms.term();\n      /* not-tokenized, but indexed field */\n      if (term != null && term.field() != DocMaker.ID_FIELD) { \n        termDocs.seek(terms.term());\n        while (termDocs.next())\n          totalTokenCount2 += termDocs.freq();\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName == DocMaker.ID_FIELD)\n        continue;\n      TermsEnum terms = fields.terms();\n      DocsEnum docs = null;\n      while(terms.next() != null) {\n        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);\n        while(docs.nextDoc() != docs.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    TermEnum terms = reader.terms();\n    TermDocs termDocs = reader.termDocs();\n    int totalTokenCount2 = 0;\n    while(terms.next()) {\n      Term term = terms.term();\n      /* not-tokenized, but indexed field */\n      if (term != null && term.field() != DocMaker.ID_FIELD) { \n        termDocs.seek(terms.term());\n        while (termDocs.next())\n          totalTokenCount2 += termDocs.freq();\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d572389229127c297dd1fa5ce4758e1cec41e799","date":1273610938,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.MockAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName == DocMaker.ID_FIELD)\n        continue;\n      TermsEnum terms = fields.terms();\n      DocsEnum docs = null;\n      while(terms.next() != null) {\n        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);\n        while(docs.nextDoc() != docs.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName == DocMaker.ID_FIELD)\n        continue;\n      TermsEnum terms = fields.terms();\n      DocsEnum docs = null;\n      while(terms.next() != null) {\n        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);\n        while(docs.nextDoc() != docs.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"af0757679472670141514cb791eafeed05abc4e5","date":1292883496,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.MockAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName == DocMaker.ID_FIELD || fieldName == DocMaker.DATE_MSEC_FIELD || fieldName == DocMaker.TIME_SEC_FIELD) {\n        continue;\n      }\n      TermsEnum terms = fields.terms();\n      DocsEnum docs = null;\n      while(terms.next() != null) {\n        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);\n        while(docs.nextDoc() != docs.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.MockAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName == DocMaker.ID_FIELD)\n        continue;\n      TermsEnum terms = fields.terms();\n      DocsEnum docs = null;\n      while(terms.next() != null) {\n        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);\n        while(docs.nextDoc() != docs.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.MockAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName == DocMaker.ID_FIELD || fieldName == DocMaker.DATE_MSEC_FIELD || fieldName == DocMaker.TIME_SEC_FIELD) {\n        continue;\n      }\n      TermsEnum terms = fields.terms();\n      DocsEnum docs = null;\n      while(terms.next() != null) {\n        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);\n        while(docs.nextDoc() != docs.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.MockAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName == DocMaker.ID_FIELD)\n        continue;\n      TermsEnum terms = fields.terms();\n      DocsEnum docs = null;\n      while(terms.next() != null) {\n        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);\n        while(docs.nextDoc() != docs.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ecc11368dc265bfdad90214f8bf5da99016ab1e2","date":1294144090,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.MockAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName == DocMaker.ID_FIELD || fieldName == DocMaker.DATE_MSEC_FIELD || fieldName == DocMaker.TIME_SEC_FIELD) {\n        continue;\n      }\n      TermsEnum terms = fields.terms();\n      DocsEnum docs = null;\n      while(terms.next() != null) {\n        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);\n        while(docs.nextDoc() != docs.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.MockAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName == DocMaker.ID_FIELD || fieldName == DocMaker.DATE_MSEC_FIELD || fieldName == DocMaker.TIME_SEC_FIELD) {\n        continue;\n      }\n      TermsEnum terms = fields.terms();\n      DocsEnum docs = null;\n      while(terms.next() != null) {\n        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);\n        while(docs.nextDoc() != docs.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":null,"sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.MockAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName == DocMaker.ID_FIELD)\n        continue;\n      TermsEnum terms = fields.terms();\n      DocsEnum docs = null;\n      while(terms.next() != null) {\n        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);\n        while(docs.nextDoc() != docs.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":5,"author":"Michael Busch","isMerge":true,"pathNew":"modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","pathOld":"lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic#testReadTokens().mjava","sourceNew":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.MockAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName == DocMaker.ID_FIELD || fieldName == DocMaker.DATE_MSEC_FIELD || fieldName == DocMaker.TIME_SEC_FIELD) {\n        continue;\n      }\n      TermsEnum terms = fields.terms();\n      DocsEnum docs = null;\n      while(terms.next() != null) {\n        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);\n        while(docs.nextDoc() != docs.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","sourceOld":"  /**\n   * Test ReadTokensTask\n   */\n  public void testReadTokens() throws Exception {\n\n    // We will call ReadTokens on this many docs\n    final int NUM_DOCS = 20;\n\n    // Read tokens from first NUM_DOCS docs from Reuters and\n    // then build index from the same docs\n    String algLines1[] = {\n      \"# ----- properties \",\n      \"analyzer=org.apache.lucene.analysis.MockAnalyzer\",\n      \"content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource\",\n      \"docs.file=\" + getReuters20LinesFile(),\n      \"# ----- alg \",\n      \"{ReadTokens}: \" + NUM_DOCS,\n      \"ResetSystemErase\",\n      \"CreateIndex\",\n      \"{AddDoc}: \" + NUM_DOCS,\n      \"CloseIndex\",\n    };\n\n    // Run algo\n    Benchmark benchmark = execBenchmark(algLines1);\n\n    List<TaskStats> stats = benchmark.getRunData().getPoints().taskStats();\n\n    // Count how many tokens all ReadTokens saw\n    int totalTokenCount1 = 0;\n    for (final TaskStats stat : stats) {\n      if (stat.getTask().getName().equals(\"ReadTokens\")) {\n        totalTokenCount1 += stat.getCount();\n      }\n    }\n\n    // Separately count how many tokens are actually in the index:\n    IndexReader reader = IndexReader.open(benchmark.getRunData().getDirectory(), true);\n    assertEquals(NUM_DOCS, reader.numDocs());\n\n    int totalTokenCount2 = 0;\n\n    FieldsEnum fields = MultiFields.getFields(reader).iterator();\n    String fieldName = null;\n    while((fieldName = fields.next()) != null) {\n      if (fieldName == DocMaker.ID_FIELD || fieldName == DocMaker.DATE_MSEC_FIELD || fieldName == DocMaker.TIME_SEC_FIELD) {\n        continue;\n      }\n      TermsEnum terms = fields.terms();\n      DocsEnum docs = null;\n      while(terms.next() != null) {\n        docs = terms.docs(MultiFields.getDeletedDocs(reader), docs);\n        while(docs.nextDoc() != docs.NO_MORE_DOCS) {\n          totalTokenCount2 += docs.freq();\n        }\n      }\n    }\n    reader.close();\n\n    // Make sure they are the same\n    assertEquals(totalTokenCount1, totalTokenCount2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70ad682703b8585f5d0a637efec044d57ec05efb":["d572389229127c297dd1fa5ce4758e1cec41e799","ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"ecc11368dc265bfdad90214f8bf5da99016ab1e2":["af0757679472670141514cb791eafeed05abc4e5"],"af0757679472670141514cb791eafeed05abc4e5":["d572389229127c297dd1fa5ce4758e1cec41e799"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d572389229127c297dd1fa5ce4758e1cec41e799":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["d572389229127c297dd1fa5ce4758e1cec41e799","af0757679472670141514cb791eafeed05abc4e5"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"70ad682703b8585f5d0a637efec044d57ec05efb":[],"ecc11368dc265bfdad90214f8bf5da99016ab1e2":["70ad682703b8585f5d0a637efec044d57ec05efb","868da859b43505d9d2a023bfeae6dd0c795f5295","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"af0757679472670141514cb791eafeed05abc4e5":["ecc11368dc265bfdad90214f8bf5da99016ab1e2","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"d572389229127c297dd1fa5ce4758e1cec41e799":["70ad682703b8585f5d0a637efec044d57ec05efb","af0757679472670141514cb791eafeed05abc4e5","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["d572389229127c297dd1fa5ce4758e1cec41e799"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"868da859b43505d9d2a023bfeae6dd0c795f5295":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["70ad682703b8585f5d0a637efec044d57ec05efb","868da859b43505d9d2a023bfeae6dd0c795f5295","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}