{"path":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","commits":[{"id":"d15f7215dbedf8f3258d977583979d3164ae8cf9","date":1298994434,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"/dev/null","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 1000 * RANDOM_MULTIPLIER;\n    int numThreads = _TestUtil.nextInt(random, 4, 8);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    BytesRef spare = new BytesRef();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = randomString();\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute bytes = ts.addAttribute(TermToBytesRefAttribute.class);\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      bytes.toBytesRef(spare);\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(spare));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            BytesRef spare = new BytesRef();\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute bytes = ts.addAttribute(TermToBytesRefAttribute.class);\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              bytes.toBytesRef(spare);\n              assertEquals(expected, spare);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7310644c70fcacbd1946d93bb21d0019114b0461","date":1299009185,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 1000;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    BytesRef spare = new BytesRef();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = randomString();\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute bytes = ts.addAttribute(TermToBytesRefAttribute.class);\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      bytes.toBytesRef(spare);\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(spare));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            BytesRef spare = new BytesRef();\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute bytes = ts.addAttribute(TermToBytesRefAttribute.class);\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              bytes.toBytesRef(spare);\n              assertEquals(expected, spare);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 1000 * RANDOM_MULTIPLIER;\n    int numThreads = _TestUtil.nextInt(random, 4, 8);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    BytesRef spare = new BytesRef();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = randomString();\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute bytes = ts.addAttribute(TermToBytesRefAttribute.class);\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      bytes.toBytesRef(spare);\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(spare));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            BytesRef spare = new BytesRef();\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute bytes = ts.addAttribute(TermToBytesRefAttribute.class);\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              bytes.toBytesRef(spare);\n              assertEquals(expected, spare);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":["a23f39c6192b8fa0fe7ee4346e03e135531df8a0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a23f39c6192b8fa0fe7ee4346e03e135531df8a0","date":1299074175,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    BytesRef spare = new BytesRef();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = randomString();\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute bytes = ts.addAttribute(TermToBytesRefAttribute.class);\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      bytes.toBytesRef(spare);\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(spare));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            BytesRef spare = new BytesRef();\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute bytes = ts.addAttribute(TermToBytesRefAttribute.class);\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              bytes.toBytesRef(spare);\n              assertEquals(expected, spare);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 1000;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    BytesRef spare = new BytesRef();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = randomString();\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute bytes = ts.addAttribute(TermToBytesRefAttribute.class);\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      bytes.toBytesRef(spare);\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(spare));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            BytesRef spare = new BytesRef();\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute bytes = ts.addAttribute(TermToBytesRefAttribute.class);\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              bytes.toBytesRef(spare);\n              assertEquals(expected, spare);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":["7310644c70fcacbd1946d93bb21d0019114b0461"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3d07f1ae3b58102f36f3393c397d78ba4e547a4","date":1300715535,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = randomString();\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    BytesRef spare = new BytesRef();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = randomString();\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute bytes = ts.addAttribute(TermToBytesRefAttribute.class);\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      bytes.toBytesRef(spare);\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(spare));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            BytesRef spare = new BytesRef();\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute bytes = ts.addAttribute(TermToBytesRefAttribute.class);\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              bytes.toBytesRef(spare);\n              assertEquals(expected, spare);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"/dev/null","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = randomString();\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"/dev/null","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = randomString();\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"048cad1ce12b3a0447b467a839e6a6ff52747d11","date":1304337794,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = randomString();\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba1f0e2f00b4449f4f1fc7473a8287cb532d631e","date":1304347497,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = randomString();\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = randomString();\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d17d4fe0503a62f6522b1dd15204dd25cd231edf","date":1313599393,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a23f39c6192b8fa0fe7ee4346e03e135531df8a0":["7310644c70fcacbd1946d93bb21d0019114b0461"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"d17d4fe0503a62f6522b1dd15204dd25cd231edf":["048cad1ce12b3a0447b467a839e6a6ff52747d11"],"7310644c70fcacbd1946d93bb21d0019114b0461":["d15f7215dbedf8f3258d977583979d3164ae8cf9"],"a3776dccca01c11e7046323cfad46a3b4a471233":["b3d07f1ae3b58102f36f3393c397d78ba4e547a4","048cad1ce12b3a0447b467a839e6a6ff52747d11"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b3d07f1ae3b58102f36f3393c397d78ba4e547a4":["a23f39c6192b8fa0fe7ee4346e03e135531df8a0"],"ba1f0e2f00b4449f4f1fc7473a8287cb532d631e":["d619839baa8ce5503e496b94a9e42ad6f079293f","048cad1ce12b3a0447b467a839e6a6ff52747d11"],"048cad1ce12b3a0447b467a839e6a6ff52747d11":["b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"d15f7215dbedf8f3258d977583979d3164ae8cf9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d17d4fe0503a62f6522b1dd15204dd25cd231edf"]},"commit2Childs":{"a23f39c6192b8fa0fe7ee4346e03e135531df8a0":["b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":[],"d619839baa8ce5503e496b94a9e42ad6f079293f":["ba1f0e2f00b4449f4f1fc7473a8287cb532d631e"],"7310644c70fcacbd1946d93bb21d0019114b0461":["a23f39c6192b8fa0fe7ee4346e03e135531df8a0"],"d17d4fe0503a62f6522b1dd15204dd25cd231edf":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","d15f7215dbedf8f3258d977583979d3164ae8cf9"],"b3d07f1ae3b58102f36f3393c397d78ba4e547a4":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233","048cad1ce12b3a0447b467a839e6a6ff52747d11"],"ba1f0e2f00b4449f4f1fc7473a8287cb532d631e":[],"048cad1ce12b3a0447b467a839e6a6ff52747d11":["d17d4fe0503a62f6522b1dd15204dd25cd231edf","a3776dccca01c11e7046323cfad46a3b4a471233","ba1f0e2f00b4449f4f1fc7473a8287cb532d631e"],"d15f7215dbedf8f3258d977583979d3164ae8cf9":["7310644c70fcacbd1946d93bb21d0019114b0461"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","a3776dccca01c11e7046323cfad46a3b4a471233","ba1f0e2f00b4449f4f1fc7473a8287cb532d631e","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}