{"path":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","commits":[{"id":"7e477c2108982ba9974f73aa8800270c75cb4971","date":1327277332,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","pathOld":"/dev/null","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 4)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        case 2:\n          text = _TestUtil.randomHtmlishString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a89676536a5d3e2e875a9eed6b3f22a63cca643","date":1327356915,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","pathOld":"/dev/null","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 4)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        case 2:\n          text = _TestUtil.randomHtmlishString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","date":1327523564,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","pathOld":"/dev/null","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 4)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        case 2:\n          text = _TestUtil.randomHtmlishString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int,boolean).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 4)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        case 2:\n          text = _TestUtil.randomHtmlishString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 4)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        case 2:\n          text = _TestUtil.randomHtmlishString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      int remainder = random.nextInt(10);\n      Reader reader = new StringReader(text);\n      TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        reader = new StringReader(text);\n        ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertTokenStreamContents(ts, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions),\n            text.length());\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              toIntArray(positions),\n              text.length());\n        } else if (offsetAtt != null) {\n          // offset\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              null,\n              null,\n              text.length());\n        } else {\n          // terms only\n          assertTokenStreamContents(ts, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5a89676536a5d3e2e875a9eed6b3f22a63cca643":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7e477c2108982ba9974f73aa8800270c75cb4971"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["7e477c2108982ba9974f73aa8800270c75cb4971"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7e477c2108982ba9974f73aa8800270c75cb4971"],"7e477c2108982ba9974f73aa8800270c75cb4971":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"5a89676536a5d3e2e875a9eed6b3f22a63cca643":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","7e477c2108982ba9974f73aa8800270c75cb4971"],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":[],"7e477c2108982ba9974f73aa8800270c75cb4971":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","3a119bbc8703c10faa329ec201c654b3a35a1e3e","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}