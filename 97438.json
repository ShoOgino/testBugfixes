{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","commits":[{"id":"c024a3e8fec0a081cbf9539845db12f0dc84d029","date":1376654698,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","pathOld":"/dev/null","sourceNew":"  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(_TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = _TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = _TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = _TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["87d6f9603307ae2ad642fb01deedf031320fd0c3","402ad3ddc9da7b70da1b167667a60ece6a1381fb","402ad3ddc9da7b70da1b167667a60ece6a1381fb","402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2ceca04c06658aeb20e0a319ade784ad9a0576dd","date":1376662287,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","pathOld":"/dev/null","sourceNew":"  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(_TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = _TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = _TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = _TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"09a42395865f791464f0bd5f6118a4abbfa3eb8a","date":1376920143,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // nocommit: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(_TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = _TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = _TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = _TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(_TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = _TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = _TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = _TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"61558dab896ba60794837a7dd3b3be5b7940044d","date":1376939269,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(_TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = _TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = _TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = _TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // nocommit: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(_TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = _TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = _TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = _TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff","date":1377034255,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(_TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = _TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = _TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = _TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(_TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = _TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = _TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = _TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","pathOld":"/dev/null","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(_TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = _TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = _TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = _TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"87d6f9603307ae2ad642fb01deedf031320fd0c3","date":1377877563,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(_TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = _TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = _TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = _TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(_TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = _TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = _TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = _TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\");\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":["c024a3e8fec0a081cbf9539845db12f0dc84d029"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(_TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = _TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = _TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = _TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":["402ad3ddc9da7b70da1b167667a60ece6a1381fb","402ad3ddc9da7b70da1b167667a60ece6a1381fb","402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<byte[]>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0d579490a72f2e6297eaa648940611234c57cf1","date":1395917140,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(TestUtil.createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c","date":1396633078,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(TestUtil.createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2a0f5bb79c600763ffe7b8141df59a3169d31e48","date":1396689440,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(TestUtil.getTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.close();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","date":1399816179,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":5,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues#testHugeBinaryValueLimit().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase#testHugeBinaryValueLimit().mjava","sourceNew":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = DirectoryReader.open(w, true);\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","sourceOld":"  // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)\n  public void testHugeBinaryValueLimit() throws Exception {\n    // We only test DVFormats that have a limit\n    assumeFalse(\"test requires codec with limits on max binary field length\", codecAcceptsHugeBinaryValues(\"field\"));\n    Analyzer analyzer = new MockAnalyzer(random());\n    // FSDirectory because SimpleText will consume gobbs of\n    // space when storing big binary values:\n    Directory d = newFSDirectory(createTempDir(\"hugeBinaryValues\"));\n    boolean doFixed = random().nextBoolean();\n    int numDocs;\n    int fixedLength = 0;\n    if (doFixed) {\n      // Sometimes make all values fixed length since some\n      // codecs have different code paths for this:\n      numDocs = TestUtil.nextInt(random(), 10, 20);\n      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n    } else {\n      numDocs = TestUtil.nextInt(random(), 100, 200);\n    }\n    IndexWriter w = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n    List<byte[]> docBytes = new ArrayList<>();\n    long totalBytes = 0;\n    for(int docID=0;docID<numDocs;docID++) {\n      // we don't use RandomIndexWriter because it might add\n      // more docvalues than we expect !!!!\n\n      // Must be > 64KB in size to ensure more than 2 pages in\n      // PagedBytes would be needed:\n      int numBytes;\n      if (doFixed) {\n        numBytes = fixedLength;\n      } else if (docID == 0 || random().nextInt(5) == 3) {\n        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;\n      } else {\n        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n      }\n      totalBytes += numBytes;\n      if (totalBytes > 5 * 1024*1024) {\n        break;\n      }\n      byte[] bytes = new byte[numBytes];\n      random().nextBytes(bytes);\n      docBytes.add(bytes);\n      Document doc = new Document();      \n      BytesRef b = new BytesRef(bytes);\n      b.length = bytes.length;\n      doc.add(new BinaryDocValuesField(\"field\", b));\n      doc.add(new StringField(\"id\", \"\"+docID, Field.Store.YES));\n      w.addDocument(doc);\n    }\n    \n    DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);\n\n    BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, \"field\", false);\n    for(int docID=0;docID<docBytes.size();docID++) {\n      StoredDocument doc = ar.document(docID);\n      BytesRef bytes = new BytesRef();\n      s.get(docID, bytes);\n      byte[] expected = docBytes.get(Integer.parseInt(doc.get(\"id\")));\n      assertEquals(expected.length, bytes.length);\n      assertEquals(new BytesRef(expected), bytes);\n    }\n\n    ar.close();\n    d.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6613659748fe4411a7dcf85266e55db1f95f7315"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c"],"61558dab896ba60794837a7dd3b3be5b7940044d":["09a42395865f791464f0bd5f6118a4abbfa3eb8a"],"6613659748fe4411a7dcf85266e55db1f95f7315":["87d6f9603307ae2ad642fb01deedf031320fd0c3"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["c024a3e8fec0a081cbf9539845db12f0dc84d029","61558dab896ba60794837a7dd3b3be5b7940044d"],"87d6f9603307ae2ad642fb01deedf031320fd0c3":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"d0d579490a72f2e6297eaa648940611234c57cf1":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"56572ec06f1407c066d6b7399413178b33176cd8":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","93dd449115a9247533e44bab47e8429e5dccbc6d"],"09a42395865f791464f0bd5f6118a4abbfa3eb8a":["2ceca04c06658aeb20e0a319ade784ad9a0576dd"],"2ceca04c06658aeb20e0a319ade784ad9a0576dd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c024a3e8fec0a081cbf9539845db12f0dc84d029"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c"],"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c":["d0d579490a72f2e6297eaa648940611234c57cf1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"c024a3e8fec0a081cbf9539845db12f0dc84d029":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["93dd449115a9247533e44bab47e8429e5dccbc6d"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["2a0f5bb79c600763ffe7b8141df59a3169d31e48","d0d579490a72f2e6297eaa648940611234c57cf1"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"61558dab896ba60794837a7dd3b3be5b7940044d":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"6613659748fe4411a7dcf85266e55db1f95f7315":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","87d6f9603307ae2ad642fb01deedf031320fd0c3"],"87d6f9603307ae2ad642fb01deedf031320fd0c3":["6613659748fe4411a7dcf85266e55db1f95f7315"],"d0d579490a72f2e6297eaa648940611234c57cf1":["a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"09a42395865f791464f0bd5f6118a4abbfa3eb8a":["61558dab896ba60794837a7dd3b3be5b7940044d"],"2ceca04c06658aeb20e0a319ade784ad9a0576dd":["09a42395865f791464f0bd5f6118a4abbfa3eb8a"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["56572ec06f1407c066d6b7399413178b33176cd8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","2ceca04c06658aeb20e0a319ade784ad9a0576dd","c024a3e8fec0a081cbf9539845db12f0dc84d029"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","56572ec06f1407c066d6b7399413178b33176cd8","93dd449115a9247533e44bab47e8429e5dccbc6d"],"c024a3e8fec0a081cbf9539845db12f0dc84d029":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff","2ceca04c06658aeb20e0a319ade784ad9a0576dd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","56572ec06f1407c066d6b7399413178b33176cd8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}