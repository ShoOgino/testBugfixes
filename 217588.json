{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Analyzer,IndexReader,Collection[String],int).mjava","commits":[{"id":"ff4227bb146f97aabae888091c19e48c88dbb0db","date":1406758576,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5cdab62f058ea765dd33deb05b4f19b7d626c801","date":1406803479,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"379db3ad24c4f0214f30a122265a6d6be003a99d","date":1407537768,"type":1,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","date":1407854805,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRefBuilder spare = new CharsRefBuilder();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            spare.copyUTF8Bytes(text);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRefBuilder spare = new CharsRefBuilder();\n      if (terms != null) {\n        TermsEnum te = terms.iterator();\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            spare.copyUTF8Bytes(text);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRefBuilder spare = new CharsRefBuilder();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            spare.copyUTF8Bytes(text);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiTerms.getTerms(indexReader, field);\n      CharsRefBuilder spare = new CharsRefBuilder();\n      if (terms != null) {\n        TermsEnum te = terms.iterator();\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            spare.copyUTF8Bytes(text);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRefBuilder spare = new CharsRefBuilder();\n      if (terms != null) {\n        TermsEnum te = terms.iterator();\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            spare.copyUTF8Bytes(text);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"ff4227bb146f97aabae888091c19e48c88dbb0db":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"04e775de416dd2d8067b10db1c8af975a1d5017e":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["ff4227bb146f97aabae888091c19e48c88dbb0db"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["04e775de416dd2d8067b10db1c8af975a1d5017e"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"ff4227bb146f97aabae888091c19e48c88dbb0db":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ff4227bb146f97aabae888091c19e48c88dbb0db"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}