{"path":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","commits":[{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","pathOld":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","sourceNew":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName(), false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            try {\n              parser.printHelp(new PrintWriter(new OutputStreamWriter(System.out, \"UTF-8\")));\n            } catch (UnsupportedEncodingException e) {\n              throw new RuntimeException(\"Won't Happen for UTF-8\");\n            }  \n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# (Re)index all files that match all of the following conditions:\\n\" +\n              \"# 1) File is contained in dir tree hdfs:///user/$USER/solrloadtest/twitter/tweets\\n\" +\n              \"# 2) file name matches the glob pattern 'sample-statuses*.gz'\\n\" +\n              \"# 3) file was last modified less than 100000 minutes ago\\n\" +\n              \"# 4) file size is between 1 MB and 1 GB\\n\" +\n              \"# Also include extra library jar file containing JSON tweet Java parser:\\n\" +\n              \"hadoop jar target/search-mr-*-job.jar \" + \"com.cloudera.cdk.morphline.hadoop.find.HdfsFindTool\" + \" \\\\\\n\" + \n              \"  -find hdfs:///user/$USER/solrloadtest/twitter/tweets \\\\\\n\" + \n              \"  -type f \\\\\\n\" + \n              \"  -name 'sample-statuses*.gz' \\\\\\n\" + \n              \"  -mmin -1000000 \\\\\\n\" + \n              \"  -size -100000000c \\\\\\n\" + \n              \"  -size +1000000c \\\\\\n\" + \n              \"| sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" + \n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadJsonTestTweets.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 100 \\\\\\n\" + \n              \"  --input-list -\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\");\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n                  + \"If you are not using --go-live, pass the --shards argument. If you are building shards for \"\n                  + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for\"\n                  + \" a replicated cluster with --shard-url, pass replica urls consecutively and also pass --shards. \" \n                  + \"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n                  + \"Using --go-live requires either --shard-url or --zk-host.\");\n\n      Argument shardUrlsArg = clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\");\n      \n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + \"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\");\n\n      Argument shardsArg = clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\");\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              \"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\");\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","sourceOld":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName(), false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            try {\n              parser.printHelp(new PrintWriter(new OutputStreamWriter(System.out, \"UTF-8\")));\n            } catch (UnsupportedEncodingException e) {\n              throw new RuntimeException(\"Won't Happen for UTF-8\");\n            }  \n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# (Re)index all files that match all of the following conditions:\\n\" +\n              \"# 1) File is contained in dir tree hdfs:///user/$USER/solrloadtest/twitter/tweets\\n\" +\n              \"# 2) file name matches the glob pattern 'sample-statuses*.gz'\\n\" +\n              \"# 3) file was last modified less than 100000 minutes ago\\n\" +\n              \"# 4) file size is between 1 MB and 1 GB\\n\" +\n              \"# Also include extra library jar file containing JSON tweet Java parser:\\n\" +\n              \"hadoop jar target/search-mr-*-job.jar \" + \"com.cloudera.cdk.morphline.hadoop.find.HdfsFindTool\" + \" \\\\\\n\" + \n              \"  -find hdfs:///user/$USER/solrloadtest/twitter/tweets \\\\\\n\" + \n              \"  -type f \\\\\\n\" + \n              \"  -name 'sample-statuses*.gz' \\\\\\n\" + \n              \"  -mmin -1000000 \\\\\\n\" + \n              \"  -size -100000000c \\\\\\n\" + \n              \"  -size +1000000c \\\\\\n\" + \n              \"| sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" + \n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadJsonTestTweets.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 100 \\\\\\n\" + \n              \"  --input-list -\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\");\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n                  + \"If you are not using --go-live, pass the --shards argument. If you are building shards for \"\n                  + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for\"\n                  + \" a replicated cluster with --shard-url, pass replica urls consecutively and also pass --shards. \" \n                  + \"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n                  + \"Using --go-live requires either --shard-url or --zk-host.\");\n\n      Argument shardUrlsArg = clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\");\n      \n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + \"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\");\n\n      Argument shardsArg = clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\");\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              \"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\");\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"42d384b06aa87eae925b668b65f3246154f0b0fa","date":1386181725,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","sourceNew":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName(), false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            try {\n              parser.printHelp(new PrintWriter(new OutputStreamWriter(System.out, \"UTF-8\")));\n            } catch (UnsupportedEncodingException e) {\n              throw new RuntimeException(\"Won't Happen for UTF-8\");\n            }  \n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# (Re)index all files that match all of the following conditions:\\n\" +\n              \"# 1) File is contained in dir tree hdfs:///user/$USER/solrloadtest/twitter/tweets\\n\" +\n              \"# 2) file name matches the glob pattern 'sample-statuses*.gz'\\n\" +\n              \"# 3) file was last modified less than 100000 minutes ago\\n\" +\n              \"# 4) file size is between 1 MB and 1 GB\\n\" +\n              \"# Also include extra library jar file containing JSON tweet Java parser:\\n\" +\n              \"hadoop jar target/search-mr-*-job.jar \" + \"com.cloudera.cdk.morphline.hadoop.find.HdfsFindTool\" + \" \\\\\\n\" + \n              \"  -find hdfs:///user/$USER/solrloadtest/twitter/tweets \\\\\\n\" + \n              \"  -type f \\\\\\n\" + \n              \"  -name 'sample-statuses*.gz' \\\\\\n\" + \n              \"  -mmin -1000000 \\\\\\n\" + \n              \"  -size -100000000c \\\\\\n\" + \n              \"  -size +1000000c \\\\\\n\" + \n              \"| sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" + \n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadJsonTestTweets.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 100 \\\\\\n\" + \n              \"  --input-list -\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","sourceOld":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName(), false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            try {\n              parser.printHelp(new PrintWriter(new OutputStreamWriter(System.out, \"UTF-8\")));\n            } catch (UnsupportedEncodingException e) {\n              throw new RuntimeException(\"Won't Happen for UTF-8\");\n            }  \n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# (Re)index all files that match all of the following conditions:\\n\" +\n              \"# 1) File is contained in dir tree hdfs:///user/$USER/solrloadtest/twitter/tweets\\n\" +\n              \"# 2) file name matches the glob pattern 'sample-statuses*.gz'\\n\" +\n              \"# 3) file was last modified less than 100000 minutes ago\\n\" +\n              \"# 4) file size is between 1 MB and 1 GB\\n\" +\n              \"# Also include extra library jar file containing JSON tweet Java parser:\\n\" +\n              \"hadoop jar target/search-mr-*-job.jar \" + \"com.cloudera.cdk.morphline.hadoop.find.HdfsFindTool\" + \" \\\\\\n\" + \n              \"  -find hdfs:///user/$USER/solrloadtest/twitter/tweets \\\\\\n\" + \n              \"  -type f \\\\\\n\" + \n              \"  -name 'sample-statuses*.gz' \\\\\\n\" + \n              \"  -mmin -1000000 \\\\\\n\" + \n              \"  -size -100000000c \\\\\\n\" + \n              \"  -size +1000000c \\\\\\n\" + \n              \"| sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" + \n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadJsonTestTweets.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 100 \\\\\\n\" + \n              \"  --input-list -\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\");\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n                  + \"If you are not using --go-live, pass the --shards argument. If you are building shards for \"\n                  + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for\"\n                  + \" a replicated cluster with --shard-url, pass replica urls consecutively and also pass --shards. \" \n                  + \"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n                  + \"Using --go-live requires either --shard-url or --zk-host.\");\n\n      Argument shardUrlsArg = clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\");\n      \n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + \"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\");\n\n      Argument shardsArg = clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\");\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              \"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\");\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d1806dbd0764a9ee0958f91e02dc8629b7a0ac02","date":1386199730,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","sourceNew":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            try {\n              parser.printHelp(new PrintWriter(new OutputStreamWriter(System.out, \"UTF-8\")));\n            } catch (UnsupportedEncodingException e) {\n              throw new RuntimeException(\"Won't Happen for UTF-8\");\n            }  \n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# (Re)index all files that match all of the following conditions:\\n\" +\n              \"# 1) File is contained in dir tree hdfs:///user/$USER/solrloadtest/twitter/tweets\\n\" +\n              \"# 2) file name matches the glob pattern 'sample-statuses*.gz'\\n\" +\n              \"# 3) file was last modified less than 100000 minutes ago\\n\" +\n              \"# 4) file size is between 1 MB and 1 GB\\n\" +\n              \"# Also include extra library jar file containing JSON tweet Java parser:\\n\" +\n              \"hadoop jar target/solr-map-reduce-*.jar \" + \"com.cloudera.cdk.morphline.hadoop.find.HdfsFindTool\" + \" \\\\\\n\" + \n              \"  -find hdfs:///user/$USER/solrloadtest/twitter/tweets \\\\\\n\" + \n              \"  -type f \\\\\\n\" + \n              \"  -name 'sample-statuses*.gz' \\\\\\n\" + \n              \"  -mmin -1000000 \\\\\\n\" + \n              \"  -size -100000000c \\\\\\n\" + \n              \"  -size +1000000c \\\\\\n\" + \n              \"| sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" + \n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadJsonTestTweets.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 100 \\\\\\n\" + \n              \"  --input-list -\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","sourceOld":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName(), false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            try {\n              parser.printHelp(new PrintWriter(new OutputStreamWriter(System.out, \"UTF-8\")));\n            } catch (UnsupportedEncodingException e) {\n              throw new RuntimeException(\"Won't Happen for UTF-8\");\n            }  \n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# (Re)index all files that match all of the following conditions:\\n\" +\n              \"# 1) File is contained in dir tree hdfs:///user/$USER/solrloadtest/twitter/tweets\\n\" +\n              \"# 2) file name matches the glob pattern 'sample-statuses*.gz'\\n\" +\n              \"# 3) file was last modified less than 100000 minutes ago\\n\" +\n              \"# 4) file size is between 1 MB and 1 GB\\n\" +\n              \"# Also include extra library jar file containing JSON tweet Java parser:\\n\" +\n              \"hadoop jar target/search-mr-*-job.jar \" + \"com.cloudera.cdk.morphline.hadoop.find.HdfsFindTool\" + \" \\\\\\n\" + \n              \"  -find hdfs:///user/$USER/solrloadtest/twitter/tweets \\\\\\n\" + \n              \"  -type f \\\\\\n\" + \n              \"  -name 'sample-statuses*.gz' \\\\\\n\" + \n              \"  -mmin -1000000 \\\\\\n\" + \n              \"  -size -100000000c \\\\\\n\" + \n              \"  -size +1000000c \\\\\\n\" + \n              \"| sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" + \n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadJsonTestTweets.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 100 \\\\\\n\" + \n              \"  --input-list -\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/search-mr-*-job.jar \" + MapReduceIndexerTool.class.getName() + \" \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","pathOld":"/dev/null","sourceNew":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            try {\n              parser.printHelp(new PrintWriter(new OutputStreamWriter(System.out, \"UTF-8\")));\n            } catch (UnsupportedEncodingException e) {\n              throw new RuntimeException(\"Won't Happen for UTF-8\");\n            }  \n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# (Re)index all files that match all of the following conditions:\\n\" +\n              \"# 1) File is contained in dir tree hdfs:///user/$USER/solrloadtest/twitter/tweets\\n\" +\n              \"# 2) file name matches the glob pattern 'sample-statuses*.gz'\\n\" +\n              \"# 3) file was last modified less than 100000 minutes ago\\n\" +\n              \"# 4) file size is between 1 MB and 1 GB\\n\" +\n              \"# Also include extra library jar file containing JSON tweet Java parser:\\n\" +\n              \"hadoop jar target/solr-map-reduce-*.jar \" + \"com.cloudera.cdk.morphline.hadoop.find.HdfsFindTool\" + \" \\\\\\n\" + \n              \"  -find hdfs:///user/$USER/solrloadtest/twitter/tweets \\\\\\n\" + \n              \"  -type f \\\\\\n\" + \n              \"  -name 'sample-statuses*.gz' \\\\\\n\" + \n              \"  -mmin -1000000 \\\\\\n\" + \n              \"  -size -100000000c \\\\\\n\" + \n              \"  -size +1000000c \\\\\\n\" + \n              \"| sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" + \n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadJsonTestTweets.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 100 \\\\\\n\" + \n              \"  --input-list -\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"32a479bfb38669c431233257a64bb039707ff339","date":1393467800,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","sourceNew":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            parser.printHelp();\n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# (Re)index all files that match all of the following conditions:\\n\" +\n              \"# 1) File is contained in dir tree hdfs:///user/$USER/solrloadtest/twitter/tweets\\n\" +\n              \"# 2) file name matches the glob pattern 'sample-statuses*.gz'\\n\" +\n              \"# 3) file was last modified less than 100000 minutes ago\\n\" +\n              \"# 4) file size is between 1 MB and 1 GB\\n\" +\n              \"# Also include extra library jar file containing JSON tweet Java parser:\\n\" +\n              \"hadoop jar target/solr-map-reduce-*.jar \" + \"com.cloudera.cdk.morphline.hadoop.find.HdfsFindTool\" + \" \\\\\\n\" + \n              \"  -find hdfs:///user/$USER/solrloadtest/twitter/tweets \\\\\\n\" + \n              \"  -type f \\\\\\n\" + \n              \"  -name 'sample-statuses*.gz' \\\\\\n\" + \n              \"  -mmin -1000000 \\\\\\n\" + \n              \"  -size -100000000c \\\\\\n\" + \n              \"  -size +1000000c \\\\\\n\" + \n              \"| sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" + \n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadJsonTestTweets.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 100 \\\\\\n\" + \n              \"  --input-list -\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","sourceOld":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            try {\n              parser.printHelp(new PrintWriter(new OutputStreamWriter(System.out, \"UTF-8\")));\n            } catch (UnsupportedEncodingException e) {\n              throw new RuntimeException(\"Won't Happen for UTF-8\");\n            }  \n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# (Re)index all files that match all of the following conditions:\\n\" +\n              \"# 1) File is contained in dir tree hdfs:///user/$USER/solrloadtest/twitter/tweets\\n\" +\n              \"# 2) file name matches the glob pattern 'sample-statuses*.gz'\\n\" +\n              \"# 3) file was last modified less than 100000 minutes ago\\n\" +\n              \"# 4) file size is between 1 MB and 1 GB\\n\" +\n              \"# Also include extra library jar file containing JSON tweet Java parser:\\n\" +\n              \"hadoop jar target/solr-map-reduce-*.jar \" + \"com.cloudera.cdk.morphline.hadoop.find.HdfsFindTool\" + \" \\\\\\n\" + \n              \"  -find hdfs:///user/$USER/solrloadtest/twitter/tweets \\\\\\n\" + \n              \"  -type f \\\\\\n\" + \n              \"  -name 'sample-statuses*.gz' \\\\\\n\" + \n              \"  -mmin -1000000 \\\\\\n\" + \n              \"  -size -100000000c \\\\\\n\" + \n              \"  -size +1000000c \\\\\\n\" + \n              \"| sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" + \n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadJsonTestTweets.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 100 \\\\\\n\" + \n              \"  --input-list -\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"13f445c5bd6f19fd57d5a3ca0a35244c96f45aa8","date":1393532551,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","sourceNew":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            parser.printHelp();\n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# (Re)index all files that match all of the following conditions:\\n\" +\n              \"# 1) File is contained in dir tree hdfs:///user/$USER/solrloadtest/twitter/tweets\\n\" +\n              \"# 2) file name matches the glob pattern 'sample-statuses*.gz'\\n\" +\n              \"# 3) file was last modified less than 100000 minutes ago\\n\" +\n              \"# 4) file size is between 1 MB and 1 GB\\n\" +\n              \"# Also include extra library jar file containing JSON tweet Java parser:\\n\" +\n              \"hadoop jar target/solr-map-reduce-*.jar \" + \"com.cloudera.cdk.morphline.hadoop.find.HdfsFindTool\" + \" \\\\\\n\" + \n              \"  -find hdfs:///user/$USER/solrloadtest/twitter/tweets \\\\\\n\" + \n              \"  -type f \\\\\\n\" + \n              \"  -name 'sample-statuses*.gz' \\\\\\n\" + \n              \"  -mmin -1000000 \\\\\\n\" + \n              \"  -size -100000000c \\\\\\n\" + \n              \"  -size +1000000c \\\\\\n\" + \n              \"| sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" + \n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadJsonTestTweets.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 100 \\\\\\n\" + \n              \"  --input-list -\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","sourceOld":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            try {\n              parser.printHelp(new PrintWriter(new OutputStreamWriter(System.out, \"UTF-8\")));\n            } catch (UnsupportedEncodingException e) {\n              throw new RuntimeException(\"Won't Happen for UTF-8\");\n            }  \n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# (Re)index all files that match all of the following conditions:\\n\" +\n              \"# 1) File is contained in dir tree hdfs:///user/$USER/solrloadtest/twitter/tweets\\n\" +\n              \"# 2) file name matches the glob pattern 'sample-statuses*.gz'\\n\" +\n              \"# 3) file was last modified less than 100000 minutes ago\\n\" +\n              \"# 4) file size is between 1 MB and 1 GB\\n\" +\n              \"# Also include extra library jar file containing JSON tweet Java parser:\\n\" +\n              \"hadoop jar target/solr-map-reduce-*.jar \" + \"com.cloudera.cdk.morphline.hadoop.find.HdfsFindTool\" + \" \\\\\\n\" + \n              \"  -find hdfs:///user/$USER/solrloadtest/twitter/tweets \\\\\\n\" + \n              \"  -type f \\\\\\n\" + \n              \"  -name 'sample-statuses*.gz' \\\\\\n\" + \n              \"  -mmin -1000000 \\\\\\n\" + \n              \"  -size -100000000c \\\\\\n\" + \n              \"  -size +1000000c \\\\\\n\" + \n              \"| sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" + \n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadJsonTestTweets.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 100 \\\\\\n\" + \n              \"  --input-list -\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8e86c971b5eeaf62acd9ddfab36b34ecf92702da","date":1393542905,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","sourceNew":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            parser.printHelp();\n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","sourceOld":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            parser.printHelp();\n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# (Re)index all files that match all of the following conditions:\\n\" +\n              \"# 1) File is contained in dir tree hdfs:///user/$USER/solrloadtest/twitter/tweets\\n\" +\n              \"# 2) file name matches the glob pattern 'sample-statuses*.gz'\\n\" +\n              \"# 3) file was last modified less than 100000 minutes ago\\n\" +\n              \"# 4) file size is between 1 MB and 1 GB\\n\" +\n              \"# Also include extra library jar file containing JSON tweet Java parser:\\n\" +\n              \"hadoop jar target/solr-map-reduce-*.jar \" + \"com.cloudera.cdk.morphline.hadoop.find.HdfsFindTool\" + \" \\\\\\n\" + \n              \"  -find hdfs:///user/$USER/solrloadtest/twitter/tweets \\\\\\n\" + \n              \"  -type f \\\\\\n\" + \n              \"  -name 'sample-statuses*.gz' \\\\\\n\" + \n              \"  -mmin -1000000 \\\\\\n\" + \n              \"  -size -100000000c \\\\\\n\" + \n              \"  -size +1000000c \\\\\\n\" + \n              \"| sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" + \n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadJsonTestTweets.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 100 \\\\\\n\" + \n              \"  --input-list -\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c74a1d30fa4439a3687ae194fa516accc89d4f35","date":1395251530,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","sourceNew":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            parser.printHelp();\n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-2, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"0 is reserved for a mapper-only feature that may ship in a future release. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"-2 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        if (opts.reducers == 0) {\n          throw new ArgumentParserException(\"--reducers must not be zero\", parser); \n        }\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","sourceOld":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            parser.printHelp();\n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"0 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1a70d04f6bb2fdf51a08d4d2cb919057f29f0b1","date":1437834887,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","sourceNew":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            parser.printHelp();\n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-2, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"0 is reserved for a mapper-only feature that may ship in a future release. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"-2 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        Utils.configureLog4jProperties(opts.log4jConfigFile.getPath());\n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        if (opts.reducers == 0) {\n          throw new ArgumentParserException(\"--reducers must not be zero\", parser); \n        }\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","sourceOld":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            parser.printHelp();\n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-2, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"0 is reserved for a mapper-only feature that may ship in a future release. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"-2 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        PropertyConfigurator.configure(opts.log4jConfigFile.getPath());        \n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        if (opts.reducers == 0) {\n          throw new ArgumentParserException(\"--reducers must not be zero\", parser); \n        }\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"12109b652e9210b8d58fca47f6c4a725d058a58e","date":1490373076,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","sourceNew":null,"sourceOld":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            parser.printHelp();\n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-2, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"0 is reserved for a mapper-only feature that may ship in a future release. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"-2 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        Utils.configureLog4jProperties(opts.log4jConfigFile.getPath());\n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        if (opts.reducers == 0) {\n          throw new ArgumentParserException(\"--reducers must not be zero\", parser); \n        }\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe1c4aa9af769a38e878f608070f672efbeac27f","date":1490594650,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/MapReduceIndexerTool.MyArgumentParser#parseArgs(String[],Configuration,Options).mjava","sourceNew":null,"sourceOld":"    /**\n     * Parses the given command line arguments.\n     * \n     * @return exitCode null indicates the caller shall proceed with processing,\n     *         non-null indicates the caller shall exit the program with the\n     *         given exit status code.\n     */\n    public Integer parseArgs(String[] args, Configuration conf, Options opts) {\n      assert args != null;\n      assert conf != null;\n      assert opts != null;\n\n      if (args.length == 0) {\n        args = new String[] { \"--help\" };\n      }\n      \n      showNonSolrCloud = Arrays.asList(args).contains(SHOW_NON_SOLR_CLOUD); // intercept it first\n      \n      ArgumentParser parser = ArgumentParsers\n        .newArgumentParser(\"hadoop [GenericOptions]... jar solr-map-reduce-*.jar \", false)\n        .defaultHelp(true)\n        .description(\n          \"MapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files \" +\n          \"and writes the indexes into HDFS, in a flexible, scalable and fault-tolerant manner. \" +\n          \"It also supports merging the output shards into a set of live customer facing Solr servers, \" +\n          \"typically a SolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\" +\n          \"\\n\\n\" +\n          \"1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread \" +\n          \"indexing load more evenly among the mappers of the subsequent phase.\" +  \n          \"\\n\\n\" +\n          \"2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it \" +\n          \"and hands SolrInputDocuments to a set of reducers. \" +\n          \"The ETL functionality is flexible and \" +\n          \"customizable using chains of arbitrary morphline commands that pipe records from one transformation command to another. \" + \n          \"Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, \" +\n          \"PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional \" +\n          \"file or data formats can be added as morphline plugins. \" +\n          \"This is done by implementing a simple Java interface that consumes a record (e.g. a file in the form of an InputStream \" +\n          \"plus some headers plus contextual metadata) and generates as output zero or more records. \" + \n          \"Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, \" +\n          \"and any custom ETL logic can be registered and executed.\\n\" +\n          \"Record fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: \" +\n          \"hadoop ... -D \" + MorphlineMapRunner.MORPHLINE_FIELD_PREFIX + Fields.ATTACHMENT_MIME_TYPE + \"=text/csv\" +\n          \"\\n\\n\" +\n          \"3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. \" +\n          \"Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their \" +\n          \"data in HDFS.\" + \n          \"\\n\\n\" +\n          \"4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr \" +\n          \"shards expected by the user, using a mapper-only job. This phase is omitted if the number \" +\n          \"of shards is already equal to the number of shards expected by the user. \" +\n          \"\\n\\n\" +\n          \"5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of \" +\n          \"live customer facing Solr servers, typically a SolrCloud. \" +\n          \"If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\" +\n          \"\\n\\n\" +\n          \"Fault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. \" +\n          \"On program startup all data in the --output-dir is deleted if that output directory already exists. \" +\n          \"If the whole job fails you can retry simply by rerunning the program again using the same arguments.\" \n          );\n\n      parser.addArgument(\"--help\", \"-help\", \"-h\")\n        .help(\"Show this help message and exit\")\n        .action(new HelpArgumentAction() {\n          @Override\n          public void run(ArgumentParser parser, Argument arg, Map<String, Object> attrs, String flag, Object value) throws ArgumentParserException {\n            parser.printHelp();\n            System.out.println();\n            System.out.print(ToolRunnerHelpFormatter.getGenericCommandUsage());\n            //ToolRunner.printGenericCommandUsage(System.out);\n            System.out.println(\n              \"Examples: \\n\\n\" + \n\n              \"# (Re)index an Avro based Twitter tweet file:\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" +\n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shards 1 \\\\\\n\" + \n              \"  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\\n\" +\n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live Solr cluster\\n\" +\n              \"# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --solr-home-dir src/test/resources/solr/minimr \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\" +  \n              \"\\n\" +\n              \"# Go live by merging resulting index shards into a live SolrCloud cluster\\n\" +\n              \"# (discover shards and Solr URLs through ZooKeeper):\\n\" +\n              \"sudo -u hdfs hadoop \\\\\\n\" + \n              \"  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\\\\n\" +\n              \"  jar target/solr-map-reduce-*.jar \\\\\\n\" +\n              \"  -D 'mapred.child.java.opts=-Xmx500m' \\\\\\n\" + \n//            \"  -D 'mapreduce.child.java.opts=-Xmx500m' \\\\\\n\" + \n              \"  --log4j src/test/resources/log4j.properties \\\\\\n\" + \n              \"  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\\\\n\" + \n              \"  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\\\\n\" + \n              \"  --zk-host zk01.mycompany.com:2181/solr \\\\\\n\" + \n              \"  --collection collection1 \\\\\\n\" + \n              \"  --go-live \\\\\\n\" + \n              \"  hdfs:///user/foo/indir\\n\"\n            );\n            throw new FoundHelpArgument(); // Trick to prevent processing of any remaining arguments\n          }\n        });\n      \n      ArgumentGroup requiredGroup = parser.addArgumentGroup(\"Required arguments\");\n      \n      Argument outputDirArg = requiredGroup.addArgument(\"--output-dir\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf) {\n          @Override\n          public Path convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            Path path = super.convert(parser, arg, value);\n            if (\"hdfs\".equals(path.toUri().getScheme()) && path.toUri().getAuthority() == null) {\n              // TODO: consider defaulting to hadoop's fs.default.name here or in SolrRecordWriter.createEmbeddedSolrServer()\n              throw new ArgumentParserException(\"Missing authority in path URI: \" + path, parser); \n            }\n            return path;\n          }\n        }.verifyHasScheme().verifyIsAbsolute().verifyCanWriteParent())\n        .required(true)\n        .help(\"HDFS directory to write Solr indexes to. Inside there one output directory per shard will be generated. \" +\n              \"Example: hdfs://c2202.mycompany.com/user/$USER/test\");\n      \n      Argument inputListArg = parser.addArgument(\"--input-list\")\n        .action(Arguments.append())\n        .metavar(\"URI\")\n  //      .type(new PathArgumentType(fs).verifyExists().verifyCanRead())\n        .type(Path.class)\n        .help(\"Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, \" +\n              \"one URI per line in the file. If '-' is specified, URIs are read from the standard input. \" + \n              \"Multiple --input-list arguments can be specified.\");\n        \n      Argument morphlineFileArg = requiredGroup.addArgument(\"--morphline-file\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .required(true)\n        .help(\"Relative or absolute path to a local config file that contains one or more morphlines. \" +\n              \"The file must be UTF-8 encoded. Example: /path/to/morphline.conf\");\n          \n      Argument morphlineIdArg = parser.addArgument(\"--morphline-id\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The identifier of the morphline that shall be executed within the morphline config file \" +\n              \"specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. \" +\n              \"top-most) morphline within the config file is used. Example: morphline1\");\n            \n      Argument solrHomeDirArg = nonSolrCloud(parser.addArgument(\"--solr-home-dir\")\n        .metavar(\"DIR\")\n        .type(new FileArgumentType() {\n          @Override\n          public File convert(ArgumentParser parser, Argument arg, String value) throws ArgumentParserException {\n            File solrHomeDir = super.convert(parser, arg, value);\n            File solrConfigFile = new File(new File(solrHomeDir, \"conf\"), \"solrconfig.xml\");\n            new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead().convert(\n                parser, arg, solrConfigFile.getPath());\n            return solrHomeDir;\n          }\n        }.verifyIsDirectory().verifyCanRead())\n        .required(false)\n        .help(\"Relative or absolute path to a local dir containing Solr conf/ dir and in particular \" +\n              \"conf/solrconfig.xml and optionally also lib/ dir. This directory will be uploaded to each MR task. \" +\n              \"Example: src/test/resources/solr/minimr\"));\n        \n      Argument updateConflictResolverArg = parser.addArgument(\"--update-conflict-resolver\")\n        .metavar(\"FQCN\")\n        .type(String.class)\n        .setDefault(RetainMostRecentUpdateConflictResolver.class.getName())\n        .help(\"Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. \" +\n            \"This enables deduplication and ordering of a series of document updates for the same unique document \" +\n            \"key. For example, a MapReduce batch job might index multiple files in the same job where some of the \" +\n            \"files contain old and new versions of the very same document, using the same unique document key.\\n\" +\n            \"Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but \" +\n            \"the most recent document version, or, in the general case, order colliding updates ascending from least \" +\n            \"recent to most recent (partial) update. The caller of this interface (i.e. the Hadoop Reducer) will then \" +\n            \"apply the updates to Solr in the order returned by the orderUpdates() method.\\n\" +\n            \"The default RetainMostRecentUpdateConflictResolver implementation ignores all but the most recent document \" +\n            \"version, based on a configurable numeric Solr field, which defaults to the file_last_modified timestamp\");\n      \n      Argument mappersArg = parser.addArgument(\"--mappers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-1, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots \" +\n            \"available on the cluster.\");\n  \n      Argument reducersArg = parser.addArgument(\"--reducers\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(-2, Integer.MAX_VALUE)) // TODO: also support X% syntax where X is an integer\n        .setDefault(-1)\n        .help(\"Tuning knob that indicates the number of reducers to index into. \" +\n            \"0 is reserved for a mapper-only feature that may ship in a future release. \" +\n            \"-1 indicates use all reduce slots available on the cluster. \" +\n            \"-2 indicates use one reducer per output shard, which disables the mtree merge MR algorithm. \" +\n            \"The mtree merge MR algorithm improves scalability by spreading load \" +\n            \"(in particular CPU load) among a number of parallel reducers that can be much larger than the number \" +\n            \"of solr shards expected by the user. It can be seen as an extension of concurrent lucene merges \" +\n            \"and tiered lucene merges to the clustered case. The subsequent mapper-only phase \" +\n            \"merges the output of said large number of reducers to the number of shards expected by the user, \" +\n            \"again by utilizing more available parallelism on the cluster.\");\n\n      Argument fanoutArg = parser.addArgument(\"--fanout\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(2, Integer.MAX_VALUE))\n        .setDefault(Integer.MAX_VALUE)\n        .help(FeatureControl.SUPPRESS);\n  \n      Argument maxSegmentsArg = parser.addArgument(\"--max-segments\")\n        .metavar(\"INTEGER\")  \n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1)\n        .help(\"Tuning knob that indicates the maximum number of segments to be contained on output in the index of \" +\n            \"each reducer shard. After a reducer has built its output index it applies a merge policy to merge segments \" +\n            \"until there are <= maxSegments lucene segments left in this index. \" + \n            \"Merging segments involves reading and rewriting all data in all these segment files, \" + \n            \"potentially multiple times, which is very I/O intensive and time consuming. \" + \n            \"However, an index with fewer segments can later be merged faster, \" +\n            \"and it can later be queried faster once deployed to a live Solr serving shard. \" + \n            \"Set maxSegments to 1 to optimize the index for low query latency. \" + \n            \"In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. \" + \n            \"This can be a reasonable trade-off for batch indexing systems.\");\n      \n      Argument fairSchedulerPoolArg = parser.addArgument(\"--fair-scheduler-pool\")\n        .metavar(\"STRING\")\n        .help(\"Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. \" +\n              \"The Fair Scheduler is a pluggable MapReduce scheduler that provides a way to share large clusters. \" +\n              \"Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an \" +\n              \"equal share of resources over time. When there is a single job running, that job uses the entire \" +\n              \"cluster. When other jobs are submitted, tasks slots that free up are assigned to the new jobs, so \" +\n              \"that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler, which \" +\n              \"forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. \" +\n              \"It is also an easy way to share a cluster between multiple of users. Fair sharing can also work with \" +\n              \"job priorities - the priorities are used as weights to determine the fraction of total compute time \" +\n              \"that each job gets.\");\n  \n      Argument dryRunArg = parser.addArgument(\"--dry-run\")\n        .action(Arguments.storeTrue())\n        .help(\"Run in local mode and print documents to stdout instead of loading them into Solr. This executes \" +\n              \"the morphline in the client process (without submitting a job to MR) for quicker turnaround during \" +\n              \"early trial & debug sessions.\");\n    \n      Argument log4jConfigFileArg = parser.addArgument(\"--log4j\")\n        .metavar(\"FILE\")\n        .type(new FileArgumentType().verifyExists().verifyIsFile().verifyCanRead())\n        .help(\"Relative or absolute path to a log4j.properties config file on the local file system. This file \" +\n            \"will be uploaded to each MR task. Example: /path/to/log4j.properties\");\n    \n      Argument verboseArg = parser.addArgument(\"--verbose\", \"-v\")\n        .action(Arguments.storeTrue())\n        .help(\"Turn on verbose output.\");\n  \n      parser.addArgument(SHOW_NON_SOLR_CLOUD)\n        .action(Arguments.storeTrue())\n        .help(\"Also show options for Non-SolrCloud mode as part of --help.\");\n      \n      ArgumentGroup clusterInfoGroup = parser\n          .addArgumentGroup(\"Cluster arguments\")\n          .description(\n              \"Arguments that provide information about your Solr cluster. \"\n            + nonSolrCloud(\"If you are building shards for a SolrCloud cluster, pass the --zk-host argument. \"\n            + \"If you are building shards for \"\n            + \"a Non-SolrCloud cluster, pass the --shard-url argument one or more times. To build indexes for \"\n            + \"a replicated Non-SolrCloud cluster with --shard-url, pass replica urls consecutively and also pass --shards. \"\n            + \"Using --go-live requires either --zk-host or --shard-url.\"));\n\n      Argument zkHostArg = clusterInfoGroup.addArgument(\"--zk-host\")\n        .metavar(\"STRING\")\n        .type(String.class)\n        .help(\"The address of a ZooKeeper ensemble being used by a SolrCloud cluster. \"\n            + \"This ZooKeeper ensemble will be examined to determine the number of output \"\n            + \"shards to create as well as the Solr URLs to merge the output shards into when using the --go-live option. \"\n            + \"Requires that you also pass the --collection to merge the shards into.\\n\"\n            + \"\\n\"\n            + \"The --zk-host option implements the same partitioning semantics as the standard SolrCloud \" \n            + \"Near-Real-Time (NRT) API. This enables to mix batch updates from MapReduce ingestion with \"\n            + \"updates from standard Solr NRT ingestion on the same SolrCloud cluster, \"\n            + \"using identical unique document keys.\\n\"\n            + \"\\n\"\n            + \"Format is: a list of comma separated host:port pairs, each corresponding to a zk \"\n            + \"server. Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183' If \"\n            + \"the optional chroot suffix is used the example would look \"\n            + \"like: '127.0.0.1:2181/solr,127.0.0.1:2182/solr,127.0.0.1:2183/solr' \"\n            + \"where the client would be rooted at '/solr' and all paths \"\n            + \"would be relative to this root - i.e. getting/setting/etc... \"\n            + \"'/foo/bar' would result in operations being run on \"\n            + \"'/solr/foo/bar' (from the server perspective).\\n\"\n            + nonSolrCloud(\"\\n\"\n            + \"If --solr-home-dir is not specified, the Solr home directory for the collection \"\n            + \"will be downloaded from this ZooKeeper ensemble.\"));\n\n      Argument shardUrlsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shard-url\")\n        .metavar(\"URL\")\n        .type(String.class)\n        .action(Arguments.append())\n        .help(\"Solr URL to merge resulting shard into if using --go-live. \" +\n              \"Example: http://solr001.mycompany.com:8983/solr/collection1. \" + \n              \"Multiple --shard-url arguments can be specified, one for each desired shard. \" +\n              \"If you are merging shards into a SolrCloud cluster, use --zk-host instead.\"));\n      \n      Argument shardsArg = nonSolrCloud(clusterInfoGroup.addArgument(\"--shards\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .help(\"Number of output shards to generate.\"));\n      \n      ArgumentGroup goLiveGroup = parser.addArgumentGroup(\"Go live arguments\")\n        .description(\"Arguments for merging the shards that are built into a live Solr cluster. \" +\n                     \"Also see the Cluster arguments.\");\n\n      Argument goLiveArg = goLiveGroup.addArgument(\"--go-live\")\n        .action(Arguments.storeTrue())\n        .help(\"Allows you to optionally merge the final index shards into a live Solr cluster after they are built. \" +\n              \"You can pass the ZooKeeper address with --zk-host and the relevant cluster information will be auto detected. \" +\n              nonSolrCloud(\"If you are not using a SolrCloud cluster, --shard-url arguments can be used to specify each SolrCore to merge \" +\n              \"each shard into.\"));\n\n      Argument collectionArg = goLiveGroup.addArgument(\"--collection\")\n        .metavar(\"STRING\")\n        .help(\"The SolrCloud collection to merge shards into when using --go-live and --zk-host. Example: collection1\");\n      \n      Argument goLiveThreadsArg = goLiveGroup.addArgument(\"--go-live-threads\")\n        .metavar(\"INTEGER\")\n        .type(Integer.class)\n        .choices(new RangeArgumentChoice(1, Integer.MAX_VALUE))\n        .setDefault(1000)\n        .help(\"Tuning knob that indicates the maximum number of live merges to run in parallel at one time.\");\n      \n      // trailing positional arguments\n      Argument inputFilesArg = parser.addArgument(\"input-files\")\n        .metavar(\"HDFS_URI\")\n        .type(new PathArgumentType(conf).verifyHasScheme().verifyExists().verifyCanRead())\n        .nargs(\"*\")\n        .setDefault()\n        .help(\"HDFS URI of file or directory tree to index.\");\n          \n      Namespace ns;\n      try {\n        ns = parser.parseArgs(args);\n      } catch (FoundHelpArgument e) {\n        return 0;\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n      \n      opts.log4jConfigFile = (File) ns.get(log4jConfigFileArg.getDest());\n      if (opts.log4jConfigFile != null) {\n        Utils.configureLog4jProperties(opts.log4jConfigFile.getPath());\n      }\n      LOG.debug(\"Parsed command line args: {}\", ns);\n      \n      opts.inputLists = ns.getList(inputListArg.getDest());\n      if (opts.inputLists == null) {\n        opts.inputLists = Collections.EMPTY_LIST;\n      }\n      opts.inputFiles = ns.getList(inputFilesArg.getDest());\n      opts.outputDir = (Path) ns.get(outputDirArg.getDest());\n      opts.mappers = ns.getInt(mappersArg.getDest());\n      opts.reducers = ns.getInt(reducersArg.getDest());\n      opts.updateConflictResolver = ns.getString(updateConflictResolverArg.getDest());\n      opts.fanout = ns.getInt(fanoutArg.getDest());\n      opts.maxSegments = ns.getInt(maxSegmentsArg.getDest());\n      opts.morphlineFile = (File) ns.get(morphlineFileArg.getDest());\n      opts.morphlineId = ns.getString(morphlineIdArg.getDest());\n      opts.solrHomeDir = (File) ns.get(solrHomeDirArg.getDest());\n      opts.fairSchedulerPool = ns.getString(fairSchedulerPoolArg.getDest());\n      opts.isDryRun = ns.getBoolean(dryRunArg.getDest());\n      opts.isVerbose = ns.getBoolean(verboseArg.getDest());\n      opts.zkHost = ns.getString(zkHostArg.getDest());\n      opts.shards = ns.getInt(shardsArg.getDest());\n      opts.shardUrls = buildShardUrls(ns.getList(shardUrlsArg.getDest()), opts.shards);\n      opts.goLive = ns.getBoolean(goLiveArg.getDest());\n      opts.goLiveThreads = ns.getInt(goLiveThreadsArg.getDest());\n      opts.collection = ns.getString(collectionArg.getDest());\n\n      try {\n        if (opts.reducers == 0) {\n          throw new ArgumentParserException(\"--reducers must not be zero\", parser); \n        }\n        verifyGoLiveArgs(opts, parser);\n      } catch (ArgumentParserException e) {\n        parser.handleError(e);\n        return 1;\n      }\n\n      if (opts.inputLists.isEmpty() && opts.inputFiles.isEmpty()) {\n        LOG.info(\"No input files specified - nothing to process\");\n        return 0; // nothing to process\n      }\n      return null;     \n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8e86c971b5eeaf62acd9ddfab36b34ecf92702da":["32a479bfb38669c431233257a64bb039707ff339"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["c1a70d04f6bb2fdf51a08d4d2cb919057f29f0b1"],"32a479bfb38669c431233257a64bb039707ff339":["d1806dbd0764a9ee0958f91e02dc8629b7a0ac02"],"fe1c4aa9af769a38e878f608070f672efbeac27f":["c1a70d04f6bb2fdf51a08d4d2cb919057f29f0b1"],"13f445c5bd6f19fd57d5a3ca0a35244c96f45aa8":["d1806dbd0764a9ee0958f91e02dc8629b7a0ac02","32a479bfb38669c431233257a64bb039707ff339"],"42d384b06aa87eae925b668b65f3246154f0b0fa":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d1806dbd0764a9ee0958f91e02dc8629b7a0ac02":["42d384b06aa87eae925b668b65f3246154f0b0fa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d1806dbd0764a9ee0958f91e02dc8629b7a0ac02"],"c74a1d30fa4439a3687ae194fa516accc89d4f35":["8e86c971b5eeaf62acd9ddfab36b34ecf92702da"],"c1a70d04f6bb2fdf51a08d4d2cb919057f29f0b1":["c74a1d30fa4439a3687ae194fa516accc89d4f35"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["12109b652e9210b8d58fca47f6c4a725d058a58e"]},"commit2Childs":{"8e86c971b5eeaf62acd9ddfab36b34ecf92702da":["c74a1d30fa4439a3687ae194fa516accc89d4f35"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"32a479bfb38669c431233257a64bb039707ff339":["8e86c971b5eeaf62acd9ddfab36b34ecf92702da","13f445c5bd6f19fd57d5a3ca0a35244c96f45aa8"],"fe1c4aa9af769a38e878f608070f672efbeac27f":[],"13f445c5bd6f19fd57d5a3ca0a35244c96f45aa8":[],"42d384b06aa87eae925b668b65f3246154f0b0fa":["d1806dbd0764a9ee0958f91e02dc8629b7a0ac02"],"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["42d384b06aa87eae925b668b65f3246154f0b0fa"],"d1806dbd0764a9ee0958f91e02dc8629b7a0ac02":["32a479bfb38669c431233257a64bb039707ff339","13f445c5bd6f19fd57d5a3ca0a35244c96f45aa8","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["70f91c8322fbffe3a3a897ef20ea19119cac10cd","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"c74a1d30fa4439a3687ae194fa516accc89d4f35":["c1a70d04f6bb2fdf51a08d4d2cb919057f29f0b1"],"c1a70d04f6bb2fdf51a08d4d2cb919057f29f0b1":["12109b652e9210b8d58fca47f6c4a725d058a58e","fe1c4aa9af769a38e878f608070f672efbeac27f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["fe1c4aa9af769a38e878f608070f672efbeac27f","13f445c5bd6f19fd57d5a3ca0a35244c96f45aa8","74f45af4339b0daf7a95c820ab88c1aea74fbce0","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}