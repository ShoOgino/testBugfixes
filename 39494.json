{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":["02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","date":1341839195,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15250ca94ba8ab3bcdd476daf6bf3f3febb92640","date":1355200097,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, 0).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.shutdown();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.shutdown();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.shutdown();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.shutdown();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3184874f7f3aca850248483485b4995343066875","date":1413876758,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a22eafe3f72a4c2945eaad9547e6c78816978f4","date":1413956657,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2bb2842e561df4e8e9ad89010605fc86ac265465","date":1414768208,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS_ONLY);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, PostingsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, PostingsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, PostingsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, PostingsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, PostingsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, PostingsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, DocsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, PostingsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, PostingsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, PostingsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, PostingsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, PostingsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, PostingsEnum.FLAG_NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    StoredDocument doc2 = ir.document(0);\n    StorableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe39f1a106531207c028defebbc9eb5bb489ac50","date":1592513789,"type":3,"author":"Michael Sokolov","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n\n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n\n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1182fe36fb5df768dc2da53f6d5338cbc07268ae","date":1592861749,"type":3,"author":"Michael Sokolov","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n\n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n\n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c2a23476693f2bd9a4b44cc3187c429a2e21dac2","date":1593289545,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n\n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n\n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    // TODO: this is evil, changing the type after creating the field:\n    customType.setIndexOptions(IndexOptions.DOCS);\n    final MockTokenizer doc1field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field1.setReader(new StringReader(\"doc1field1\"));\n    f.setTokenStream(doc1field1);\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    final MockTokenizer doc1field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc1field2.setReader(new StringReader(\"doc1field2\"));\n    f2.setTokenStream(doc1field2);\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    final MockTokenizer doc2field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field1.setReader(new StringReader(\"doc2field1\"));\n    f.setTokenStream(doc2field1);\n    final MockTokenizer doc2field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc2field2.setReader(new StringReader(\"doc2field2\"));\n    f2.setTokenStream(doc2field2);\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    final MockTokenizer doc3field1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field1.setReader(new StringReader(\"doc3field1\"));\n    f.setTokenStream(doc3field1);\n    final MockTokenizer doc3field2 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    doc3field2.setReader(new StringReader(\"doc3field2\"));\n    f2.setTokenStream(doc3field2);\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = DirectoryReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc1field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc2field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"binary\", new BytesRef(\"doc3field1\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc1field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc2field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(TestUtil.docs(random(), ir, \"string\", new BytesRef(\"doc3field2\"), null, PostingsEnum.NONE).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["1d028314cced5858683a1bb4741423d0f934257b","15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["3184874f7f3aca850248483485b4995343066875"],"1d028314cced5858683a1bb4741423d0f934257b":["02331260bb246364779cb6f04919ca47900d01bb","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"fe39f1a106531207c028defebbc9eb5bb489ac50":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"51f5280f31484820499077f41fcdfe92d527d9dc":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"3184874f7f3aca850248483485b4995343066875":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"c2a23476693f2bd9a4b44cc3187c429a2e21dac2":["1182fe36fb5df768dc2da53f6d5338cbc07268ae"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":["d0ef034a4f10871667ae75181537775ddcf8ade4","3184874f7f3aca850248483485b4995343066875"],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["1d028314cced5858683a1bb4741423d0f934257b"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","02331260bb246364779cb6f04919ca47900d01bb"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","02331260bb246364779cb6f04919ca47900d01bb"],"1182fe36fb5df768dc2da53f6d5338cbc07268ae":["fe39f1a106531207c028defebbc9eb5bb489ac50"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c2a23476693f2bd9a4b44cc3187c429a2e21dac2"],"02331260bb246364779cb6f04919ca47900d01bb":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4":["d6f074e73200c07d54f242d3880a8da5a35ff97b"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["6613659748fe4411a7dcf85266e55db1f95f7315"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["51f5280f31484820499077f41fcdfe92d527d9dc"],"1d028314cced5858683a1bb4741423d0f934257b":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","15250ca94ba8ab3bcdd476daf6bf3f3febb92640"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","02331260bb246364779cb6f04919ca47900d01bb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["fe39f1a106531207c028defebbc9eb5bb489ac50"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"fe39f1a106531207c028defebbc9eb5bb489ac50":["1182fe36fb5df768dc2da53f6d5338cbc07268ae"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"3184874f7f3aca850248483485b4995343066875":["2bb2842e561df4e8e9ad89010605fc86ac265465","0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"c2a23476693f2bd9a4b44cc3187c429a2e21dac2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":[],"15250ca94ba8ab3bcdd476daf6bf3f3febb92640":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":[],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["1d028314cced5858683a1bb4741423d0f934257b"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["3184874f7f3aca850248483485b4995343066875","0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"1182fe36fb5df768dc2da53f6d5338cbc07268ae":["c2a23476693f2bd9a4b44cc3187c429a2e21dac2"],"02331260bb246364779cb6f04919ca47900d01bb":["1d028314cced5858683a1bb4741423d0f934257b","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","0a22eafe3f72a4c2945eaad9547e6c78816978f4","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}