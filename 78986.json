{"path":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","commits":[{"id":"6c18273ea5b3974d2f30117f46f1ae416c28f727","date":1279708040,"type":1,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fieldInfo);\n\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte(defaultNorm);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fieldInfo);\n\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fieldInfo);\n\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte(defaultNorm);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fieldInfo);\n\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"/dev/null","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"/dev/null","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"/dev/null","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8b76d8c77c9c069618078344054ff4c6a3374f80","date":1306168582,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"efb7a19703a037c29e30440260d393500febc1f4","date":1306648116,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, normsOut);\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, normsOut);\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, normsOut);\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName, state.context);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, normsOut);\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, normsOut);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName, state.context);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, normsOut);\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, normsOut);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName, state.context);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, normsOut);\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, normsOut);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24230fe54121f9be9d85f2c2067536296785e421","date":1314462346,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName, state.context);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(normsOut);\n      } else {\n        IOUtils.closeWhileHandlingException(normsOut);\n      }\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName, state.context);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, normsOut);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"143d45d6fe43d56d1f541059577c929fc0ad27a1","date":1323026648,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsConsumer#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    NormsWriter normsOut = null;\n    boolean success = false;\n    try {\n      normsOut = normsFormat.normsWriter(state);\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsConsumerPerField toWrite = (NormsConsumerPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (!fi.omitNorms && toWrite != null && toWrite.upto > 0) {\n          normsOut.startField(fi);\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeNorm(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeNorm((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          // Fill entire field with default norm:\n          normsOut.startField(fi);\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeNorm((byte) 0);\n        }\n      }\n      normsOut.finish(state.numDocs);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(normsOut);\n      } else {\n        IOUtils.closeWhileHandlingException(normsOut);\n      }\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName, state.context);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(normsOut);\n      } else {\n        IOUtils.closeWhileHandlingException(normsOut);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e8099eba57cdbcce07a786d4f70916be3f02e365","date":1323094558,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName, state.context);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        // we must check the final value of omitNorms for the fieldinfo, it could have \n        // changed for this field since the first time we added it.\n        if (!fi.omitNorms && toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*(long)state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*(long)state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(normsOut);\n      } else {\n        IOUtils.closeWhileHandlingException(normsOut);\n      }\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName, state.context);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(normsOut);\n      } else {\n        IOUtils.closeWhileHandlingException(normsOut);\n      }\n    }\n  }\n\n","bugFix":["6c18273ea5b3974d2f30117f46f1ae416c28f727"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":5,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsConsumer#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    NormsWriter normsOut = null;\n    boolean success = false;\n    try {\n      normsOut = normsFormat.normsWriter(state);\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsConsumerPerField toWrite = (NormsConsumerPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        // we must check the final value of omitNorms for the fieldinfo, it could have \n        // changed for this field since the first time we added it.\n        if (!fi.omitNorms && toWrite != null && toWrite.upto > 0) {\n          normsOut.startField(fi);\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeNorm(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeNorm((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          // Fill entire field with default norm:\n          normsOut.startField(fi);\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeNorm((byte) 0);\n        }\n      }\n      normsOut.finish(state.numDocs);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(normsOut);\n      } else {\n        IOUtils.closeWhileHandlingException(normsOut);\n      }\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName, state.context);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        // we must check the final value of omitNorms for the fieldinfo, it could have \n        // changed for this field since the first time we added it.\n        if (!fi.omitNorms && toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*(long)state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*(long)state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(normsOut);\n      } else {\n        IOUtils.closeWhileHandlingException(normsOut);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsConsumer#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    NormsWriter normsOut = null;\n    boolean success = false;\n    try {\n      normsOut = normsFormat.normsWriter(state);\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsConsumerPerField toWrite = (NormsConsumerPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        // we must check the final value of omitNorms for the fieldinfo, it could have \n        // changed for this field since the first time we added it.\n        if (!fi.omitNorms && toWrite != null && toWrite.upto > 0) {\n          normsOut.startField(fi);\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeNorm(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeNorm((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          // Fill entire field with default norm:\n          normsOut.startField(fi);\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeNorm((byte) 0);\n        }\n      }\n      normsOut.finish(state.numDocs);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(normsOut);\n      } else {\n        IOUtils.closeWhileHandlingException(normsOut);\n      }\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName, state.context);\n    boolean success = false;\n    try {\n      normsOut.writeBytes(SegmentNorms.NORMS_HEADER, 0, SegmentNorms.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fi);\n        int upto = 0;\n        // we must check the final value of omitNorms for the fieldinfo, it could have \n        // changed for this field since the first time we added it.\n        if (!fi.omitNorms && toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte((byte) 0);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*(long)state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*(long)state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(normsOut);\n      } else {\n        IOUtils.closeWhileHandlingException(normsOut);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"24230fe54121f9be9d85f2c2067536296785e421":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"8b76d8c77c9c069618078344054ff4c6a3374f80":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["e8099eba57cdbcce07a786d4f70916be3f02e365","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"efb7a19703a037c29e30440260d393500febc1f4":["8b76d8c77c9c069618078344054ff4c6a3374f80"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["efb7a19703a037c29e30440260d393500febc1f4"],"e8099eba57cdbcce07a786d4f70916be3f02e365":["24230fe54121f9be9d85f2c2067536296785e421"],"143d45d6fe43d56d1f541059577c929fc0ad27a1":["24230fe54121f9be9d85f2c2067536296785e421"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["bde51b089eb7f86171eb3406e38a274743f9b7ac","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a3776dccca01c11e7046323cfad46a3b4a471233","efb7a19703a037c29e30440260d393500febc1f4"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["e8099eba57cdbcce07a786d4f70916be3f02e365","143d45d6fe43d56d1f541059577c929fc0ad27a1"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["efb7a19703a037c29e30440260d393500febc1f4","639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["6c18273ea5b3974d2f30117f46f1ae416c28f727","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["868da859b43505d9d2a023bfeae6dd0c795f5295","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["135621f3a0670a9394eb563224a3b76cc4dddc0f","efb7a19703a037c29e30440260d393500febc1f4"]},"commit2Childs":{"24230fe54121f9be9d85f2c2067536296785e421":["e8099eba57cdbcce07a786d4f70916be3f02e365","143d45d6fe43d56d1f541059577c929fc0ad27a1"],"8b76d8c77c9c069618078344054ff4c6a3374f80":["efb7a19703a037c29e30440260d393500febc1f4"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":[],"efb7a19703a037c29e30440260d393500febc1f4":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","ddc4c914be86e34b54f70023f45a60fa7f04e929","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["2e10cb22a8bdb44339e282925a29182bb2f3174d"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["8b76d8c77c9c069618078344054ff4c6a3374f80","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"e8099eba57cdbcce07a786d4f70916be3f02e365":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"143d45d6fe43d56d1f541059577c929fc0ad27a1":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"a3776dccca01c11e7046323cfad46a3b4a471233":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6c18273ea5b3974d2f30117f46f1ae416c28f727","135621f3a0670a9394eb563224a3b76cc4dddc0f","b3e06be49006ecac364d39d12b9c9f74882f9b9f","c0ef0193974807e4bddf5432a6b0287fe4d6c9df","a3776dccca01c11e7046323cfad46a3b4a471233","868da859b43505d9d2a023bfeae6dd0c795f5295","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["24230fe54121f9be9d85f2c2067536296785e421","5d004d0e0b3f65bb40da76d476d659d7888270e8"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[]},"heads":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","5d004d0e0b3f65bb40da76d476d659d7888270e8","cd5edd1f2b162a5cfa08efd17851a07373a96817","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}