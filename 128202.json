{"path":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","commits":[{"id":"ce05d6b4ba1d00f03884a62f237cd38b1ce5c0b0","date":1374088456,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"/dev/null","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    if (overshard) {\n      createCollection(DELETE_DATA_DIR_COLLECTION, shardCount * 2, 1, 2);\n    } else {\n      int rep = shardCount / 2;\n      if (rep == 0) rep = 1;\n      createCollection(DELETE_DATA_DIR_COLLECTION, rep, 2, 1);\n    }\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client) + \"/delete_data_dir\");\n      c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) {\n        c.commit();\n      } else {\n        c.commit(true, true, true);\n      }\n      \n      c.query(new SolrQuery(\"id:\" + i));\n      c.setSoTimeout(30000);\n      c.setConnectionTimeout(30000);\n      NamedList<Object> response = c.query(\n          new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n      NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n      String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\"))\n          .get(\"data\");\n      dataDirs.add(dataDir);\n      c.shutdown();\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","date":1376375609,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"/dev/null","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    if (overshard) {\n      createCollection(DELETE_DATA_DIR_COLLECTION, shardCount * 2, 1, 2);\n    } else {\n      int rep = shardCount / 2;\n      if (rep == 0) rep = 1;\n      createCollection(DELETE_DATA_DIR_COLLECTION, rep, 2, 1);\n    }\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client) + \"/delete_data_dir\");\n      c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) {\n        c.commit();\n      } else {\n        c.commit(true, true, true);\n      }\n      \n      c.query(new SolrQuery(\"id:\" + i));\n      c.setSoTimeout(30000);\n      c.setConnectionTimeout(30000);\n      NamedList<Object> response = c.query(\n          new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n      NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n      String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\"))\n          .get(\"data\");\n      dataDirs.add(dataDir);\n      c.shutdown();\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3c3e46d3417c353d7be14509cfab11b315927fe","date":1382292560,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    if (overshard) {\n      createCollection(DELETE_DATA_DIR_COLLECTION, shardCount * 2, 1, 2);\n    } else {\n      int rep = shardCount / 2;\n      if (rep == 0) rep = 1;\n      createCollection(DELETE_DATA_DIR_COLLECTION, rep, 2, 1);\n    }\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client) + \"/delete_data_dir\");\n      c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) {\n        c.commit();\n      } else {\n        c.commit(true, true, true);\n      }\n      \n      c.query(new SolrQuery(\"id:\" + i));\n      c.setSoTimeout(60000);\n      c.setConnectionTimeout(30000);\n      NamedList<Object> response = c.query(\n          new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n      NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n      String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\"))\n          .get(\"data\");\n      dataDirs.add(dataDir);\n      c.shutdown();\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    if (overshard) {\n      createCollection(DELETE_DATA_DIR_COLLECTION, shardCount * 2, 1, 2);\n    } else {\n      int rep = shardCount / 2;\n      if (rep == 0) rep = 1;\n      createCollection(DELETE_DATA_DIR_COLLECTION, rep, 2, 1);\n    }\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client) + \"/delete_data_dir\");\n      c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) {\n        c.commit();\n      } else {\n        c.commit(true, true, true);\n      }\n      \n      c.query(new SolrQuery(\"id:\" + i));\n      c.setSoTimeout(30000);\n      c.setConnectionTimeout(30000);\n      NamedList<Object> response = c.query(\n          new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n      NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n      String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\"))\n          .get(\"data\");\n      dataDirs.add(dataDir);\n      c.shutdown();\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2d750082a4223c20902ef11ff6a9831d55738ea4","date":1382309916,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    if (overshard) {\n      createCollection(DELETE_DATA_DIR_COLLECTION, shardCount * 2, 1, 2);\n    } else {\n      int rep = shardCount / 2;\n      if (rep == 0) rep = 1;\n      createCollection(DELETE_DATA_DIR_COLLECTION, rep, 2, 1);\n    }\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setSoTimeout(60000);\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo\n            .get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    if (overshard) {\n      createCollection(DELETE_DATA_DIR_COLLECTION, shardCount * 2, 1, 2);\n    } else {\n      int rep = shardCount / 2;\n      if (rep == 0) rep = 1;\n      createCollection(DELETE_DATA_DIR_COLLECTION, rep, 2, 1);\n    }\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client) + \"/delete_data_dir\");\n      c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n      if (random().nextBoolean()) {\n        c.commit();\n      } else {\n        c.commit(true, true, true);\n      }\n      \n      c.query(new SolrQuery(\"id:\" + i));\n      c.setSoTimeout(60000);\n      c.setConnectionTimeout(30000);\n      NamedList<Object> response = c.query(\n          new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n      NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n      String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\"))\n          .get(\"data\");\n      dataDirs.add(dataDir);\n      c.shutdown();\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0f17bf9fcdeb66b745313664b7517b75f03012b4","date":1388346469,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    if (overshard) {\n      createCollection(DELETE_DATA_DIR_COLLECTION, shardCount * 2, 1, 2);\n    } else {\n      int rep = shardCount / 2;\n      if (rep == 0) rep = 1;\n      createCollection(DELETE_DATA_DIR_COLLECTION, rep, 2, 1);\n    }\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo\n            .get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    if (overshard) {\n      createCollection(DELETE_DATA_DIR_COLLECTION, shardCount * 2, 1, 2);\n    } else {\n      int rep = shardCount / 2;\n      if (rep == 0) rep = 1;\n      createCollection(DELETE_DATA_DIR_COLLECTION, rep, 2, 1);\n    }\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setSoTimeout(60000);\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo\n            .get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"214f5a1364eaf033f5d28ad610cee4c577021f66","date":1390178171,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 2;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 15000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo\n            .get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    if (overshard) {\n      createCollection(DELETE_DATA_DIR_COLLECTION, shardCount * 2, 1, 2);\n    } else {\n      int rep = shardCount / 2;\n      if (rep == 0) rep = 1;\n      createCollection(DELETE_DATA_DIR_COLLECTION, rep, 2, 1);\n    }\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo\n            .get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9e8d92cc399a85ed36e6f232cdbb61077a84f5c","date":1390232749,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo\n            .get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 2;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 15000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo\n            .get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"15e9b3e6106d8c4fa41008d4967413d4df973d00","date":1392220901,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo\n            .get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo\n            .get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo\n            .get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<String>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo\n            .get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0d579490a72f2e6297eaa648940611234c57cf1","date":1395917140,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo\n            .get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2a0f5bb79c600763ffe7b8141df59a3169d31e48","date":1396689440,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo\n            .get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294","date":1408633409,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION);\n      try {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client)\n          + \"/delete_data_dir\");\n      try {\n        c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) c.add(getDoc(\"id\", i++));\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.query(new SolrQuery(\"id:\" + i));\n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":["2d750082a4223c20902ef11ff6a9831d55738ea4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bafca15d8e408346a67f4282ad1143b88023893b","date":1420034748,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION);\n      try {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrServer client : clients) {\n      HttpSolrServer c = new HttpSolrServer(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION);\n      try {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION);\n      try {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = shardCount * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = shardCount / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION);\n      try {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cc3b13b430571c2e169f98fe38e1e7666f88522d","date":1422446157,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION);\n      try {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      } finally {\n        c.shutdown();\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197","date":1427044180,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":["ce05d6b4ba1d00f03884a62f237cd38b1ce5c0b0"],"bugIntro":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      FileSystem fs = FileSystem.newInstance(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3def6e0e7b7566dd7f04a3514e77ee97a40fc78a","date":1429888091,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = new Configuration();\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(dataDir), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"102da6baafc0f534a59f31729343dbab9d3b9e9a","date":1438410244,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState();\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState();\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState(true);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState(true);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bcf9886c8ff537aafde14de48ebf744f5673f08b","date":1439041198,"type":3,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState();\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState();\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState();\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n    \n    long timeout = System.currentTimeMillis() + 10000;\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (System.currentTimeMillis() > timeout) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState();\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f","date":1457343183,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().updateClusterState();\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n      cloudClient.getZkStateReader().updateClusterState();\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e3c94a8b8bf47db4f968d9ae510ec8bbe1372088","date":1460069869,"type":3,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5bdaf2cee03ff78b0a0cbf23df0095a3590b493b","date":1460110033,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = new HttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ff94e4cae4a92d7aeee5e1b15b8ca2229738ee7e","date":1461168315,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["344b0840364d990b29b97467bfcc766ff8325d11"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9b701e0fcef214f86bab1daf107adce0b0131212","date":1461241874,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"197bbedf08450ade98a11f4a0001448059666bec","date":1498534625,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","date":1498540685,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"43d1e498704edd2bba13548a189eed4dfccff11b","date":1499143458,"type":3,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2ea161f828a3a7a6eb9410a431aecda6d7ab1065","date":1499213384,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        c.setConnectionTimeout(30000);\n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"344b0840364d990b29b97467bfcc766ff8325d11","date":1501574100,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocCollection docCollection = clusterState.getCollectionOrNull(DELETE_DATA_DIR_COLLECTION);\n    assertNotNull(\"Could not find :\"+DELETE_DATA_DIR_COLLECTION, docCollection);\n    Slice slice = docCollection.getSlice(\"shard1\");\n    assertNotNull(docCollection.getSlices().toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":["ff94e4cae4a92d7aeee5e1b15b8ca2229738ee7e"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","date":1502192746,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocCollection docCollection = clusterState.getCollectionOrNull(DELETE_DATA_DIR_COLLECTION);\n    assertNotNull(\"Could not find :\"+DELETE_DATA_DIR_COLLECTION, docCollection);\n    Slice slice = docCollection.getSlice(\"shard1\");\n    assertNotNull(docCollection.getSlices().toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    Slice slice = clusterState.getSlice(DELETE_DATA_DIR_COLLECTION, \"shard1\");\n    assertNotNull(clusterState.getSlices(DELETE_DATA_DIR_COLLECTION).toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1d4bf9d5308dfef350829c28f2b3b2648df1e9b1","date":1513252583,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocCollection docCollection = clusterState.getCollectionOrNull(DELETE_DATA_DIR_COLLECTION);\n    assertNotNull(\"Could not find :\"+DELETE_DATA_DIR_COLLECTION, docCollection);\n    Slice slice = docCollection.getSlice(\"shard1\");\n    assertNotNull(docCollection.getSlices().toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocCollection docCollection = clusterState.getCollectionOrNull(DELETE_DATA_DIR_COLLECTION);\n    assertNotNull(\"Could not find :\"+DELETE_DATA_DIR_COLLECTION, docCollection);\n    Slice slice = docCollection.getSlice(\"shard1\");\n    assertNotNull(docCollection.getSlices().toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f","date":1552317217,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws Exception {\n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocCollection docCollection = clusterState.getCollectionOrNull(DELETE_DATA_DIR_COLLECTION);\n    assertNotNull(\"Could not find :\"+DELETE_DATA_DIR_COLLECTION, docCollection);\n    Slice slice = docCollection.getSlice(\"shard1\");\n    assertNotNull(docCollection.getSlices().toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      try(FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf)) {\n        assertFalse(\n            \"Data directory exists after collection removal : \" + dataDir,\n            fs.exists(new Path(dataDir)));\n      }\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws SolrServerException,\n      IOException, Exception, KeeperException, InterruptedException,\n      URISyntaxException {\n    \n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocCollection docCollection = clusterState.getCollectionOrNull(DELETE_DATA_DIR_COLLECTION);\n    assertNotNull(\"Could not find :\"+DELETE_DATA_DIR_COLLECTION, docCollection);\n    Slice slice = docCollection.getSlice(\"shard1\");\n    assertNotNull(docCollection.getSlices().toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf);\n      assertFalse(\n          \"Data directory exists after collection removal : \" + dataDir,\n          fs.exists(new Path(dataDir)));\n      fs.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"add53de9835b2cd1a7a80b4e0036afee171c9fdf","date":1552937136,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws Exception {\n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocCollection docCollection = clusterState.getCollectionOrNull(DELETE_DATA_DIR_COLLECTION);\n    assertNotNull(\"Could not find :\"+DELETE_DATA_DIR_COLLECTION, docCollection);\n    Slice slice = docCollection.getSlice(\"shard1\");\n    assertNotNull(docCollection.getSlices().toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      try(FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf)) {\n        assertFalse(\n            \"Data directory exists after collection removal : \" + dataDir,\n            fs.exists(new Path(dataDir)));\n      }\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws Exception {\n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocCollection docCollection = clusterState.getCollectionOrNull(DELETE_DATA_DIR_COLLECTION);\n    assertNotNull(\"Could not find :\"+DELETE_DATA_DIR_COLLECTION, docCollection);\n    Slice slice = docCollection.getSlice(\"shard1\");\n    assertNotNull(docCollection.getSlices().toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      try(FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf)) {\n        assertFalse(\n            \"Data directory exists after collection removal : \" + dataDir,\n            fs.exists(new Path(dataDir)));\n      }\n    }\n  }\n\n","bugFix":["a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aa2585c33d5d66a1c837c312221eb55ddb3c4300","date":1592493170,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws Exception {\n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocCollection docCollection = clusterState.getCollectionOrNull(DELETE_DATA_DIR_COLLECTION);\n    assertNotNull(\"Could not find :\"+DELETE_DATA_DIR_COLLECTION, docCollection);\n    Slice slice = docCollection.getSlice(\"shard1\");\n    assertNotNull(docCollection.getSlices().toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        @SuppressWarnings({\"unchecked\"})\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        @SuppressWarnings({\"unchecked\"})\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      try(FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf)) {\n        assertFalse(\n            \"Data directory exists after collection removal : \" + dataDir,\n            fs.exists(new Path(dataDir)));\n      }\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws Exception {\n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocCollection docCollection = clusterState.getCollectionOrNull(DELETE_DATA_DIR_COLLECTION);\n    assertNotNull(\"Could not find :\"+DELETE_DATA_DIR_COLLECTION, docCollection);\n    Slice slice = docCollection.getSlice(\"shard1\");\n    assertNotNull(docCollection.getSlices().toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      try(FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf)) {\n        assertFalse(\n            \"Data directory exists after collection removal : \" + dataDir,\n            fs.exists(new Path(dataDir)));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/StressHdfsTest#createAndDeleteCollection().mjava","sourceNew":"  private void createAndDeleteCollection() throws Exception {\n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocCollection docCollection = clusterState.getCollectionOrNull(DELETE_DATA_DIR_COLLECTION);\n    assertNotNull(\"Could not find :\"+DELETE_DATA_DIR_COLLECTION, docCollection);\n    Slice slice = docCollection.getSlice(\"shard1\");\n    assertNotNull(docCollection.getSlices().toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        @SuppressWarnings({\"unchecked\"})\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        @SuppressWarnings({\"unchecked\"})\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      try(FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf)) {\n        assertFalse(\n            \"Data directory exists after collection removal : \" + dataDir,\n            fs.exists(new Path(dataDir)));\n      }\n    }\n  }\n\n","sourceOld":"  private void createAndDeleteCollection() throws Exception {\n    boolean overshard = random().nextBoolean();\n    int rep;\n    int nShards;\n    int maxReplicasPerNode;\n    if (overshard) {\n      nShards = getShardCount() * 2;\n      maxReplicasPerNode = 8;\n      rep = 1;\n    } else {\n      nShards = getShardCount() / 2;\n      maxReplicasPerNode = 1;\n      rep = 2;\n      if (nShards == 0) nShards = 1;\n    }\n    \n    createCollection(DELETE_DATA_DIR_COLLECTION, \"conf1\", nShards, rep, maxReplicasPerNode);\n\n    waitForRecoveriesToFinish(DELETE_DATA_DIR_COLLECTION, false);\n    \n    // data dirs should be in zk, SOLR-8913\n    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();\n    final DocCollection docCollection = clusterState.getCollectionOrNull(DELETE_DATA_DIR_COLLECTION);\n    assertNotNull(\"Could not find :\"+DELETE_DATA_DIR_COLLECTION, docCollection);\n    Slice slice = docCollection.getSlice(\"shard1\");\n    assertNotNull(docCollection.getSlices().toString(), slice);\n    Collection<Replica> replicas = slice.getReplicas();\n    for (Replica replica : replicas) {\n      assertNotNull(replica.getProperties().toString(), replica.get(\"dataDir\"));\n      assertNotNull(replica.getProperties().toString(), replica.get(\"ulogDir\"));\n    }\n    \n    cloudClient.setDefaultCollection(DELETE_DATA_DIR_COLLECTION);\n    cloudClient.getZkStateReader().forceUpdateCollection(DELETE_DATA_DIR_COLLECTION);\n    \n    for (int i = 1; i < nShards + 1; i++) {\n      cloudClient.getZkStateReader().getLeaderRetry(DELETE_DATA_DIR_COLLECTION, \"shard\" + i, 30000);\n    }\n    \n    // collect the data dirs\n    List<String> dataDirs = new ArrayList<>();\n    \n    int i = 0;\n    for (SolrClient client : clients) {\n      try (HttpSolrClient c = getHttpSolrClient(getBaseUrl(client) + \"/\" + DELETE_DATA_DIR_COLLECTION, 30000)) {\n        int docCnt = random().nextInt(1000) + 1;\n        for (int j = 0; j < docCnt; j++) {\n          c.add(getDoc(\"id\", i++, \"txt_t\", \"just some random text for a doc\"));\n        }\n\n        if (random().nextBoolean()) {\n          c.commit();\n        } else {\n          c.commit(true, true, true);\n        }\n        \n        NamedList<Object> response = c.query(\n            new SolrQuery().setRequestHandler(\"/admin/system\")).getResponse();\n        @SuppressWarnings({\"unchecked\"})\n        NamedList<Object> coreInfo = (NamedList<Object>) response.get(\"core\");\n        @SuppressWarnings({\"unchecked\"})\n        String dataDir = (String) ((NamedList<Object>) coreInfo.get(\"directory\")).get(\"data\");\n        dataDirs.add(dataDir);\n      }\n    }\n    \n    if (random().nextBoolean()) {\n      cloudClient.deleteByQuery(\"*:*\");\n      cloudClient.commit();\n      \n      assertEquals(0, cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound());\n    }\n    \n    cloudClient.commit();\n    cloudClient.query(new SolrQuery(\"*:*\"));\n    \n    // delete collection\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"action\", CollectionAction.DELETE.toString());\n    params.set(\"name\", DELETE_DATA_DIR_COLLECTION);\n    QueryRequest request = new QueryRequest(params);\n    request.setPath(\"/admin/collections\");\n    cloudClient.request(request);\n\n    final TimeOut timeout = new TimeOut(10, TimeUnit.SECONDS, TimeSource.NANO_TIME);\n    while (cloudClient.getZkStateReader().getClusterState().hasCollection(DELETE_DATA_DIR_COLLECTION)) {\n      if (timeout.hasTimedOut()) {\n        throw new AssertionError(\"Timeout waiting to see removed collection leave clusterstate\");\n      }\n      \n      Thread.sleep(200);\n    }\n    \n    // check that all dirs are gone\n    for (String dataDir : dataDirs) {\n      Configuration conf = HdfsTestUtil.getClientConfiguration(dfsCluster);\n      try(FileSystem fs = FileSystem.get(new URI(HdfsTestUtil.getURI(dfsCluster)), conf)) {\n        assertFalse(\n            \"Data directory exists after collection removal : \" + dataDir,\n            fs.exists(new Path(dataDir)));\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f":["1d4bf9d5308dfef350829c28f2b3b2648df1e9b1"],"197bbedf08450ade98a11f4a0001448059666bec":["9b701e0fcef214f86bab1daf107adce0b0131212"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","d0d579490a72f2e6297eaa648940611234c57cf1"],"abb23fcc2461782ab204e61213240feb77d355aa":["bafca15d8e408346a67f4282ad1143b88023893b"],"344b0840364d990b29b97467bfcc766ff8325d11":["43d1e498704edd2bba13548a189eed4dfccff11b"],"e3c94a8b8bf47db4f968d9ae510ec8bbe1372088":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f"],"ff94e4cae4a92d7aeee5e1b15b8ca2229738ee7e":["5bdaf2cee03ff78b0a0cbf23df0095a3590b493b"],"e9e8d92cc399a85ed36e6f232cdbb61077a84f5c":["214f5a1364eaf033f5d28ad610cee4c577021f66"],"bafca15d8e408346a67f4282ad1143b88023893b":["0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294"],"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f":["bcf9886c8ff537aafde14de48ebf744f5673f08b"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["aa2585c33d5d66a1c837c312221eb55ddb3c4300"],"add53de9835b2cd1a7a80b4e0036afee171c9fdf":["2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f"],"43d1e498704edd2bba13548a189eed4dfccff11b":["28288370235ed02234a64753cdbf0c6ec096304a"],"0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["cc3b13b430571c2e169f98fe38e1e7666f88522d","a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197"],"aa2585c33d5d66a1c837c312221eb55ddb3c4300":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"15e9b3e6106d8c4fa41008d4967413d4df973d00":["e9e8d92cc399a85ed36e6f232cdbb61077a84f5c"],"2ea161f828a3a7a6eb9410a431aecda6d7ab1065":["fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","43d1e498704edd2bba13548a189eed4dfccff11b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":["9b701e0fcef214f86bab1daf107adce0b0131212","197bbedf08450ade98a11f4a0001448059666bec"],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["abb23fcc2461782ab204e61213240feb77d355aa"],"1d4bf9d5308dfef350829c28f2b3b2648df1e9b1":["344b0840364d990b29b97467bfcc766ff8325d11"],"3def6e0e7b7566dd7f04a3514e77ee97a40fc78a":["a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["15e9b3e6106d8c4fa41008d4967413d4df973d00"],"9b701e0fcef214f86bab1daf107adce0b0131212":["5bdaf2cee03ff78b0a0cbf23df0095a3590b493b","ff94e4cae4a92d7aeee5e1b15b8ca2229738ee7e"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ce05d6b4ba1d00f03884a62f237cd38b1ce5c0b0"],"102da6baafc0f534a59f31729343dbab9d3b9e9a":["3def6e0e7b7566dd7f04a3514e77ee97a40fc78a"],"d0d579490a72f2e6297eaa648940611234c57cf1":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"214f5a1364eaf033f5d28ad610cee4c577021f66":["0f17bf9fcdeb66b745313664b7517b75f03012b4"],"a3c3e46d3417c353d7be14509cfab11b315927fe":["ce05d6b4ba1d00f03884a62f237cd38b1ce5c0b0"],"28288370235ed02234a64753cdbf0c6ec096304a":["9b701e0fcef214f86bab1daf107adce0b0131212","197bbedf08450ade98a11f4a0001448059666bec"],"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac":["2ea161f828a3a7a6eb9410a431aecda6d7ab1065","344b0840364d990b29b97467bfcc766ff8325d11"],"a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"0f17bf9fcdeb66b745313664b7517b75f03012b4":["2d750082a4223c20902ef11ff6a9831d55738ea4"],"ce05d6b4ba1d00f03884a62f237cd38b1ce5c0b0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2d750082a4223c20902ef11ff6a9831d55738ea4":["a3c3e46d3417c353d7be14509cfab11b315927fe"],"5bdaf2cee03ff78b0a0cbf23df0095a3590b493b":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f","e3c94a8b8bf47db4f968d9ae510ec8bbe1372088"],"bcf9886c8ff537aafde14de48ebf744f5673f08b":["102da6baafc0f534a59f31729343dbab9d3b9e9a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"]},"commit2Childs":{"2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f":["add53de9835b2cd1a7a80b4e0036afee171c9fdf"],"197bbedf08450ade98a11f4a0001448059666bec":["fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","28288370235ed02234a64753cdbf0c6ec096304a"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294"],"abb23fcc2461782ab204e61213240feb77d355aa":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"344b0840364d990b29b97467bfcc766ff8325d11":["1d4bf9d5308dfef350829c28f2b3b2648df1e9b1","7a23cf16c8fa265dc0a564adcabb55e3f054e0ac"],"e3c94a8b8bf47db4f968d9ae510ec8bbe1372088":["5bdaf2cee03ff78b0a0cbf23df0095a3590b493b"],"ff94e4cae4a92d7aeee5e1b15b8ca2229738ee7e":["9b701e0fcef214f86bab1daf107adce0b0131212"],"e9e8d92cc399a85ed36e6f232cdbb61077a84f5c":["15e9b3e6106d8c4fa41008d4967413d4df973d00"],"bafca15d8e408346a67f4282ad1143b88023893b":["abb23fcc2461782ab204e61213240feb77d355aa"],"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f":["e3c94a8b8bf47db4f968d9ae510ec8bbe1372088","5bdaf2cee03ff78b0a0cbf23df0095a3590b493b"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"add53de9835b2cd1a7a80b4e0036afee171c9fdf":["aa2585c33d5d66a1c837c312221eb55ddb3c4300"],"43d1e498704edd2bba13548a189eed4dfccff11b":["344b0840364d990b29b97467bfcc766ff8325d11","2ea161f828a3a7a6eb9410a431aecda6d7ab1065"],"0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294":["bafca15d8e408346a67f4282ad1143b88023893b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"aa2585c33d5d66a1c837c312221eb55ddb3c4300":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"15e9b3e6106d8c4fa41008d4967413d4df973d00":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"2ea161f828a3a7a6eb9410a431aecda6d7ab1065":["7a23cf16c8fa265dc0a564adcabb55e3f054e0ac"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","ce05d6b4ba1d00f03884a62f237cd38b1ce5c0b0"],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":["2ea161f828a3a7a6eb9410a431aecda6d7ab1065"],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197"],"1d4bf9d5308dfef350829c28f2b3b2648df1e9b1":["2c801a37c38aedbd2ddbd27f2aaeb30cd5c7af0f"],"3def6e0e7b7566dd7f04a3514e77ee97a40fc78a":["102da6baafc0f534a59f31729343dbab9d3b9e9a"],"9b701e0fcef214f86bab1daf107adce0b0131212":["197bbedf08450ade98a11f4a0001448059666bec","fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","28288370235ed02234a64753cdbf0c6ec096304a"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["2a0f5bb79c600763ffe7b8141df59a3169d31e48","d0d579490a72f2e6297eaa648940611234c57cf1"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":[],"102da6baafc0f534a59f31729343dbab9d3b9e9a":["bcf9886c8ff537aafde14de48ebf744f5673f08b"],"d0d579490a72f2e6297eaa648940611234c57cf1":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"214f5a1364eaf033f5d28ad610cee4c577021f66":["e9e8d92cc399a85ed36e6f232cdbb61077a84f5c"],"28288370235ed02234a64753cdbf0c6ec096304a":["43d1e498704edd2bba13548a189eed4dfccff11b"],"a3c3e46d3417c353d7be14509cfab11b315927fe":["2d750082a4223c20902ef11ff6a9831d55738ea4"],"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac":[],"a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","3def6e0e7b7566dd7f04a3514e77ee97a40fc78a"],"0f17bf9fcdeb66b745313664b7517b75f03012b4":["214f5a1364eaf033f5d28ad610cee4c577021f66"],"ce05d6b4ba1d00f03884a62f237cd38b1ce5c0b0":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","a3c3e46d3417c353d7be14509cfab11b315927fe"],"5bdaf2cee03ff78b0a0cbf23df0095a3590b493b":["ff94e4cae4a92d7aeee5e1b15b8ca2229738ee7e","9b701e0fcef214f86bab1daf107adce0b0131212"],"2d750082a4223c20902ef11ff6a9831d55738ea4":["0f17bf9fcdeb66b745313664b7517b75f03012b4"],"bcf9886c8ff537aafde14de48ebf744f5673f08b":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}