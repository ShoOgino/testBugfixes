{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testRandomFiles().mjava","commits":[{"id":"3257de94b910b1c34362d2f90d9407daf63dd68b","date":1412367119,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testRandomFiles().mjava","pathOld":"/dev/null","sourceNew":"  /** \n   * This test creates a compound file based on a large number of files of\n   * various length. The file content is generated randomly. The sizes range\n   * from 0 to 1Mb. Some of the sizes are selected to test the buffering\n   * logic in the file reading code. For this the chunk variable is set to\n   * the length of the buffer used internally by the compound file logic.\n   */\n  public void testRandomFiles() throws IOException {\n    Directory dir = newDirectory();\n    // Setup the test segment\n    String segment = \"_123\";\n    int chunk = 1024; // internal buffer size used by the stream\n    createRandomFile(dir, segment + \".zero\", 0);\n    createRandomFile(dir, segment + \".one\", 1);\n    createRandomFile(dir, segment + \".ten\", 10);\n    createRandomFile(dir, segment + \".hundred\", 100);\n    createRandomFile(dir, segment + \".big1\", chunk);\n    createRandomFile(dir, segment + \".big2\", chunk - 1);\n    createRandomFile(dir, segment + \".big3\", chunk + 1);\n    createRandomFile(dir, segment + \".big4\", 3 * chunk);\n    createRandomFile(dir, segment + \".big5\", 3 * chunk - 1);\n    createRandomFile(dir, segment + \".big6\", 3 * chunk + 1);\n    createRandomFile(dir, segment + \".big7\", 1000 * chunk);\n    \n    String files[] = dir.listAll();\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(files), MergeState.CheckAbort.NONE, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    for (int i = 0; i < files.length; i++) {\n      IndexInput check = dir.openInput(files[i], newIOContext(random()));\n      IndexInput test = cfs.openInput(files[i], newIOContext(random()));\n      assertSameStreams(files[i], check, test);\n      assertSameSeekBehavior(files[i], check, test);\n      test.close();\n      check.close();\n    }\n    cfs.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testRandomFiles().mjava","pathOld":"/dev/null","sourceNew":"  /** \n   * This test creates a compound file based on a large number of files of\n   * various length. The file content is generated randomly. The sizes range\n   * from 0 to 1Mb. Some of the sizes are selected to test the buffering\n   * logic in the file reading code. For this the chunk variable is set to\n   * the length of the buffer used internally by the compound file logic.\n   */\n  public void testRandomFiles() throws IOException {\n    Directory dir = newDirectory();\n    // Setup the test segment\n    String segment = \"_123\";\n    int chunk = 1024; // internal buffer size used by the stream\n    createRandomFile(dir, segment + \".zero\", 0);\n    createRandomFile(dir, segment + \".one\", 1);\n    createRandomFile(dir, segment + \".ten\", 10);\n    createRandomFile(dir, segment + \".hundred\", 100);\n    createRandomFile(dir, segment + \".big1\", chunk);\n    createRandomFile(dir, segment + \".big2\", chunk - 1);\n    createRandomFile(dir, segment + \".big3\", chunk + 1);\n    createRandomFile(dir, segment + \".big4\", 3 * chunk);\n    createRandomFile(dir, segment + \".big5\", 3 * chunk - 1);\n    createRandomFile(dir, segment + \".big6\", 3 * chunk + 1);\n    createRandomFile(dir, segment + \".big7\", 1000 * chunk);\n    \n    String files[] = dir.listAll();\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(files), MergeState.CheckAbort.NONE, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    for (int i = 0; i < files.length; i++) {\n      IndexInput check = dir.openInput(files[i], newIOContext(random()));\n      IndexInput test = cfs.openInput(files[i], newIOContext(random()));\n      assertSameStreams(files[i], check, test);\n      assertSameSeekBehavior(files[i], check, test);\n      test.close();\n      check.close();\n    }\n    cfs.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5faf65b6692f15cca0f87bf8666c87899afc619f","date":1420468108,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testRandomFiles().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testRandomFiles().mjava","sourceNew":"  /** \n   * This test creates a compound file based on a large number of files of\n   * various length. The file content is generated randomly. The sizes range\n   * from 0 to 1Mb. Some of the sizes are selected to test the buffering\n   * logic in the file reading code. For this the chunk variable is set to\n   * the length of the buffer used internally by the compound file logic.\n   */\n  public void testRandomFiles() throws IOException {\n    Directory dir = newDirectory();\n    // Setup the test segment\n    String segment = \"_123\";\n    int chunk = 1024; // internal buffer size used by the stream\n    createRandomFile(dir, segment + \".zero\", 0);\n    createRandomFile(dir, segment + \".one\", 1);\n    createRandomFile(dir, segment + \".ten\", 10);\n    createRandomFile(dir, segment + \".hundred\", 100);\n    createRandomFile(dir, segment + \".big1\", chunk);\n    createRandomFile(dir, segment + \".big2\", chunk - 1);\n    createRandomFile(dir, segment + \".big3\", chunk + 1);\n    createRandomFile(dir, segment + \".big4\", 3 * chunk);\n    createRandomFile(dir, segment + \".big5\", 3 * chunk - 1);\n    createRandomFile(dir, segment + \".big6\", 3 * chunk + 1);\n    createRandomFile(dir, segment + \".big7\", 1000 * chunk);\n    \n    String files[] = dir.listAll();\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(files), IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    for (int i = 0; i < files.length; i++) {\n      IndexInput check = dir.openInput(files[i], newIOContext(random()));\n      IndexInput test = cfs.openInput(files[i], newIOContext(random()));\n      assertSameStreams(files[i], check, test);\n      assertSameSeekBehavior(files[i], check, test);\n      test.close();\n      check.close();\n    }\n    cfs.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** \n   * This test creates a compound file based on a large number of files of\n   * various length. The file content is generated randomly. The sizes range\n   * from 0 to 1Mb. Some of the sizes are selected to test the buffering\n   * logic in the file reading code. For this the chunk variable is set to\n   * the length of the buffer used internally by the compound file logic.\n   */\n  public void testRandomFiles() throws IOException {\n    Directory dir = newDirectory();\n    // Setup the test segment\n    String segment = \"_123\";\n    int chunk = 1024; // internal buffer size used by the stream\n    createRandomFile(dir, segment + \".zero\", 0);\n    createRandomFile(dir, segment + \".one\", 1);\n    createRandomFile(dir, segment + \".ten\", 10);\n    createRandomFile(dir, segment + \".hundred\", 100);\n    createRandomFile(dir, segment + \".big1\", chunk);\n    createRandomFile(dir, segment + \".big2\", chunk - 1);\n    createRandomFile(dir, segment + \".big3\", chunk + 1);\n    createRandomFile(dir, segment + \".big4\", 3 * chunk);\n    createRandomFile(dir, segment + \".big5\", 3 * chunk - 1);\n    createRandomFile(dir, segment + \".big6\", 3 * chunk + 1);\n    createRandomFile(dir, segment + \".big7\", 1000 * chunk);\n    \n    String files[] = dir.listAll();\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(files), MergeState.CheckAbort.NONE, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    for (int i = 0; i < files.length; i++) {\n      IndexInput check = dir.openInput(files[i], newIOContext(random()));\n      IndexInput test = cfs.openInput(files[i], newIOContext(random()));\n      assertSameStreams(files[i], check, test);\n      assertSameSeekBehavior(files[i], check, test);\n      test.close();\n      check.close();\n    }\n    cfs.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"75d243fa001c0783996918dbbe60b55cbaeeff46","date":1422502815,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testRandomFiles().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testRandomFiles().mjava","sourceNew":"  /** \n   * This test creates a compound file based on a large number of files of\n   * various length. The file content is generated randomly. The sizes range\n   * from 0 to 1Mb. Some of the sizes are selected to test the buffering\n   * logic in the file reading code. For this the chunk variable is set to\n   * the length of the buffer used internally by the compound file logic.\n   */\n  public void testRandomFiles() throws IOException {\n    Directory dir = newDirectory();\n    // Setup the test segment\n    String segment = \"_123\";\n    int chunk = 1024; // internal buffer size used by the stream\n    createRandomFile(dir, segment + \".zero\", 0);\n    createRandomFile(dir, segment + \".one\", 1);\n    createRandomFile(dir, segment + \".ten\", 10);\n    createRandomFile(dir, segment + \".hundred\", 100);\n    createRandomFile(dir, segment + \".big1\", chunk);\n    createRandomFile(dir, segment + \".big2\", chunk - 1);\n    createRandomFile(dir, segment + \".big3\", chunk + 1);\n    createRandomFile(dir, segment + \".big4\", 3 * chunk);\n    createRandomFile(dir, segment + \".big5\", 3 * chunk - 1);\n    createRandomFile(dir, segment + \".big6\", 3 * chunk + 1);\n    createRandomFile(dir, segment + \".big7\", 1000 * chunk);\n    \n    String files[] = dir.listAll();\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.setFiles(Arrays.asList(files));\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    for (int i = 0; i < files.length; i++) {\n      IndexInput check = dir.openInput(files[i], newIOContext(random()));\n      IndexInput test = cfs.openInput(files[i], newIOContext(random()));\n      assertSameStreams(files[i], check, test);\n      assertSameSeekBehavior(files[i], check, test);\n      test.close();\n      check.close();\n    }\n    cfs.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** \n   * This test creates a compound file based on a large number of files of\n   * various length. The file content is generated randomly. The sizes range\n   * from 0 to 1Mb. Some of the sizes are selected to test the buffering\n   * logic in the file reading code. For this the chunk variable is set to\n   * the length of the buffer used internally by the compound file logic.\n   */\n  public void testRandomFiles() throws IOException {\n    Directory dir = newDirectory();\n    // Setup the test segment\n    String segment = \"_123\";\n    int chunk = 1024; // internal buffer size used by the stream\n    createRandomFile(dir, segment + \".zero\", 0);\n    createRandomFile(dir, segment + \".one\", 1);\n    createRandomFile(dir, segment + \".ten\", 10);\n    createRandomFile(dir, segment + \".hundred\", 100);\n    createRandomFile(dir, segment + \".big1\", chunk);\n    createRandomFile(dir, segment + \".big2\", chunk - 1);\n    createRandomFile(dir, segment + \".big3\", chunk + 1);\n    createRandomFile(dir, segment + \".big4\", 3 * chunk);\n    createRandomFile(dir, segment + \".big5\", 3 * chunk - 1);\n    createRandomFile(dir, segment + \".big6\", 3 * chunk + 1);\n    createRandomFile(dir, segment + \".big7\", 1000 * chunk);\n    \n    String files[] = dir.listAll();\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(files), IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    for (int i = 0; i < files.length; i++) {\n      IndexInput check = dir.openInput(files[i], newIOContext(random()));\n      IndexInput test = cfs.openInput(files[i], newIOContext(random()));\n      assertSameStreams(files[i], check, test);\n      assertSameSeekBehavior(files[i], check, test);\n      test.close();\n      check.close();\n    }\n    cfs.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"22989c36ff05c657df26dd3377b37c9ad35859bc","date":1424477375,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testRandomFiles().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testRandomFiles().mjava","sourceNew":"  /** \n   * This test creates a compound file based on a large number of files of\n   * various length. The file content is generated randomly. The sizes range\n   * from 0 to 1Mb. Some of the sizes are selected to test the buffering\n   * logic in the file reading code. For this the chunk variable is set to\n   * the length of the buffer used internally by the compound file logic.\n   */\n  public void testRandomFiles() throws IOException {\n    Directory dir = newDirectory();\n    // Setup the test segment\n    String segment = \"_123\";\n    int chunk = 1024; // internal buffer size used by the stream\n    createRandomFile(dir, segment + \".zero\", 0);\n    createRandomFile(dir, segment + \".one\", 1);\n    createRandomFile(dir, segment + \".ten\", 10);\n    createRandomFile(dir, segment + \".hundred\", 100);\n    createRandomFile(dir, segment + \".big1\", chunk);\n    createRandomFile(dir, segment + \".big2\", chunk - 1);\n    createRandomFile(dir, segment + \".big3\", chunk + 1);\n    createRandomFile(dir, segment + \".big4\", 3 * chunk);\n    createRandomFile(dir, segment + \".big5\", 3 * chunk - 1);\n    createRandomFile(dir, segment + \".big6\", 3 * chunk + 1);\n    createRandomFile(dir, segment + \".big7\", 1000 * chunk);\n    \n    List<String> files = new ArrayList<>();\n    for (String file : dir.listAll()) {\n      if (file.startsWith(segment)) {\n        files.add(file);\n      }\n    }\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.setFiles(files);\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    for (String file : files) {\n      IndexInput check = dir.openInput(file, newIOContext(random()));\n      IndexInput test = cfs.openInput(file, newIOContext(random()));\n      assertSameStreams(file, check, test);\n      assertSameSeekBehavior(file, check, test);\n      test.close();\n      check.close();\n    }\n    cfs.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** \n   * This test creates a compound file based on a large number of files of\n   * various length. The file content is generated randomly. The sizes range\n   * from 0 to 1Mb. Some of the sizes are selected to test the buffering\n   * logic in the file reading code. For this the chunk variable is set to\n   * the length of the buffer used internally by the compound file logic.\n   */\n  public void testRandomFiles() throws IOException {\n    Directory dir = newDirectory();\n    // Setup the test segment\n    String segment = \"_123\";\n    int chunk = 1024; // internal buffer size used by the stream\n    createRandomFile(dir, segment + \".zero\", 0);\n    createRandomFile(dir, segment + \".one\", 1);\n    createRandomFile(dir, segment + \".ten\", 10);\n    createRandomFile(dir, segment + \".hundred\", 100);\n    createRandomFile(dir, segment + \".big1\", chunk);\n    createRandomFile(dir, segment + \".big2\", chunk - 1);\n    createRandomFile(dir, segment + \".big3\", chunk + 1);\n    createRandomFile(dir, segment + \".big4\", 3 * chunk);\n    createRandomFile(dir, segment + \".big5\", 3 * chunk - 1);\n    createRandomFile(dir, segment + \".big6\", 3 * chunk + 1);\n    createRandomFile(dir, segment + \".big7\", 1000 * chunk);\n    \n    String files[] = dir.listAll();\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.setFiles(Arrays.asList(files));\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    for (int i = 0; i < files.length; i++) {\n      IndexInput check = dir.openInput(files[i], newIOContext(random()));\n      IndexInput test = cfs.openInput(files[i], newIOContext(random()));\n      assertSameStreams(files[i], check, test);\n      assertSameSeekBehavior(files[i], check, test);\n      test.close();\n      check.close();\n    }\n    cfs.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71488d7f5786ae87541276121ecb69705a11a295","date":1465498138,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testRandomFiles().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testRandomFiles().mjava","sourceNew":"  /** \n   * This test creates a compound file based on a large number of files of\n   * various length. The file content is generated randomly. The sizes range\n   * from 0 to 1Mb. Some of the sizes are selected to test the buffering\n   * logic in the file reading code. For this the chunk variable is set to\n   * the length of the buffer used internally by the compound file logic.\n   */\n  public void testRandomFiles() throws IOException {\n    Directory dir = newDirectory();\n    // Setup the test segment\n    String segment = \"_123\";\n    int chunk = 1024; // internal buffer size used by the stream\n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    byte[] segId = si.getId();\n    createRandomFile(dir, segment + \".zero\", 0, segId);\n    createRandomFile(dir, segment + \".one\", 1, segId);\n    createRandomFile(dir, segment + \".ten\", 10, segId);\n    createRandomFile(dir, segment + \".hundred\", 100, segId);\n    createRandomFile(dir, segment + \".big1\", chunk, segId);\n    createRandomFile(dir, segment + \".big2\", chunk - 1, segId);\n    createRandomFile(dir, segment + \".big3\", chunk + 1, segId);\n    createRandomFile(dir, segment + \".big4\", 3 * chunk, segId);\n    createRandomFile(dir, segment + \".big5\", 3 * chunk - 1, segId);\n    createRandomFile(dir, segment + \".big6\", 3 * chunk + 1, segId);\n    createRandomFile(dir, segment + \".big7\", 1000 * chunk, segId);\n    \n    List<String> files = new ArrayList<>();\n    for (String file : dir.listAll()) {\n      if (file.startsWith(segment)) {\n        files.add(file);\n      }\n    }\n    \n    si.setFiles(files);\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    for (String file : files) {\n      IndexInput check = dir.openInput(file, newIOContext(random()));\n      IndexInput test = cfs.openInput(file, newIOContext(random()));\n      assertSameStreams(file, check, test);\n      assertSameSeekBehavior(file, check, test);\n      test.close();\n      check.close();\n    }\n    cfs.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** \n   * This test creates a compound file based on a large number of files of\n   * various length. The file content is generated randomly. The sizes range\n   * from 0 to 1Mb. Some of the sizes are selected to test the buffering\n   * logic in the file reading code. For this the chunk variable is set to\n   * the length of the buffer used internally by the compound file logic.\n   */\n  public void testRandomFiles() throws IOException {\n    Directory dir = newDirectory();\n    // Setup the test segment\n    String segment = \"_123\";\n    int chunk = 1024; // internal buffer size used by the stream\n    createRandomFile(dir, segment + \".zero\", 0);\n    createRandomFile(dir, segment + \".one\", 1);\n    createRandomFile(dir, segment + \".ten\", 10);\n    createRandomFile(dir, segment + \".hundred\", 100);\n    createRandomFile(dir, segment + \".big1\", chunk);\n    createRandomFile(dir, segment + \".big2\", chunk - 1);\n    createRandomFile(dir, segment + \".big3\", chunk + 1);\n    createRandomFile(dir, segment + \".big4\", 3 * chunk);\n    createRandomFile(dir, segment + \".big5\", 3 * chunk - 1);\n    createRandomFile(dir, segment + \".big6\", 3 * chunk + 1);\n    createRandomFile(dir, segment + \".big7\", 1000 * chunk);\n    \n    List<String> files = new ArrayList<>();\n    for (String file : dir.listAll()) {\n      if (file.startsWith(segment)) {\n        files.add(file);\n      }\n    }\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.setFiles(files);\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    for (String file : files) {\n      IndexInput check = dir.openInput(file, newIOContext(random()));\n      IndexInput test = cfs.openInput(file, newIOContext(random()));\n      assertSameStreams(file, check, test);\n      assertSameSeekBehavior(file, check, test);\n      test.close();\n      check.close();\n    }\n    cfs.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testRandomFiles().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase#testRandomFiles().mjava","sourceNew":"  /** \n   * This test creates a compound file based on a large number of files of\n   * various length. The file content is generated randomly. The sizes range\n   * from 0 to 1Mb. Some of the sizes are selected to test the buffering\n   * logic in the file reading code. For this the chunk variable is set to\n   * the length of the buffer used internally by the compound file logic.\n   */\n  public void testRandomFiles() throws IOException {\n    Directory dir = newDirectory();\n    // Setup the test segment\n    String segment = \"_123\";\n    int chunk = 1024; // internal buffer size used by the stream\n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    byte[] segId = si.getId();\n    createRandomFile(dir, segment + \".zero\", 0, segId);\n    createRandomFile(dir, segment + \".one\", 1, segId);\n    createRandomFile(dir, segment + \".ten\", 10, segId);\n    createRandomFile(dir, segment + \".hundred\", 100, segId);\n    createRandomFile(dir, segment + \".big1\", chunk, segId);\n    createRandomFile(dir, segment + \".big2\", chunk - 1, segId);\n    createRandomFile(dir, segment + \".big3\", chunk + 1, segId);\n    createRandomFile(dir, segment + \".big4\", 3 * chunk, segId);\n    createRandomFile(dir, segment + \".big5\", 3 * chunk - 1, segId);\n    createRandomFile(dir, segment + \".big6\", 3 * chunk + 1, segId);\n    createRandomFile(dir, segment + \".big7\", 1000 * chunk, segId);\n    \n    List<String> files = new ArrayList<>();\n    for (String file : dir.listAll()) {\n      if (file.startsWith(segment)) {\n        files.add(file);\n      }\n    }\n    \n    si.setFiles(files);\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    for (String file : files) {\n      IndexInput check = dir.openInput(file, newIOContext(random()));\n      IndexInput test = cfs.openInput(file, newIOContext(random()));\n      assertSameStreams(file, check, test);\n      assertSameSeekBehavior(file, check, test);\n      test.close();\n      check.close();\n    }\n    cfs.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** \n   * This test creates a compound file based on a large number of files of\n   * various length. The file content is generated randomly. The sizes range\n   * from 0 to 1Mb. Some of the sizes are selected to test the buffering\n   * logic in the file reading code. For this the chunk variable is set to\n   * the length of the buffer used internally by the compound file logic.\n   */\n  public void testRandomFiles() throws IOException {\n    Directory dir = newDirectory();\n    // Setup the test segment\n    String segment = \"_123\";\n    int chunk = 1024; // internal buffer size used by the stream\n    createRandomFile(dir, segment + \".zero\", 0);\n    createRandomFile(dir, segment + \".one\", 1);\n    createRandomFile(dir, segment + \".ten\", 10);\n    createRandomFile(dir, segment + \".hundred\", 100);\n    createRandomFile(dir, segment + \".big1\", chunk);\n    createRandomFile(dir, segment + \".big2\", chunk - 1);\n    createRandomFile(dir, segment + \".big3\", chunk + 1);\n    createRandomFile(dir, segment + \".big4\", 3 * chunk);\n    createRandomFile(dir, segment + \".big5\", 3 * chunk - 1);\n    createRandomFile(dir, segment + \".big6\", 3 * chunk + 1);\n    createRandomFile(dir, segment + \".big7\", 1000 * chunk);\n    \n    List<String> files = new ArrayList<>();\n    for (String file : dir.listAll()) {\n      if (file.startsWith(segment)) {\n        files.add(file);\n      }\n    }\n    \n    SegmentInfo si = newSegmentInfo(dir, \"_123\");\n    si.setFiles(files);\n    si.getCodec().compoundFormat().write(dir, si, IOContext.DEFAULT);\n    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);\n    \n    for (String file : files) {\n      IndexInput check = dir.openInput(file, newIOContext(random()));\n      IndexInput test = cfs.openInput(file, newIOContext(random()));\n      assertSameStreams(file, check, test);\n      assertSameSeekBehavior(file, check, test);\n      test.close();\n      check.close();\n    }\n    cfs.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9bb9a29a5e71a90295f175df8919802993142c9a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3257de94b910b1c34362d2f90d9407daf63dd68b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3257de94b910b1c34362d2f90d9407daf63dd68b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"71488d7f5786ae87541276121ecb69705a11a295":["22989c36ff05c657df26dd3377b37c9ad35859bc"],"22989c36ff05c657df26dd3377b37c9ad35859bc":["75d243fa001c0783996918dbbe60b55cbaeeff46"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["9bb9a29a5e71a90295f175df8919802993142c9a"],"75d243fa001c0783996918dbbe60b55cbaeeff46":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["22989c36ff05c657df26dd3377b37c9ad35859bc","71488d7f5786ae87541276121ecb69705a11a295"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71488d7f5786ae87541276121ecb69705a11a295"]},"commit2Childs":{"9bb9a29a5e71a90295f175df8919802993142c9a":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9bb9a29a5e71a90295f175df8919802993142c9a","3257de94b910b1c34362d2f90d9407daf63dd68b"],"3257de94b910b1c34362d2f90d9407daf63dd68b":["9bb9a29a5e71a90295f175df8919802993142c9a"],"71488d7f5786ae87541276121ecb69705a11a295":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"22989c36ff05c657df26dd3377b37c9ad35859bc":["71488d7f5786ae87541276121ecb69705a11a295","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["75d243fa001c0783996918dbbe60b55cbaeeff46"],"75d243fa001c0783996918dbbe60b55cbaeeff46":["22989c36ff05c657df26dd3377b37c9ad35859bc"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}