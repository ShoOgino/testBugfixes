{"path":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader...)\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader...)\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(directory, termIndexInterval, mergedName, null, codecs);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx(), merger.getCodec());\n            setDiagnostics(info, \"addIndexes(IndexReader...)\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader...)\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"433777d1eaf9998136cd16515dc0e1eb26f5d535","date":1273839120,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(directory, termIndexInterval, mergedName, null, codecs, payloadProcessorProvider);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx(), merger.getCodec());\n            setDiagnostics(info, \"addIndexes(IndexReader...)\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(directory, termIndexInterval, mergedName, null, codecs);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx(), merger.getCodec());\n            setDiagnostics(info, \"addIndexes(IndexReader...)\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fb10b6bcde550b87d8f10e5f010bd8f3021023b6","date":1274974592,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, true,\n            -1, null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(directory, termIndexInterval, mergedName, null, codecs, payloadProcessorProvider);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx(), merger.getCodec());\n            setDiagnostics(info, \"addIndexes(IndexReader...)\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6267e1ce56c2eec111425690cd04e251b6f14952","date":1275222352,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, true,\n            -1, null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9b832cbed6eb3d54a8bb9339296bdda8eeb53014","date":1279708040,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        // nocommit\n        //docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getLogMergePolicy().getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"334c1175813aea771a71728cd2c4ee4754fd0603","date":1279710173,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        // nocommit\n        //docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getLogMergePolicy().getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8fe956d65251358d755c56f14fe8380644790e47","date":1279711318,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        // nocommit\n        //docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getLogMergePolicy().getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"833a7987bc1c94455fde83e3311f72bddedcfb93","date":1279951470,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        // nocommit\n        //docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getLogMergePolicy().getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        // nocommit\n        //docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getLogMergePolicy().getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"975460c64e0309e8a78d230321879ec18e65dc0a","date":1281550753,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            checkpoint();\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"07719bf99e16ca75605269caeeea3f2f0b0b40ff","date":1282687996,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            checkpoint();\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a493e6d0c3ad86bd55c0a1360d110142e948f2bd","date":1289406991,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getSegmentCodecs());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"85a883878c0af761245ab048babc63d099f835f3","date":1289553330,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getSegmentCodecs());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a","date":1289919830,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getSegmentCodecs());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            synchronized(this) {\n              deleter.decRef(files);\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getSegmentCodecs());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":["2586f96f60332eb97ecd2934b0763791462568b2"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9ab1f5591dc05f1f2b5407d809c9699f75554a32","date":1290008586,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getSegmentCodecs());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            synchronized(this) {\n              deleter.decRef(files);\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getSegmentCodecs());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4e8cc373c801e54cec75daf9f52792cb4b17f536","date":1291116159,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getSegmentCodecs());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && ((LogMergePolicy) mergePolicy).getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            synchronized(this) {\n              deleter.decRef(files);\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getSegmentCodecs());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            synchronized(this) {\n              deleter.decRef(files);\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"38a62612cfa4e104080d89d7751a8f1a258ac335","date":1291442315,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n          false, -1, null, false, merger.hasProx(), merger.getSegmentCodecs());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n        \n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getSegmentCodecs());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && ((LogMergePolicy) mergePolicy).getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            synchronized(this) {\n              deleter.decRef(files);\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getSegmentCodecs());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && ((LogMergePolicy) mergePolicy).getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            synchronized(this) {\n              deleter.decRef(files);\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getSegmentCodecs());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            synchronized(this) {\n              deleter.decRef(files);\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e","date":1291833341,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n          false, -1, null, false, merger.hasProx(), merger.getSegmentCodecs());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n        \n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, -1,\n            null, false, merger.hasProx(), merger.getSegmentCodecs());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && ((LogMergePolicy) mergePolicy).getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n              checkpoint();\n            }\n          } finally {\n            synchronized(this) {\n              deleter.decRef(files);\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"44fcbde6fb2ac44ee3b45e013e54a42911e689ff","date":1292065621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n          false, -1, null, false, merger.hasProx(), merger.getSegmentCodecs());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n          false, -1, null, false, merger.hasProx(), merger.getSegmentCodecs());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n        \n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e1cbd7e289dc1243c7a59e1a83d078163a147fe","date":1292268032,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, -1, null, false, merger.hasProx(), merger.getSegmentCodecs(),\n                                         merger.hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n          false, -1, null, false, merger.hasProx(), merger.getSegmentCodecs());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e06c9d5fba0a2f937941d199d64ccb32aac502d1","date":1292411167,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, -1, null, false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, -1, null, false, merger.hasProx(), merger.getSegmentCodecs(),\n                                         merger.hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, -1, null, false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5","date":1292711882,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n          false, -1, null, false, merger.hasProx(), merger.getSegmentCodecs());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n        \n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional\n   * semantics, temporary free space required in the Directory,\n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n          mergedName, null, codecs, payloadProcessorProvider);\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = null;\n      synchronized(this) {\n        info = new SegmentInfo(mergedName, docCount, directory, false, merger.hasProx(), merger.getCodec());\n        setDiagnostics(info, \"addIndexes(IndexReader...)\");\n        segmentInfos.add(info);\n        checkpoint();\n        \n        // Notify DocumentsWriter that the flushed count just increased\n        // nocommit\n        //docWriter.updateFlushedDocCount(docCount);\n      }\n      \n      // Now create the compound file if needed\n      if (mergePolicy instanceof LogMergePolicy && getLogMergePolicy().getUseCompoundFile()) {\n\n        List<String> files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a chance to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\", info);\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          } finally {\n            deleter.decRef(files);\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe","date":1294227869,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["f7719bda090a2ae5bab940a27ba7bb9054b29818"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional\n   * semantics, temporary free space required in the Directory,\n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional\n   * semantics, temporary free space required in the Directory,\n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        info.setUseCompoundFile(true);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(merger.getMergedFiles(info));\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"86c34ea6a885f625f2e464756450d45b72653ef3","date":1295256222,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * <p>\n   * After this completes, the index is optimized.\n   * </p>\n   * <p>\n   * The provided IndexReaders are not closed.\n   * </p>\n   *\n   * <p>\n   * <b>NOTE:</b> while this is running, any attempts to add or delete documents\n   * (with another thread) will be paused until this method completes.\n   *\n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * </p>\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * </p>\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional\n   * semantics, temporary free space required in the Directory,\n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad27cbdf7398b36c6a478859f546c84d71cb251b","date":1296069528,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0c7a8f7304b75b1528814c5820fa23a96816c27","date":1298314239,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               fieldInfos.newFieldInfosWithGlobalFieldNumberMap());\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         merger.fieldInfos());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * <p>\n   * After this completes, the index is optimized.\n   * </p>\n   * <p>\n   * The provided IndexReaders are not closed.\n   * </p>\n   *\n   * <p>\n   * <b>NOTE:</b> while this is running, any attempts to add or delete documents\n   * (with another thread) will be paused until this method completes.\n   *\n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * </p>\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * </p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               fieldInfos.newFieldInfosWithGlobalFieldNumberMap());\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         merger.fieldInfos());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * <p>\n   * After this completes, the index is optimized.\n   * </p>\n   * <p>\n   * The provided IndexReaders are not closed.\n   * </p>\n   *\n   * <p>\n   * <b>NOTE:</b> while this is running, any attempts to add or delete documents\n   * (with another thread) will be paused until this method completes.\n   *\n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * </p>\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * </p>\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"14ec33385f6fbb6ce172882d14605790418a5d31","date":1298910796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               fieldInfos.newFieldInfosWithGlobalFieldNumberMap());\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         merger.fieldInfos());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"00b21520fafb9860ce0318d7be5ea84619c185ad","date":1300444600,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos());\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e92442af786151ee55bc283eb472f629e3c7b52b","date":1301070252,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos());\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               ((FieldInfos) docWriter.getFieldInfos().clone()));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      \n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.fieldInfos().hasProx(), merger.getSegmentCodecs(),\n                                         merger.fieldInfos().hasVectors());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1123ce2cdec6831731a4eea0f7c3367b30e9788c","date":1301327679,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * <p>\n   * After this completes, the index is optimized.\n   * </p>\n   * <p>\n   * The provided IndexReaders are not closed.\n   * </p>\n   *\n   * <p>\n   * <b>NOTE:</b> while this is running, any attempts to add or delete documents\n   * (with another thread) will be paused until this method completes.\n   *\n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * </p>\n   *\n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * </p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, termIndexInterval,\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               fieldInfos.newFieldInfosWithGlobalFieldNumberMap());\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         merger.fieldInfos());\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fe2fc74577855eadfb5eae3153c2fffdaaf791","date":1305237079,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, codecs, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n      \n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      \n      int docCount = merger.merge();                // merge 'em\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, fieldInfos.hasProx(), merger.getSegmentCodecs(),\n                                         fieldInfos.hasVectors(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n      \n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n        \n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"850aca8525380ccb7df9626d7caa89678005dd31","date":1307569142,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      // nocommit - we should pass a MergeInfo here into merge to create corresponding IOContext instances?\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(true), true, false));\n        merger.createCompoundFile(mergedName + \".cfs\", info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b6f9be74ca7baaef11857ad002cad40419979516","date":1309449808,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, false));      \n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)), context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      // nocommit - we should pass a MergeInfo here into merge to create corresponding IOContext instances?\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      // nocommit - we should pass a MergeInfo here into merge to create corresponding IOContext instances?\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(true), true, false));\n        merger.createCompoundFile(mergedName + \".cfs\", info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e248526ae3a33286a678d7833da022fd95695f2d","date":1309450587,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, false));      \n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)), context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, false));      \n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)), context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      // nocommit - we should pass a MergeInfo here into merge to create corresponding IOContext instances?\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, false));      \n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)), context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, false));      \n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)), context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f7719bda090a2ae5bab940a27ba7bb9054b29818","date":1315503197,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, false));      \n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)), context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, false));      \n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)), context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        deleter.deleteNewFiles(info.files());\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":["fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, false));      \n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getCodec(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, false));      \n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)), context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getSegmentCodecs(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(mergedName + \".cfs\", info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, false));      \n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, false));      \n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(directory, config.getTermIndexInterval(),\n                                               mergedName, null, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      int docCount = merger.merge();                // merge 'em\n\n      final FieldInfos fieldInfos = merger.fieldInfos();\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, merger.getCodec(),\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, false));      \n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        merger.createCompoundFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"58c6bbc222f074c844e736e6fb23647e3db9cfe3","date":1322743940,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream != null)\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c5df35ab57c223ea11aec64b53bf611904f3dced","date":1323640545,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","date":1323720782,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"c5df35ab57c223ea11aec64b53bf611904f3dced":["58c6bbc222f074c844e736e6fb23647e3db9cfe3"],"975460c64e0309e8a78d230321879ec18e65dc0a":["334c1175813aea771a71728cd2c4ee4754fd0603"],"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["38a62612cfa4e104080d89d7751a8f1a258ac335"],"d3fe2fc74577855eadfb5eae3153c2fffdaaf791":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["c5df35ab57c223ea11aec64b53bf611904f3dced"],"5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["07719bf99e16ca75605269caeeea3f2f0b0b40ff"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["833a7987bc1c94455fde83e3311f72bddedcfb93","94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"14ec33385f6fbb6ce172882d14605790418a5d31":["b0c7a8f7304b75b1528814c5820fa23a96816c27"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["70ad682703b8585f5d0a637efec044d57ec05efb","ad27cbdf7398b36c6a478859f546c84d71cb251b"],"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"433777d1eaf9998136cd16515dc0e1eb26f5d535":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["e06c9d5fba0a2f937941d199d64ccb32aac502d1"],"07719bf99e16ca75605269caeeea3f2f0b0b40ff":["975460c64e0309e8a78d230321879ec18e65dc0a"],"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe":["94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["86c34ea6a885f625f2e464756450d45b72653ef3","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a3776dccca01c11e7046323cfad46a3b4a471233","850aca8525380ccb7df9626d7caa89678005dd31"],"70ad682703b8585f5d0a637efec044d57ec05efb":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"1123ce2cdec6831731a4eea0f7c3367b30e9788c":["e92442af786151ee55bc283eb472f629e3c7b52b"],"38a62612cfa4e104080d89d7751a8f1a258ac335":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["85a883878c0af761245ab048babc63d099f835f3","5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"00b21520fafb9860ce0318d7be5ea84619c185ad":["14ec33385f6fbb6ce172882d14605790418a5d31"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["6267e1ce56c2eec111425690cd04e251b6f14952"],"6267e1ce56c2eec111425690cd04e251b6f14952":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["29ef99d61cda9641b6250bf9567329a6e65f901d","e92442af786151ee55bc283eb472f629e3c7b52b"],"a3776dccca01c11e7046323cfad46a3b4a471233":["1123ce2cdec6831731a4eea0f7c3367b30e9788c","d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"ad27cbdf7398b36c6a478859f546c84d71cb251b":["fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"e248526ae3a33286a678d7833da022fd95695f2d":["b6f9be74ca7baaef11857ad002cad40419979516"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","850aca8525380ccb7df9626d7caa89678005dd31"],"58c6bbc222f074c844e736e6fb23647e3db9cfe3":["3cc749c053615f5871f3b95715fe292f34e70a53"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["58c6bbc222f074c844e736e6fb23647e3db9cfe3","c5df35ab57c223ea11aec64b53bf611904f3dced"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["850aca8525380ccb7df9626d7caa89678005dd31"],"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e":["3bb13258feba31ab676502787ab2e1779f129b7a"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["06584e6e98d592b34e1329b384182f368d2025e8"],"e92442af786151ee55bc283eb472f629e3c7b52b":["1224a4027481acce15495b03bce9b48b93b42722"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["bde51b089eb7f86171eb3406e38a274743f9b7ac","1123ce2cdec6831731a4eea0f7c3367b30e9788c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b6f9be74ca7baaef11857ad002cad40419979516":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["850aca8525380ccb7df9626d7caa89678005dd31","e248526ae3a33286a678d7833da022fd95695f2d"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"1224a4027481acce15495b03bce9b48b93b42722":["00b21520fafb9860ce0318d7be5ea84619c185ad"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["8fe956d65251358d755c56f14fe8380644790e47"],"850aca8525380ccb7df9626d7caa89678005dd31":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"e06c9d5fba0a2f937941d199d64ccb32aac502d1":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"85a883878c0af761245ab048babc63d099f835f3":["07719bf99e16ca75605269caeeea3f2f0b0b40ff","a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"3cc749c053615f5871f3b95715fe292f34e70a53":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["1123ce2cdec6831731a4eea0f7c3367b30e9788c","c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["4a69e5860d014751cc9329dfeb441a6d8fd1ed8e","94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"334c1175813aea771a71728cd2c4ee4754fd0603":["9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["433777d1eaf9998136cd16515dc0e1eb26f5d535"],"7b91922b55d15444d554721b352861d028eb8278":["f7719bda090a2ae5bab940a27ba7bb9054b29818"],"f7719bda090a2ae5bab940a27ba7bb9054b29818":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["ad27cbdf7398b36c6a478859f546c84d71cb251b"],"86c34ea6a885f625f2e464756450d45b72653ef3":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"8fe956d65251358d755c56f14fe8380644790e47":["6267e1ce56c2eec111425690cd04e251b6f14952"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"3bb13258feba31ab676502787ab2e1779f129b7a":["9ab1f5591dc05f1f2b5407d809c9699f75554a32","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a"]},"commit2Childs":{"c5df35ab57c223ea11aec64b53bf611904f3dced":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"975460c64e0309e8a78d230321879ec18e65dc0a":["07719bf99e16ca75605269caeeea3f2f0b0b40ff"],"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"d3fe2fc74577855eadfb5eae3153c2fffdaaf791":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","a3776dccca01c11e7046323cfad46a3b4a471233","850aca8525380ccb7df9626d7caa89678005dd31"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a":["9ab1f5591dc05f1f2b5407d809c9699f75554a32","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a","85a883878c0af761245ab048babc63d099f835f3"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"14ec33385f6fbb6ce172882d14605790418a5d31":["00b21520fafb9860ce0318d7be5ea84619c185ad"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"433777d1eaf9998136cd16515dc0e1eb26f5d535":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"07719bf99e16ca75605269caeeea3f2f0b0b40ff":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd","85a883878c0af761245ab048babc63d099f835f3"],"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe":["70ad682703b8585f5d0a637efec044d57ec05efb","ad27cbdf7398b36c6a478859f546c84d71cb251b","868da859b43505d9d2a023bfeae6dd0c795f5295"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"70ad682703b8585f5d0a637efec044d57ec05efb":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"1123ce2cdec6831731a4eea0f7c3367b30e9788c":["a3776dccca01c11e7046323cfad46a3b4a471233","c0ef0193974807e4bddf5432a6b0287fe4d6c9df","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"38a62612cfa4e104080d89d7751a8f1a258ac335":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["3bb13258feba31ab676502787ab2e1779f129b7a"],"06584e6e98d592b34e1329b384182f368d2025e8":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"00b21520fafb9860ce0318d7be5ea84619c185ad":["1224a4027481acce15495b03bce9b48b93b42722"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["334c1175813aea771a71728cd2c4ee4754fd0603"],"6267e1ce56c2eec111425690cd04e251b6f14952":["9b832cbed6eb3d54a8bb9339296bdda8eeb53014","8fe956d65251358d755c56f14fe8380644790e47"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"ad27cbdf7398b36c6a478859f546c84d71cb251b":["29ef99d61cda9641b6250bf9567329a6e65f901d","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"e248526ae3a33286a678d7833da022fd95695f2d":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"58c6bbc222f074c844e736e6fb23647e3db9cfe3":["c5df35ab57c223ea11aec64b53bf611904f3dced","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":[],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["433777d1eaf9998136cd16515dc0e1eb26f5d535"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["b6f9be74ca7baaef11857ad002cad40419979516"],"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["3cc749c053615f5871f3b95715fe292f34e70a53"],"e92442af786151ee55bc283eb472f629e3c7b52b":["1123ce2cdec6831731a4eea0f7c3367b30e9788c","d619839baa8ce5503e496b94a9e42ad6f079293f"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"b6f9be74ca7baaef11857ad002cad40419979516":["e248526ae3a33286a678d7833da022fd95695f2d"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["5d004d0e0b3f65bb40da76d476d659d7888270e8","f7719bda090a2ae5bab940a27ba7bb9054b29818"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["e06c9d5fba0a2f937941d199d64ccb32aac502d1"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"1224a4027481acce15495b03bce9b48b93b42722":["e92442af786151ee55bc283eb472f629e3c7b52b"],"850aca8525380ccb7df9626d7caa89678005dd31":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","639c36565ce03aed5b0fce7c9e4448e53a1f7efd","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"e06c9d5fba0a2f937941d199d64ccb32aac502d1":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"85a883878c0af761245ab048babc63d099f835f3":["9ab1f5591dc05f1f2b5407d809c9699f75554a32"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791","135621f3a0670a9394eb563224a3b76cc4dddc0f"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["70ad682703b8585f5d0a637efec044d57ec05efb"],"3cc749c053615f5871f3b95715fe292f34e70a53":["58c6bbc222f074c844e736e6fb23647e3db9cfe3"],"334c1175813aea771a71728cd2c4ee4754fd0603":["975460c64e0309e8a78d230321879ec18e65dc0a"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["6267e1ce56c2eec111425690cd04e251b6f14952"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"f7719bda090a2ae5bab940a27ba7bb9054b29818":["7b91922b55d15444d554721b352861d028eb8278"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["14ec33385f6fbb6ce172882d14605790418a5d31","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"86c34ea6a885f625f2e464756450d45b72653ef3":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"8fe956d65251358d755c56f14fe8380644790e47":["833a7987bc1c94455fde83e3311f72bddedcfb93"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["86c34ea6a885f625f2e464756450d45b72653ef3"],"3bb13258feba31ab676502787ab2e1779f129b7a":["4a69e5860d014751cc9329dfeb441a6d8fd1ed8e"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["38a62612cfa4e104080d89d7751a8f1a258ac335","3bb13258feba31ab676502787ab2e1779f129b7a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","5d004d0e0b3f65bb40da76d476d659d7888270e8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}