{"path":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b","date":1337136355,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new MutableFieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d2dee33619431ada2a7a07f5fe2dbd94bac6a460","date":1337274029,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new MutableFieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         SegmentInfo.NO, -1, mergedName, false, null, false, 0,\n                                         mergeState.fieldInfos.hasProx(), codec, null,\n                                         mergeState.fieldInfos.hasVectors(),\n                                         mergeState.fieldInfos.hasDocValues(),\n                                         mergeState.fieldInfos.hasNorms(),\n                                         mergeState.fieldInfos.hasFreq());\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new MutableFieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fc834f3412d287003cc04691da380b69ab983239","date":1337276089,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new MutableFieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         SegmentInfo.NO, -1, mergedName, false, null, false, 0,\n                                         mergeState.fieldInfos.hasProx(), codec, null,\n                                         mergeState.fieldInfos.hasVectors());\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new MutableFieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         SegmentInfo.NO, -1, mergedName, false, null, false, 0,\n                                         mergeState.fieldInfos.hasProx(), codec, null,\n                                         mergeState.fieldInfos.hasVectors(),\n                                         mergeState.fieldInfos.hasDocValues(),\n                                         mergeState.fieldInfos.hasNorms(),\n                                         mergeState.fieldInfos.hasFreq());\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4a8b14bc4241c302311422d5c6f7627f8febb86e","date":1337291675,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new MutableFieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         SegmentInfo.NO, -1, mergedName, false, null, false, 0,\n                                         mergeState.fieldInfos.hasProx(), codec, null);\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new MutableFieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         SegmentInfo.NO, -1, mergedName, false, null, false, 0,\n                                         mergeState.fieldInfos.hasProx(), codec, null,\n                                         mergeState.fieldInfos.hasVectors());\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dc97c61094c5498702b29cc2e8309beac50c23dc","date":1337293692,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new MutableFieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         SegmentInfo.NO, -1, mergedName, false, null, false, 0,\n                                         codec, null);\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new MutableFieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         SegmentInfo.NO, -1, mergedName, false, null, false, 0,\n                                         mergeState.fieldInfos.hasProx(), codec, null);\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a851824c09818632c94eba41e60ef5e72e323c8e","date":1337355760,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         SegmentInfo.NO, -1, mergedName, false, null, false, 0,\n                                         codec, null);\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new MutableFieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         SegmentInfo.NO, -1, mergedName, false, null, false, 0,\n                                         codec, null);\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4356000e349e38c9fb48034695b7c309abd54557","date":1337460341,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         -1, mergedName, false, null, false, 0,\n                                         codec, null);\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(info, mergeState.fieldInfos);\n      info.clearFilesCache();\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         SegmentInfo.NO, -1, mergedName, false, null, false, 0,\n                                         codec, null);\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"352763be0465236f8e2ac188aa1b761cb3e1c9ee","date":1337516554,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         -1, mergedName, false, null, false, 0,\n                                         codec, null);\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(directory, info, mergeState.fieldInfos, context);\n      info.clearFilesCache();\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         -1, mergedName, false, null, false, 0,\n                                         codec, null);\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(info, mergeState.fieldInfos);\n      info.clearFilesCache();\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9b2af6b2c05418fb9df466c739ed5b3a153eadde","date":1337520269,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         -1, mergedName, false, null, false, 0,\n                                         codec, null);\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(infoStream, directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(directory, info, mergeState.fieldInfos, context);\n      info.clearFilesCache();\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         -1, mergedName, false, null, false, 0,\n                                         codec, null);\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(directory, info, mergeState.fieldInfos, context);\n      info.clearFilesCache();\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1494abe5dc85557ec2e2772f87660d48f831c3a5","date":1337614370,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      SegmentMerger merger = new SegmentMerger(infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         -1, mergedName, false, null, false, 0,\n                                         codec, null);\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = info.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.clearFilesCache();\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         -1, mergedName, false, null, false, 0,\n                                         codec, null);\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(infoStream, directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(directory, info, mergeState.fieldInfos, context);\n      info.clearFilesCache();\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9a46feaa8775cb79964b568371b8eedaef5f576b","date":1337620767,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      SegmentMerger merger = new SegmentMerger(infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         -1, mergedName, false, null, false, 0,\n                                         codec, null);\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = info.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      SegmentMerger merger = new SegmentMerger(infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         -1, mergedName, false, null, false, 0,\n                                         codec, null);\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = info.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.clearFilesCache();\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"22b3128eea8c61f8f1f387dac6b3e9504bc8036e","date":1337625491,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      SegmentMerger merger = new SegmentMerger(infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, mergeState.mergedDocCount,\n                                         -1, mergedName, false, null, false, 0,\n                                         codec, null);\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = info.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      SegmentMerger merger = new SegmentMerger(infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, docCount,\n                                         -1, mergedName, false, null, false, 0,\n                                         codec, null);\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = info.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d153abcf92dc5329d98571a8c3035df9bd80648","date":1337702630,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      SegmentMerger merger = new SegmentMerger(infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, mergeState.mergedDocCount,\n                                         -1, mergedName, false, null, false,\n                                         codec, null);\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      SegmentMerger merger = new SegmentMerger(infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, mergeState.mergedDocCount,\n                                         -1, mergedName, false, null, false, 0,\n                                         codec, null);\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = info.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"63caed6eb28209e181e97822c4c8fdf808884c3b","date":1337712793,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      SegmentMerger merger = new SegmentMerger(infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, mergeState.mergedDocCount,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      SegmentMerger merger = new SegmentMerger(infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, mergeState.mergedDocCount,\n                                         -1, mergedName, false, null, false,\n                                         codec, null);\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9ee159418514037b0fa456cf8b5d6c91e2bf5557","date":1337721836,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      SegmentMerger merger = new SegmentMerger(infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, mergeState.mergedDocCount,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      SegmentMerger merger = new SegmentMerger(infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, mergeState.mergedDocCount,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfosFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["2edef7afebca00bf81a8bef95d44ea971ba309fa"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"129c6e8ac0c0d9a110ba29e4b5f1889374f30076","date":1337725510,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, 0,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      info.docCount = mergeState.mergedDocCount;\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      SegmentMerger merger = new SegmentMerger(infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, mergeState.mergedDocCount,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"00f06a4178989089b29a77d6dce7c86dfb8b6931","date":1337729247,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, 0,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      info.docCount = mergeState.mergedDocCount;\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, 0,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      info.docCount = mergeState.mergedDocCount;\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4e84d639980c2b2eb5d41330d5ff68d143239495","date":1337729749,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, 0,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      info.docCount = mergeState.mergedDocCount;\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, 0,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos.Builder(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      info.docCount = mergeState.mergedDocCount;\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0f6a6419266ce0a74e9f1501938a86a4c94d5af7","date":1337731230,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, 0,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      info.docCount = mergeState.mergedDocCount;\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, 0,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, codec, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      info.docCount = mergeState.mergedDocCount;\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"662f233ff219b7c334eb6c65cd68cc71b27a4ffe","date":1337732885,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, 0,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      // nocommit use setter and make this a SetOnce:\n      info.docCount = mergeState.mergedDocCount;\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, 0,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n      info.docCount = mergeState.mergedDocCount;\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fb5728b83dbb3e002cdd22adfe6caf103a96ef15","date":1337791289,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, 0,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      // nocommit use setter and make this a SetOnce:\n      info.docCount = mergeState.mergedDocCount;\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, 0,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      // nocommit use setter and make this a SetOnce:\n      info.docCount = mergeState.mergedDocCount;\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.getFiles().addAll(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"203d7d3cb7712e10ef33009a63247ae40c302d7a","date":1337798111,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      info.setDocCount(mergeState.mergedDocCount);\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, 0,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      // nocommit use setter and make this a SetOnce:\n      info.docCount = mergeState.mergedDocCount;\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"16cbef32b882ec68df422af3f08845ec82620335","date":1337802266,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      info.setDocCount(mergeState.mergedDocCount);\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6a917aca07a305ab70118a83e84d931503441271","date":1337826487,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         null, false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         -1, mergedName, false, null, false,\n                                         codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"764b942fd30efcae6e532c19771f32eeeb0037b2","date":1337868546,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         null, false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n       }\n       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),\n                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               new FieldInfos(globalFieldNumberMap), codec, context);\n\n      for (IndexReader reader : readers)      // add new indexes\n        merger.add(reader);\n      MergeState mergeState = merger.merge();                // merge 'em\n      int docCount = mergeState.mergedDocCount;\n      final FieldInfos fieldInfos = mergeState.fieldInfos;\n      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,\n                                         false, codec,\n                                         fieldInfos);\n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(info.files());\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(info);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2edef7afebca00bf81a8bef95d44ea971ba309fa","date":1339101284,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":["9ee159418514037b0fa456cf8b5d6c91e2bf5557"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f9e3bc279c689c218d5cd0906648a0d9049d45c","date":1340695604,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["15fbe8579d34349a8c79cbc5c933530dd5b6742a"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4d3e8520fd031bab31fd0e4d480e55958bc45efe","date":1340901565,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":["38a62612cfa4e104080d89d7751a8f1a258ac335"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes} for details on transactional \n   * semantics, temporary free space required in the Directory, \n   * and non-CFS segments on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * <p><b>NOTE</b>: if you call {@link #close(boolean)}\n   * with <tt>false</tt>, which aborts all running merges,\n   * then any thread still running this method might hit a\n   * {@link MergePolicy.MergeAbortedException}.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws CorruptIndexException, IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bc124b3b129ef11a255212f3af482b771c5b3a6c","date":1344947616,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","date":1345029782,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3c188105a9aae04f56c24996f98f8333fc825d2e","date":1345031914,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c93396a1df03720cb20e2c2f513a6fa59b21e4c","date":1345032673,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b05c56a41b733e02a189c48895922b5bd8c7f3d1","date":1345033322,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,\n                                               globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9eae2a56dc810a17cf807d831f720dec931a03de","date":1349262073,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try{\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState = merger.merge();                // merge 'em\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n\n        // delete new non cfs files directly: they were never\n        // registered with IFD\n        synchronized(this) {\n          deleter.deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b2266ed5528eed5eee00e1d3ff77d6cce6ef0b1f","date":1349264427,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try{\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad252c98ff183bc59bd0617be14fa46f9696d6fc","date":1363962178,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      for (IndexReader reader : readers) {    // add new indexes\n        merger.add(reader);\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4d1c249f01722fe2de6d60de2f0aade417fbb638","date":1365517193,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, \"addIndexes(IndexReader...)\");\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e847ef0151948fe67f4f07d70d7730578444ac6f","date":1374237791,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        if (indexReader.numDocs() > 0) {\n          numDocs += indexReader.numDocs();\n          for (AtomicReaderContext ctx : indexReader.leaves()) {\n            if (ctx.reader().numDocs() > 0) { // drop empty (or all deleted) segments\n              mergeReaders.add(ctx.reader());\n            }\n          }\n        }\n      }\n      \n      if (mergeReaders.isEmpty()) { // no segments with documents to add\n        return;\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["0859dec0aa7a485aa0081147f533c5987b4b47ac"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a45bec74b98f6fc05f52770cfb425739e6563960","date":1375119292,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        if (indexReader.numDocs() > 0) {\n          numDocs += indexReader.numDocs();\n          for (AtomicReaderContext ctx : indexReader.leaves()) {\n            if (ctx.reader().numDocs() > 0) { // drop empty (or all deleted) segments\n              mergeReaders.add(ctx.reader());\n            }\n          }\n        }\n      }\n      \n      if (mergeReaders.isEmpty()) { // no segments with documents to add\n        return;\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        if (indexReader.numDocs() > 0) {\n          numDocs += indexReader.numDocs();\n          for (AtomicReaderContext ctx : indexReader.leaves()) {\n            if (ctx.reader().numDocs() > 0) { // drop empty (or all deleted) segments\n              mergeReaders.add(ctx.reader());\n            }\n          }\n        }\n      }\n      \n      if (mergeReaders.isEmpty()) { // no segments with documents to add\n        return;\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        if (indexReader.numDocs() > 0) {\n          numDocs += indexReader.numDocs();\n          for (AtomicReaderContext ctx : indexReader.leaves()) {\n            if (ctx.reader().numDocs() > 0) { // drop empty (or all deleted) segments\n              mergeReaders.add(ctx.reader());\n            }\n          }\n        }\n      }\n      \n      if (mergeReaders.isEmpty()) { // no segments with documents to add\n        return;\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call. Also, if the given readers are {@link DirectoryReader}s, they can be\n   * opened with {@code termIndexInterval=-1} to save RAM, since during merge\n   * the in-memory structure is not used. See\n   * {@link DirectoryReader#open(Directory, int)}.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir, config.getTermIndexInterval(),\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0859dec0aa7a485aa0081147f533c5987b4b47ac","date":1376498602,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        if (indexReader.numDocs() > 0) {\n          numDocs += indexReader.numDocs();\n          for (AtomicReaderContext ctx : indexReader.leaves()) {\n            if (ctx.reader().numDocs() > 0) { // drop empty (or all deleted) segments\n              mergeReaders.add(ctx.reader());\n            }\n          }\n        }\n      }\n      \n      if (mergeReaders.isEmpty()) { // no segments with documents to add\n        return;\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":["e847ef0151948fe67f4f07d70d7730578444ac6f"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        if (indexReader.numDocs() > 0) {\n          numDocs += indexReader.numDocs();\n          for (AtomicReaderContext ctx : indexReader.leaves()) {\n            if (ctx.reader().numDocs() > 0) { // drop empty (or all deleted) segments\n              mergeReaders.add(ctx.reader());\n            }\n          }\n        }\n      }\n      \n      if (mergeReaders.isEmpty()) { // no segments with documents to add\n        return;\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e072d0b1fc19e0533d8ce432eed245196bca6fde","date":1379265112,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"77f264c55cbf75404f8601ae7290d69157273a56","date":1380484282,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["0567bdc5c86c94ced64201187cfcef2417d76dda"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f3b037cd083286b2af89f96e768f85dcd8072d6","date":1396337805,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["15fbe8579d34349a8c79cbc5c933530dd5b6742a"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0567bdc5c86c94ced64201187cfcef2417d76dda","date":1400678298,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a656b32c3aa151037a8c52e9b134acc3cbf482bc","date":1400688195,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"027bee21e09164c9ee230395405076d1e0034b30","date":1401521821,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d6b7c6630218ed9693cdb8643276513f9f0043f4","date":1406648084,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ee59f646cf24586a449cad77391a60a3ac8d8959","date":1408015131,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"057a1793765d068ea9302f1a29e21734ee58d41e","date":1408130117,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e3cc329405ce41b8ef462b4cd30611eca1567620","date":1408661910,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null);\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["8521d944f9dfb45692ec28235dbf116d47ef69ba"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c3b36e97ff7b3f9377144e86f395df5f4eee5c20","date":1408670441,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"949847c0040cd70a68222d526cb0da7bf6cbb3c2","date":1410997182,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE</b>: if this method hits an OutOfMemoryError you should immediately\n   * close the writer. See <a href=\"#OOME\">above</a> for details.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<AtomicReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (AtomicReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2131047ecceac64b54ba70feec3d26bbd7e483d7","date":1411862069,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.mergeFieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.mergeFieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context, config.getCheckIntegrityAtMerge());\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2c3c58609ce8cbaa9116c281d30aa3cdc6a87051","date":1412632911,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.mergeFieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e2fe60a17a7a0cfd101b1169acf089221bc6c166","date":1412767493,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7599427f762eb1b4265584fd6e96521e4a1a4f3c","date":1413100083,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"55980207f1977bd1463465de1659b821347e2fa8","date":1413336386,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      MergeState mergeState;\n      boolean success = false;\n      try {\n        mergeState = merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.mergeFieldInfos, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8521d944f9dfb45692ec28235dbf116d47ef69ba","date":1417535150,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":["e3cc329405ce41b8ef462b4cd30611eca1567620"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"15fbe8579d34349a8c79cbc5c933530dd5b6742a","date":1418066328,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE</b>: if you call {@link #abortMerges}, which\n   * aborts all running merges, then any thread still running this method might\n   * hit a {@link MergePolicy.MergeAbortedException}.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","1f9e3bc279c689c218d5cd0906648a0d9049d45c"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5faf65b6692f15cca0f87bf8666c87899afc619f","date":1420468108,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(mergeDirectory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, mergeDirectory, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               MergeState.CheckAbort.NONE, globalFieldNumberMap, \n                                               context);\n      \n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa","date":1420599177,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(LeafReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(LeafReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(LeafReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (LeafReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(mergeDirectory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, mergeDirectory, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(mergeDirectory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, mergeDirectory, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"4a8b14bc4241c302311422d5c6f7627f8febb86e":["fc834f3412d287003cc04691da380b69ab983239"],"7599427f762eb1b4265584fd6e96521e4a1a4f3c":["e2fe60a17a7a0cfd101b1169acf089221bc6c166"],"1f3b037cd083286b2af89f96e768f85dcd8072d6":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"00f06a4178989089b29a77d6dce7c86dfb8b6931":["129c6e8ac0c0d9a110ba29e4b5f1889374f30076"],"b2266ed5528eed5eee00e1d3ff77d6cce6ef0b1f":["9eae2a56dc810a17cf807d831f720dec931a03de"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c","bc124b3b129ef11a255212f3af482b771c5b3a6c"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"63caed6eb28209e181e97822c4c8fdf808884c3b":["9d153abcf92dc5329d98571a8c3035df9bd80648"],"16cbef32b882ec68df422af3f08845ec82620335":["203d7d3cb7712e10ef33009a63247ae40c302d7a"],"4d1c249f01722fe2de6d60de2f0aade417fbb638":["ad252c98ff183bc59bd0617be14fa46f9696d6fc"],"f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"c3b36e97ff7b3f9377144e86f395df5f4eee5c20":["e3cc329405ce41b8ef462b4cd30611eca1567620"],"ad252c98ff183bc59bd0617be14fa46f9696d6fc":["b2266ed5528eed5eee00e1d3ff77d6cce6ef0b1f"],"129c6e8ac0c0d9a110ba29e4b5f1889374f30076":["9ee159418514037b0fa456cf8b5d6c91e2bf5557"],"d6b7c6630218ed9693cdb8643276513f9f0043f4":["027bee21e09164c9ee230395405076d1e0034b30"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["1f3b037cd083286b2af89f96e768f85dcd8072d6"],"77f264c55cbf75404f8601ae7290d69157273a56":["e072d0b1fc19e0533d8ce432eed245196bca6fde"],"bc124b3b129ef11a255212f3af482b771c5b3a6c":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"949847c0040cd70a68222d526cb0da7bf6cbb3c2":["c3b36e97ff7b3f9377144e86f395df5f4eee5c20"],"027bee21e09164c9ee230395405076d1e0034b30":["0567bdc5c86c94ced64201187cfcef2417d76dda"],"203d7d3cb7712e10ef33009a63247ae40c302d7a":["fb5728b83dbb3e002cdd22adfe6caf103a96ef15"],"fb5728b83dbb3e002cdd22adfe6caf103a96ef15":["662f233ff219b7c334eb6c65cd68cc71b27a4ffe"],"a45bec74b98f6fc05f52770cfb425739e6563960":["e847ef0151948fe67f4f07d70d7730578444ac6f"],"2edef7afebca00bf81a8bef95d44ea971ba309fa":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"9b2af6b2c05418fb9df466c739ed5b3a153eadde":["352763be0465236f8e2ac188aa1b761cb3e1c9ee"],"1f9e3bc279c689c218d5cd0906648a0d9049d45c":["2edef7afebca00bf81a8bef95d44ea971ba309fa"],"8521d944f9dfb45692ec28235dbf116d47ef69ba":["7599427f762eb1b4265584fd6e96521e4a1a4f3c"],"d2dee33619431ada2a7a07f5fe2dbd94bac6a460":["f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b"],"e3cc329405ce41b8ef462b4cd30611eca1567620":["057a1793765d068ea9302f1a29e21734ee58d41e"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["2edef7afebca00bf81a8bef95d44ea971ba309fa","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","0567bdc5c86c94ced64201187cfcef2417d76dda"],"9ee159418514037b0fa456cf8b5d6c91e2bf5557":["63caed6eb28209e181e97822c4c8fdf808884c3b"],"15fbe8579d34349a8c79cbc5c933530dd5b6742a":["8521d944f9dfb45692ec28235dbf116d47ef69ba"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["77f264c55cbf75404f8601ae7290d69157273a56"],"9a46feaa8775cb79964b568371b8eedaef5f576b":["1494abe5dc85557ec2e2772f87660d48f831c3a5"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["fe33227f6805edab2036cbb80645cc4e2d1fa424"],"6a917aca07a305ab70118a83e84d931503441271":["16cbef32b882ec68df422af3f08845ec82620335"],"ee59f646cf24586a449cad77391a60a3ac8d8959":["d6b7c6630218ed9693cdb8643276513f9f0043f4"],"764b942fd30efcae6e532c19771f32eeeb0037b2":["6a917aca07a305ab70118a83e84d931503441271"],"a851824c09818632c94eba41e60ef5e72e323c8e":["dc97c61094c5498702b29cc2e8309beac50c23dc"],"b7605579001505896d48b07160075a5c8b8e128e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","0567bdc5c86c94ced64201187cfcef2417d76dda"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["4d3e8520fd031bab31fd0e4d480e55958bc45efe","bc124b3b129ef11a255212f3af482b771c5b3a6c"],"057a1793765d068ea9302f1a29e21734ee58d41e":["ee59f646cf24586a449cad77391a60a3ac8d8959"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["949847c0040cd70a68222d526cb0da7bf6cbb3c2"],"1494abe5dc85557ec2e2772f87660d48f831c3a5":["9b2af6b2c05418fb9df466c739ed5b3a153eadde"],"2c3c58609ce8cbaa9116c281d30aa3cdc6a87051":["9bb9a29a5e71a90295f175df8919802993142c9a"],"4356000e349e38c9fb48034695b7c309abd54557":["a851824c09818632c94eba41e60ef5e72e323c8e"],"9bb9a29a5e71a90295f175df8919802993142c9a":["c9fb5f46e264daf5ba3860defe623a89d202dd87","2131047ecceac64b54ba70feec3d26bbd7e483d7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["4d1c249f01722fe2de6d60de2f0aade417fbb638"],"22b3128eea8c61f8f1f387dac6b3e9504bc8036e":["9a46feaa8775cb79964b568371b8eedaef5f576b"],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["1f9e3bc279c689c218d5cd0906648a0d9049d45c"],"4e84d639980c2b2eb5d41330d5ff68d143239495":["00f06a4178989089b29a77d6dce7c86dfb8b6931"],"e2fe60a17a7a0cfd101b1169acf089221bc6c166":["2c3c58609ce8cbaa9116c281d30aa3cdc6a87051"],"e072d0b1fc19e0533d8ce432eed245196bca6fde":["0859dec0aa7a485aa0081147f533c5987b4b47ac"],"5eb2511ababf862ea11e10761c70ee560cd84510":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","1f3b037cd083286b2af89f96e768f85dcd8072d6"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"55980207f1977bd1463465de1659b821347e2fa8":["2131047ecceac64b54ba70feec3d26bbd7e483d7","7599427f762eb1b4265584fd6e96521e4a1a4f3c"],"0f6a6419266ce0a74e9f1501938a86a4c94d5af7":["4e84d639980c2b2eb5d41330d5ff68d143239495"],"0859dec0aa7a485aa0081147f533c5987b4b47ac":["a45bec74b98f6fc05f52770cfb425739e6563960"],"e847ef0151948fe67f4f07d70d7730578444ac6f":["4d1c249f01722fe2de6d60de2f0aade417fbb638"],"9eae2a56dc810a17cf807d831f720dec931a03de":["bc124b3b129ef11a255212f3af482b771c5b3a6c"],"352763be0465236f8e2ac188aa1b761cb3e1c9ee":["4356000e349e38c9fb48034695b7c309abd54557"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","0859dec0aa7a485aa0081147f533c5987b4b47ac"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["22b3128eea8c61f8f1f387dac6b3e9504bc8036e"],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"0567bdc5c86c94ced64201187cfcef2417d76dda":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["3c188105a9aae04f56c24996f98f8333fc825d2e"],"dc97c61094c5498702b29cc2e8309beac50c23dc":["4a8b14bc4241c302311422d5c6f7627f8febb86e"],"fc834f3412d287003cc04691da380b69ab983239":["d2dee33619431ada2a7a07f5fe2dbd94bac6a460"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","764b942fd30efcae6e532c19771f32eeeb0037b2"],"2131047ecceac64b54ba70feec3d26bbd7e483d7":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["15fbe8579d34349a8c79cbc5c933530dd5b6742a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"662f233ff219b7c334eb6c65cd68cc71b27a4ffe":["0f6a6419266ce0a74e9f1501938a86a4c94d5af7"]},"commit2Childs":{"4a8b14bc4241c302311422d5c6f7627f8febb86e":["dc97c61094c5498702b29cc2e8309beac50c23dc"],"7599427f762eb1b4265584fd6e96521e4a1a4f3c":["8521d944f9dfb45692ec28235dbf116d47ef69ba","55980207f1977bd1463465de1659b821347e2fa8"],"1f3b037cd083286b2af89f96e768f85dcd8072d6":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","5eb2511ababf862ea11e10761c70ee560cd84510"],"00f06a4178989089b29a77d6dce7c86dfb8b6931":["4e84d639980c2b2eb5d41330d5ff68d143239495"],"b2266ed5528eed5eee00e1d3ff77d6cce6ef0b1f":["ad252c98ff183bc59bd0617be14fa46f9696d6fc"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"63caed6eb28209e181e97822c4c8fdf808884c3b":["9ee159418514037b0fa456cf8b5d6c91e2bf5557"],"16cbef32b882ec68df422af3f08845ec82620335":["6a917aca07a305ab70118a83e84d931503441271"],"4d1c249f01722fe2de6d60de2f0aade417fbb638":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","e847ef0151948fe67f4f07d70d7730578444ac6f"],"f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b":["d2dee33619431ada2a7a07f5fe2dbd94bac6a460"],"c3b36e97ff7b3f9377144e86f395df5f4eee5c20":["949847c0040cd70a68222d526cb0da7bf6cbb3c2"],"ad252c98ff183bc59bd0617be14fa46f9696d6fc":["4d1c249f01722fe2de6d60de2f0aade417fbb638"],"129c6e8ac0c0d9a110ba29e4b5f1889374f30076":["00f06a4178989089b29a77d6dce7c86dfb8b6931"],"d6b7c6630218ed9693cdb8643276513f9f0043f4":["ee59f646cf24586a449cad77391a60a3ac8d8959"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["a656b32c3aa151037a8c52e9b134acc3cbf482bc","b7605579001505896d48b07160075a5c8b8e128e","0567bdc5c86c94ced64201187cfcef2417d76dda"],"77f264c55cbf75404f8601ae7290d69157273a56":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"bc124b3b129ef11a255212f3af482b771c5b3a6c":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","9eae2a56dc810a17cf807d831f720dec931a03de"],"949847c0040cd70a68222d526cb0da7bf6cbb3c2":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"027bee21e09164c9ee230395405076d1e0034b30":["d6b7c6630218ed9693cdb8643276513f9f0043f4"],"203d7d3cb7712e10ef33009a63247ae40c302d7a":["16cbef32b882ec68df422af3f08845ec82620335"],"fb5728b83dbb3e002cdd22adfe6caf103a96ef15":["203d7d3cb7712e10ef33009a63247ae40c302d7a"],"a45bec74b98f6fc05f52770cfb425739e6563960":["0859dec0aa7a485aa0081147f533c5987b4b47ac"],"2edef7afebca00bf81a8bef95d44ea971ba309fa":["1f9e3bc279c689c218d5cd0906648a0d9049d45c","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"9b2af6b2c05418fb9df466c739ed5b3a153eadde":["1494abe5dc85557ec2e2772f87660d48f831c3a5"],"1f9e3bc279c689c218d5cd0906648a0d9049d45c":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"8521d944f9dfb45692ec28235dbf116d47ef69ba":["15fbe8579d34349a8c79cbc5c933530dd5b6742a"],"d2dee33619431ada2a7a07f5fe2dbd94bac6a460":["fc834f3412d287003cc04691da380b69ab983239"],"e3cc329405ce41b8ef462b4cd30611eca1567620":["c3b36e97ff7b3f9377144e86f395df5f4eee5c20"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["3c188105a9aae04f56c24996f98f8333fc825d2e"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":[],"9ee159418514037b0fa456cf8b5d6c91e2bf5557":["129c6e8ac0c0d9a110ba29e4b5f1889374f30076"],"15fbe8579d34349a8c79cbc5c933530dd5b6742a":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"9a46feaa8775cb79964b568371b8eedaef5f576b":["22b3128eea8c61f8f1f387dac6b3e9504bc8036e"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c"],"6a917aca07a305ab70118a83e84d931503441271":["764b942fd30efcae6e532c19771f32eeeb0037b2"],"ee59f646cf24586a449cad77391a60a3ac8d8959":["057a1793765d068ea9302f1a29e21734ee58d41e"],"764b942fd30efcae6e532c19771f32eeeb0037b2":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"a851824c09818632c94eba41e60ef5e72e323c8e":["4356000e349e38c9fb48034695b7c309abd54557"],"b7605579001505896d48b07160075a5c8b8e128e":[],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":[],"057a1793765d068ea9302f1a29e21734ee58d41e":["e3cc329405ce41b8ef462b4cd30611eca1567620"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["9bb9a29a5e71a90295f175df8919802993142c9a","2131047ecceac64b54ba70feec3d26bbd7e483d7"],"1494abe5dc85557ec2e2772f87660d48f831c3a5":["9a46feaa8775cb79964b568371b8eedaef5f576b"],"2c3c58609ce8cbaa9116c281d30aa3cdc6a87051":["e2fe60a17a7a0cfd101b1169acf089221bc6c166"],"4356000e349e38c9fb48034695b7c309abd54557":["352763be0465236f8e2ac188aa1b761cb3e1c9ee"],"9bb9a29a5e71a90295f175df8919802993142c9a":["2c3c58609ce8cbaa9116c281d30aa3cdc6a87051"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"22b3128eea8c61f8f1f387dac6b3e9504bc8036e":["9d153abcf92dc5329d98571a8c3035df9bd80648"],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["bc124b3b129ef11a255212f3af482b771c5b3a6c","fe33227f6805edab2036cbb80645cc4e2d1fa424","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198"],"4e84d639980c2b2eb5d41330d5ff68d143239495":["0f6a6419266ce0a74e9f1501938a86a4c94d5af7"],"e2fe60a17a7a0cfd101b1169acf089221bc6c166":["7599427f762eb1b4265584fd6e96521e4a1a4f3c"],"e072d0b1fc19e0533d8ce432eed245196bca6fde":["77f264c55cbf75404f8601ae7290d69157273a56"],"5eb2511ababf862ea11e10761c70ee560cd84510":[],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["1f3b037cd083286b2af89f96e768f85dcd8072d6","5eb2511ababf862ea11e10761c70ee560cd84510"],"55980207f1977bd1463465de1659b821347e2fa8":[],"0f6a6419266ce0a74e9f1501938a86a4c94d5af7":["662f233ff219b7c334eb6c65cd68cc71b27a4ffe"],"9eae2a56dc810a17cf807d831f720dec931a03de":["b2266ed5528eed5eee00e1d3ff77d6cce6ef0b1f"],"e847ef0151948fe67f4f07d70d7730578444ac6f":["a45bec74b98f6fc05f52770cfb425739e6563960"],"352763be0465236f8e2ac188aa1b761cb3e1c9ee":["9b2af6b2c05418fb9df466c739ed5b3a153eadde"],"0859dec0aa7a485aa0081147f533c5987b4b47ac":["e072d0b1fc19e0533d8ce432eed245196bca6fde","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"9d153abcf92dc5329d98571a8c3035df9bd80648":["63caed6eb28209e181e97822c4c8fdf808884c3b"],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["b05c56a41b733e02a189c48895922b5bd8c7f3d1"],"0567bdc5c86c94ced64201187cfcef2417d76dda":["027bee21e09164c9ee230395405076d1e0034b30","a656b32c3aa151037a8c52e9b134acc3cbf482bc","b7605579001505896d48b07160075a5c8b8e128e"],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"dc97c61094c5498702b29cc2e8309beac50c23dc":["a851824c09818632c94eba41e60ef5e72e323c8e"],"fc834f3412d287003cc04691da380b69ab983239":["4a8b14bc4241c302311422d5c6f7627f8febb86e"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["2edef7afebca00bf81a8bef95d44ea971ba309fa"],"2131047ecceac64b54ba70feec3d26bbd7e483d7":["9bb9a29a5e71a90295f175df8919802993142c9a","55980207f1977bd1463465de1659b821347e2fa8"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"662f233ff219b7c334eb6c65cd68cc71b27a4ffe":["fb5728b83dbb3e002cdd22adfe6caf103a96ef15"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","a656b32c3aa151037a8c52e9b134acc3cbf482bc","b7605579001505896d48b07160075a5c8b8e128e","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","5eb2511ababf862ea11e10761c70ee560cd84510","55980207f1977bd1463465de1659b821347e2fa8","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}