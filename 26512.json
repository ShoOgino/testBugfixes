{"path":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","commits":[{"id":"1a51ec81f1fd009bf893bd88ec1c7b964fae6fab","date":1354403647,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","pathOld":"lucene/codecs/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene41Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene41Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene41Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene41Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"470cd91a3cf3f140cfc5bbff1a11e2ab0cb2229a","date":1354406688,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == NON_COMPRESSING_CODEC) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(NON_COMPRESSING_CODEC);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // switch codecs\n        if (iwConf.getCodec() instanceof Lucene41Codec) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(new Lucene41Codec());\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"407687e67faf6e1f02a211ca078d8e3eed631027","date":1355157407,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","pathOld":"/dev/null","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == NON_COMPRESSING_CODEC) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(NON_COMPRESSING_CODEC);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70b55953b6a72596cb534ead735a8b849a473cac","date":1363634568,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene42Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == NON_COMPRESSING_CODEC) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(NON_COMPRESSING_CODEC);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n  }\n\n","bugFix":null,"bugIntro":["4ce24aa081e44190692bbebc8aead342ad7060e8"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"470cd91a3cf3f140cfc5bbff1a11e2ab0cb2229a":["1a51ec81f1fd009bf893bd88ec1c7b964fae6fab"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"407687e67faf6e1f02a211ca078d8e3eed631027":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","470cd91a3cf3f140cfc5bbff1a11e2ab0cb2229a"],"70b55953b6a72596cb534ead735a8b849a473cac":["470cd91a3cf3f140cfc5bbff1a11e2ab0cb2229a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["70b55953b6a72596cb534ead735a8b849a473cac"],"1a51ec81f1fd009bf893bd88ec1c7b964fae6fab":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"470cd91a3cf3f140cfc5bbff1a11e2ab0cb2229a":["407687e67faf6e1f02a211ca078d8e3eed631027","70b55953b6a72596cb534ead735a8b849a473cac"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["407687e67faf6e1f02a211ca078d8e3eed631027","1a51ec81f1fd009bf893bd88ec1c7b964fae6fab"],"407687e67faf6e1f02a211ca078d8e3eed631027":[],"70b55953b6a72596cb534ead735a8b849a473cac":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1a51ec81f1fd009bf893bd88ec1c7b964fae6fab":["470cd91a3cf3f140cfc5bbff1a11e2ab0cb2229a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["407687e67faf6e1f02a211ca078d8e3eed631027","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}