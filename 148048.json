{"path":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","commits":[{"id":"c495edcca4d0bc51bf62d9be3527c87bf9b44ded","date":1498673617,"type":0,"author":"Dennis Gove","isMerge":false,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Send a shard request to each chosen replica, streaming \n   * the responses back to the {@link AnalyticsRequestManager}\n   * through the {@link AnalyticsShardResponseParser}.\n   * <p>\n   * A thread pool is used to send the requests simultaneously, \n   * and therefore importing the results is also done in parallel.\n   * However the manager can only import one shard response at a time,\n   * so the {@link AnalyticsShardResponseParser} is blocked until each import is finished.\n   * \n   * @throws IOException if an exception occurs while sending requests.\n   */\n  private void streamFromShards() throws IOException {\n    ExecutorService service = ExecutorUtil.newMDCAwareCachedThreadPool(new SolrjNamedThreadFactory(\"SolrAnalyticsStream\"));\n    List<Future<SolrException>> futures = new ArrayList<>();\n    List<AnalyticsShardRequester> openers = new ArrayList<>();\n    for (String replicaUrl : replicaUrls) {\n      AnalyticsShardRequester opener = new AnalyticsShardRequester(replicaUrl);\n      openers.add(opener);\n      Future<SolrException> future = service.submit(opener);\n      futures.add(future);\n    }\n    try {\n      for (Future<SolrException> f : futures) {\n        SolrException e = f.get();\n        if (e != null) {\n          throw e;\n        }\n      }\n    } catch (InterruptedException e1) {\n      throw new RuntimeException(e1);\n    } catch (ExecutionException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      service.shutdown();\n      for (AnalyticsShardRequester opener : openers) {\n        opener.close();\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Send a shard request to each chosen replica, streaming \n   * the responses back to the {@link AnalyticsRequestManager}\n   * through the {@link AnalyticsShardResponseParser}.\n   * <p>\n   * A thread pool is used to send the requests simultaneously, \n   * and therefore importing the results is also done in parallel.\n   * However the manager can only import one shard response at a time,\n   * so the {@link AnalyticsShardResponseParser} is blocked until each import is finished.\n   * \n   * @throws IOException if an exception occurs while sending requests.\n   */\n  private void streamFromShards() throws IOException {\n    ExecutorService service = ExecutorUtil.newMDCAwareCachedThreadPool(new SolrjNamedThreadFactory(\"SolrAnalyticsStream\"));\n    List<Future<SolrException>> futures = new ArrayList<>();\n    List<AnalyticsShardRequester> openers = new ArrayList<>();\n    for (String replicaUrl : replicaUrls) {\n      AnalyticsShardRequester opener = new AnalyticsShardRequester(replicaUrl);\n      openers.add(opener);\n      Future<SolrException> future = service.submit(opener);\n      futures.add(future);\n    }\n    try {\n      for (Future<SolrException> f : futures) {\n        SolrException e = f.get();\n        if (e != null) {\n          throw e;\n        }\n      }\n    } catch (InterruptedException e1) {\n      throw new RuntimeException(e1);\n    } catch (ExecutionException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      service.shutdown();\n      for (AnalyticsShardRequester opener : openers) {\n        opener.close();\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30c8e5574b55d57947e989443dfde611646530ee","date":1499131153,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Send a shard request to each chosen replica, streaming \n   * the responses back to the {@link AnalyticsRequestManager}\n   * through the {@link AnalyticsShardResponseParser}.\n   * <p>\n   * A thread pool is used to send the requests simultaneously, \n   * and therefore importing the results is also done in parallel.\n   * However the manager can only import one shard response at a time,\n   * so the {@link AnalyticsShardResponseParser} is blocked until each import is finished.\n   * \n   * @throws IOException if an exception occurs while sending requests.\n   */\n  private void streamFromShards() throws IOException {\n    ExecutorService service = ExecutorUtil.newMDCAwareCachedThreadPool(new SolrjNamedThreadFactory(\"SolrAnalyticsStream\"));\n    List<Future<SolrException>> futures = new ArrayList<>();\n    List<AnalyticsShardRequester> openers = new ArrayList<>();\n    for (String replicaUrl : replicaUrls) {\n      AnalyticsShardRequester opener = new AnalyticsShardRequester(replicaUrl);\n      openers.add(opener);\n      Future<SolrException> future = service.submit(opener);\n      futures.add(future);\n    }\n    try {\n      for (Future<SolrException> f : futures) {\n        SolrException e = f.get();\n        if (e != null) {\n          throw e;\n        }\n      }\n    } catch (InterruptedException e1) {\n      throw new RuntimeException(e1);\n    } catch (ExecutionException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      service.shutdown();\n      for (AnalyticsShardRequester opener : openers) {\n        opener.close();\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1e4e64b7199d2f2a17be7f3926c7532553910dce","date":1564342581,"type":3,"author":"Jason Gerlowski","isMerge":false,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","pathOld":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","sourceNew":"  /**\n   * Send a shard request to each chosen replica, streaming\n   * the responses back to the {@link AnalyticsRequestManager}\n   * through the {@link AnalyticsShardResponseParser}.\n   * <p>\n   * A thread pool is used to send the requests simultaneously,\n   * and therefore importing the results is also done in parallel.\n   * However the manager can only import one shard response at a time,\n   * so the {@link AnalyticsShardResponseParser} is blocked until each import is finished.\n   *\n   * @throws IOException if an exception occurs while sending requests.\n   */\n  private void streamFromShards() throws IOException {\n    ExecutorService service = ExecutorUtil.newMDCAwareCachedThreadPool(new SolrjNamedThreadFactory(\"SolrAnalyticsStream\"));\n    List<Future<SolrException>> futures = new ArrayList<>();\n    List<AnalyticsShardRequester> openers = new ArrayList<>();\n    for (String replicaUrl : replicaUrls) {\n      AnalyticsShardRequester opener = new AnalyticsShardRequester(replicaUrl);\n      openers.add(opener);\n      Future<SolrException> future = service.submit(opener);\n      futures.add(future);\n    }\n    try {\n      for (Future<SolrException> f : futures) {\n        SolrException e = f.get();\n        if (e != null) {\n          throw e;\n        }\n      }\n    } catch (InterruptedException e1) {\n      throw new RuntimeException(e1);\n    } catch (ExecutionException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      service.shutdown();\n      for (AnalyticsShardRequester opener : openers) {\n        opener.close();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Send a shard request to each chosen replica, streaming \n   * the responses back to the {@link AnalyticsRequestManager}\n   * through the {@link AnalyticsShardResponseParser}.\n   * <p>\n   * A thread pool is used to send the requests simultaneously, \n   * and therefore importing the results is also done in parallel.\n   * However the manager can only import one shard response at a time,\n   * so the {@link AnalyticsShardResponseParser} is blocked until each import is finished.\n   * \n   * @throws IOException if an exception occurs while sending requests.\n   */\n  private void streamFromShards() throws IOException {\n    ExecutorService service = ExecutorUtil.newMDCAwareCachedThreadPool(new SolrjNamedThreadFactory(\"SolrAnalyticsStream\"));\n    List<Future<SolrException>> futures = new ArrayList<>();\n    List<AnalyticsShardRequester> openers = new ArrayList<>();\n    for (String replicaUrl : replicaUrls) {\n      AnalyticsShardRequester opener = new AnalyticsShardRequester(replicaUrl);\n      openers.add(opener);\n      Future<SolrException> future = service.submit(opener);\n      futures.add(future);\n    }\n    try {\n      for (Future<SolrException> f : futures) {\n        SolrException e = f.get();\n        if (e != null) {\n          throw e;\n        }\n      }\n    } catch (InterruptedException e1) {\n      throw new RuntimeException(e1);\n    } catch (ExecutionException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      service.shutdown();\n      for (AnalyticsShardRequester opener : openers) {\n        opener.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8061ddd97f3352007d927dae445884a6f3d857b","date":1564988276,"type":3,"author":"Atri Sharma","isMerge":true,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","pathOld":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","sourceNew":"  /**\n   * Send a shard request to each chosen replica, streaming\n   * the responses back to the {@link AnalyticsRequestManager}\n   * through the {@link AnalyticsShardResponseParser}.\n   * <p>\n   * A thread pool is used to send the requests simultaneously,\n   * and therefore importing the results is also done in parallel.\n   * However the manager can only import one shard response at a time,\n   * so the {@link AnalyticsShardResponseParser} is blocked until each import is finished.\n   *\n   * @throws IOException if an exception occurs while sending requests.\n   */\n  private void streamFromShards() throws IOException {\n    ExecutorService service = ExecutorUtil.newMDCAwareCachedThreadPool(new SolrjNamedThreadFactory(\"SolrAnalyticsStream\"));\n    List<Future<SolrException>> futures = new ArrayList<>();\n    List<AnalyticsShardRequester> openers = new ArrayList<>();\n    for (String replicaUrl : replicaUrls) {\n      AnalyticsShardRequester opener = new AnalyticsShardRequester(replicaUrl);\n      openers.add(opener);\n      Future<SolrException> future = service.submit(opener);\n      futures.add(future);\n    }\n    try {\n      for (Future<SolrException> f : futures) {\n        SolrException e = f.get();\n        if (e != null) {\n          throw e;\n        }\n      }\n    } catch (InterruptedException e1) {\n      throw new RuntimeException(e1);\n    } catch (ExecutionException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      service.shutdown();\n      for (AnalyticsShardRequester opener : openers) {\n        opener.close();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Send a shard request to each chosen replica, streaming \n   * the responses back to the {@link AnalyticsRequestManager}\n   * through the {@link AnalyticsShardResponseParser}.\n   * <p>\n   * A thread pool is used to send the requests simultaneously, \n   * and therefore importing the results is also done in parallel.\n   * However the manager can only import one shard response at a time,\n   * so the {@link AnalyticsShardResponseParser} is blocked until each import is finished.\n   * \n   * @throws IOException if an exception occurs while sending requests.\n   */\n  private void streamFromShards() throws IOException {\n    ExecutorService service = ExecutorUtil.newMDCAwareCachedThreadPool(new SolrjNamedThreadFactory(\"SolrAnalyticsStream\"));\n    List<Future<SolrException>> futures = new ArrayList<>();\n    List<AnalyticsShardRequester> openers = new ArrayList<>();\n    for (String replicaUrl : replicaUrls) {\n      AnalyticsShardRequester opener = new AnalyticsShardRequester(replicaUrl);\n      openers.add(opener);\n      Future<SolrException> future = service.submit(opener);\n      futures.add(future);\n    }\n    try {\n      for (Future<SolrException> f : futures) {\n        SolrException e = f.get();\n        if (e != null) {\n          throw e;\n        }\n      }\n    } catch (InterruptedException e1) {\n      throw new RuntimeException(e1);\n    } catch (ExecutionException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      service.shutdown();\n      for (AnalyticsShardRequester opener : openers) {\n        opener.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab30b5737e6a5d4ee74fdc889750d18d2a624471","date":1576097297,"type":3,"author":"Mikhail Khludnev","isMerge":false,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","pathOld":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","sourceNew":"  /**\n   * Send a shard request to each chosen replica, streaming\n   * the responses back to the {@link AnalyticsRequestManager}\n   * through the {@link AnalyticsShardResponseParser}.\n   * <p>\n   * A thread pool is used to send the requests simultaneously,\n   * and therefore importing the results is also done in parallel.\n   * However the manager can only import one shard response at a time,\n   * so the {@link AnalyticsShardResponseParser} is blocked until each import is finished.\n   *\n   * @throws IOException if an exception occurs while sending requests.\n   */\n  private void streamFromShards() throws IOException {\n    ExecutorService service = ExecutorUtil.newMDCAwareCachedThreadPool(new SolrjNamedThreadFactory(\"SolrAnalyticsStream\"));\n    List<Future<SolrException>> futures = new ArrayList<>();\n    List<AnalyticsShardRequester> openers = new ArrayList<>();\n    for (String replicaUrl : replicaUrls) {\n      AnalyticsShardRequester opener = new AnalyticsShardRequester(replicaUrl);\n      openers.add(opener);\n      Future<SolrException> future = service.submit(opener);\n      futures.add(future);\n    }\n    try {\n      for (Future<SolrException> f : futures) {\n        SolrException e = f.get();\n        if (e != null) {\n          if (TimeExceededStubException.isIt(e)) {\n            manager.setPartialResults(true);\n          } else {\n            throw e;\n          }\n        }\n      }\n    } catch (InterruptedException e1) {\n      throw new RuntimeException(e1);\n    } catch (ExecutionException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      service.shutdown();\n      for (AnalyticsShardRequester opener : openers) {\n        opener.close();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Send a shard request to each chosen replica, streaming\n   * the responses back to the {@link AnalyticsRequestManager}\n   * through the {@link AnalyticsShardResponseParser}.\n   * <p>\n   * A thread pool is used to send the requests simultaneously,\n   * and therefore importing the results is also done in parallel.\n   * However the manager can only import one shard response at a time,\n   * so the {@link AnalyticsShardResponseParser} is blocked until each import is finished.\n   *\n   * @throws IOException if an exception occurs while sending requests.\n   */\n  private void streamFromShards() throws IOException {\n    ExecutorService service = ExecutorUtil.newMDCAwareCachedThreadPool(new SolrjNamedThreadFactory(\"SolrAnalyticsStream\"));\n    List<Future<SolrException>> futures = new ArrayList<>();\n    List<AnalyticsShardRequester> openers = new ArrayList<>();\n    for (String replicaUrl : replicaUrls) {\n      AnalyticsShardRequester opener = new AnalyticsShardRequester(replicaUrl);\n      openers.add(opener);\n      Future<SolrException> future = service.submit(opener);\n      futures.add(future);\n    }\n    try {\n      for (Future<SolrException> f : futures) {\n        SolrException e = f.get();\n        if (e != null) {\n          throw e;\n        }\n      }\n    } catch (InterruptedException e1) {\n      throw new RuntimeException(e1);\n    } catch (ExecutionException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      service.shutdown();\n      for (AnalyticsShardRequester opener : openers) {\n        opener.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"df724d84dab24a0cc54bec95a8680867adc7f171","date":1576156608,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","pathOld":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","sourceNew":"  /**\n   * Send a shard request to each chosen replica, streaming\n   * the responses back to the {@link AnalyticsRequestManager}\n   * through the {@link AnalyticsShardResponseParser}.\n   * <p>\n   * A thread pool is used to send the requests simultaneously,\n   * and therefore importing the results is also done in parallel.\n   * However the manager can only import one shard response at a time,\n   * so the {@link AnalyticsShardResponseParser} is blocked until each import is finished.\n   *\n   * @throws IOException if an exception occurs while sending requests.\n   */\n  private void streamFromShards() throws IOException {\n    ExecutorService service = ExecutorUtil.newMDCAwareCachedThreadPool(new SolrjNamedThreadFactory(\"SolrAnalyticsStream\"));\n    List<Future<SolrException>> futures = new ArrayList<>();\n    List<AnalyticsShardRequester> openers = new ArrayList<>();\n    for (String replicaUrl : replicaUrls) {\n      AnalyticsShardRequester opener = new AnalyticsShardRequester(replicaUrl);\n      openers.add(opener);\n      Future<SolrException> future = service.submit(opener);\n      futures.add(future);\n    }\n    try {\n      for (Future<SolrException> f : futures) {\n        SolrException e = f.get();\n        if (e != null) {\n          if (TimeExceededStubException.isIt(e)) {\n            manager.setPartialResults(true);\n          } else {\n            throw e;\n          }\n        }\n      }\n    } catch (InterruptedException e1) {\n      throw new RuntimeException(e1);\n    } catch (ExecutionException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      service.shutdown();\n      for (AnalyticsShardRequester opener : openers) {\n        opener.close();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Send a shard request to each chosen replica, streaming\n   * the responses back to the {@link AnalyticsRequestManager}\n   * through the {@link AnalyticsShardResponseParser}.\n   * <p>\n   * A thread pool is used to send the requests simultaneously,\n   * and therefore importing the results is also done in parallel.\n   * However the manager can only import one shard response at a time,\n   * so the {@link AnalyticsShardResponseParser} is blocked until each import is finished.\n   *\n   * @throws IOException if an exception occurs while sending requests.\n   */\n  private void streamFromShards() throws IOException {\n    ExecutorService service = ExecutorUtil.newMDCAwareCachedThreadPool(new SolrjNamedThreadFactory(\"SolrAnalyticsStream\"));\n    List<Future<SolrException>> futures = new ArrayList<>();\n    List<AnalyticsShardRequester> openers = new ArrayList<>();\n    for (String replicaUrl : replicaUrls) {\n      AnalyticsShardRequester opener = new AnalyticsShardRequester(replicaUrl);\n      openers.add(opener);\n      Future<SolrException> future = service.submit(opener);\n      futures.add(future);\n    }\n    try {\n      for (Future<SolrException> f : futures) {\n        SolrException e = f.get();\n        if (e != null) {\n          throw e;\n        }\n      }\n    } catch (InterruptedException e1) {\n      throw new RuntimeException(e1);\n    } catch (ExecutionException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      service.shutdown();\n      for (AnalyticsShardRequester opener : openers) {\n        opener.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb03700c9690d16b15fb4f56f6ec36b128fd894e","date":1586745995,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","pathOld":"solr/contrib/analytics/src/java/org/apache/solr/analytics/stream/AnalyticsShardRequestManager#streamFromShards().mjava","sourceNew":"  /**\n   * Send a shard request to each chosen replica, streaming\n   * the responses back to the {@link AnalyticsRequestManager}\n   * through the {@link AnalyticsShardResponseParser}.\n   * <p>\n   * A thread pool is used to send the requests simultaneously,\n   * and therefore importing the results is also done in parallel.\n   * However the manager can only import one shard response at a time,\n   * so the {@link AnalyticsShardResponseParser} is blocked until each import is finished.\n   *\n   * @throws IOException if an exception occurs while sending requests.\n   */\n  private void streamFromShards() throws IOException {\n    ExecutorService service = ExecutorUtil.newMDCAwareCachedThreadPool(new SolrNamedThreadFactory(\"SolrAnalyticsStream\"));\n    List<Future<SolrException>> futures = new ArrayList<>();\n    List<AnalyticsShardRequester> openers = new ArrayList<>();\n    for (String replicaUrl : replicaUrls) {\n      AnalyticsShardRequester opener = new AnalyticsShardRequester(replicaUrl);\n      openers.add(opener);\n      Future<SolrException> future = service.submit(opener);\n      futures.add(future);\n    }\n    try {\n      for (Future<SolrException> f : futures) {\n        SolrException e = f.get();\n        if (e != null) {\n          if (TimeExceededStubException.isIt(e)) {\n            manager.setPartialResults(true);\n          } else {\n            throw e;\n          }\n        }\n      }\n    } catch (InterruptedException e1) {\n      throw new RuntimeException(e1);\n    } catch (ExecutionException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      service.shutdown();\n      for (AnalyticsShardRequester opener : openers) {\n        opener.close();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Send a shard request to each chosen replica, streaming\n   * the responses back to the {@link AnalyticsRequestManager}\n   * through the {@link AnalyticsShardResponseParser}.\n   * <p>\n   * A thread pool is used to send the requests simultaneously,\n   * and therefore importing the results is also done in parallel.\n   * However the manager can only import one shard response at a time,\n   * so the {@link AnalyticsShardResponseParser} is blocked until each import is finished.\n   *\n   * @throws IOException if an exception occurs while sending requests.\n   */\n  private void streamFromShards() throws IOException {\n    ExecutorService service = ExecutorUtil.newMDCAwareCachedThreadPool(new SolrjNamedThreadFactory(\"SolrAnalyticsStream\"));\n    List<Future<SolrException>> futures = new ArrayList<>();\n    List<AnalyticsShardRequester> openers = new ArrayList<>();\n    for (String replicaUrl : replicaUrls) {\n      AnalyticsShardRequester opener = new AnalyticsShardRequester(replicaUrl);\n      openers.add(opener);\n      Future<SolrException> future = service.submit(opener);\n      futures.add(future);\n    }\n    try {\n      for (Future<SolrException> f : futures) {\n        SolrException e = f.get();\n        if (e != null) {\n          if (TimeExceededStubException.isIt(e)) {\n            manager.setPartialResults(true);\n          } else {\n            throw e;\n          }\n        }\n      }\n    } catch (InterruptedException e1) {\n      throw new RuntimeException(e1);\n    } catch (ExecutionException e1) {\n      throw new RuntimeException(e1);\n    } finally {\n      service.shutdown();\n      for (AnalyticsShardRequester opener : openers) {\n        opener.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"df724d84dab24a0cc54bec95a8680867adc7f171":["1e4e64b7199d2f2a17be7f3926c7532553910dce","ab30b5737e6a5d4ee74fdc889750d18d2a624471"],"1e4e64b7199d2f2a17be7f3926c7532553910dce":["28288370235ed02234a64753cdbf0c6ec096304a"],"ab30b5737e6a5d4ee74fdc889750d18d2a624471":["1e4e64b7199d2f2a17be7f3926c7532553910dce"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fb03700c9690d16b15fb4f56f6ec36b128fd894e":["ab30b5737e6a5d4ee74fdc889750d18d2a624471"],"30c8e5574b55d57947e989443dfde611646530ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","28288370235ed02234a64753cdbf0c6ec096304a"],"c495edcca4d0bc51bf62d9be3527c87bf9b44ded":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c495edcca4d0bc51bf62d9be3527c87bf9b44ded"],"f8061ddd97f3352007d927dae445884a6f3d857b":["28288370235ed02234a64753cdbf0c6ec096304a","1e4e64b7199d2f2a17be7f3926c7532553910dce"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["fb03700c9690d16b15fb4f56f6ec36b128fd894e"]},"commit2Childs":{"df724d84dab24a0cc54bec95a8680867adc7f171":[],"1e4e64b7199d2f2a17be7f3926c7532553910dce":["df724d84dab24a0cc54bec95a8680867adc7f171","ab30b5737e6a5d4ee74fdc889750d18d2a624471","f8061ddd97f3352007d927dae445884a6f3d857b"],"ab30b5737e6a5d4ee74fdc889750d18d2a624471":["df724d84dab24a0cc54bec95a8680867adc7f171","fb03700c9690d16b15fb4f56f6ec36b128fd894e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["30c8e5574b55d57947e989443dfde611646530ee","c495edcca4d0bc51bf62d9be3527c87bf9b44ded","28288370235ed02234a64753cdbf0c6ec096304a"],"fb03700c9690d16b15fb4f56f6ec36b128fd894e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"30c8e5574b55d57947e989443dfde611646530ee":[],"c495edcca4d0bc51bf62d9be3527c87bf9b44ded":["28288370235ed02234a64753cdbf0c6ec096304a"],"28288370235ed02234a64753cdbf0c6ec096304a":["1e4e64b7199d2f2a17be7f3926c7532553910dce","30c8e5574b55d57947e989443dfde611646530ee","f8061ddd97f3352007d927dae445884a6f3d857b"],"f8061ddd97f3352007d927dae445884a6f3d857b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["df724d84dab24a0cc54bec95a8680867adc7f171","30c8e5574b55d57947e989443dfde611646530ee","f8061ddd97f3352007d927dae445884a6f3d857b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}