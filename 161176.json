{"path":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"modules/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param termFreqMap a Map of terms and their frequencies\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Int> termFreqMap, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    TokenStream ts = analyzer.tokenStream(fieldName, r);\n    int tokenCount = 0;\n    // for every token\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    ts.reset();\n    while (ts.incrementToken()) {\n      String word = termAtt.toString();\n      tokenCount++;\n      if (tokenCount > maxNumTokensParsed) {\n        break;\n      }\n      if (isNoiseWord(word)) {\n        continue;\n      }\n\n      // increment frequency\n      Int cnt = termFreqMap.get(word);\n      if (cnt == null) {\n        termFreqMap.put(word, new Int());\n      } else {\n        cnt.x++;\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","sourceOld":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param termFreqMap a Map of terms and their frequencies\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Int> termFreqMap, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    TokenStream ts = analyzer.tokenStream(fieldName, r);\n    int tokenCount = 0;\n    // for every token\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    ts.reset();\n    while (ts.incrementToken()) {\n      String word = termAtt.toString();\n      tokenCount++;\n      if (tokenCount > maxNumTokensParsed) {\n        break;\n      }\n      if (isNoiseWord(word)) {\n        continue;\n      }\n\n      // increment frequency\n      Int cnt = termFreqMap.get(word);\n      if (cnt == null) {\n        termFreqMap.put(word, new Int());\n      } else {\n        cnt.x++;\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"782ed6a4b4ba50ec19734fc8db4e570ee193d627","date":1381127065,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param termFreqMap a Map of terms and their frequencies\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Int> termFreqMap, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    try (TokenStream ts = analyzer.tokenStream(fieldName, r)) {\n      int tokenCount = 0;\n      // for every token\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      ts.reset();\n      while (ts.incrementToken()) {\n        String word = termAtt.toString();\n        tokenCount++;\n        if (tokenCount > maxNumTokensParsed) {\n          break;\n        }\n        if (isNoiseWord(word)) {\n          continue;\n        }\n\n        // increment frequency\n        Int cnt = termFreqMap.get(word);\n        if (cnt == null) {\n          termFreqMap.put(word, new Int());\n        } else {\n          cnt.x++;\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param termFreqMap a Map of terms and their frequencies\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Int> termFreqMap, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    TokenStream ts = analyzer.tokenStream(fieldName, r);\n    int tokenCount = 0;\n    // for every token\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    ts.reset();\n    while (ts.incrementToken()) {\n      String word = termAtt.toString();\n      tokenCount++;\n      if (tokenCount > maxNumTokensParsed) {\n        break;\n      }\n      if (isNoiseWord(word)) {\n        continue;\n      }\n\n      // increment frequency\n      Int cnt = termFreqMap.get(word);\n      if (cnt == null) {\n        termFreqMap.put(word, new Int());\n      } else {\n        cnt.x++;\n      }\n    }\n    ts.end();\n    ts.close();\n  }\n\n","bugFix":["e141595402370bee958745de8b1c9de1fa182581","69e043c521d4e8db770cc140c63f5ef51f03426a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e562682007e295029696e354ac6947531b083c79","date":1459152450,"type":5,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Map[String,Int]],String).mjava","pathOld":"lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis#addTermFrequencies(Reader,Map[String,Int],String).mjava","sourceNew":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param perFieldTermFrequencies a Map of terms and their frequencies per field\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Map<String, Int>> perFieldTermFrequencies, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    Map<String, Int> termFreqMap = perFieldTermFrequencies.get(fieldName);\n    if (termFreqMap == null) {\n      termFreqMap = new HashMap<>();\n      perFieldTermFrequencies.put(fieldName, termFreqMap);\n    }\n    try (TokenStream ts = analyzer.tokenStream(fieldName, r)) {\n      int tokenCount = 0;\n      // for every token\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      ts.reset();\n      while (ts.incrementToken()) {\n        String word = termAtt.toString();\n        tokenCount++;\n        if (tokenCount > maxNumTokensParsed) {\n          break;\n        }\n        if (isNoiseWord(word)) {\n          continue;\n        }\n\n        // increment frequency\n        Int cnt = termFreqMap.get(word);\n        if (cnt == null) {\n          termFreqMap.put(word, new Int());\n        } else {\n          cnt.x++;\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  /**\n   * Adds term frequencies found by tokenizing text from reader into the Map words\n   *\n   * @param r a source of text to be tokenized\n   * @param termFreqMap a Map of terms and their frequencies\n   * @param fieldName Used by analyzer for any special per-field analysis\n   */\n  private void addTermFrequencies(Reader r, Map<String, Int> termFreqMap, String fieldName)\n      throws IOException {\n    if (analyzer == null) {\n      throw new UnsupportedOperationException(\"To use MoreLikeThis without \" +\n          \"term vectors, you must provide an Analyzer\");\n    }\n    try (TokenStream ts = analyzer.tokenStream(fieldName, r)) {\n      int tokenCount = 0;\n      // for every token\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      ts.reset();\n      while (ts.incrementToken()) {\n        String word = termAtt.toString();\n        tokenCount++;\n        if (tokenCount > maxNumTokensParsed) {\n          break;\n        }\n        if (isNoiseWord(word)) {\n          continue;\n        }\n\n        // increment frequency\n        Int cnt = termFreqMap.get(word);\n        if (cnt == null) {\n          termFreqMap.put(word, new Int());\n        } else {\n          cnt.x++;\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"e562682007e295029696e354ac6947531b083c79":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e562682007e295029696e354ac6947531b083c79"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["e562682007e295029696e354ac6947531b083c79"],"e562682007e295029696e354ac6947531b083c79":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}