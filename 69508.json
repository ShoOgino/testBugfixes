{"path":"lucene/core/src/test/org/apache/lucene/index/TestReaderPool#testPassReaderToMergePolicyConcurrently().mjava","commits":[{"id":"5640f95cd73b5d4138e0a0988164b0fa398a3256","date":1527054899,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestReaderPool#testPassReaderToMergePolicyConcurrently().mjava","pathOld":"/dev/null","sourceNew":"  public void testPassReaderToMergePolicyConcurrently() throws Exception {\n    Directory directory = newDirectory();\n    FieldInfos.FieldNumbers fieldNumbers = buildIndex(directory);\n    StandardDirectoryReader reader = (StandardDirectoryReader) DirectoryReader.open(directory);\n    SegmentInfos segmentInfos = reader.segmentInfos.clone();\n    ReaderPool pool = new ReaderPool(directory, directory, segmentInfos, fieldNumbers, () -> 0L,\n        new NullInfoStream(), null, null);\n    if (random().nextBoolean()) {\n      pool.enableReaderPooling();\n    }\n    AtomicBoolean isDone = new AtomicBoolean();\n    CountDownLatch latch = new CountDownLatch(1);\n    Thread refresher = new Thread(() -> {\n      try {\n        latch.countDown();\n        while (isDone.get() == false) {\n          for (SegmentCommitInfo commitInfo : segmentInfos) {\n            ReadersAndUpdates readersAndUpdates = pool.get(commitInfo, true);\n            SegmentReader segmentReader = readersAndUpdates.getReader(IOContext.READ);\n            readersAndUpdates.release(segmentReader);\n            pool.release(readersAndUpdates, random().nextBoolean());\n          }\n        }\n      } catch (Exception ex) {\n        throw new AssertionError(ex);\n      }\n    });\n    refresher.start();\n    MergePolicy mergePolicy = new FilterMergePolicy(newMergePolicy()) {\n      @Override\n      public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) throws IOException {\n        CodecReader reader = readerIOSupplier.get();\n        assert reader.maxDoc() > 0; // just try to access the reader\n        return true;\n      }\n    };\n    latch.await();\n    for (int i = 0; i < reader.maxDoc(); i++) {\n      for (SegmentCommitInfo commitInfo : segmentInfos) {\n        ReadersAndUpdates readersAndUpdates = pool.get(commitInfo, true);\n        SegmentReader sr = readersAndUpdates.getReadOnlyClone(IOContext.READ);\n        PostingsEnum postings = sr.postings(new Term(\"id\", \"\" + i));\n        sr.decRef();\n        if (postings != null) {\n          for (int docId = postings.nextDoc(); docId != DocIdSetIterator.NO_MORE_DOCS; docId = postings.nextDoc()) {\n            readersAndUpdates.delete(docId);\n            assertTrue(readersAndUpdates.keepFullyDeletedSegment(mergePolicy));\n          }\n        }\n        assertTrue(readersAndUpdates.keepFullyDeletedSegment(mergePolicy));\n        pool.release(readersAndUpdates, random().nextBoolean());\n      }\n    }\n    isDone.set(true);\n    refresher.join();\n    IOUtils.close(pool, reader, directory);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"763da4a9605e47013078edc323b9d4b608f0f9e0","date":1555353576,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestReaderPool#testPassReaderToMergePolicyConcurrently().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestReaderPool#testPassReaderToMergePolicyConcurrently().mjava","sourceNew":"  public void testPassReaderToMergePolicyConcurrently() throws Exception {\n    Directory directory = newDirectory();\n    FieldInfos.FieldNumbers fieldNumbers = buildIndex(directory);\n    StandardDirectoryReader reader = (StandardDirectoryReader) DirectoryReader.open(directory);\n    SegmentInfos segmentInfos = reader.segmentInfos.clone();\n    ReaderPool pool = new ReaderPool(directory, directory, segmentInfos, fieldNumbers, () -> 0L,\n        new NullInfoStream(), null, null, Collections.emptyMap());\n    if (random().nextBoolean()) {\n      pool.enableReaderPooling();\n    }\n    AtomicBoolean isDone = new AtomicBoolean();\n    CountDownLatch latch = new CountDownLatch(1);\n    Thread refresher = new Thread(() -> {\n      try {\n        latch.countDown();\n        while (isDone.get() == false) {\n          for (SegmentCommitInfo commitInfo : segmentInfos) {\n            ReadersAndUpdates readersAndUpdates = pool.get(commitInfo, true);\n            SegmentReader segmentReader = readersAndUpdates.getReader(IOContext.READ);\n            readersAndUpdates.release(segmentReader);\n            pool.release(readersAndUpdates, random().nextBoolean());\n          }\n        }\n      } catch (Exception ex) {\n        throw new AssertionError(ex);\n      }\n    });\n    refresher.start();\n    MergePolicy mergePolicy = new FilterMergePolicy(newMergePolicy()) {\n      @Override\n      public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) throws IOException {\n        CodecReader reader = readerIOSupplier.get();\n        assert reader.maxDoc() > 0; // just try to access the reader\n        return true;\n      }\n    };\n    latch.await();\n    for (int i = 0; i < reader.maxDoc(); i++) {\n      for (SegmentCommitInfo commitInfo : segmentInfos) {\n        ReadersAndUpdates readersAndUpdates = pool.get(commitInfo, true);\n        SegmentReader sr = readersAndUpdates.getReadOnlyClone(IOContext.READ);\n        PostingsEnum postings = sr.postings(new Term(\"id\", \"\" + i));\n        sr.decRef();\n        if (postings != null) {\n          for (int docId = postings.nextDoc(); docId != DocIdSetIterator.NO_MORE_DOCS; docId = postings.nextDoc()) {\n            readersAndUpdates.delete(docId);\n            assertTrue(readersAndUpdates.keepFullyDeletedSegment(mergePolicy));\n          }\n        }\n        assertTrue(readersAndUpdates.keepFullyDeletedSegment(mergePolicy));\n        pool.release(readersAndUpdates, random().nextBoolean());\n      }\n    }\n    isDone.set(true);\n    refresher.join();\n    IOUtils.close(pool, reader, directory);\n  }\n\n","sourceOld":"  public void testPassReaderToMergePolicyConcurrently() throws Exception {\n    Directory directory = newDirectory();\n    FieldInfos.FieldNumbers fieldNumbers = buildIndex(directory);\n    StandardDirectoryReader reader = (StandardDirectoryReader) DirectoryReader.open(directory);\n    SegmentInfos segmentInfos = reader.segmentInfos.clone();\n    ReaderPool pool = new ReaderPool(directory, directory, segmentInfos, fieldNumbers, () -> 0L,\n        new NullInfoStream(), null, null);\n    if (random().nextBoolean()) {\n      pool.enableReaderPooling();\n    }\n    AtomicBoolean isDone = new AtomicBoolean();\n    CountDownLatch latch = new CountDownLatch(1);\n    Thread refresher = new Thread(() -> {\n      try {\n        latch.countDown();\n        while (isDone.get() == false) {\n          for (SegmentCommitInfo commitInfo : segmentInfos) {\n            ReadersAndUpdates readersAndUpdates = pool.get(commitInfo, true);\n            SegmentReader segmentReader = readersAndUpdates.getReader(IOContext.READ);\n            readersAndUpdates.release(segmentReader);\n            pool.release(readersAndUpdates, random().nextBoolean());\n          }\n        }\n      } catch (Exception ex) {\n        throw new AssertionError(ex);\n      }\n    });\n    refresher.start();\n    MergePolicy mergePolicy = new FilterMergePolicy(newMergePolicy()) {\n      @Override\n      public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) throws IOException {\n        CodecReader reader = readerIOSupplier.get();\n        assert reader.maxDoc() > 0; // just try to access the reader\n        return true;\n      }\n    };\n    latch.await();\n    for (int i = 0; i < reader.maxDoc(); i++) {\n      for (SegmentCommitInfo commitInfo : segmentInfos) {\n        ReadersAndUpdates readersAndUpdates = pool.get(commitInfo, true);\n        SegmentReader sr = readersAndUpdates.getReadOnlyClone(IOContext.READ);\n        PostingsEnum postings = sr.postings(new Term(\"id\", \"\" + i));\n        sr.decRef();\n        if (postings != null) {\n          for (int docId = postings.nextDoc(); docId != DocIdSetIterator.NO_MORE_DOCS; docId = postings.nextDoc()) {\n            readersAndUpdates.delete(docId);\n            assertTrue(readersAndUpdates.keepFullyDeletedSegment(mergePolicy));\n          }\n        }\n        assertTrue(readersAndUpdates.keepFullyDeletedSegment(mergePolicy));\n        pool.release(readersAndUpdates, random().nextBoolean());\n      }\n    }\n    isDone.set(true);\n    refresher.join();\n    IOUtils.close(pool, reader, directory);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a4e83191a3e02851a0b67e5335e6922f3e9ea86d","date":1583489709,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestReaderPool#testPassReaderToMergePolicyConcurrently().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestReaderPool#testPassReaderToMergePolicyConcurrently().mjava","sourceNew":"  public void testPassReaderToMergePolicyConcurrently() throws Exception {\n    Directory directory = newDirectory();\n    FieldInfos.FieldNumbers fieldNumbers = buildIndex(directory);\n    StandardDirectoryReader reader = (StandardDirectoryReader) DirectoryReader.open(directory);\n    SegmentInfos segmentInfos = reader.segmentInfos.clone();\n    ReaderPool pool = new ReaderPool(directory, directory, segmentInfos, fieldNumbers, () -> 0L,\n        new NullInfoStream(), null, null);\n    if (random().nextBoolean()) {\n      pool.enableReaderPooling();\n    }\n    AtomicBoolean isDone = new AtomicBoolean();\n    CountDownLatch latch = new CountDownLatch(1);\n    Thread refresher = new Thread(() -> {\n      try {\n        latch.countDown();\n        while (isDone.get() == false) {\n          for (SegmentCommitInfo commitInfo : segmentInfos) {\n            ReadersAndUpdates readersAndUpdates = pool.get(commitInfo, true);\n            SegmentReader segmentReader = readersAndUpdates.getReader(IOContext.READ);\n            readersAndUpdates.release(segmentReader);\n            pool.release(readersAndUpdates, random().nextBoolean());\n          }\n        }\n      } catch (Exception ex) {\n        throw new AssertionError(ex);\n      }\n    });\n    refresher.start();\n    MergePolicy mergePolicy = new FilterMergePolicy(newMergePolicy()) {\n      @Override\n      public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) throws IOException {\n        CodecReader reader = readerIOSupplier.get();\n        assert reader.maxDoc() > 0; // just try to access the reader\n        return true;\n      }\n    };\n    latch.await();\n    for (int i = 0; i < reader.maxDoc(); i++) {\n      for (SegmentCommitInfo commitInfo : segmentInfos) {\n        ReadersAndUpdates readersAndUpdates = pool.get(commitInfo, true);\n        SegmentReader sr = readersAndUpdates.getReadOnlyClone(IOContext.READ);\n        PostingsEnum postings = sr.postings(new Term(\"id\", \"\" + i));\n        sr.decRef();\n        if (postings != null) {\n          for (int docId = postings.nextDoc(); docId != DocIdSetIterator.NO_MORE_DOCS; docId = postings.nextDoc()) {\n            readersAndUpdates.delete(docId);\n            assertTrue(readersAndUpdates.keepFullyDeletedSegment(mergePolicy));\n          }\n        }\n        assertTrue(readersAndUpdates.keepFullyDeletedSegment(mergePolicy));\n        pool.release(readersAndUpdates, random().nextBoolean());\n      }\n    }\n    isDone.set(true);\n    refresher.join();\n    IOUtils.close(pool, reader, directory);\n  }\n\n","sourceOld":"  public void testPassReaderToMergePolicyConcurrently() throws Exception {\n    Directory directory = newDirectory();\n    FieldInfos.FieldNumbers fieldNumbers = buildIndex(directory);\n    StandardDirectoryReader reader = (StandardDirectoryReader) DirectoryReader.open(directory);\n    SegmentInfos segmentInfos = reader.segmentInfos.clone();\n    ReaderPool pool = new ReaderPool(directory, directory, segmentInfos, fieldNumbers, () -> 0L,\n        new NullInfoStream(), null, null, Collections.emptyMap());\n    if (random().nextBoolean()) {\n      pool.enableReaderPooling();\n    }\n    AtomicBoolean isDone = new AtomicBoolean();\n    CountDownLatch latch = new CountDownLatch(1);\n    Thread refresher = new Thread(() -> {\n      try {\n        latch.countDown();\n        while (isDone.get() == false) {\n          for (SegmentCommitInfo commitInfo : segmentInfos) {\n            ReadersAndUpdates readersAndUpdates = pool.get(commitInfo, true);\n            SegmentReader segmentReader = readersAndUpdates.getReader(IOContext.READ);\n            readersAndUpdates.release(segmentReader);\n            pool.release(readersAndUpdates, random().nextBoolean());\n          }\n        }\n      } catch (Exception ex) {\n        throw new AssertionError(ex);\n      }\n    });\n    refresher.start();\n    MergePolicy mergePolicy = new FilterMergePolicy(newMergePolicy()) {\n      @Override\n      public boolean keepFullyDeletedSegment(IOSupplier<CodecReader> readerIOSupplier) throws IOException {\n        CodecReader reader = readerIOSupplier.get();\n        assert reader.maxDoc() > 0; // just try to access the reader\n        return true;\n      }\n    };\n    latch.await();\n    for (int i = 0; i < reader.maxDoc(); i++) {\n      for (SegmentCommitInfo commitInfo : segmentInfos) {\n        ReadersAndUpdates readersAndUpdates = pool.get(commitInfo, true);\n        SegmentReader sr = readersAndUpdates.getReadOnlyClone(IOContext.READ);\n        PostingsEnum postings = sr.postings(new Term(\"id\", \"\" + i));\n        sr.decRef();\n        if (postings != null) {\n          for (int docId = postings.nextDoc(); docId != DocIdSetIterator.NO_MORE_DOCS; docId = postings.nextDoc()) {\n            readersAndUpdates.delete(docId);\n            assertTrue(readersAndUpdates.keepFullyDeletedSegment(mergePolicy));\n          }\n        }\n        assertTrue(readersAndUpdates.keepFullyDeletedSegment(mergePolicy));\n        pool.release(readersAndUpdates, random().nextBoolean());\n      }\n    }\n    isDone.set(true);\n    refresher.join();\n    IOUtils.close(pool, reader, directory);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5640f95cd73b5d4138e0a0988164b0fa398a3256":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["5640f95cd73b5d4138e0a0988164b0fa398a3256"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["763da4a9605e47013078edc323b9d4b608f0f9e0"]},"commit2Childs":{"5640f95cd73b5d4138e0a0988164b0fa398a3256":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5640f95cd73b5d4138e0a0988164b0fa398a3256"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}