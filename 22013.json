{"path":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","commits":[{"id":"2c007e7c4cf8c55bc2a5884e315123afaaeec87f","date":1327520966,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"/dev/null","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      CommonsHttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new CommonsHttpSolrServer(recoveryUrl);\n        server.setSoTimeout(5000);\n        server.setConnectionTimeout(5000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["91e069c492cf4895697ef7b81df0ffb9a8bd4b48"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","date":1327523564,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"/dev/null","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      CommonsHttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new CommonsHttpSolrServer(recoveryUrl);\n        server.setSoTimeout(5000);\n        server.setConnectionTimeout(5000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0d22ac6a4146774c1bc8400160fc0b6150294e92","date":1327528604,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"/dev/null","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      CommonsHttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new CommonsHttpSolrServer(recoveryUrl);\n        server.setSoTimeout(5000);\n        server.setConnectionTimeout(5000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7321b77a7bc3edfebd637ef273e9dfaa9969eba6","date":1333023097,"type":3,"author":"Sami Siren","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(5000);\n        server.setConnectionTimeout(5000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      CommonsHttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new CommonsHttpSolrServer(recoveryUrl);\n        server.setSoTimeout(5000);\n        server.setConnectionTimeout(5000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6013b4c7388f1627659c8f96c44abd10a294d3a6","date":1346343796,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode || error.e instanceof SolrException) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        // and we assume SolrException means\n        // the node went down\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(5000);\n        server.setConnectionTimeout(5000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(5000);\n        server.setConnectionTimeout(5000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05a14b2611ead08655a2b2bdc61632eb31316e57","date":1346366621,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode || error.e instanceof SolrException) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        // and we assume SolrException means\n        // the node went down\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(5000);\n        server.setConnectionTimeout(5000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(5000);\n        server.setConnectionTimeout(5000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"617053eaceb9d43aa4f14e350900c966e53d23bc","date":1356303510,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(5000);\n        server.setConnectionTimeout(5000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode || error.e instanceof SolrException) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        // and we assume SolrException means\n        // the node went down\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(5000);\n        server.setConnectionTimeout(5000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":["91e069c492cf4895697ef7b81df0ffb9a8bd4b48"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"34ea1d605e0261f0f714f9e444df24839d75161c","date":1357147436,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(15000);\n        server.setConnectionTimeout(15000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(5000);\n        server.setConnectionTimeout(5000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(15000);\n        server.setConnectionTimeout(15000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode || error.e instanceof SolrException) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        // and we assume SolrException means\n        // the node went down\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(5000);\n        server.setConnectionTimeout(5000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"91e069c492cf4895697ef7b81df0ffb9a8bd4b48","date":1382134253,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.req.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(15000);\n        server.setConnectionTimeout(15000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    Response response = cmdDistrib.getResponse();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (response.errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (response.errors.get(0).node instanceof RetryNode) {\n        rsp.setException(response.errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : response.errors) {\n      if (error.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(15000);\n        server.setConnectionTimeout(15000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","bugFix":["2c007e7c4cf8c55bc2a5884e315123afaaeec87f","617053eaceb9d43aa4f14e350900c966e53d23bc"],"bugIntro":["c0cf9c2ec975506bab465b6b2be92cb9bffc84d3"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3c3e46d3417c353d7be14509cfab11b315927fe","date":1382292560,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.req.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(60000);\n        server.setConnectionTimeout(15000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.req.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(15000);\n        server.setConnectionTimeout(15000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"677676f8b2da1378ca646add0b9862033c912d0b","date":1382308296,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (final SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      final String recoveryUrl = error.req.node.getBaseUrl();\n      \n      Thread thread = new Thread() {\n        {\n          setDaemon(true);\n        }\n        @Override\n        public void run() {\n          log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n          HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n          server.setSoTimeout(60000);\n          server.setConnectionTimeout(15000);\n          \n          RequestRecovery recoverRequestCmd = new RequestRecovery();\n          recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n          recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n          try {\n            server.request(recoverRequestCmd);\n          } catch (Throwable t) {\n            SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          }\n        }\n      };\n      ExecutorService executor = req.getCore().getCoreDescriptor().getCoreContainer().getUpdateExecutor();\n      executor.execute(thread);\n      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: could be sent in parallel\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      String recoveryUrl = error.req.node.getBaseUrl();\n      HttpSolrServer server;\n      log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n      try {\n        server = new HttpSolrServer(recoveryUrl);\n        server.setSoTimeout(60000);\n        server.setConnectionTimeout(15000);\n        \n        RequestRecovery recoverRequestCmd = new RequestRecovery();\n        recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n        recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n        \n        server.request(recoverRequestCmd);\n      } catch (Exception e) {\n        log.info(\"Could not tell a replica to recover\", e);\n      }\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2d750082a4223c20902ef11ff6a9831d55738ea4","date":1382309916,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (final SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      final String recoveryUrl = error.req.node.getBaseUrl();\n      \n      Thread thread = new Thread() {\n        {\n          setDaemon(true);\n        }\n        @Override\n        public void run() {\n          log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n          HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n          try {\n            server.setSoTimeout(60000);\n            server.setConnectionTimeout(15000);\n            \n            RequestRecovery recoverRequestCmd = new RequestRecovery();\n            recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n            recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n            try {\n              server.request(recoverRequestCmd);\n            } catch (Throwable t) {\n              SolrException.log(log, recoveryUrl\n                  + \": Could not tell a replica to recover\", t);\n            }\n          } finally {\n            server.shutdown();\n          }\n        }\n      };\n      ExecutorService executor = req.getCore().getCoreDescriptor().getCoreContainer().getUpdateExecutor();\n      executor.execute(thread);\n      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (final SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      final String recoveryUrl = error.req.node.getBaseUrl();\n      \n      Thread thread = new Thread() {\n        {\n          setDaemon(true);\n        }\n        @Override\n        public void run() {\n          log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n          HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n          server.setSoTimeout(60000);\n          server.setConnectionTimeout(15000);\n          \n          RequestRecovery recoverRequestCmd = new RequestRecovery();\n          recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n          recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n          try {\n            server.request(recoverRequestCmd);\n          } catch (Throwable t) {\n            SolrException.log(log, recoveryUrl + \": Could not tell a replica to recover\", t);\n          }\n        }\n      };\n      ExecutorService executor = req.getCore().getCoreDescriptor().getCoreContainer().getUpdateExecutor();\n      executor.execute(thread);\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":["22859cb40e09867e7da8de84a31956c07259f82f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a7035935aa89f6951286e9005cbeb16e89a082a2","date":1385258396,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (final SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      final String recoveryUrl = error.req.node.getBaseUrl();\n      \n      Thread thread = new Thread() {\n        {\n          setDaemon(true);\n        }\n        @Override\n        public void run() {\n          log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n          HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n          try {\n            server.setSoTimeout(60000);\n            server.setConnectionTimeout(15000);\n            \n            RequestRecovery recoverRequestCmd = new RequestRecovery();\n            recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n            recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n            try {\n              server.request(recoverRequestCmd);\n            } catch (Throwable t) {\n              SolrException.log(log, recoveryUrl\n                  + \": Could not tell a replica to recover\", t);\n            }\n          } finally {\n            server.shutdown();\n          }\n        }\n      };\n      ExecutorService executor = req.getCore().getCoreDescriptor().getCoreContainer().getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(thread);\n      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (final SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      final String recoveryUrl = error.req.node.getBaseUrl();\n      \n      Thread thread = new Thread() {\n        {\n          setDaemon(true);\n        }\n        @Override\n        public void run() {\n          log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n          HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n          try {\n            server.setSoTimeout(60000);\n            server.setConnectionTimeout(15000);\n            \n            RequestRecovery recoverRequestCmd = new RequestRecovery();\n            recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n            recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n            try {\n              server.request(recoverRequestCmd);\n            } catch (Throwable t) {\n              SolrException.log(log, recoveryUrl\n                  + \": Could not tell a replica to recover\", t);\n            }\n          } finally {\n            server.shutdown();\n          }\n        }\n      };\n      ExecutorService executor = req.getCore().getCoreDescriptor().getCoreContainer().getUpdateExecutor();\n      executor.execute(thread);\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (final SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      final String recoveryUrl = error.req.node.getBaseUrl();\n      \n      Thread thread = new Thread() {\n        {\n          setDaemon(true);\n        }\n        @Override\n        public void run() {\n          log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n          HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n          try {\n            server.setSoTimeout(60000);\n            server.setConnectionTimeout(15000);\n            \n            RequestRecovery recoverRequestCmd = new RequestRecovery();\n            recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n            recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n            try {\n              server.request(recoverRequestCmd);\n            } catch (Throwable t) {\n              SolrException.log(log, recoveryUrl\n                  + \": Could not tell a replica to recover\", t);\n            }\n          } finally {\n            server.shutdown();\n          }\n        }\n      };\n      ExecutorService executor = req.getCore().getCoreDescriptor().getCoreContainer().getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(thread);\n      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (final SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      final String recoveryUrl = error.req.node.getBaseUrl();\n      \n      Thread thread = new Thread() {\n        {\n          setDaemon(true);\n        }\n        @Override\n        public void run() {\n          log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n          HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n          try {\n            server.setSoTimeout(60000);\n            server.setConnectionTimeout(15000);\n            \n            RequestRecovery recoverRequestCmd = new RequestRecovery();\n            recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n            recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n            try {\n              server.request(recoverRequestCmd);\n            } catch (Throwable t) {\n              SolrException.log(log, recoveryUrl\n                  + \": Could not tell a replica to recover\", t);\n            }\n          } finally {\n            server.shutdown();\n          }\n        }\n      };\n      ExecutorService executor = req.getCore().getCoreDescriptor().getCoreContainer().getUpdateExecutor();\n      executor.execute(thread);\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3392214a5d43ed5dd25d84dbdce00681c6add436","date":1388363110,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      final String recoveryUrl = error.req.node.getBaseUrl();\n      \n      Thread thread = new Thread() {\n        {\n          setDaemon(true);\n        }\n        @Override\n        public void run() {\n          log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n          HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n          try {\n            server.setSoTimeout(60000);\n            server.setConnectionTimeout(15000);\n            \n            RequestRecovery recoverRequestCmd = new RequestRecovery();\n            recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n            recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n            try {\n              server.request(recoverRequestCmd);\n            } catch (Throwable t) {\n              SolrException.log(log, recoveryUrl\n                  + \": Could not tell a replica to recover\", t);\n            }\n          } finally {\n            server.shutdown();\n          }\n        }\n      };\n      ExecutorService executor = req.getCore().getCoreDescriptor().getCoreContainer().getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(thread);\n      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    // TODO: we should do this in the background it would seem\n    for (final SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      final String recoveryUrl = error.req.node.getBaseUrl();\n      \n      Thread thread = new Thread() {\n        {\n          setDaemon(true);\n        }\n        @Override\n        public void run() {\n          log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n          HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n          try {\n            server.setSoTimeout(60000);\n            server.setConnectionTimeout(15000);\n            \n            RequestRecovery recoverRequestCmd = new RequestRecovery();\n            recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n            recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n            try {\n              server.request(recoverRequestCmd);\n            } catch (Throwable t) {\n              SolrException.log(log, recoveryUrl\n                  + \": Could not tell a replica to recover\", t);\n            }\n          } finally {\n            server.shutdown();\n          }\n        }\n      };\n      ExecutorService executor = req.getCore().getCoreDescriptor().getCoreContainer().getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(thread);\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":["6784d0cc613dc1ee97030eaaa5e0754edc22d164"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"14d5815ecbef89580f5c48990bcd433f04f8563a","date":1399564106,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n      \n      final String replicaUrl = error.req.node.getUrl();      \n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n      \n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n        try {\n          // if false, then the node is probably not \"live\" anymore\n          sendRecoveryCommand = \n              zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                                                                  shardId, \n                                                                  replicaUrl, \n                                                                  stdNode.getNodeProps(), \n                                                                  false);\n          \n          // we want to try more than once, ~10 minutes\n          if (sendRecoveryCommand) {\n            maxTries = 120;\n          } // else the node is no longer \"live\" so no need to send any recovery command\n          \n        } catch (Exception e) {\n          log.error(\"Leader failed to set replica \"+\n              error.req.node.getUrl()+\" state to DOWN due to: \"+e, e);\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n      \n      Throwable rootCause = SolrException.getRootCause(error.e);      \n      log.error(\"Setting up to try to start recovery on replica \"+replicaUrl+\" after: \"+rootCause);\n      \n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();      \n      LeaderInitiatedRecoveryThread lirThread = \n          new LeaderInitiatedRecoveryThread(zkController,\n                                            coreContainer,\n                                            collection,\n                                            shardId,\n                                            error.req.node.getNodeProps(),\n                                            maxTries);\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);      \n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n      // TODO: we should force their state to recovering ??\n      // TODO: do retries??\n      // TODO: what if its is already recovering? Right now recoveries queue up -\n      // should they?\n      final String recoveryUrl = error.req.node.getBaseUrl();\n      \n      Thread thread = new Thread() {\n        {\n          setDaemon(true);\n        }\n        @Override\n        public void run() {\n          log.info(\"try and ask \" + recoveryUrl + \" to recover\");\n          HttpSolrServer server = new HttpSolrServer(recoveryUrl);\n          try {\n            server.setSoTimeout(60000);\n            server.setConnectionTimeout(15000);\n            \n            RequestRecovery recoverRequestCmd = new RequestRecovery();\n            recoverRequestCmd.setAction(CoreAdminAction.REQUESTRECOVERY);\n            recoverRequestCmd.setCoreName(error.req.node.getCoreName());\n            try {\n              server.request(recoverRequestCmd);\n            } catch (Throwable t) {\n              SolrException.log(log, recoveryUrl\n                  + \": Could not tell a replica to recover\", t);\n            }\n          } finally {\n            server.shutdown();\n          }\n        }\n      };\n      ExecutorService executor = req.getCore().getCoreDescriptor().getCoreContainer().getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(thread);\n      \n    }\n  }\n\n","bugFix":null,"bugIntro":["22859cb40e09867e7da8de84a31956c07259f82f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d22d0f5940f155267b250876fa797ff69e8e6e7","date":1400540695,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n      \n      final String replicaUrl = error.req.node.getUrl();      \n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n      \n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n        try {\n          // if false, then the node is probably not \"live\" anymore\n          sendRecoveryCommand = \n              zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                                                                  shardId, \n                                                                  replicaUrl, \n                                                                  stdNode.getNodeProps(), \n                                                                  false);\n          \n          // we want to try more than once, ~10 minutes\n          if (sendRecoveryCommand) {\n            maxTries = 120;\n          } // else the node is no longer \"live\" so no need to send any recovery command\n          \n        } catch (Exception e) {\n          log.error(\"Leader failed to set replica \"+\n              error.req.node.getUrl()+\" state to DOWN due to: \"+e, e);\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n      \n      Throwable rootCause = SolrException.getRootCause(error.e);      \n      log.error(\"Setting up to try to start recovery on replica \"+replicaUrl+\" after: \"+rootCause);\n      \n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();      \n      LeaderInitiatedRecoveryThread lirThread = \n          new LeaderInitiatedRecoveryThread(zkController,\n                                            coreContainer,\n                                            collection,\n                                            shardId,\n                                            error.req.node.getNodeProps(),\n                                            maxTries);\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);      \n    }\n    \n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n      \n      final String replicaUrl = error.req.node.getUrl();      \n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n      \n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n        try {\n          // if false, then the node is probably not \"live\" anymore\n          sendRecoveryCommand = \n              zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                                                                  shardId, \n                                                                  replicaUrl, \n                                                                  stdNode.getNodeProps(), \n                                                                  false);\n          \n          // we want to try more than once, ~10 minutes\n          if (sendRecoveryCommand) {\n            maxTries = 120;\n          } // else the node is no longer \"live\" so no need to send any recovery command\n          \n        } catch (Exception e) {\n          log.error(\"Leader failed to set replica \"+\n              error.req.node.getUrl()+\" state to DOWN due to: \"+e, e);\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n      \n      Throwable rootCause = SolrException.getRootCause(error.e);      \n      log.error(\"Setting up to try to start recovery on replica \"+replicaUrl+\" after: \"+rootCause);\n      \n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();      \n      LeaderInitiatedRecoveryThread lirThread = \n          new LeaderInitiatedRecoveryThread(zkController,\n                                            coreContainer,\n                                            collection,\n                                            shardId,\n                                            error.req.node.getNodeProps(),\n                                            maxTries);\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);      \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n      \n      final String replicaUrl = error.req.node.getUrl();      \n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n      \n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n        try {\n          // if false, then the node is probably not \"live\" anymore\n          sendRecoveryCommand = \n              zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                                                                  shardId, \n                                                                  replicaUrl, \n                                                                  stdNode.getNodeProps(), \n                                                                  false);\n          \n          // we want to try more than once, ~10 minutes\n          if (sendRecoveryCommand) {\n            maxTries = 120;\n          } // else the node is no longer \"live\" so no need to send any recovery command\n          \n        } catch (Exception e) {\n          log.error(\"Leader failed to set replica \"+\n              error.req.node.getUrl()+\" state to DOWN due to: \"+e, e);\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n      \n      Throwable rootCause = SolrException.getRootCause(error.e);      \n      log.error(\"Setting up to try to start recovery on replica \"+replicaUrl+\" after: \"+rootCause);\n      \n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();      \n      LeaderInitiatedRecoveryThread lirThread = \n          new LeaderInitiatedRecoveryThread(zkController,\n                                            coreContainer,\n                                            collection,\n                                            shardId,\n                                            error.req.node.getNodeProps(),\n                                            maxTries);\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);      \n    }\n    \n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n      \n      final String replicaUrl = error.req.node.getUrl();      \n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n      \n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n        try {\n          // if false, then the node is probably not \"live\" anymore\n          sendRecoveryCommand = \n              zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                                                                  shardId, \n                                                                  replicaUrl, \n                                                                  stdNode.getNodeProps(), \n                                                                  false);\n          \n          // we want to try more than once, ~10 minutes\n          if (sendRecoveryCommand) {\n            maxTries = 120;\n          } // else the node is no longer \"live\" so no need to send any recovery command\n          \n        } catch (Exception e) {\n          log.error(\"Leader failed to set replica \"+\n              error.req.node.getUrl()+\" state to DOWN due to: \"+e, e);\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n      \n      Throwable rootCause = SolrException.getRootCause(error.e);      \n      log.error(\"Setting up to try to start recovery on replica \"+replicaUrl+\" after: \"+rootCause);\n      \n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();      \n      LeaderInitiatedRecoveryThread lirThread = \n          new LeaderInitiatedRecoveryThread(zkController,\n                                            coreContainer,\n                                            collection,\n                                            shardId,\n                                            error.req.node.getNodeProps(),\n                                            maxTries);\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);      \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6e36353d7461af8d2329a78a71457cf8e3c1e88f","date":1411572107,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName)) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n\n          } catch (KeeperException.SessionExpiredException see) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + see, see);\n            // our session is expired, which means our state is suspect, so don't go\n            // putting other replicas in recovery (see SOLR-6511)\n            sendRecoveryCommand = false;\n          } catch (Exception e) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + e, e);\n            // will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe?\n          sendRecoveryCommand = false;\n          log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n              shardId+\", no request recovery command will be sent!\");\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n      \n      final String replicaUrl = error.req.node.getUrl();      \n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n      \n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n        try {\n          // if false, then the node is probably not \"live\" anymore\n          sendRecoveryCommand = \n              zkController.ensureReplicaInLeaderInitiatedRecovery(collection, \n                                                                  shardId, \n                                                                  replicaUrl, \n                                                                  stdNode.getNodeProps(), \n                                                                  false);\n          \n          // we want to try more than once, ~10 minutes\n          if (sendRecoveryCommand) {\n            maxTries = 120;\n          } // else the node is no longer \"live\" so no need to send any recovery command\n          \n        } catch (Exception e) {\n          log.error(\"Leader failed to set replica \"+\n              error.req.node.getUrl()+\" state to DOWN due to: \"+e, e);\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n      \n      Throwable rootCause = SolrException.getRootCause(error.e);      \n      log.error(\"Setting up to try to start recovery on replica \"+replicaUrl+\" after: \"+rootCause);\n      \n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();      \n      LeaderInitiatedRecoveryThread lirThread = \n          new LeaderInitiatedRecoveryThread(zkController,\n                                            coreContainer,\n                                            collection,\n                                            shardId,\n                                            error.req.node.getNodeProps(),\n                                            maxTries);\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);      \n    }\n    \n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":null,"bugIntro":["dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc","22859cb40e09867e7da8de84a31956c07259f82f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b76fad1f8492334e597826072e178dab0b21e02b","date":1412249476,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        for (ZkCoreNodeProps replicaProp : myReplicas) {\n          if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n            foundErrorNodeInReplicaList = true;\n            break;\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n\n          } catch (KeeperException.SessionExpiredException see) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + see, see);\n            // our session is expired, which means our state is suspect, so don't go\n            // putting other replicas in recovery (see SOLR-6511)\n            sendRecoveryCommand = false;\n          } catch (Exception e) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + e, e);\n            // will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName)) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n\n          } catch (KeeperException.SessionExpiredException see) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + see, see);\n            // our session is expired, which means our state is suspect, so don't go\n            // putting other replicas in recovery (see SOLR-6511)\n            sendRecoveryCommand = false;\n          } catch (Exception e) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + e, e);\n            // will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe?\n          sendRecoveryCommand = false;\n          log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n              shardId+\", no request recovery command will be sent!\");\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":null,"bugIntro":["6dd5104ff91bf9fe2184d702f43f11e400c8d3a9","7f6d2237ba87daf0df9c6e7bd692d5262b447790"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d9a47902d6207303f5ed3e7aaca62ca33433af66","date":1412435312,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        for (ZkCoreNodeProps replicaProp : myReplicas) {\n          if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n            foundErrorNodeInReplicaList = true;\n            break;\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n\n          } catch (KeeperException.SessionExpiredException see) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + see, see);\n            // our session is expired, which means our state is suspect, so don't go\n            // putting other replicas in recovery (see SOLR-6511)\n            sendRecoveryCommand = false;\n          } catch (Exception e) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + e, e);\n            // will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName)) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n\n          } catch (KeeperException.SessionExpiredException see) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + see, see);\n            // our session is expired, which means our state is suspect, so don't go\n            // putting other replicas in recovery (see SOLR-6511)\n            sendRecoveryCommand = false;\n          } catch (Exception e) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + e, e);\n            // will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe?\n          sendRecoveryCommand = false;\n          log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n              shardId+\", no request recovery command will be sent!\");\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"28b6c6f2489e2262f68c2bb555908565774437a6","date":1412617849,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        for (ZkCoreNodeProps replicaProp : myReplicas) {\n          if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n            foundErrorNodeInReplicaList = true;\n            break;\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        for (ZkCoreNodeProps replicaProp : myReplicas) {\n          if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n            foundErrorNodeInReplicaList = true;\n            break;\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n\n          } catch (KeeperException.SessionExpiredException see) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + see, see);\n            // our session is expired, which means our state is suspect, so don't go\n            // putting other replicas in recovery (see SOLR-6511)\n            sendRecoveryCommand = false;\n          } catch (Exception e) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + e, e);\n            // will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":null,"bugIntro":["22859cb40e09867e7da8de84a31956c07259f82f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"55980207f1977bd1463465de1659b821347e2fa8","date":1413336386,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        for (ZkCoreNodeProps replicaProp : myReplicas) {\n          if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n            foundErrorNodeInReplicaList = true;\n            break;\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        for (ZkCoreNodeProps replicaProp : myReplicas) {\n          if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n            foundErrorNodeInReplicaList = true;\n            break;\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n\n          } catch (KeeperException.SessionExpiredException see) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + see, see);\n            // our session is expired, which means our state is suspect, so don't go\n            // putting other replicas in recovery (see SOLR-6511)\n            sendRecoveryCommand = false;\n          } catch (Exception e) {\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + e, e);\n            // will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7f6d2237ba87daf0df9c6e7bd692d5262b447790","date":1413871818,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        for (ZkCoreNodeProps replicaProp : myReplicas) {\n          if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n            foundErrorNodeInReplicaList = true;\n            break;\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":["b76fad1f8492334e597826072e178dab0b21e02b"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0a22eafe3f72a4c2945eaad9547e6c78816978f4","date":1413956657,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        for (ZkCoreNodeProps replicaProp : myReplicas) {\n          if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n            foundErrorNodeInReplicaList = true;\n            break;\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","date":1419400138,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if its a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6784d0cc613dc1ee97030eaaa5e0754edc22d164","date":1420824784,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update\", error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":["3392214a5d43ed5dd25d84dbdce00681c6add436"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc","date":1426444850,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    stdNode.getNodeProps(),\n                    false,\n                    leaderCoreNodeName);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":["6e36353d7461af8d2329a78a71457cf8e3c1e88f"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0932eb10135843758b2ca508d5aa2b4798aa07f9","date":1426947197,"type":3,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        try {\n          Replica leader = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leader != null) {\n            leaderCoreNodeName = leader.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    stdNode.getNodeProps(),\n                    leaderCoreNodeName,\n                    false /* forcePublishState */,\n                    false /* retryOnConnLoss */\n                );\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    stdNode.getNodeProps(),\n                    false,\n                    leaderCoreNodeName);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":null,"bugIntro":["6dd5104ff91bf9fe2184d702f43f11e400c8d3a9","22859cb40e09867e7da8de84a31956c07259f82f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        try {\n          Replica leader = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leader != null) {\n            leaderCoreNodeName = leader.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    stdNode.getNodeProps(),\n                    leaderCoreNodeName,\n                    false /* forcePublishState */,\n                    false /* retryOnConnLoss */\n                );\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        try {\n          leaderCoreNodeName = zkController.getZkStateReader().getLeaderRetry(collection, shardId).getName();\n        } catch (Exception exc) {\n          log.error(\"Failed to determine if \" + cloudDesc.getCoreNodeName() + \" is still the leader for \" + collection +\n              \" \" + shardId + \" before putting \" + replicaUrl + \" into leader-initiated recovery due to: \" + exc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    replicaUrl,\n                    stdNode.getNodeProps(),\n                    false);\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica \" + replicaUrl + \" after: \" + rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6a96b173f2f573ac82bc8279dfdd3cbe2b948f95","date":1429031018,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        try {\n          Replica leader = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leader != null) {\n            leaderCoreNodeName = leader.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    stdNode.getNodeProps(),\n                    leaderCoreNodeName,\n                    false /* forcePublishState */,\n                    false /* retryOnConnLoss */\n                );\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      try {\n        MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", error.req.node.getNodeProps().getCoreUrl());\n        executor.execute(lirThread);\n      } finally {\n        MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        try {\n          Replica leader = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leader != null) {\n            leaderCoreNodeName = leader.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    stdNode.getNodeProps(),\n                    leaderCoreNodeName,\n                    false /* forcePublishState */,\n                    false /* retryOnConnLoss */\n                );\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      executor.execute(lirThread);\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":null,"bugIntro":["22859cb40e09867e7da8de84a31956c07259f82f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"22859cb40e09867e7da8de84a31956c07259f82f","date":1441822065,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        try {\n          Replica leader = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leader != null) {\n            leaderCoreNodeName = leader.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreDescriptor().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                leaderCoreNodeName,\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      int maxTries = 1;       \n      boolean sendRecoveryCommand = true;\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        try {\n          Replica leader = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leader != null) {\n            leaderCoreNodeName = leader.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            sendRecoveryCommand =\n                zkController.ensureReplicaInLeaderInitiatedRecovery(collection,\n                    shardId,\n                    stdNode.getNodeProps(),\n                    leaderCoreNodeName,\n                    false /* forcePublishState */,\n                    false /* retryOnConnLoss */\n                );\n\n            // we want to try more than once, ~10 minutes\n            if (sendRecoveryCommand) {\n              maxTries = 120;\n            } // else the node is no longer \"live\" so no need to send any recovery command\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n            if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException ||\n                setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException) {\n              // our session is expired, which means our state is suspect, so don't go\n              // putting other replicas in recovery (see SOLR-6511)\n              sendRecoveryCommand = false;\n            } // else will go ahead and try to send the recovery command once after this error\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          sendRecoveryCommand = false;\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      } // else not a StdNode, recovery command still gets sent once\n            \n      if (!sendRecoveryCommand)\n        continue; // the replica is already in recovery handling or is not live   \n\n      Throwable rootCause = SolrException.getRootCause(error.e);\n      log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n\n      // try to send the recovery command to the downed replica in a background thread\n      CoreContainer coreContainer = req.getCore().getCoreDescriptor().getCoreContainer();\n      LeaderInitiatedRecoveryThread lirThread =\n          new LeaderInitiatedRecoveryThread(zkController,\n              coreContainer,\n              collection,\n              shardId,\n              error.req.node.getNodeProps(),\n              maxTries,\n              cloudDesc.getCoreNodeName()); // core node name of current leader\n      ExecutorService executor = coreContainer.getUpdateShardHandler().getUpdateExecutor();\n      try {\n        MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", error.req.node.getNodeProps().getCoreUrl());\n        executor.execute(lirThread);\n      } finally {\n        MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":["2d750082a4223c20902ef11ff6a9831d55738ea4","14d5815ecbef89580f5c48990bcd433f04f8563a","6a96b173f2f573ac82bc8279dfdd3cbe2b948f95","6e36353d7461af8d2329a78a71457cf8e3c1e88f","28b6c6f2489e2262f68c2bb555908565774437a6","0932eb10135843758b2ca508d5aa2b4798aa07f9"],"bugIntro":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"120dcb9902dc31423bf7d82c10c5439b88325390","date":1442349554,"type":3,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        try {\n          Replica leader = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leader != null) {\n            leaderCoreNodeName = leader.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (replicationTracker != null && replicationTracker.getAchievedRf() < replicationTracker.minRf) {\n          continue;\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreDescriptor().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                leaderCoreNodeName,\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        try {\n          Replica leader = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leader != null) {\n            leaderCoreNodeName = leader.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreDescriptor().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                leaderCoreNodeName,\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3e0300d1df37d7e9662d491269e91b6f66dca8bd","date":1443011762,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        try {\n          Replica leader = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leader != null) {\n            leaderCoreNodeName = leader.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (replicationTracker != null && replicationTracker.getAchievedRf() < replicationTracker.minRf) {\n          continue;\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreDescriptor().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                req.getCore().getCoreDescriptor(),\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        try {\n          Replica leader = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leader != null) {\n            leaderCoreNodeName = leader.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (replicationTracker != null && replicationTracker.getAchievedRf() < replicationTracker.minRf) {\n          continue;\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreDescriptor().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                leaderCoreNodeName,\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":["22859cb40e09867e7da8de84a31956c07259f82f"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6dd5104ff91bf9fe2184d702f43f11e400c8d3a9","date":1447452434,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (replicationTracker != null && replicationTracker.getAchievedRf() < replicationTracker.minRf) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreDescriptor().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                req.getCore().getCoreDescriptor(),\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        try {\n          Replica leader = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leader != null) {\n            leaderCoreNodeName = leader.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (replicationTracker != null && replicationTracker.getAchievedRf() < replicationTracker.minRf) {\n          continue;\n        }\n\n        if (cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) && foundErrorNodeInReplicaList) {\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreDescriptor().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                req.getCore().getCoreDescriptor(),\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else  {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" is no longer the leader for \"+collection+\" \"+\n                shardId+\", no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":["b76fad1f8492334e597826072e178dab0b21e02b","0932eb10135843758b2ca508d5aa2b4798aa07f9"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f57cf082c4d2ee975c6a2034fcf3c13f9514e6ef","date":1458928975,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    \n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (replicationTracker != null && replicationTracker.getAchievedRf() < replicationTracker.minRf) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreDescriptor().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                req.getCore().getCoreDescriptor(),\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }\n\n    \n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n    \n    // if it's a forward, any fail is a problem - \n    // otherwise we assume things are fine if we got it locally\n    // until we start allowing min replication param\n    if (errors.size() > 0) {\n      // if one node is a RetryNode, this was a forward request\n      if (errors.get(0).req.node instanceof RetryNode) {\n        rsp.setException(errors.get(0).e);\n      } else {\n        if (log.isWarnEnabled()) {\n          for (Error error : errors) {\n            log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n          }\n        }\n      }\n      // else\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n    }\n   \n    \n    // if it is not a forward request, for each fail, try to tell them to\n    // recover - the doc was already added locally, so it should have been\n    // legit\n\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // we don't try to force a leader to recover\n        // when we cannot forward to it\n        continue;\n      }\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        rsp.setException(error.e);\n        break;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (replicationTracker != null && replicationTracker.getAchievedRf() < replicationTracker.minRf) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreDescriptor().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                req.getCore().getCoreDescriptor(),\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"729cb470f975115d4c60517b2cb7c42e37a7a2e1","date":1492041760,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    \n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (replicationTracker != null && replicationTracker.getAchievedRf() < replicationTracker.minRf) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                req.getCore().getCoreDescriptor(),\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }\n\n    \n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    \n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (replicationTracker != null && replicationTracker.getAchievedRf() < replicationTracker.minRf) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreDescriptor().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                req.getCore().getCoreDescriptor(),\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }\n\n    \n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54ca69905c5d9d1529286f06ab1d12c68f6c13cb","date":1492683554,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    \n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (replicationTracker != null && replicationTracker.getAchievedRf() < replicationTracker.minRf) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                req.getCore().getCoreDescriptor(),\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }\n\n    \n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    \n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (replicationTracker != null && replicationTracker.getAchievedRf() < replicationTracker.minRf) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreDescriptor().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                req.getCore().getCoreDescriptor(),\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }\n\n    \n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"615bf5b56d95ed8c9bf06a402db6c817d6bff21a","date":1509492118,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    \n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                req.getCore().getCoreDescriptor(),\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread    \n    \n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    \n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n       \n      DistribPhase phase =\n          DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));       \n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (replicationTracker != null && replicationTracker.getAchievedRf() < replicationTracker.minRf) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                req.getCore().getCoreDescriptor(),\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n\n    if (replicationTracker != null) {\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, replicationTracker.getAchievedRf());\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, replicationTracker.minRf);\n      replicationTracker = null;\n    }\n\n    \n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"84f20f331d8001864545c7021812d8c6509c7593","date":1517216128,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Map<ShardInfo, Set<String>> failedReplicas = new HashMap<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              ShardInfo shardInfo = new ShardInfo(collection, shardId, leaderCoreNodeName);\n              failedReplicas.putIfAbsent(shardInfo, new HashSet<>());\n              failedReplicas.get(shardInfo).add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode) {\n      for (Map.Entry<ShardInfo, Set<String>> entry : failedReplicas.entrySet()) {\n        ShardInfo shardInfo = entry.getKey();\n        zkController.getShardTerms(shardInfo.collection, shardInfo.shard).ensureTermsIsHigher(shardInfo.leader, entry.getValue());\n      }\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    \n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n            zkController.ensureReplicaInLeaderInitiatedRecovery(\n                req.getCore().getCoreContainer(),\n                collection,\n                shardId,\n                stdNode.getNodeProps(),\n                req.getCore().getCoreDescriptor(),\n                false /* forcePublishState */\n            );\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b82b38f6df64b40b9ae76575b234a8ddd4238cd4","date":1519975758,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Map<ShardInfo, Set<String>> failedReplicas = new HashMap<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              ShardInfo shardInfo = new ShardInfo(collection, shardId, leaderCoreNodeName);\n              failedReplicas.putIfAbsent(shardInfo, new HashSet<>());\n              failedReplicas.get(shardInfo).add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            // some replicas did not receive the updates, exception must be notified to clients\n            errorsForClient.add(error);\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode) {\n      for (Map.Entry<ShardInfo, Set<String>> entry : failedReplicas.entrySet()) {\n        ShardInfo shardInfo = entry.getKey();\n        zkController.getShardTerms(shardInfo.collection, shardInfo.shard).ensureTermsIsHigher(shardInfo.leader, entry.getValue());\n      }\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Map<ShardInfo, Set<String>> failedReplicas = new HashMap<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              ShardInfo shardInfo = new ShardInfo(collection, shardId, leaderCoreNodeName);\n              failedReplicas.putIfAbsent(shardInfo, new HashSet<>());\n              failedReplicas.get(shardInfo).add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode) {\n      for (Map.Entry<ShardInfo, Set<String>> entry : failedReplicas.entrySet()) {\n        ShardInfo shardInfo = entry.getKey();\n        zkController.getShardTerms(shardInfo.collection, shardInfo.shard).ensureTermsIsHigher(shardInfo.leader, entry.getValue());\n      }\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8c30d22eaf1287a88a402fba9d8b7b9d20d6ef94","date":1520143025,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && !isOldLIRMode && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              replicasShouldBeInLowerTerms.add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            // some replicas did not receive the updates, exception must be notified to clients\n            errorsForClient.add(error);\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode && !replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Map<ShardInfo, Set<String>> failedReplicas = new HashMap<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              ShardInfo shardInfo = new ShardInfo(collection, shardId, leaderCoreNodeName);\n              failedReplicas.putIfAbsent(shardInfo, new HashSet<>());\n              failedReplicas.get(shardInfo).add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            // some replicas did not receive the updates, exception must be notified to clients\n            errorsForClient.add(error);\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode) {\n      for (Map.Entry<ShardInfo, Set<String>> entry : failedReplicas.entrySet()) {\n        ShardInfo shardInfo = entry.getKey();\n        zkController.getShardTerms(shardInfo.collection, shardInfo.shard).ensureTermsIsHigher(shardInfo.leader, entry.getValue());\n      }\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"da4a8b166c17d13e0b123fa57a4efbc66d442f73","date":1520566499,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && !isOldLIRMode && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              replicasShouldBeInLowerTerms.add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode && !replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && !isOldLIRMode && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              replicasShouldBeInLowerTerms.add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            // some replicas did not receive the updates, exception must be notified to clients\n            errorsForClient.add(error);\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode && !replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2ffc8d70d9f57a62a24c3dd15b66e353de935054","date":1533178472,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && !isOldLIRMode && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              replicasShouldBeInLowerTerms.add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                cloudDesc.getShardId()+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode && !replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && !isOldLIRMode && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              replicasShouldBeInLowerTerms.add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                shardId+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode && !replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c0cf9c2ec975506bab465b6b2be92cb9bffc84d3","date":1533596209,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && !isOldLIRMode && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof ForwardNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              replicasShouldBeInLowerTerms.add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                cloudDesc.getShardId()+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode && !replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && !isOldLIRMode && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof RetryNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              replicasShouldBeInLowerTerms.add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                cloudDesc.getShardId()+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode && !replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":["91e069c492cf4895697ef7b81df0ffb9a8bd4b48"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"43f5f8344e80b4bfb2069917069430266753d2f0","date":1538584815,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && !isOldLIRMode && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof ForwardNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              replicasShouldBeInLowerTerms.add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                cloudDesc.getShardId()+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode && !replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    handleReplicationFactor();\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && !isOldLIRMode && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof ForwardNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        // If the client specified minRf and we didn't achieve the minRf, don't send recovery and let client retry\n        if (leaderReplicationTracker != null &&\n            leaderReplicationTracker.getAchievedRf() < leaderReplicationTracker.getRequestedRf()) {\n          continue;\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              replicasShouldBeInLowerTerms.add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                cloudDesc.getShardId()+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode && !replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    // in either case, we need to attach the achieved and min rf to the response.\n    if (leaderReplicationTracker != null || rollupReplicationTracker != null) {\n      int achievedRf = Integer.MAX_VALUE;\n      int requestedRf = Integer.MAX_VALUE;\n\n      if (leaderReplicationTracker != null) {\n\n        achievedRf = leaderReplicationTracker.getAchievedRf();\n        requestedRf = leaderReplicationTracker.getRequestedRf();\n\n        // Transfer this to the rollup tracker if it exists\n        if (rollupReplicationTracker != null) {\n          rollupReplicationTracker.testAndSetAchievedRf(achievedRf);\n        }\n      }\n\n      // Rollup tracker has accumulated stats.\n      if (rollupReplicationTracker != null) {\n        achievedRf = rollupReplicationTracker.getAchievedRf();\n        requestedRf = rollupReplicationTracker.getRequestedRf();\n      }\n      rsp.getResponseHeader().add(UpdateRequest.MIN_REPFACT, requestedRf);\n      rsp.getResponseHeader().add(UpdateRequest.REPFACT, achievedRf);\n      rollupReplicationTracker = null;\n      leaderReplicationTracker = null;\n\n    }\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180","date":1539076849,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof ForwardNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n            replicasShouldBeInLowerTerms.add(coreNodeName);\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                cloudDesc.getShardId()+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    handleReplicationFactor();\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && !isOldLIRMode && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof ForwardNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            if (!isOldLIRMode && zkController.getShardTerms(collection, shardId).registered(coreNodeName)) {\n              log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n              replicasShouldBeInLowerTerms.add(coreNodeName);\n            } else {\n              // The replica did not registered its term, so it must run with old LIR implementation\n              log.error(\"Setting up to try to start recovery on replica {}\", replicaUrl, rootCause);\n              zkController.ensureReplicaInLeaderInitiatedRecovery(\n                  req.getCore().getCoreContainer(),\n                  collection,\n                  shardId,\n                  stdNode.getNodeProps(),\n                  req.getCore().getCoreDescriptor(),\n                  false /* forcePublishState */\n              );\n            }\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                cloudDesc.getShardId()+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!isOldLIRMode && !replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    handleReplicationFactor();\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9d70e774cb25c8a8d2c3e5e84200f235f9168d87","date":1553016391,"type":5,"author":"Bar Rotstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedZkUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<SolrCmdDistributor.Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<SolrCmdDistributor.Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n\n      if (error.req.node instanceof SolrCmdDistributor.ForwardNode) {\n        // if it's a forward, any fail is a problem -\n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded\n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n\n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof SolrCmdDistributor.StdNode) {\n        SolrCmdDistributor.StdNode stdNode = (SolrCmdDistributor.StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n          leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n            replicasShouldBeInLowerTerms.add(coreNodeName);\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                cloudDesc.getShardId()+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    handleReplicationFactor();\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof ForwardNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n            replicasShouldBeInLowerTerms.add(coreNodeName);\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                cloudDesc.getShardId()+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    handleReplicationFactor();\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6a96b173f2f573ac82bc8279dfdd3cbe2b948f95":["0932eb10135843758b2ca508d5aa2b4798aa07f9"],"617053eaceb9d43aa4f14e350900c966e53d23bc":["6013b4c7388f1627659c8f96c44abd10a294d3a6"],"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180":["43f5f8344e80b4bfb2069917069430266753d2f0"],"8c30d22eaf1287a88a402fba9d8b7b9d20d6ef94":["b82b38f6df64b40b9ae76575b234a8ddd4238cd4"],"22859cb40e09867e7da8de84a31956c07259f82f":["6a96b173f2f573ac82bc8279dfdd3cbe2b948f95"],"3392214a5d43ed5dd25d84dbdce00681c6add436":["a7035935aa89f6951286e9005cbeb16e89a082a2"],"14d5815ecbef89580f5c48990bcd433f04f8563a":["3392214a5d43ed5dd25d84dbdce00681c6add436"],"6784d0cc613dc1ee97030eaaa5e0754edc22d164":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"6e36353d7461af8d2329a78a71457cf8e3c1e88f":["9d22d0f5940f155267b250876fa797ff69e8e6e7"],"3e0300d1df37d7e9662d491269e91b6f66dca8bd":["120dcb9902dc31423bf7d82c10c5439b88325390"],"7321b77a7bc3edfebd637ef273e9dfaa9969eba6":["2c007e7c4cf8c55bc2a5884e315123afaaeec87f"],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2c007e7c4cf8c55bc2a5884e315123afaaeec87f"],"d9a47902d6207303f5ed3e7aaca62ca33433af66":["6e36353d7461af8d2329a78a71457cf8e3c1e88f","b76fad1f8492334e597826072e178dab0b21e02b"],"05a14b2611ead08655a2b2bdc61632eb31316e57":["7321b77a7bc3edfebd637ef273e9dfaa9969eba6","6013b4c7388f1627659c8f96c44abd10a294d3a6"],"729cb470f975115d4c60517b2cb7c42e37a7a2e1":["f57cf082c4d2ee975c6a2034fcf3c13f9514e6ef"],"7f6d2237ba87daf0df9c6e7bd692d5262b447790":["28b6c6f2489e2262f68c2bb555908565774437a6"],"9d22d0f5940f155267b250876fa797ff69e8e6e7":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"28b6c6f2489e2262f68c2bb555908565774437a6":["b76fad1f8492334e597826072e178dab0b21e02b"],"2d750082a4223c20902ef11ff6a9831d55738ea4":["677676f8b2da1378ca646add0b9862033c912d0b"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["2d750082a4223c20902ef11ff6a9831d55738ea4","a7035935aa89f6951286e9005cbeb16e89a082a2"],"677676f8b2da1378ca646add0b9862033c912d0b":["a3c3e46d3417c353d7be14509cfab11b315927fe"],"da4a8b166c17d13e0b123fa57a4efbc66d442f73":["8c30d22eaf1287a88a402fba9d8b7b9d20d6ef94"],"9d70e774cb25c8a8d2c3e5e84200f235f9168d87":["b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180"],"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc":["6784d0cc613dc1ee97030eaaa5e0754edc22d164"],"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":["f57cf082c4d2ee975c6a2034fcf3c13f9514e6ef"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["6013b4c7388f1627659c8f96c44abd10a294d3a6","34ea1d605e0261f0f714f9e444df24839d75161c"],"b76fad1f8492334e597826072e178dab0b21e02b":["6e36353d7461af8d2329a78a71457cf8e3c1e88f"],"43f5f8344e80b4bfb2069917069430266753d2f0":["c0cf9c2ec975506bab465b6b2be92cb9bffc84d3"],"91e069c492cf4895697ef7b81df0ffb9a8bd4b48":["34ea1d605e0261f0f714f9e444df24839d75161c"],"84f20f331d8001864545c7021812d8c6509c7593":["615bf5b56d95ed8c9bf06a402db6c817d6bff21a"],"b7605579001505896d48b07160075a5c8b8e128e":["14d5815ecbef89580f5c48990bcd433f04f8563a","9d22d0f5940f155267b250876fa797ff69e8e6e7"],"c0cf9c2ec975506bab465b6b2be92cb9bffc84d3":["2ffc8d70d9f57a62a24c3dd15b66e353de935054"],"6013b4c7388f1627659c8f96c44abd10a294d3a6":["7321b77a7bc3edfebd637ef273e9dfaa9969eba6"],"6dd5104ff91bf9fe2184d702f43f11e400c8d3a9":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["6784d0cc613dc1ee97030eaaa5e0754edc22d164","0932eb10135843758b2ca508d5aa2b4798aa07f9"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["7f6d2237ba87daf0df9c6e7bd692d5262b447790"],"0d22ac6a4146774c1bc8400160fc0b6150294e92":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2c007e7c4cf8c55bc2a5884e315123afaaeec87f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"120dcb9902dc31423bf7d82c10c5439b88325390":["22859cb40e09867e7da8de84a31956c07259f82f"],"0932eb10135843758b2ca508d5aa2b4798aa07f9":["dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc"],"a7035935aa89f6951286e9005cbeb16e89a082a2":["2d750082a4223c20902ef11ff6a9831d55738ea4"],"b82b38f6df64b40b9ae76575b234a8ddd4238cd4":["84f20f331d8001864545c7021812d8c6509c7593"],"55980207f1977bd1463465de1659b821347e2fa8":["d9a47902d6207303f5ed3e7aaca62ca33433af66","28b6c6f2489e2262f68c2bb555908565774437a6"],"34ea1d605e0261f0f714f9e444df24839d75161c":["617053eaceb9d43aa4f14e350900c966e53d23bc"],"f57cf082c4d2ee975c6a2034fcf3c13f9514e6ef":["6dd5104ff91bf9fe2184d702f43f11e400c8d3a9"],"a3c3e46d3417c353d7be14509cfab11b315927fe":["91e069c492cf4895697ef7b81df0ffb9a8bd4b48"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":["55980207f1977bd1463465de1659b821347e2fa8","7f6d2237ba87daf0df9c6e7bd692d5262b447790"],"2c007e7c4cf8c55bc2a5884e315123afaaeec87f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"615bf5b56d95ed8c9bf06a402db6c817d6bff21a":["729cb470f975115d4c60517b2cb7c42e37a7a2e1"],"2ffc8d70d9f57a62a24c3dd15b66e353de935054":["da4a8b166c17d13e0b123fa57a4efbc66d442f73"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9d70e774cb25c8a8d2c3e5e84200f235f9168d87"]},"commit2Childs":{"6a96b173f2f573ac82bc8279dfdd3cbe2b948f95":["22859cb40e09867e7da8de84a31956c07259f82f"],"617053eaceb9d43aa4f14e350900c966e53d23bc":["34ea1d605e0261f0f714f9e444df24839d75161c"],"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180":["9d70e774cb25c8a8d2c3e5e84200f235f9168d87"],"8c30d22eaf1287a88a402fba9d8b7b9d20d6ef94":["da4a8b166c17d13e0b123fa57a4efbc66d442f73"],"22859cb40e09867e7da8de84a31956c07259f82f":["120dcb9902dc31423bf7d82c10c5439b88325390"],"3392214a5d43ed5dd25d84dbdce00681c6add436":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"14d5815ecbef89580f5c48990bcd433f04f8563a":["9d22d0f5940f155267b250876fa797ff69e8e6e7","b7605579001505896d48b07160075a5c8b8e128e"],"6784d0cc613dc1ee97030eaaa5e0754edc22d164":["dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"6e36353d7461af8d2329a78a71457cf8e3c1e88f":["d9a47902d6207303f5ed3e7aaca62ca33433af66","b76fad1f8492334e597826072e178dab0b21e02b"],"3e0300d1df37d7e9662d491269e91b6f66dca8bd":["6dd5104ff91bf9fe2184d702f43f11e400c8d3a9"],"7321b77a7bc3edfebd637ef273e9dfaa9969eba6":["05a14b2611ead08655a2b2bdc61632eb31316e57","6013b4c7388f1627659c8f96c44abd10a294d3a6"],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":[],"d9a47902d6207303f5ed3e7aaca62ca33433af66":["55980207f1977bd1463465de1659b821347e2fa8"],"05a14b2611ead08655a2b2bdc61632eb31316e57":[],"729cb470f975115d4c60517b2cb7c42e37a7a2e1":["615bf5b56d95ed8c9bf06a402db6c817d6bff21a"],"9d22d0f5940f155267b250876fa797ff69e8e6e7":["6e36353d7461af8d2329a78a71457cf8e3c1e88f","b7605579001505896d48b07160075a5c8b8e128e"],"7f6d2237ba87daf0df9c6e7bd692d5262b447790":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"28b6c6f2489e2262f68c2bb555908565774437a6":["7f6d2237ba87daf0df9c6e7bd692d5262b447790","55980207f1977bd1463465de1659b821347e2fa8"],"2d750082a4223c20902ef11ff6a9831d55738ea4":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","a7035935aa89f6951286e9005cbeb16e89a082a2"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"677676f8b2da1378ca646add0b9862033c912d0b":["2d750082a4223c20902ef11ff6a9831d55738ea4"],"da4a8b166c17d13e0b123fa57a4efbc66d442f73":["2ffc8d70d9f57a62a24c3dd15b66e353de935054"],"9d70e774cb25c8a8d2c3e5e84200f235f9168d87":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc":["0932eb10135843758b2ca508d5aa2b4798aa07f9"],"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":[],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"b76fad1f8492334e597826072e178dab0b21e02b":["d9a47902d6207303f5ed3e7aaca62ca33433af66","28b6c6f2489e2262f68c2bb555908565774437a6"],"43f5f8344e80b4bfb2069917069430266753d2f0":["b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180"],"91e069c492cf4895697ef7b81df0ffb9a8bd4b48":["a3c3e46d3417c353d7be14509cfab11b315927fe"],"84f20f331d8001864545c7021812d8c6509c7593":["b82b38f6df64b40b9ae76575b234a8ddd4238cd4"],"b7605579001505896d48b07160075a5c8b8e128e":[],"6013b4c7388f1627659c8f96c44abd10a294d3a6":["617053eaceb9d43aa4f14e350900c966e53d23bc","05a14b2611ead08655a2b2bdc61632eb31316e57","d3fcb70cf561547c7bb1506e0cf32ca7b1287064"],"c0cf9c2ec975506bab465b6b2be92cb9bffc84d3":["43f5f8344e80b4bfb2069917069430266753d2f0"],"6dd5104ff91bf9fe2184d702f43f11e400c8d3a9":["f57cf082c4d2ee975c6a2034fcf3c13f9514e6ef"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["6784d0cc613dc1ee97030eaaa5e0754edc22d164"],"0d22ac6a4146774c1bc8400160fc0b6150294e92":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","0d22ac6a4146774c1bc8400160fc0b6150294e92","2c007e7c4cf8c55bc2a5884e315123afaaeec87f"],"120dcb9902dc31423bf7d82c10c5439b88325390":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"],"0932eb10135843758b2ca508d5aa2b4798aa07f9":["6a96b173f2f573ac82bc8279dfdd3cbe2b948f95","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"a7035935aa89f6951286e9005cbeb16e89a082a2":["3392214a5d43ed5dd25d84dbdce00681c6add436","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"b82b38f6df64b40b9ae76575b234a8ddd4238cd4":["8c30d22eaf1287a88a402fba9d8b7b9d20d6ef94"],"55980207f1977bd1463465de1659b821347e2fa8":["0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"34ea1d605e0261f0f714f9e444df24839d75161c":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","91e069c492cf4895697ef7b81df0ffb9a8bd4b48"],"f57cf082c4d2ee975c6a2034fcf3c13f9514e6ef":["729cb470f975115d4c60517b2cb7c42e37a7a2e1","54ca69905c5d9d1529286f06ab1d12c68f6c13cb"],"a3c3e46d3417c353d7be14509cfab11b315927fe":["677676f8b2da1378ca646add0b9862033c912d0b"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":[],"2c007e7c4cf8c55bc2a5884e315123afaaeec87f":["7321b77a7bc3edfebd637ef273e9dfaa9969eba6","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","0d22ac6a4146774c1bc8400160fc0b6150294e92"],"615bf5b56d95ed8c9bf06a402db6c817d6bff21a":["84f20f331d8001864545c7021812d8c6509c7593"],"2ffc8d70d9f57a62a24c3dd15b66e353de935054":["c0cf9c2ec975506bab465b6b2be92cb9bffc84d3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","05a14b2611ead08655a2b2bdc61632eb31316e57","74f45af4339b0daf7a95c820ab88c1aea74fbce0","54ca69905c5d9d1529286f06ab1d12c68f6c13cb","d3fcb70cf561547c7bb1506e0cf32ca7b1287064","b7605579001505896d48b07160075a5c8b8e128e","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","0d22ac6a4146774c1bc8400160fc0b6150294e92","0a22eafe3f72a4c2945eaad9547e6c78816978f4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}