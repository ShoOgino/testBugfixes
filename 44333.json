{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","sourceNew":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer1.toString()), MockTokenizer.WHITESPACE, false));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer2.toString()), MockTokenizer.WHITESPACE, false));\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase();\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","sourceOld":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer1.toString()), MockTokenizer.WHITESPACE, false));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer2.toString()), MockTokenizer.WHITESPACE, false));\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase();\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"68f4ad0129ad3f60268f3df42c238366082da936","date":1341666620,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","sourceNew":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer1.toString()), MockTokenizer.WHITESPACE, false));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer2.toString()), MockTokenizer.WHITESPACE, false));\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","sourceOld":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer1.toString()), MockTokenizer.WHITESPACE, false));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer2.toString()), MockTokenizer.WHITESPACE, false));\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase();\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2acf500f78aa12b92e371fd89c719291986b6b90","date":1341846236,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","sourceNew":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer1.toString()), MockTokenizer.WHITESPACE, false));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer2.toString()), MockTokenizer.WHITESPACE, false));\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","sourceOld":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer1.toString()), MockTokenizer.WHITESPACE, false));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer2.toString()), MockTokenizer.WHITESPACE, false));\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase();\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"46d8ada1fff8d18cb197c38c7983225162599948","date":1341853497,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","sourceNew":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer1.toString()), MockTokenizer.WHITESPACE, false));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer2.toString()), MockTokenizer.WHITESPACE, false));\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","sourceOld":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer1.toString()), MockTokenizer.WHITESPACE, false));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer2.toString()), MockTokenizer.WHITESPACE, false));\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase();\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","sourceNew":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer1.toString()), MockTokenizer.WHITESPACE, false));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer2.toString()), MockTokenizer.WHITESPACE, false));\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","sourceOld":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer1.toString()), MockTokenizer.WHITESPACE, false));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer2.toString()), MockTokenizer.WHITESPACE, false));\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase();\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","sourceNew":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer1.toString()));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer2.toString()));\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","sourceOld":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer1.toString()), MockTokenizer.WHITESPACE, false));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(new MockTokenizer(new StringReader(buffer2.toString()), MockTokenizer.WHITESPACE, false));\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75","date":1399205975,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","sourceNew":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer1.toString()));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    MockTokenizer tokenizer = new MockTokenizer(tee1.getAttributeFactory(), MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(new StringReader(buffer2.toString()));\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(tokenizer);\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","sourceOld":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer1.toString()));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer2.toString()));\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff4227bb146f97aabae888091c19e48c88dbb0db","date":1406758576,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","sourceNew":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer1.toString()));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    MockTokenizer tokenizer = new MockTokenizer(tee1.getAttributeFactory(), MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(new StringReader(buffer2.toString()));\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(tokenizer);\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","sourceOld":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer1.toString()));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    MockTokenizer tokenizer = new MockTokenizer(tee1.getAttributeFactory(), MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(new StringReader(buffer2.toString()));\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(tokenizer);\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5cdab62f058ea765dd33deb05b4f19b7d626c801","date":1406803479,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","sourceNew":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer1.toString()));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    MockTokenizer tokenizer = new MockTokenizer(tee1.getAttributeFactory(), MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(new StringReader(buffer2.toString()));\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(tokenizer);\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","sourceOld":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer1.toString()));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    MockTokenizer tokenizer = new MockTokenizer(tee1.getAttributeFactory(), MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(new StringReader(buffer2.toString()));\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(tokenizer);\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"379db3ad24c4f0214f30a122265a6d6be003a99d","date":1407537768,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","sourceNew":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer1.toString()));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    MockTokenizer tokenizer = new MockTokenizer(tee1.getAttributeFactory(), MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(new StringReader(buffer2.toString()));\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(tokenizer);\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","sourceOld":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer1.toString()));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    MockTokenizer tokenizer = new MockTokenizer(tee1.getAttributeFactory(), MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(new StringReader(buffer2.toString()));\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(tokenizer);\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55244759f906151d96839f8451dee793acb06e75","date":1418999882,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","sourceNew":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer1.toString()));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n\n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    MockTokenizer tokenizer = new MockTokenizer(tee1.getAttributeFactory(), MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(new StringReader(buffer2.toString()));\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(tokenizer);\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    TokenStream lowerCasing = new LowerCaseFilter(source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","sourceOld":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer1.toString()));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    tee1.reset();\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n    \n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    MockTokenizer tokenizer = new MockTokenizer(tee1.getAttributeFactory(), MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(new StringReader(buffer2.toString()));\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(tokenizer);\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    source1.reset();\n    TokenStream lowerCasing = new LowerCaseFilter(source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3eac7b217fb57548b6fc21f0117e74698afde766","date":1452862547,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter#testMultipleSources().mjava","sourceNew":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer1.toString()));\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n\n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n\n    MockTokenizer tokenizer = new MockTokenizer(tee1.getAttributeFactory(), MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(new StringReader(buffer2.toString()));\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(tokenizer);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    TokenStream lowerCasing = new LowerCaseFilter(source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","sourceOld":"  public void testMultipleSources() throws Exception {\n    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(whitespaceMockTokenizer(buffer1.toString()));\n    final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);\n    final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);\n    final TokenStream source1 = new CachingTokenFilter(tee1);\n\n    tee1.addAttribute(CheckClearAttributesAttribute.class);\n    dogDetector.addAttribute(CheckClearAttributesAttribute.class);\n    theDetector.addAttribute(CheckClearAttributesAttribute.class);\n\n    MockTokenizer tokenizer = new MockTokenizer(tee1.getAttributeFactory(), MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader(new StringReader(buffer2.toString()));\n    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(tokenizer);\n    tee2.addSinkTokenStream(dogDetector);\n    tee2.addSinkTokenStream(theDetector);\n    final TokenStream source2 = tee2;\n\n    assertTokenStreamContents(source1, tokens1);\n    assertTokenStreamContents(source2, tokens2);\n\n    assertTokenStreamContents(theDetector, new String[]{\"The\", \"the\", \"The\", \"the\"});\n    assertTokenStreamContents(dogDetector, new String[]{\"Dogs\", \"Dogs\"});\n    \n    TokenStream lowerCasing = new LowerCaseFilter(source1);\n    String[] lowerCaseTokens = new String[tokens1.length];\n    for (int i = 0; i < tokens1.length; i++)\n      lowerCaseTokens[i] = tokens1[i].toLowerCase(Locale.ROOT);\n    assertTokenStreamContents(lowerCasing, lowerCaseTokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ff4227bb146f97aabae888091c19e48c88dbb0db":["923f36bb0db6f793cf62dbb68723ae3bfbaf1d75"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["2acf500f78aa12b92e371fd89c719291986b6b90"],"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"68f4ad0129ad3f60268f3df42c238366082da936":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"2acf500f78aa12b92e371fd89c719291986b6b90":["b89678825b68eccaf09e6ab71675fc0b0af1e099","68f4ad0129ad3f60268f3df42c238366082da936"],"3eac7b217fb57548b6fc21f0117e74698afde766":["55244759f906151d96839f8451dee793acb06e75"],"46d8ada1fff8d18cb197c38c7983225162599948":["b89678825b68eccaf09e6ab71675fc0b0af1e099","2acf500f78aa12b92e371fd89c719291986b6b90"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["b89678825b68eccaf09e6ab71675fc0b0af1e099","2acf500f78aa12b92e371fd89c719291986b6b90"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["ff4227bb146f97aabae888091c19e48c88dbb0db"],"55244759f906151d96839f8451dee793acb06e75":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3eac7b217fb57548b6fc21f0117e74698afde766"]},"commit2Childs":{"ff4227bb146f97aabae888091c19e48c88dbb0db":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["923f36bb0db6f793cf62dbb68723ae3bfbaf1d75"],"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75":["ff4227bb146f97aabae888091c19e48c88dbb0db"],"68f4ad0129ad3f60268f3df42c238366082da936":["2acf500f78aa12b92e371fd89c719291986b6b90"],"2acf500f78aa12b92e371fd89c719291986b6b90":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","46d8ada1fff8d18cb197c38c7983225162599948","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"3eac7b217fb57548b6fc21f0117e74698afde766":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"46d8ada1fff8d18cb197c38c7983225162599948":[],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["68f4ad0129ad3f60268f3df42c238366082da936","2acf500f78aa12b92e371fd89c719291986b6b90","46d8ada1fff8d18cb197c38c7983225162599948","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["55244759f906151d96839f8451dee793acb06e75"],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"55244759f906151d96839f8451dee793acb06e75":["3eac7b217fb57548b6fc21f0117e74698afde766"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["46d8ada1fff8d18cb197c38c7983225162599948","fe33227f6805edab2036cbb80645cc4e2d1fa424","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}