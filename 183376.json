{"path":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsCache#createValue(AtomicReader,CacheKey,boolean).mjava","commits":[{"id":"a4d374b2bebd0d52acaa61038fbf23068620fba7","date":1353240004,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        // nocommit used packed ints like below!\n        final byte[][] values = new byte[maxDoc][];\n        BytesRef scratch = new BytesRef();\n        for(int docID=0;docID<maxDoc;docID++) {\n          valuesIn.get(docID, scratch);\n          values[docID] = new byte[scratch.length];\n          System.arraycopy(scratch.bytes, scratch.offset, values[docID], 0, scratch.length);\n        }\n\n        return new DocTerms() {\n          @Override\n          public int size() {\n            return maxDoc;\n          }\n\n          @Override\n          public boolean exists(int docID) {\n            // nocommit lying ...?\n            return true;\n          }\n\n          @Override\n          public BytesRef getTerm(int docID, BytesRef ret) {\n            ret.bytes = values[docID];\n            ret.length = ret.bytes.length;\n            ret.offset = 0;\n            return ret;\n          }      \n        };\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["87d6f9603307ae2ad642fb01deedf031320fd0c3"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e05b279040cd7b938223b77c3772786678160cf6","date":1353297629,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        final BinaryDocValues ramInstance = valuesIn.newRAMInstance();\n        return new DocTerms() {\n\n          @Override\n          public BytesRef getTerm(int docID, BytesRef ret) {\n            ramInstance.get(docID, ret);\n            return ret;\n          }\n\n          @Override\n          public boolean exists(int docID) {\n            // nocommit lying ...?\n            return true;\n          }\n\n          @Override\n          public int size() {\n            return ramInstance.size();\n          }     \n        };\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        // nocommit used packed ints like below!\n        final byte[][] values = new byte[maxDoc][];\n        BytesRef scratch = new BytesRef();\n        for(int docID=0;docID<maxDoc;docID++) {\n          valuesIn.get(docID, scratch);\n          values[docID] = new byte[scratch.length];\n          System.arraycopy(scratch.bytes, scratch.offset, values[docID], 0, scratch.length);\n        }\n\n        return new DocTerms() {\n          @Override\n          public int size() {\n            return maxDoc;\n          }\n\n          @Override\n          public boolean exists(int docID) {\n            // nocommit lying ...?\n            return true;\n          }\n\n          @Override\n          public BytesRef getTerm(int docID, BytesRef ret) {\n            ret.bytes = values[docID];\n            ret.length = ret.bytes.length;\n            ret.offset = 0;\n            return ret;\n          }      \n        };\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2dd9934a49477c83301120ba51827d91eb3606d5","date":1353767072,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.BinaryDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn;\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      BinaryDocValues valuesIn = reader.getBinaryDocValues(key.field);\n      if (valuesIn != null) {\n        final BinaryDocValues ramInstance = valuesIn.newRAMInstance();\n        return new DocTerms() {\n\n          @Override\n          public BytesRef getTerm(int docID, BytesRef ret) {\n            ramInstance.get(docID, ret);\n            return ret;\n          }\n\n          @Override\n          public boolean exists(int docID) {\n            // nocommit lying ...?\n            return true;\n          }\n\n          @Override\n          public int size() {\n            return ramInstance.size();\n          }     \n        };\n      } else {\n        final int maxDoc = reader.maxDoc();\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final int termCountHardLimit = maxDoc;\n\n        // Holds the actual term data, expanded.\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBPV;\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              numUniqueTerms = termCountHardLimit;\n            }\n            startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          } else {\n            startBPV = 1;\n          }\n        } else {\n          startBPV = 1;\n        }\n\n        final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);\n      \n        // pointer==0 means not set\n        bytes.copyUsingLengthPrefix(new BytesRef());\n\n        if (terms != null) {\n          int termCount = 0;\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n          while(true) {\n            if (termCount++ == termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              break;\n            }\n\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final long pointer = bytes.copyUsingLengthPrefix(term);\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToOffset.set(docID, pointer);\n            }\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"e05b279040cd7b938223b77c3772786678160cf6":["a4d374b2bebd0d52acaa61038fbf23068620fba7"],"2dd9934a49477c83301120ba51827d91eb3606d5":["e05b279040cd7b938223b77c3772786678160cf6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a4d374b2bebd0d52acaa61038fbf23068620fba7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"e05b279040cd7b938223b77c3772786678160cf6":["2dd9934a49477c83301120ba51827d91eb3606d5"],"2dd9934a49477c83301120ba51827d91eb3606d5":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a4d374b2bebd0d52acaa61038fbf23068620fba7","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a4d374b2bebd0d52acaa61038fbf23068620fba7":["e05b279040cd7b938223b77c3772786678160cf6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["2dd9934a49477c83301120ba51827d91eb3606d5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}