{"path":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame#loadBlock().mjava","commits":[{"id":"3f4ef2de7b0fd59ef22e20888773ad260c90bfb4","date":1400183621,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame#loadBlock().mjava","pathOld":"/dev/null","sourceNew":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n    assert arc == null || (isLastInFloor || isFloor);\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    numBytes = ste.in.readVInt();\n    if (statBytes.length < numBytes) {\n      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(statBytes, 0, numBytes);\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4d637064d608752565d4f9f41b2497dfdfdde50e","date":1400798123,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame#loadBlock().mjava","pathOld":"/dev/null","sourceNew":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n    assert arc == null || (isLastInFloor || isFloor);\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    numBytes = ste.in.readVInt();\n    if (statBytes.length < numBytes) {\n      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(statBytes, 0, numBytes);\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f838187609fee3a1afa5f162f93c796046242c84","date":1406216791,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame#loadBlock().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame#loadBlock().mjava","sourceNew":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n\n    assert arc == null || (isLastInFloor || isFloor): \"fp=\" + fp + \" arc=\" + arc + \" isFloor=\" + isFloor + \" isLastInFloor=\" + isLastInFloor;\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    numBytes = ste.in.readVInt();\n    if (statBytes.length < numBytes) {\n      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(statBytes, 0, numBytes);\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","sourceOld":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n    assert arc == null || (isLastInFloor || isFloor);\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    numBytes = ste.in.readVInt();\n    if (statBytes.length < numBytes) {\n      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(statBytes, 0, numBytes);\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"59db5e5f780185e0155d296a323e440a6ecfd3b6","date":1435089559,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame#loadBlock().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame#loadBlock().mjava","sourceNew":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n\n    assert arc == null || (isLastInFloor || isFloor): \"fp=\" + fp + \" arc=\" + arc + \" isFloor=\" + isFloor + \" isLastInFloor=\" + isLastInFloor;\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    numBytes = ste.in.readVInt();\n    if (statBytes.length < numBytes) {\n      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(statBytes, 0, numBytes);\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","sourceOld":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n\n    assert arc == null || (isLastInFloor || isFloor): \"fp=\" + fp + \" arc=\" + arc + \" isFloor=\" + isFloor + \" isLastInFloor=\" + isLastInFloor;\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    numBytes = ste.in.readVInt();\n    if (statBytes.length < numBytes) {\n      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(statBytes, 0, numBytes);\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9407318969e8504257b4c5764c65755a043e5404","date":1579873617,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame#loadBlock().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame#loadBlock().mjava","sourceNew":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n\n    assert arc == null || (isLastInFloor || isFloor): \"fp=\" + fp + \" arc=\" + arc + \" isFloor=\" + isFloor + \" isLastInFloor=\" + isLastInFloor;\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    final long startSuffixFP = ste.in.getFilePointer();\n    // term suffixes:\n    if (version >= BlockTreeTermsReader.VERSION_COMPRESSED_SUFFIXES) {\n      final long codeL = ste.in.readVLong();\n      isLeafBlock = (codeL & 0x04) != 0;\n      final int numSuffixBytes = (int) (codeL >>> 3);\n      if (suffixBytes.length < numSuffixBytes) {\n        suffixBytes = new byte[ArrayUtil.oversize(numSuffixBytes, 1)];\n      }\n      try {\n        compressionAlg = CompressionAlgorithm.byCode((int) codeL & 0x03);\n      } catch (IllegalArgumentException e) {\n        throw new CorruptIndexException(e.getMessage(), ste.in, e);\n      }\n      compressionAlg.read(ste.in, suffixBytes, numSuffixBytes);\n      suffixesReader.reset(suffixBytes, 0, numSuffixBytes);\n\n      int numSuffixLengthBytes = ste.in.readVInt();\n      final boolean allEqual = (numSuffixLengthBytes & 0x01) != 0;\n      numSuffixLengthBytes >>>= 1;\n      if (suffixLengthBytes.length < numSuffixLengthBytes) {\n        suffixLengthBytes = new byte[ArrayUtil.oversize(numSuffixLengthBytes, 1)];\n      }\n      if (allEqual) {\n        Arrays.fill(suffixLengthBytes, 0, numSuffixLengthBytes, ste.in.readByte());\n      } else {\n        LZ4.decompress(ste.in, numSuffixLengthBytes, suffixLengthBytes, 0);\n      }\n      suffixLengthsReader.reset(suffixLengthBytes, 0, numSuffixLengthBytes);\n    } else {\n      code = ste.in.readVInt();\n      isLeafBlock = (code & 1) != 0;\n      int numBytes = code >>> 1;\n      if (suffixBytes.length < numBytes) {\n        suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      }\n      ste.in.readBytes(suffixBytes, 0, numBytes);\n      suffixesReader.reset(suffixBytes, 0, numBytes);\n    }\n    totalSuffixBytes = ste.in.getFilePointer() - startSuffixFP;\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    final long startStatsFP = ste.in.getFilePointer();\n    int numBytes = ste.in.readVInt();\n    if (version >= BlockTreeTermsReader.VERSION_COMPRESSED_SUFFIXES) {\n      final boolean allOnes = (numBytes & 0x01) != 0;\n      numBytes >>>= 1;\n      if (statBytes.length < numBytes) {\n        statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      }\n      if (allOnes) {\n        Arrays.fill(statBytes, 0, numBytes, (byte) 1);\n      } else {\n        LZ4.decompress(ste.in, numBytes, statBytes, 0);\n      }\n    } else {\n      if (statBytes.length < numBytes) {\n        statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      }\n      ste.in.readBytes(statBytes, 0, numBytes);\n    }\n    totalStatsBytes = ste.in.getFilePointer() - startStatsFP;\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","sourceOld":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n\n    assert arc == null || (isLastInFloor || isFloor): \"fp=\" + fp + \" arc=\" + arc + \" isFloor=\" + isFloor + \" isLastInFloor=\" + isLastInFloor;\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    numBytes = ste.in.readVInt();\n    if (statBytes.length < numBytes) {\n      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(statBytes, 0, numBytes);\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a356e37aed258bcd168680472f8d1dbc6f396935","date":1580233110,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame#loadBlock().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/SegmentTermsEnumFrame#loadBlock().mjava","sourceNew":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n\n    assert arc == null || (isLastInFloor || isFloor): \"fp=\" + fp + \" arc=\" + arc + \" isFloor=\" + isFloor + \" isLastInFloor=\" + isLastInFloor;\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    final long startSuffixFP = ste.in.getFilePointer();\n    // term suffixes:\n    if (version >= BlockTreeTermsReader.VERSION_COMPRESSED_SUFFIXES) {\n      final long codeL = ste.in.readVLong();\n      isLeafBlock = (codeL & 0x04) != 0;\n      final int numSuffixBytes = (int) (codeL >>> 3);\n      if (suffixBytes.length < numSuffixBytes) {\n        suffixBytes = new byte[ArrayUtil.oversize(numSuffixBytes, 1)];\n      }\n      try {\n        compressionAlg = CompressionAlgorithm.byCode((int) codeL & 0x03);\n      } catch (IllegalArgumentException e) {\n        throw new CorruptIndexException(e.getMessage(), ste.in, e);\n      }\n      compressionAlg.read(ste.in, suffixBytes, numSuffixBytes);\n      suffixesReader.reset(suffixBytes, 0, numSuffixBytes);\n\n      int numSuffixLengthBytes = ste.in.readVInt();\n      final boolean allEqual = (numSuffixLengthBytes & 0x01) != 0;\n      numSuffixLengthBytes >>>= 1;\n      if (suffixLengthBytes.length < numSuffixLengthBytes) {\n        suffixLengthBytes = new byte[ArrayUtil.oversize(numSuffixLengthBytes, 1)];\n      }\n      if (allEqual) {\n        Arrays.fill(suffixLengthBytes, 0, numSuffixLengthBytes, ste.in.readByte());\n      } else {\n        ste.in.readBytes(suffixLengthBytes, 0, numSuffixLengthBytes);\n      }\n      suffixLengthsReader.reset(suffixLengthBytes, 0, numSuffixLengthBytes);\n    } else {\n      code = ste.in.readVInt();\n      isLeafBlock = (code & 1) != 0;\n      int numBytes = code >>> 1;\n      if (suffixBytes.length < numBytes) {\n        suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      }\n      ste.in.readBytes(suffixBytes, 0, numBytes);\n      suffixesReader.reset(suffixBytes, 0, numBytes);\n    }\n    totalSuffixBytes = ste.in.getFilePointer() - startSuffixFP;\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    int numBytes = ste.in.readVInt();\n    if (statBytes.length < numBytes) {\n      statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(statBytes, 0, numBytes);\n    statsReader.reset(statBytes, 0, numBytes);\n    statsSingletonRunLength = 0;\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","sourceOld":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n\n    assert arc == null || (isLastInFloor || isFloor): \"fp=\" + fp + \" arc=\" + arc + \" isFloor=\" + isFloor + \" isLastInFloor=\" + isLastInFloor;\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    final long startSuffixFP = ste.in.getFilePointer();\n    // term suffixes:\n    if (version >= BlockTreeTermsReader.VERSION_COMPRESSED_SUFFIXES) {\n      final long codeL = ste.in.readVLong();\n      isLeafBlock = (codeL & 0x04) != 0;\n      final int numSuffixBytes = (int) (codeL >>> 3);\n      if (suffixBytes.length < numSuffixBytes) {\n        suffixBytes = new byte[ArrayUtil.oversize(numSuffixBytes, 1)];\n      }\n      try {\n        compressionAlg = CompressionAlgorithm.byCode((int) codeL & 0x03);\n      } catch (IllegalArgumentException e) {\n        throw new CorruptIndexException(e.getMessage(), ste.in, e);\n      }\n      compressionAlg.read(ste.in, suffixBytes, numSuffixBytes);\n      suffixesReader.reset(suffixBytes, 0, numSuffixBytes);\n\n      int numSuffixLengthBytes = ste.in.readVInt();\n      final boolean allEqual = (numSuffixLengthBytes & 0x01) != 0;\n      numSuffixLengthBytes >>>= 1;\n      if (suffixLengthBytes.length < numSuffixLengthBytes) {\n        suffixLengthBytes = new byte[ArrayUtil.oversize(numSuffixLengthBytes, 1)];\n      }\n      if (allEqual) {\n        Arrays.fill(suffixLengthBytes, 0, numSuffixLengthBytes, ste.in.readByte());\n      } else {\n        LZ4.decompress(ste.in, numSuffixLengthBytes, suffixLengthBytes, 0);\n      }\n      suffixLengthsReader.reset(suffixLengthBytes, 0, numSuffixLengthBytes);\n    } else {\n      code = ste.in.readVInt();\n      isLeafBlock = (code & 1) != 0;\n      int numBytes = code >>> 1;\n      if (suffixBytes.length < numBytes) {\n        suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      }\n      ste.in.readBytes(suffixBytes, 0, numBytes);\n      suffixesReader.reset(suffixBytes, 0, numBytes);\n    }\n    totalSuffixBytes = ste.in.getFilePointer() - startSuffixFP;\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    // stats\n    final long startStatsFP = ste.in.getFilePointer();\n    int numBytes = ste.in.readVInt();\n    if (version >= BlockTreeTermsReader.VERSION_COMPRESSED_SUFFIXES) {\n      final boolean allOnes = (numBytes & 0x01) != 0;\n      numBytes >>>= 1;\n      if (statBytes.length < numBytes) {\n        statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      }\n      if (allOnes) {\n        Arrays.fill(statBytes, 0, numBytes, (byte) 1);\n      } else {\n        LZ4.decompress(ste.in, numBytes, statBytes, 0);\n      }\n    } else {\n      if (statBytes.length < numBytes) {\n        statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      }\n      ste.in.readBytes(statBytes, 0, numBytes);\n    }\n    totalStatsBytes = ste.in.getFilePointer() - startStatsFP;\n    statsReader.reset(statBytes, 0, numBytes);\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"4d637064d608752565d4f9f41b2497dfdfdde50e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3f4ef2de7b0fd59ef22e20888773ad260c90bfb4"],"3f4ef2de7b0fd59ef22e20888773ad260c90bfb4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"59db5e5f780185e0155d296a323e440a6ecfd3b6":["f838187609fee3a1afa5f162f93c796046242c84"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9407318969e8504257b4c5764c65755a043e5404":["59db5e5f780185e0155d296a323e440a6ecfd3b6"],"a356e37aed258bcd168680472f8d1dbc6f396935":["9407318969e8504257b4c5764c65755a043e5404"],"f838187609fee3a1afa5f162f93c796046242c84":["4d637064d608752565d4f9f41b2497dfdfdde50e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a356e37aed258bcd168680472f8d1dbc6f396935"]},"commit2Childs":{"4d637064d608752565d4f9f41b2497dfdfdde50e":["f838187609fee3a1afa5f162f93c796046242c84"],"3f4ef2de7b0fd59ef22e20888773ad260c90bfb4":["4d637064d608752565d4f9f41b2497dfdfdde50e"],"59db5e5f780185e0155d296a323e440a6ecfd3b6":["9407318969e8504257b4c5764c65755a043e5404"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4d637064d608752565d4f9f41b2497dfdfdde50e","3f4ef2de7b0fd59ef22e20888773ad260c90bfb4"],"9407318969e8504257b4c5764c65755a043e5404":["a356e37aed258bcd168680472f8d1dbc6f396935"],"a356e37aed258bcd168680472f8d1dbc6f396935":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f838187609fee3a1afa5f162f93c796046242c84":["59db5e5f780185e0155d296a323e440a6ecfd3b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}