{"path":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","commits":[{"id":"dd745d580729e528151b58aeda87ef82f1b95c9b","date":1248369082,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"contrib/analyzers/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The IndexReader class which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet stopWords = new HashSet();\n    String internedFieldName = fieldName.intern();\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The IndexReader class which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet stopWords = new HashSet();\n    String internedFieldName = fieldName.intern();\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"99a7ff0fa358e0b2513ba76ee19b04663ff35be8","date":1249665593,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The IndexReader class which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet stopWords = new HashSet();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The IndexReader class which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet stopWords = new HashSet();\n    String internedFieldName = fieldName.intern();\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"18359c8e12d55f66c27cfe7babe86283f06a6aa5","date":1250426225,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The IndexReader class which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet stopWords = new HashSet();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map streamMap = (Map) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The IndexReader class which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet stopWords = new HashSet();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45b06c3ff8a4d9d3d751dd311e51c59303c2719c","date":1250600126,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet stopWords = new HashSet();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map streamMap = (Map) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The IndexReader class which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet stopWords = new HashSet();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map streamMap = (Map) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60cdc0e643184821eb066795a8791cd82559f46e","date":1257941914,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet stopWords = new HashSet();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map streamMap = (Map) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"60cdc0e643184821eb066795a8791cd82559f46e":["45b06c3ff8a4d9d3d751dd311e51c59303c2719c"],"dd745d580729e528151b58aeda87ef82f1b95c9b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"18359c8e12d55f66c27cfe7babe86283f06a6aa5":["99a7ff0fa358e0b2513ba76ee19b04663ff35be8"],"99a7ff0fa358e0b2513ba76ee19b04663ff35be8":["dd745d580729e528151b58aeda87ef82f1b95c9b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"45b06c3ff8a4d9d3d751dd311e51c59303c2719c":["18359c8e12d55f66c27cfe7babe86283f06a6aa5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["60cdc0e643184821eb066795a8791cd82559f46e"]},"commit2Childs":{"60cdc0e643184821eb066795a8791cd82559f46e":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"dd745d580729e528151b58aeda87ef82f1b95c9b":["99a7ff0fa358e0b2513ba76ee19b04663ff35be8"],"18359c8e12d55f66c27cfe7babe86283f06a6aa5":["45b06c3ff8a4d9d3d751dd311e51c59303c2719c"],"99a7ff0fa358e0b2513ba76ee19b04663ff35be8":["18359c8e12d55f66c27cfe7babe86283f06a6aa5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["dd745d580729e528151b58aeda87ef82f1b95c9b"],"45b06c3ff8a4d9d3d751dd311e51c59303c2719c":["60cdc0e643184821eb066795a8791cd82559f46e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}