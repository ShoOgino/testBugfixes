{"path":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc1field1\")));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc1field2\")));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc2field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc2field2\")));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc3field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc3field2\")));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc1field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc2field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc3field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc1field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc2field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc3field2\")).next());\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc1field1\")));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc1field2\")));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc2field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc2field2\")));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc3field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc3field2\")));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc1field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc2field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc3field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc1field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc2field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc3field2\")).next());\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d572389229127c297dd1fa5ce4758e1cec41e799","date":1273610938,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc1field1\")));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc1field2\")));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc2field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc2field2\")));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc3field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc3field2\")));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc1field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc2field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc3field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc1field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc2field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc3field2\")).next());\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc1field1\")));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc1field2\")));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc2field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc2field2\")));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc3field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc3field2\")));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc1field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc2field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc3field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc1field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc2field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc3field2\")).next());\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"03276b2744036b1b19a7a2dd4b74ba7bc484f107","date":1274048508,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc1field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc2field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc3field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc1field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc2field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc3field2\")).next());\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc1field1\")));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc1field2\")));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc2field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc2field2\")));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc3field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\"doc3field2\")));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc1field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc2field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc3field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc1field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc2field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc3field2\")).next());\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28427ef110c4c5bf5b4057731b83110bd1e13724","date":1276701452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc1field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc2field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc3field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc1field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc2field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc3field2\")).next());\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc1field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc2field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc3field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc1field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc2field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc3field2\")).next());\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b21422ff1d1d56499dec481f193b402e5e8def5b","date":1281472367,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = newDirectory(random);\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a05409176bd65129d67a785ee70e881e238a9aef","date":1282582843,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory(random);\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = newDirectory(random);\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory(random);\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a13a126d15299d5c1e117ea99ddae6fb0fa3f209","date":1291909583,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", customType, b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n    Field f2 = newField(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":["fa0f44f887719e97183771e977cfc4bfb485b766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd","date":1317197236,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", customType, b, 10, 17);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"872cff1d3a554e0cd64014cd97f88d3002b0f491","date":1323024658,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b65b350ca9588f9fc76ce7d6804160d06c45ff42","date":1323026297,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc1field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc2field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"binary\", new BytesRef(\"doc3field1\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc1field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc2field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(MultiFields.getTermDocsEnum(ir, null, \"string\", new BytesRef(\"doc3field2\")).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1c5b026d03cbbb03ca4c0b97d14e9839682281dc","date":1323049298,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir, true);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fa0f44f887719e97183771e977cfc4bfb485b766","date":1326668713,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(BinaryField.TYPE_STORED);\n    customType.setTokenized(true);\n    customType.setIndexed(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n\n    FieldType customType = new FieldType(StoredField.TYPE);\n    customType.setTokenized(true);\n    \n    Field f = new Field(\"binary\", b, 10, 17, customType);\n    customType.setIndexed(true);\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc1field1\"), MockTokenizer.WHITESPACE, false));\n\n    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n    \n    Field f2 = newField(\"string\", \"value\", customType2);\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc1field2\"), MockTokenizer.WHITESPACE, false));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n\n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc2field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc2field2\"), MockTokenizer.WHITESPACE, false));\n    w.addDocument(doc);\n\n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new MockTokenizer(new StringReader(\"doc3field1\"), MockTokenizer.WHITESPACE, false));\n    f2.setTokenStream(new MockTokenizer(new StringReader(\"doc3field2\"), MockTokenizer.WHITESPACE, false));\n\n    w.addDocument(doc);\n    w.commit();\n    w.forceMerge(1);   // force segment merge.\n    w.close();\n\n    IndexReader ir = IndexReader.open(dir);\n    Document doc2 = ir.document(0);\n    IndexableField f3 = doc2.getField(\"binary\");\n    b = f3.binaryValue().bytes;\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(1).getField(\"binary\").binaryValue()!=null);\n    assertTrue(ir.document(2).getField(\"binary\").binaryValue()!=null);\n\n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc1field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc2field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"binary\", new BytesRef(\"doc3field1\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc1field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc2field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertTrue(_TestUtil.docs(random, ir, \"string\", new BytesRef(\"doc3field2\"), null, null, false).nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["872cff1d3a554e0cd64014cd97f88d3002b0f491","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["fa0f44f887719e97183771e977cfc4bfb485b766"],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","a13a126d15299d5c1e117ea99ddae6fb0fa3f209"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["a13a126d15299d5c1e117ea99ddae6fb0fa3f209"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["a05409176bd65129d67a785ee70e881e238a9aef"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a05409176bd65129d67a785ee70e881e238a9aef":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["03276b2744036b1b19a7a2dd4b74ba7bc484f107"],"fa0f44f887719e97183771e977cfc4bfb485b766":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["28427ef110c4c5bf5b4057731b83110bd1e13724"],"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a13a126d15299d5c1e117ea99ddae6fb0fa3f209":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"03276b2744036b1b19a7a2dd4b74ba7bc484f107":["d572389229127c297dd1fa5ce4758e1cec41e799"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"d572389229127c297dd1fa5ce4758e1cec41e799":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["f2c5f0cb44df114db4228c8f77861714b5cabaea","962d04139994fce5193143ef35615499a9a96d78"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["132903c28af3aa6f67284b78de91c0f0a99488c2","a13a126d15299d5c1e117ea99ddae6fb0fa3f209"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"5f4e87790277826a2aea119328600dfb07761f32":["03276b2744036b1b19a7a2dd4b74ba7bc484f107","28427ef110c4c5bf5b4057731b83110bd1e13724"],"962d04139994fce5193143ef35615499a9a96d78":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd","872cff1d3a554e0cd64014cd97f88d3002b0f491"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a13a126d15299d5c1e117ea99ddae6fb0fa3f209","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["872cff1d3a554e0cd64014cd97f88d3002b0f491","1c5b026d03cbbb03ca4c0b97d14e9839682281dc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["872cff1d3a554e0cd64014cd97f88d3002b0f491","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["a13a126d15299d5c1e117ea99ddae6fb0fa3f209","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["962d04139994fce5193143ef35615499a9a96d78"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","962d04139994fce5193143ef35615499a9a96d78"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a05409176bd65129d67a785ee70e881e238a9aef":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["b21422ff1d1d56499dec481f193b402e5e8def5b","5f4e87790277826a2aea119328600dfb07761f32"],"fa0f44f887719e97183771e977cfc4bfb485b766":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["d572389229127c297dd1fa5ce4758e1cec41e799"],"a13a126d15299d5c1e117ea99ddae6fb0fa3f209":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","f2c5f0cb44df114db4228c8f77861714b5cabaea","ab5cb6a74aefb78aa0569857970b9151dfe2e787","a3776dccca01c11e7046323cfad46a3b4a471233"],"03276b2744036b1b19a7a2dd4b74ba7bc484f107":["28427ef110c4c5bf5b4057731b83110bd1e13724","5f4e87790277826a2aea119328600dfb07761f32"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["a05409176bd65129d67a785ee70e881e238a9aef"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["1509f151d7692d84fae414b2b799ac06ba60fcb4","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233"],"d572389229127c297dd1fa5ce4758e1cec41e799":["03276b2744036b1b19a7a2dd4b74ba7bc484f107"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","b65b350ca9588f9fc76ce7d6804160d06c45ff42","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"962d04139994fce5193143ef35615499a9a96d78":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":["1c5b026d03cbbb03ca4c0b97d14e9839682281dc"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","fa0f44f887719e97183771e977cfc4bfb485b766"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}