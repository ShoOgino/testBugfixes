{"path":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(List[String[]]).mjava","commits":[{"id":"3e0e5dacb8158de7670b41d1a749a4b7487e6acf","date":1431331436,"type":1,"author":"Christian Moen","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(List[String[]]).mjava","pathOld":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  private UserDictionary(List<String[]> featureEntries) throws IOException {\n\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if it's needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.setLength(token.length());\n      for (int i = 0; i < token.length(); i++) {\n        scratch.setIntAt(i, (int) token.charAt(i));\n      }\n      fstBuilder.add(scratch.get(), ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if it's needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.setLength(token.length());\n      for (int i = 0; i < token.length(); i++) {\n        scratch.setIntAt(i, (int) token.charAt(i));\n      }\n      fstBuilder.add(scratch.get(), ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd1eeead920e82d87bc0f8ff7d2d5353aba27842","date":1565751892,"type":3,"author":"Tomoko Uchida","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(List[String[]]).mjava","pathOld":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(List[String[]]).mjava","sourceNew":"  private UserDictionary(List<String[]> featureEntries) throws IOException {\n\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if it's needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String surface = values[0].replaceAll(\"\\\\s\", \"\");\n      String concatenatedSegment = values[1].replaceAll(\"\\\\s\", \"\");\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n\n      if (!surface.equals(concatenatedSegment)) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the concatenated segmentation (\" + concatenatedSegment + \")\" +\n                                   \" does not match the surface form (\" + surface + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.setLength(token.length());\n      for (int i = 0; i < token.length(); i++) {\n        scratch.setIntAt(i, (int) token.charAt(i));\n      }\n      fstBuilder.add(scratch.get(), ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  private UserDictionary(List<String[]> featureEntries) throws IOException {\n\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if it's needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.setLength(token.length());\n      for (int i = 0; i < token.length(); i++) {\n        scratch.setIntAt(i, (int) token.charAt(i));\n      }\n      fstBuilder.add(scratch.get(), ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f5661e6a04d3172e262ad741b717924f2f1b6a5","date":1576244274,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(List[String[]]).mjava","pathOld":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(List[String[]]).mjava","sourceNew":"  private UserDictionary(List<String[]> featureEntries) throws IOException {\n\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if it's needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    FSTCompiler<Long> fstCompiler = new FSTCompiler<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String surface = values[0].replaceAll(\"\\\\s\", \"\");\n      String concatenatedSegment = values[1].replaceAll(\"\\\\s\", \"\");\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n\n      if (!surface.equals(concatenatedSegment)) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the concatenated segmentation (\" + concatenatedSegment + \")\" +\n                                   \" does not match the surface form (\" + surface + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.setLength(token.length());\n      for (int i = 0; i < token.length(); i++) {\n        scratch.setIntAt(i, (int) token.charAt(i));\n      }\n      fstCompiler.add(scratch.get(), ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstCompiler.compile(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  private UserDictionary(List<String[]> featureEntries) throws IOException {\n\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if it's needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String surface = values[0].replaceAll(\"\\\\s\", \"\");\n      String concatenatedSegment = values[1].replaceAll(\"\\\\s\", \"\");\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n\n      if (!surface.equals(concatenatedSegment)) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the concatenated segmentation (\" + concatenatedSegment + \")\" +\n                                   \" does not match the surface form (\" + surface + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.setLength(token.length());\n      for (int i = 0; i < token.length(); i++) {\n        scratch.setIntAt(i, (int) token.charAt(i));\n      }\n      fstBuilder.add(scratch.get(), ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c5db9bff3aeb942c848a2ab8fa4b8b0737377deb","date":1576247714,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(List[String[]]).mjava","pathOld":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(List[String[]]).mjava","sourceNew":"  private UserDictionary(List<String[]> featureEntries) throws IOException {\n\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if it's needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    FSTCompiler<Long> fstCompiler = new FSTCompiler<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String surface = values[0].replaceAll(\"\\\\s\", \"\");\n      String concatenatedSegment = values[1].replaceAll(\"\\\\s\", \"\");\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n\n      if (!surface.equals(concatenatedSegment)) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the concatenated segmentation (\" + concatenatedSegment + \")\" +\n                                   \" does not match the surface form (\" + surface + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.setLength(token.length());\n      for (int i = 0; i < token.length(); i++) {\n        scratch.setIntAt(i, (int) token.charAt(i));\n      }\n      fstCompiler.add(scratch.get(), ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstCompiler.compile(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  private UserDictionary(List<String[]> featureEntries) throws IOException {\n\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if it's needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String surface = values[0].replaceAll(\"\\\\s\", \"\");\n      String concatenatedSegment = values[1].replaceAll(\"\\\\s\", \"\");\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n\n      if (!surface.equals(concatenatedSegment)) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the concatenated segmentation (\" + concatenatedSegment + \")\" +\n                                   \" does not match the surface form (\" + surface + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.setLength(token.length());\n      for (int i = 0; i < token.length(); i++) {\n        scratch.setIntAt(i, (int) token.charAt(i));\n      }\n      fstBuilder.add(scratch.get(), ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3e0e5dacb8158de7670b41d1a749a4b7487e6acf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c5db9bff3aeb942c848a2ab8fa4b8b0737377deb":["bd1eeead920e82d87bc0f8ff7d2d5353aba27842","0f5661e6a04d3172e262ad741b717924f2f1b6a5"],"0f5661e6a04d3172e262ad741b717924f2f1b6a5":["bd1eeead920e82d87bc0f8ff7d2d5353aba27842"],"bd1eeead920e82d87bc0f8ff7d2d5353aba27842":["3e0e5dacb8158de7670b41d1a749a4b7487e6acf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f5661e6a04d3172e262ad741b717924f2f1b6a5"]},"commit2Childs":{"3e0e5dacb8158de7670b41d1a749a4b7487e6acf":["bd1eeead920e82d87bc0f8ff7d2d5353aba27842"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3e0e5dacb8158de7670b41d1a749a4b7487e6acf"],"c5db9bff3aeb942c848a2ab8fa4b8b0737377deb":[],"0f5661e6a04d3172e262ad741b717924f2f1b6a5":["c5db9bff3aeb942c848a2ab8fa4b8b0737377deb","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"bd1eeead920e82d87bc0f8ff7d2d5353aba27842":["c5db9bff3aeb942c848a2ab8fa4b8b0737377deb","0f5661e6a04d3172e262ad741b717924f2f1b6a5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c5db9bff3aeb942c848a2ab8fa4b8b0737377deb","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}