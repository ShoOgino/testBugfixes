{"path":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6267e1ce56c2eec111425690cd04e251b6f14952","date":1275222352,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9b832cbed6eb3d54a8bb9339296bdda8eeb53014","date":1279708040,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    // nocommit\n    //final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      // nocommit\n//      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n//        doFlushDocStore = true;\n//      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"334c1175813aea771a71728cd2c4ee4754fd0603","date":1279710173,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    // nocommit\n    //final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      // nocommit\n//      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n//        doFlushDocStore = true;\n//      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8fe956d65251358d755c56f14fe8380644790e47","date":1279711318,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    // nocommit\n    //final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      // nocommit\n//      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n//        doFlushDocStore = true;\n//      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"833a7987bc1c94455fde83e3311f72bddedcfb93","date":1279951470,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    merge.increfDone = true;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    // nocommit\n    //final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      // nocommit\n//      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n//        doFlushDocStore = true;\n//      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"619e84c4a25ac93018dce34af2bb41dafbeac829","date":1281217684,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2a186ae8733084223c22044e935e4ef848a143d1","date":1289694819,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c498d3f8d75170b121f5eda2c6210ac5beb5d411","date":1289726298,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5","date":1290247889,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":["878eedeaae8b281cc57edbb48be7876469cec585"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"44fcbde6fb2ac44ee3b45e013e54a42911e689ff","date":1292065621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n    \n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e1cbd7e289dc1243c7a59e1a83d078163a147fe","date":1292268032,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n    \n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    boolean hasVectors = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      if (si.getHasVectors()) {\n        hasVectors = true;\n      }\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.hasVectors = hasVectors;\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null,\n                                 false);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name + \" mergeDocStores=\" + mergeDocStores);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n    \n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e06c9d5fba0a2f937941d199d64ccb32aac502d1","date":1292411167,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n    \n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    boolean hasVectors = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      if (si.getHasVectors()) {\n        hasVectors = true;\n      }\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null,\n                                 hasVectors);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name + \" mergeDocStores=\" + mergeDocStores);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n    \n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    boolean hasVectors = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      if (si.getHasVectors()) {\n        hasVectors = true;\n      }\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.hasVectors = hasVectors;\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null,\n                                 false);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name + \" mergeDocStores=\" + mergeDocStores);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean hasVectors = false;\n    for (SegmentInfo sourceSegment : merge.segments) {\n      if (sourceSegment.getHasVectors()) {\n        hasVectors = true;\n      }\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, hasVectors);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n    \n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    boolean hasVectors = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      if (si.getHasVectors()) {\n        hasVectors = true;\n      }\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null,\n                                 hasVectors);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name + \" mergeDocStores=\" + mergeDocStores);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f82bdb39e96d0f03d4e6482f4c835775856ccdef","date":1292757839,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean hasVectors = false;\n    for (SegmentInfo sourceSegment : merge.segments) {\n      if (sourceSegment.getHasVectors()) {\n        hasVectors = true;\n      }\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, hasVectors);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    // if a mergedSegmentWarmer is installed, we must merge\n    // the doc stores because we will open a full\n    // SegmentReader on the merged segment:\n    if (!mergeDocStores && mergedSegmentWarmer != null && currentDocStoreSegment != null && lastDocStoreSegment != null && lastDocStoreSegment.equals(currentDocStoreSegment)) {\n      mergeDocStores = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      updatePendingMerges(1, false);\n    }\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    merge.increfDone = true;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false,\n                                 false,\n                                 null);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30981e39c01d6738e774177bc068fa8827e11950","date":1296088978,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Lock order: IW -> BD\n    if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, merge.segments)) {\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c19f985e36a65cc969e8e564fe337a0d41512075","date":1296330536,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Lock order: IW -> BD\n    if (bufferedDeletes.applyDeletes(readerPool, segmentInfos, merge.segments)) {\n      checkpoint();\n    }\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","date":1297940445,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f1bdbf92da222965b46c0a942c3857ba56e5c638","date":1298297608,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0c7a8f7304b75b1528814c5820fa23a96816c27","date":1298314239,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, fieldInfos.newFieldInfosWithGlobalFieldNumberMap());\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, fieldInfos.newFieldInfosWithGlobalFieldNumberMap());\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14ec33385f6fbb6ce172882d14605790418a5d31","date":1298910796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, fieldInfos.newFieldInfosWithGlobalFieldNumberMap());\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos());\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e92442af786151ee55bc283eb472f629e3c7b52b","date":1301070252,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos());\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false);\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, fieldInfos.newFieldInfosWithGlobalFieldNumberMap());\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01e5948db9a07144112d2f08f28ca2e3cd880348","date":1301759232,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"327863a2fd61e831028b6c56c8fef6b00a44eb0b","date":1302686439,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c50bf8a3310d2aec44c01b0818c308b2e0ac6b33","date":1304363189,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fe2fc74577855eadfb5eae3153c2fffdaaf791","date":1305237079,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, false, null, false, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, globalFieldNumberMap.newFieldInfos(SegmentCodecsBuilder.create(codecs)));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        message(\"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      message(\"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes(true) * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"58c6bbc222f074c844e736e6fb23647e3db9cfe3","date":1322743940,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream != null) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream != null) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ce667c6d3400b22523701c549c0d35e26da8b46","date":1324405053,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n      }\n      if (readerPool != null) {\n        readerPool.drop(result.allDeleted);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert merge.maxNumSegments == -1 || merge.maxNumSegments > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    // TODO: is there any perf benefit to sorting\n    // merged segments?  eg biggest to smallest?\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));\n\n    // TODO: in the non-pool'd case this is somewhat\n    // wasteful, because we open these readers, close them,\n    // and then open them again for merging.  Maybe  we\n    // could pre-pool them somehow in that case...\n\n    // Lock order: IW -> BD\n    final BufferedDeletesStream.ApplyDeletesResult result = bufferedDeletesStream.applyDeletes(readerPool, merge.segments);\n\n    if (result.anyDeletes) {\n      checkpoint();\n    }\n\n    if (!keepFullyDeletedSegments && result.allDeleted != null) {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"drop 100% deleted segments: \" + result.allDeleted);\n      }\n      for(SegmentInfo info : result.allDeleted) {\n        segmentInfos.remove(info);\n        if (merge.segments.contains(info)) {\n          mergingSegments.remove(info);\n          merge.segments.remove(info);\n        }\n        readerPool.drop(info);\n      }\n      checkpoint();\n    }\n\n    merge.info.setBufferedDeletesGen(result.gen);\n\n    // Lock order: IW -> BD\n    bufferedDeletesStream.prune(segmentInfos);\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"mergeMaxNumSegments\", \"\"+merge.maxNumSegments);\n    details.put(\"mergeFactor\", Integer.toString(merge.segments.size()));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merge seg=\" + merge.info.name);\n    }\n\n    assert merge.estimatedMergeBytes == 0;\n    for(SegmentInfo info : merge.segments) {\n      if (info.docCount > 0) {\n        final int delCount = numDeletedDocs(info);\n        assert delCount <= info.docCount;\n        final double delRatio = ((double) delCount)/info.docCount;\n        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);\n      }\n    }\n\n    // TODO: I think this should no longer be needed (we\n    // now build CFS before adding segment to the infos);\n    // however, on removing it, tests fail for some reason!\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"d3fe2fc74577855eadfb5eae3153c2fffdaaf791":["c50bf8a3310d2aec44c01b0818c308b2e0ac6b33"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["29ef99d61cda9641b6250bf9567329a6e65f901d","e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["9ce667c6d3400b22523701c549c0d35e26da8b46"],"f82bdb39e96d0f03d4e6482f4c835775856ccdef":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["833a7987bc1c94455fde83e3311f72bddedcfb93","f82bdb39e96d0f03d4e6482f4c835775856ccdef"],"c19f985e36a65cc969e8e564fe337a0d41512075":["30981e39c01d6738e774177bc068fa8827e11950"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["e92442af786151ee55bc283eb472f629e3c7b52b"],"14ec33385f6fbb6ce172882d14605790418a5d31":["b0c7a8f7304b75b1528814c5820fa23a96816c27"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","c19f985e36a65cc969e8e564fe337a0d41512075"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"9ce667c6d3400b22523701c549c0d35e26da8b46":["58c6bbc222f074c844e736e6fb23647e3db9cfe3"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["e06c9d5fba0a2f937941d199d64ccb32aac502d1"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2a186ae8733084223c22044e935e4ef848a143d1":["619e84c4a25ac93018dce34af2bb41dafbeac829"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"327863a2fd61e831028b6c56c8fef6b00a44eb0b":["45669a651c970812a680841b97a77cce06af559f"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["6267e1ce56c2eec111425690cd04e251b6f14952"],"6267e1ce56c2eec111425690cd04e251b6f14952":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["f1bdbf92da222965b46c0a942c3857ba56e5c638","e92442af786151ee55bc283eb472f629e3c7b52b"],"a3776dccca01c11e7046323cfad46a3b4a471233":["e92442af786151ee55bc283eb472f629e3c7b52b","d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"619e84c4a25ac93018dce34af2bb41dafbeac829":["334c1175813aea771a71728cd2c4ee4754fd0603"],"30981e39c01d6738e774177bc068fa8827e11950":["f82bdb39e96d0f03d4e6482f4c835775856ccdef"],"58c6bbc222f074c844e736e6fb23647e3db9cfe3":["3cc749c053615f5871f3b95715fe292f34e70a53"],"8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5":["2a186ae8733084223c22044e935e4ef848a143d1"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["06584e6e98d592b34e1329b384182f368d2025e8"],"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3":["c19f985e36a65cc969e8e564fe337a0d41512075"],"e92442af786151ee55bc283eb472f629e3c7b52b":["1224a4027481acce15495b03bce9b48b93b42722"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["bde51b089eb7f86171eb3406e38a274743f9b7ac","e92442af786151ee55bc283eb472f629e3c7b52b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"c50bf8a3310d2aec44c01b0818c308b2e0ac6b33":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"1224a4027481acce15495b03bce9b48b93b42722":["14ec33385f6fbb6ce172882d14605790418a5d31"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["8fe956d65251358d755c56f14fe8380644790e47"],"e06c9d5fba0a2f937941d199d64ccb32aac502d1":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"c498d3f8d75170b121f5eda2c6210ac5beb5d411":["619e84c4a25ac93018dce34af2bb41dafbeac829","2a186ae8733084223c22044e935e4ef848a143d1"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"3cc749c053615f5871f3b95715fe292f34e70a53":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["01e5948db9a07144112d2f08f28ca2e3cd880348","327863a2fd61e831028b6c56c8fef6b00a44eb0b"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["3bb13258feba31ab676502787ab2e1779f129b7a","f82bdb39e96d0f03d4e6482f4c835775856ccdef"],"334c1175813aea771a71728cd2c4ee4754fd0603":["9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"7b91922b55d15444d554721b352861d028eb8278":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"8fe956d65251358d755c56f14fe8380644790e47":["6267e1ce56c2eec111425690cd04e251b6f14952"],"45669a651c970812a680841b97a77cce06af559f":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","01e5948db9a07144112d2f08f28ca2e3cd880348"],"3bb13258feba31ab676502787ab2e1779f129b7a":["c498d3f8d75170b121f5eda2c6210ac5beb5d411","8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"d3fe2fc74577855eadfb5eae3153c2fffdaaf791":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","a3776dccca01c11e7046323cfad46a3b4a471233","7b91922b55d15444d554721b352861d028eb8278"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f82bdb39e96d0f03d4e6482f4c835775856ccdef":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","30981e39c01d6738e774177bc068fa8827e11950","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"c19f985e36a65cc969e8e564fe337a0d41512075":["29ef99d61cda9641b6250bf9567329a6e65f901d","e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","45669a651c970812a680841b97a77cce06af559f"],"14ec33385f6fbb6ce172882d14605790418a5d31":["1224a4027481acce15495b03bce9b48b93b42722"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["f1bdbf92da222965b46c0a942c3857ba56e5c638"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":[],"9ce667c6d3400b22523701c549c0d35e26da8b46":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["f82bdb39e96d0f03d4e6482f4c835775856ccdef"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"2a186ae8733084223c22044e935e4ef848a143d1":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5","c498d3f8d75170b121f5eda2c6210ac5beb5d411"],"06584e6e98d592b34e1329b384182f368d2025e8":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"327863a2fd61e831028b6c56c8fef6b00a44eb0b":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["334c1175813aea771a71728cd2c4ee4754fd0603"],"6267e1ce56c2eec111425690cd04e251b6f14952":["9b832cbed6eb3d54a8bb9339296bdda8eeb53014","8fe956d65251358d755c56f14fe8380644790e47"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"619e84c4a25ac93018dce34af2bb41dafbeac829":["2a186ae8733084223c22044e935e4ef848a143d1","c498d3f8d75170b121f5eda2c6210ac5beb5d411"],"30981e39c01d6738e774177bc068fa8827e11950":["c19f985e36a65cc969e8e564fe337a0d41512075"],"58c6bbc222f074c844e736e6fb23647e3db9cfe3":["9ce667c6d3400b22523701c549c0d35e26da8b46"],"8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff","3bb13258feba31ab676502787ab2e1779f129b7a"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["6267e1ce56c2eec111425690cd04e251b6f14952"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["3cc749c053615f5871f3b95715fe292f34e70a53"],"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3":["f1bdbf92da222965b46c0a942c3857ba56e5c638","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"e92442af786151ee55bc283eb472f629e3c7b52b":["01e5948db9a07144112d2f08f28ca2e3cd880348","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233","c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["45669a651c970812a680841b97a77cce06af559f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["e06c9d5fba0a2f937941d199d64ccb32aac502d1"],"c50bf8a3310d2aec44c01b0818c308b2e0ac6b33":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"1224a4027481acce15495b03bce9b48b93b42722":["e92442af786151ee55bc283eb472f629e3c7b52b"],"e06c9d5fba0a2f937941d199d64ccb32aac502d1":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"c498d3f8d75170b121f5eda2c6210ac5beb5d411":["3bb13258feba31ab676502787ab2e1779f129b7a"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"3cc749c053615f5871f3b95715fe292f34e70a53":["58c6bbc222f074c844e736e6fb23647e3db9cfe3"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["c50bf8a3310d2aec44c01b0818c308b2e0ac6b33","135621f3a0670a9394eb563224a3b76cc4dddc0f"],"334c1175813aea771a71728cd2c4ee4754fd0603":["619e84c4a25ac93018dce34af2bb41dafbeac829"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["14ec33385f6fbb6ce172882d14605790418a5d31","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"8fe956d65251358d755c56f14fe8380644790e47":["833a7987bc1c94455fde83e3311f72bddedcfb93"],"45669a651c970812a680841b97a77cce06af559f":["327863a2fd61e831028b6c56c8fef6b00a44eb0b"],"3bb13258feba31ab676502787ab2e1779f129b7a":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}