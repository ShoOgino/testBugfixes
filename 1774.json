{"path":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","pathOld":"src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, IndexFileNames.NORMS_EXTENSION);\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, IndexFileNames.NORMS_EXTENSION);\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb10b6bcde550b87d8f10e5f010bd8f3021023b6","date":1274974592,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, IndexFileNames.NORMS_EXTENSION);\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6c18273ea5b3974d2f30117f46f1ae416c28f727","date":1279708040,"type":5,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[FieldInfo,InvertedDocEndConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        NormsWriterPerField toWrite = (NormsWriterPerField) fieldsToFlush.get(fieldInfo);\n\n        int upto = 0;\n        if (toWrite != null && toWrite.upto > 0) {\n          normCount++;\n\n          int docID = 0;\n          for (; docID < state.numDocs; docID++) {\n            if (upto < toWrite.upto && toWrite.docIDs[upto] == docID) {\n              normsOut.writeByte(toWrite.norms[upto]);\n              upto++;\n            } else {\n              normsOut.writeByte(defaultNorm);\n            }\n          }\n\n          // we should have consumed every norm\n          assert upto == toWrite.upto;\n\n          toWrite.reset();\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["e8099eba57cdbcce07a786d4f70916be3f02e365"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe","date":1294227869,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05","date":1294877328,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    if (!fieldInfos.hasNorms()) {\n      return;\n    }\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte((byte) 0);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    if (!fieldInfos.hasNorms()) {\n      return;\n    }\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte((byte) 0);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0c7a8f7304b75b1528814c5820fa23a96816c27","date":1298314239,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        List<NormsWriterPerField> toMerge = byField.get(fi);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte((byte) 0);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    if (!fieldInfos.hasNorms()) {\n      return;\n    }\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte((byte) 0);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14ec33385f6fbb6ce172882d14605790418a5d31","date":1298910796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    if (!fieldInfos.hasNorms()) {\n      return;\n    }\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte((byte) 0);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        List<NormsWriterPerField> toMerge = byField.get(fi);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte((byte) 0);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final List<NormsWriterPerField> toMerge = byField.get(fi);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte((byte) 0);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    if (!fieldInfos.hasNorms()) {\n      return;\n    }\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte((byte) 0);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final List<NormsWriterPerField> toMerge = byField.get(fi);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte((byte) 0);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    if (!fieldInfos.hasNorms()) {\n      return;\n    }\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List<NormsWriterPerField> toMerge = byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte((byte) 0);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":4,"author":"Uwe Schindler","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":null,"sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final List<NormsWriterPerField> toMerge = byField.get(fi);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte((byte) 0);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":null,"sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final List<NormsWriterPerField> toMerge = byField.get(fi);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte((byte) 0);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":4,"author":"Steven Rowe","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/NormsWriter#flush(Map[InvertedDocEndConsumerPerThread,Collection[InvertedDocEndConsumerPerField]],SegmentWriteState).mjava","sourceNew":null,"sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  @Override\n  public void flush(Map<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map<FieldInfo,List<NormsWriterPerField>> byField = new HashMap<FieldInfo,List<NormsWriterPerField>>();\n\n    if (!state.fieldInfos.hasNorms()) {\n      return;\n    }\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    for (final Map.Entry<InvertedDocEndConsumerPerThread,Collection<InvertedDocEndConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      final Collection<InvertedDocEndConsumerPerField> fields = entry.getValue();\n      final Iterator<InvertedDocEndConsumerPerField> fieldsIt = fields.iterator();\n\n      while (fieldsIt.hasNext()) {\n        final NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List<NormsWriterPerField> l = byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList<NormsWriterPerField>();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.NORMS_EXTENSION);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      int normCount = 0;\n\n      for (FieldInfo fi : state.fieldInfos) {\n        final List<NormsWriterPerField> toMerge = byField.get(fi);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte((byte) 0);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        } else if (fi.isIndexed && !fi.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte((byte) 0);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05":["fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"70ad682703b8585f5d0a637efec044d57ec05efb":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6","fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["1224a4027481acce15495b03bce9b48b93b42722","6c18273ea5b3974d2f30117f46f1ae416c28f727"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"14ec33385f6fbb6ce172882d14605790418a5d31":["b0c7a8f7304b75b1528814c5820fa23a96816c27"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["29ef99d61cda9641b6250bf9567329a6e65f901d","1224a4027481acce15495b03bce9b48b93b42722"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["7a54e23e03b47f3d568ab3020bdd386e4b2f0a05"],"a3776dccca01c11e7046323cfad46a3b4a471233":["1224a4027481acce15495b03bce9b48b93b42722","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["70ad682703b8585f5d0a637efec044d57ec05efb","7a54e23e03b47f3d568ab3020bdd386e4b2f0a05"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"1224a4027481acce15495b03bce9b48b93b42722":["14ec33385f6fbb6ce172882d14605790418a5d31"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05":["b0c7a8f7304b75b1528814c5820fa23a96816c27","29ef99d61cda9641b6250bf9567329a6e65f901d"],"70ad682703b8585f5d0a637efec044d57ec05efb":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["70ad682703b8585f5d0a637efec044d57ec05efb","6c18273ea5b3974d2f30117f46f1ae416c28f727","fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"14ec33385f6fbb6ce172882d14605790418a5d31":["1224a4027481acce15495b03bce9b48b93b42722"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["14ec33385f6fbb6ce172882d14605790418a5d31"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe":["7a54e23e03b47f3d568ab3020bdd386e4b2f0a05","70ad682703b8585f5d0a637efec044d57ec05efb"],"1224a4027481acce15495b03bce9b48b93b42722":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}