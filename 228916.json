{"path":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","commits":[{"id":"99761ac3b0c4136140f9cd2d081b80934bba16fa","date":1456263279,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (bytesPerDim != fieldInfo.getPointNumBytes()) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n        int bytesPerDim = fieldInfo.getPointNumBytes();\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        int[] hitCount = new int[1];\n        final TermIterator iterator = sortedPackedPoints.iterator();\n        byte[] pointBytes = new byte[bytesPerDim * numDims];\n\n        if (numDims == 1) {\n\n          final BytesRef scratch = new BytesRef();\n          scratch.length = bytesPerDim;\n\n          // Optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field,\n                           new IntersectVisitor() {\n\n                             private BytesRef nextQueryPoint = iterator.next();\n\n                             @Override\n                             public void grow(int count) {\n                               result.grow(count);\n                             }\n\n                             @Override\n                             public void visit(int docID) {\n                               hitCount[0]++;\n                               result.add(docID);\n                             }\n\n                             @Override\n                             public void visit(int docID, byte[] packedValue) {\n                               scratch.bytes = packedValue;\n                               while (nextQueryPoint != null) {\n                                 int cmp = nextQueryPoint.compareTo(scratch);\n                                 if (cmp == 0) {\n                                   // Query point equals index point, so collect and return\n                                   hitCount[0]++;\n                                   result.add(docID);\n                                   break;\n                                 } else if (cmp < 0) {\n                                   // Query point is before index point, so we move to next query point\n                                   nextQueryPoint = iterator.next();\n                                 } else {\n                                   // Query point is after index point, so we don't collect and we return:\n                                   break;\n                                 }\n                               }\n                             }\n\n                             @Override\n                             public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n\n                               while (nextQueryPoint != null) {\n                                 scratch.bytes = minPackedValue;\n                                 int cmpMin = nextQueryPoint.compareTo(scratch);\n                                 if (cmpMin < 0) {\n                                   // query point is before the start of this cell\n                                   nextQueryPoint = iterator.next();\n                                   continue;\n                                 }\n                                 scratch.bytes = maxPackedValue;\n                                 int cmpMax = nextQueryPoint.compareTo(scratch);\n                                 if (cmpMax > 0) {\n                                   // query point is after the end of this cell\n                                   return Relation.CELL_OUTSIDE_QUERY;\n                                 }\n\n                                 if (cmpMin == 0 && cmpMax == 0) {\n                                   // NOTE: we only hit this if we are on a cell whose min and max values are exactly equal to our point,\n                                   // which can easily happen if many (> 1024) docs share this one value\n                                   return Relation.CELL_INSIDE_QUERY;\n                                 } else {\n                                   return Relation.CELL_CROSSES_QUERY;\n                                 }\n                               }\n\n                               // We exhausted all points in the query:\n                               return Relation.CELL_OUTSIDE_QUERY;\n                             }\n                           });\n        } else {\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            // nocommit make sure a test tests this:\n            assert point.length == pointBytes.length;\n            System.arraycopy(point.bytes, point.offset, pointBytes, 0, pointBytes.length);\n\n            final BytesRef finalPoint = point;\n\n            values.intersect(field,\n                             // nocommit don't make new instance of this for each point?\n                             new IntersectVisitor() {\n\n                               @Override\n                               public void grow(int count) {\n                                 result.grow(count);\n                               }\n\n                               @Override\n                               public void visit(int docID) {\n                                 hitCount[0]++;\n                                 result.add(docID);\n                               }\n\n                               @Override\n                               public void visit(int docID, byte[] packedValue) {\n                                 assert packedValue.length == finalPoint.length;\n                                 if (Arrays.equals(packedValue, pointBytes)) {\n                                   // The point for this doc matches the point we are querying on\n                                   hitCount[0]++;\n                                   result.add(docID);\n                                 }\n                               }\n\n                               @Override\n                               public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n\n                                 boolean crosses = false;\n\n                                 for(int dim=0;dim<numDims;dim++) {\n                                   int offset = dim*bytesPerDim;\n\n                                   int cmpMin = StringHelper.compare(bytesPerDim, minPackedValue, offset, pointBytes, offset);\n                                   if (cmpMin > 0) {\n                                     return Relation.CELL_OUTSIDE_QUERY;\n                                   }\n\n                                   int cmpMax = StringHelper.compare(bytesPerDim, maxPackedValue, offset, pointBytes, offset);\n                                   if (cmpMax < 0) {\n                                     return Relation.CELL_OUTSIDE_QUERY;\n                                   }\n\n                                   if (cmpMin != 0 || cmpMax != 0) {\n                                     crosses = true;\n                                   }\n                                 }\n\n                                 if (crosses) {\n                                   return Relation.CELL_CROSSES_QUERY;\n                                 } else {\n                                   // nocommit make sure tests hit this case:\n                                   // NOTE: we only hit this if we are on a cell whose min and max values are exactly equal to our point,\n                                   // which can easily happen if many docs share this one value\n                                   return Relation.CELL_INSIDE_QUERY;\n                                 }\n                               }\n                             });\n          }\n        }\n\n        // NOTE: hitCount[0] will be over-estimate in multi-valued case\n        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["5839bca64b33c24668e37476ee168d00dc0bb96d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ee299c4e4c019174aa433f564b5de03a7a40e00d","date":1456264115,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (bytesPerDim != fieldInfo.getPointNumBytes()) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n        int bytesPerDim = fieldInfo.getPointNumBytes();\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        int[] hitCount = new int[1];\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints.iterator(), hitCount, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(hitCount, result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        // NOTE: hitCount[0] will be over-estimate in multi-valued case\n        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (bytesPerDim != fieldInfo.getPointNumBytes()) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n        int bytesPerDim = fieldInfo.getPointNumBytes();\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        int[] hitCount = new int[1];\n        final TermIterator iterator = sortedPackedPoints.iterator();\n        byte[] pointBytes = new byte[bytesPerDim * numDims];\n\n        if (numDims == 1) {\n\n          final BytesRef scratch = new BytesRef();\n          scratch.length = bytesPerDim;\n\n          // Optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field,\n                           new IntersectVisitor() {\n\n                             private BytesRef nextQueryPoint = iterator.next();\n\n                             @Override\n                             public void grow(int count) {\n                               result.grow(count);\n                             }\n\n                             @Override\n                             public void visit(int docID) {\n                               hitCount[0]++;\n                               result.add(docID);\n                             }\n\n                             @Override\n                             public void visit(int docID, byte[] packedValue) {\n                               scratch.bytes = packedValue;\n                               while (nextQueryPoint != null) {\n                                 int cmp = nextQueryPoint.compareTo(scratch);\n                                 if (cmp == 0) {\n                                   // Query point equals index point, so collect and return\n                                   hitCount[0]++;\n                                   result.add(docID);\n                                   break;\n                                 } else if (cmp < 0) {\n                                   // Query point is before index point, so we move to next query point\n                                   nextQueryPoint = iterator.next();\n                                 } else {\n                                   // Query point is after index point, so we don't collect and we return:\n                                   break;\n                                 }\n                               }\n                             }\n\n                             @Override\n                             public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n\n                               while (nextQueryPoint != null) {\n                                 scratch.bytes = minPackedValue;\n                                 int cmpMin = nextQueryPoint.compareTo(scratch);\n                                 if (cmpMin < 0) {\n                                   // query point is before the start of this cell\n                                   nextQueryPoint = iterator.next();\n                                   continue;\n                                 }\n                                 scratch.bytes = maxPackedValue;\n                                 int cmpMax = nextQueryPoint.compareTo(scratch);\n                                 if (cmpMax > 0) {\n                                   // query point is after the end of this cell\n                                   return Relation.CELL_OUTSIDE_QUERY;\n                                 }\n\n                                 if (cmpMin == 0 && cmpMax == 0) {\n                                   // NOTE: we only hit this if we are on a cell whose min and max values are exactly equal to our point,\n                                   // which can easily happen if many (> 1024) docs share this one value\n                                   return Relation.CELL_INSIDE_QUERY;\n                                 } else {\n                                   return Relation.CELL_CROSSES_QUERY;\n                                 }\n                               }\n\n                               // We exhausted all points in the query:\n                               return Relation.CELL_OUTSIDE_QUERY;\n                             }\n                           });\n        } else {\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            // nocommit make sure a test tests this:\n            assert point.length == pointBytes.length;\n            System.arraycopy(point.bytes, point.offset, pointBytes, 0, pointBytes.length);\n\n            final BytesRef finalPoint = point;\n\n            values.intersect(field,\n                             // nocommit don't make new instance of this for each point?\n                             new IntersectVisitor() {\n\n                               @Override\n                               public void grow(int count) {\n                                 result.grow(count);\n                               }\n\n                               @Override\n                               public void visit(int docID) {\n                                 hitCount[0]++;\n                                 result.add(docID);\n                               }\n\n                               @Override\n                               public void visit(int docID, byte[] packedValue) {\n                                 assert packedValue.length == finalPoint.length;\n                                 if (Arrays.equals(packedValue, pointBytes)) {\n                                   // The point for this doc matches the point we are querying on\n                                   hitCount[0]++;\n                                   result.add(docID);\n                                 }\n                               }\n\n                               @Override\n                               public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n\n                                 boolean crosses = false;\n\n                                 for(int dim=0;dim<numDims;dim++) {\n                                   int offset = dim*bytesPerDim;\n\n                                   int cmpMin = StringHelper.compare(bytesPerDim, minPackedValue, offset, pointBytes, offset);\n                                   if (cmpMin > 0) {\n                                     return Relation.CELL_OUTSIDE_QUERY;\n                                   }\n\n                                   int cmpMax = StringHelper.compare(bytesPerDim, maxPackedValue, offset, pointBytes, offset);\n                                   if (cmpMax < 0) {\n                                     return Relation.CELL_OUTSIDE_QUERY;\n                                   }\n\n                                   if (cmpMin != 0 || cmpMax != 0) {\n                                     crosses = true;\n                                   }\n                                 }\n\n                                 if (crosses) {\n                                   return Relation.CELL_CROSSES_QUERY;\n                                 } else {\n                                   // nocommit make sure tests hit this case:\n                                   // NOTE: we only hit this if we are on a cell whose min and max values are exactly equal to our point,\n                                   // which can easily happen if many docs share this one value\n                                   return Relation.CELL_INSIDE_QUERY;\n                                 }\n                               }\n                             });\n          }\n        }\n\n        // NOTE: hitCount[0] will be over-estimate in multi-valued case\n        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e460f059c59ca6be827d6de9a0e26f526b9414c0","date":1456270863,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n        int bytesPerDim = fieldInfo.getPointNumBytes();\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        int[] hitCount = new int[1];\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints.iterator(), hitCount, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(hitCount, result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        // NOTE: hitCount[0] will be over-estimate in multi-valued case\n        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (bytesPerDim != fieldInfo.getPointNumBytes()) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n        int bytesPerDim = fieldInfo.getPointNumBytes();\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        int[] hitCount = new int[1];\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints.iterator(), hitCount, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(hitCount, result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        // NOTE: hitCount[0] will be over-estimate in multi-valued case\n        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f429037bedccb27766e77f5e772b291a8d1bc97c","date":1456353956,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        int[] hitCount = new int[1];\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints.iterator(), hitCount, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(hitCount, result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        // NOTE: hitCount[0] will be over-estimate in multi-valued case\n        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n        int bytesPerDim = fieldInfo.getPointNumBytes();\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        int[] hitCount = new int[1];\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints.iterator(), hitCount, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(hitCount, result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        // NOTE: hitCount[0] will be over-estimate in multi-valued case\n        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"868f63baffb79407d4b49f3c0be8dc4725ffc6e1","date":1456423715,"type":0,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        int[] hitCount = new int[1];\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints.iterator(), hitCount, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(hitCount, result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        // NOTE: hitCount[0] will be over-estimate in multi-valued case\n        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c18695b5c27b72d1fdc966eee5e01d07a81b5c52","date":1456532984,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        int[] hitCount = new int[1];\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, hitCount, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(hitCount, result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        // NOTE: hitCount[0] will be over-estimate in multi-valued case\n        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        int[] hitCount = new int[1];\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints.iterator(), hitCount, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(hitCount, result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        // NOTE: hitCount[0] will be over-estimate in multi-valued case\n        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2b63007489248c99b5cdc766ce55938891f5d969","date":1456737032,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        int[] hitCount = new int[1];\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, hitCount, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(hitCount, result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        // NOTE: hitCount[0] will be over-estimate in multi-valued case\n        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"af2638813028b254a88b418ebeafb541afb49653","date":1456804822,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        int[] hitCount = new int[1];\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints.iterator(), hitCount, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(hitCount, result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        // NOTE: hitCount[0] will be over-estimate in multi-valued case\n        return new ConstantScoreScorer(this, score(), result.build(hitCount[0]).iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5839bca64b33c24668e37476ee168d00dc0bb96d","date":1457536035,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","bugFix":["99761ac3b0c4136140f9cd2d081b80934bba16fa"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9ee4c03e3ee986704eeeb45c571d001905a6430","date":1462194267,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30540ec27130887a9372c159e8fe971200f37727","date":1462223109,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55b50463286869f584cf849d1587a0fcd54d1dfa","date":1462378517,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc());\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"02e175abd2c4c1611c5a9647486ae8ba249a94c1","date":1468327116,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/search/PointInSetQuery#createWeight(IndexSearcher,boolean).mjava","sourceNew":null,"sourceOld":"  @Override\n  public final Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n\n    // We don't use RandomAccessWeight here: it's no good to approximate with \"match all docs\".\n    // This is an inverted structure and should be used in the first pass:\n\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        LeafReader reader = context.reader();\n        PointValues values = reader.getPointValues();\n        if (values == null) {\n          // No docs in this segment indexed any points\n          return null;\n        }\n        FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);\n        if (fieldInfo == null) {\n          // No docs in this segment indexed this field at all\n          return null;\n        }\n        if (fieldInfo.getPointDimensionCount() != numDims) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with numDims=\" + fieldInfo.getPointDimensionCount() + \" but this query has numDims=\" + numDims);\n        }\n        if (fieldInfo.getPointNumBytes() != bytesPerDim) {\n          throw new IllegalArgumentException(\"field=\\\"\" + field + \"\\\" was indexed with bytesPerDim=\" + fieldInfo.getPointNumBytes() + \" but this query has bytesPerDim=\" + bytesPerDim);\n        }\n\n        DocIdSetBuilder result = new DocIdSetBuilder(reader.maxDoc(), values, field);\n\n        if (numDims == 1) {\n\n          // We optimize this common case, effectively doing a merge sort of the indexed values vs the queried set:\n          values.intersect(field, new MergePointVisitor(sortedPackedPoints, result));\n\n        } else {\n          // NOTE: this is naive implementation, where for each point we re-walk the KD tree to intersect.  We could instead do a similar\n          // optimization as the 1D case, but I think it'd mean building a query-time KD tree so we could efficiently intersect against the\n          // index, which is probably tricky!\n          SinglePointVisitor visitor = new SinglePointVisitor(result);\n          TermIterator iterator = sortedPackedPoints.iterator();\n          for (BytesRef point = iterator.next(); point != null; point = iterator.next()) {\n            visitor.setPoint(point);\n            values.intersect(field, visitor);\n          }\n        }\n\n        return new ConstantScoreScorer(this, score(), result.build().iterator());\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"af2638813028b254a88b418ebeafb541afb49653":["868f63baffb79407d4b49f3c0be8dc4725ffc6e1","2b63007489248c99b5cdc766ce55938891f5d969"],"e460f059c59ca6be827d6de9a0e26f526b9414c0":["ee299c4e4c019174aa433f564b5de03a7a40e00d"],"ee299c4e4c019174aa433f564b5de03a7a40e00d":["99761ac3b0c4136140f9cd2d081b80934bba16fa"],"5839bca64b33c24668e37476ee168d00dc0bb96d":["af2638813028b254a88b418ebeafb541afb49653"],"30540ec27130887a9372c159e8fe971200f37727":["5839bca64b33c24668e37476ee168d00dc0bb96d","c9ee4c03e3ee986704eeeb45c571d001905a6430"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["55b50463286869f584cf849d1587a0fcd54d1dfa","02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"55b50463286869f584cf849d1587a0fcd54d1dfa":["5839bca64b33c24668e37476ee168d00dc0bb96d","30540ec27130887a9372c159e8fe971200f37727"],"c9ee4c03e3ee986704eeeb45c571d001905a6430":["5839bca64b33c24668e37476ee168d00dc0bb96d"],"f429037bedccb27766e77f5e772b291a8d1bc97c":["e460f059c59ca6be827d6de9a0e26f526b9414c0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"868f63baffb79407d4b49f3c0be8dc4725ffc6e1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f429037bedccb27766e77f5e772b291a8d1bc97c"],"2b63007489248c99b5cdc766ce55938891f5d969":["c18695b5c27b72d1fdc966eee5e01d07a81b5c52"],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["30540ec27130887a9372c159e8fe971200f37727"],"99761ac3b0c4136140f9cd2d081b80934bba16fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c18695b5c27b72d1fdc966eee5e01d07a81b5c52":["868f63baffb79407d4b49f3c0be8dc4725ffc6e1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["02e175abd2c4c1611c5a9647486ae8ba249a94c1"]},"commit2Childs":{"af2638813028b254a88b418ebeafb541afb49653":["5839bca64b33c24668e37476ee168d00dc0bb96d"],"e460f059c59ca6be827d6de9a0e26f526b9414c0":["f429037bedccb27766e77f5e772b291a8d1bc97c"],"ee299c4e4c019174aa433f564b5de03a7a40e00d":["e460f059c59ca6be827d6de9a0e26f526b9414c0"],"5839bca64b33c24668e37476ee168d00dc0bb96d":["30540ec27130887a9372c159e8fe971200f37727","55b50463286869f584cf849d1587a0fcd54d1dfa","c9ee4c03e3ee986704eeeb45c571d001905a6430"],"30540ec27130887a9372c159e8fe971200f37727":["55b50463286869f584cf849d1587a0fcd54d1dfa","02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"55b50463286869f584cf849d1587a0fcd54d1dfa":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"c9ee4c03e3ee986704eeeb45c571d001905a6430":["30540ec27130887a9372c159e8fe971200f37727"],"f429037bedccb27766e77f5e772b291a8d1bc97c":["868f63baffb79407d4b49f3c0be8dc4725ffc6e1"],"868f63baffb79407d4b49f3c0be8dc4725ffc6e1":["af2638813028b254a88b418ebeafb541afb49653","c18695b5c27b72d1fdc966eee5e01d07a81b5c52"],"2b63007489248c99b5cdc766ce55938891f5d969":["af2638813028b254a88b418ebeafb541afb49653"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["868f63baffb79407d4b49f3c0be8dc4725ffc6e1","99761ac3b0c4136140f9cd2d081b80934bba16fa"],"99761ac3b0c4136140f9cd2d081b80934bba16fa":["ee299c4e4c019174aa433f564b5de03a7a40e00d"],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c18695b5c27b72d1fdc966eee5e01d07a81b5c52":["2b63007489248c99b5cdc766ce55938891f5d969"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}