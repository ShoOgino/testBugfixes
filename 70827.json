{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"38061899d760e06a12fe186bc1f09ca9ff0e64a6","date":1376491296,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff4227bb146f97aabae888091c19e48c88dbb0db","date":1406758576,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5cdab62f058ea765dd33deb05b4f19b7d626c801","date":1406803479,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"379db3ad24c4f0214f30a122265a6d6be003a99d","date":1407537768,"type":5,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    super(delegate.getReuseStrategy());\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"ff4227bb146f97aabae888091c19e48c88dbb0db":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["38061899d760e06a12fe186bc1f09ca9ff0e64a6"],"38061899d760e06a12fe186bc1f09ca9ff0e64a6":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["b89678825b68eccaf09e6ab71675fc0b0af1e099","38061899d760e06a12fe186bc1f09ca9ff0e64a6"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["ff4227bb146f97aabae888091c19e48c88dbb0db"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["379db3ad24c4f0214f30a122265a6d6be003a99d"]},"commit2Childs":{"ff4227bb146f97aabae888091c19e48c88dbb0db":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ff4227bb146f97aabae888091c19e48c88dbb0db"],"38061899d760e06a12fe186bc1f09ca9ff0e64a6":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["38061899d760e06a12fe186bc1f09ca9ff0e64a6","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}