{"path":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb10b6bcde550b87d8f10e5f010bd8f3021023b6","date":1274974592,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"406e7055a3e99d3fa6ce49a555a51dd18b321806","date":1282520243,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            final Bits delDocs = MultiFields.getDeletedDocs(reader);\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":["bcc1e326920283403c98c028a51787e36e348983"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f367dfb9086b92a13c77e2d31112c715cd4502c","date":1290190924,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    final BytesRef normBuffer = new BytesRef();\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            normBuffer.grow(maxDoc);\n            final byte[] norms = normBuffer.bytes;\n            reader.norms(fi.name, norms, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(norms, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(norms[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            final Bits delDocs = MultiFields.getDeletedDocs(reader);\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f0c367848b77b4804089201619add2663800223c","date":1290208739,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    // get needed buffer size by finding the largest segment\n    int bufferSize = 0;\n    for (IndexReader reader : readers) {\n      bufferSize = Math.max(bufferSize, reader.maxDoc());\n    }\n    \n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          if (normBuffer == null) {\n            normBuffer = new byte[bufferSize];\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    final BytesRef normBuffer = new BytesRef();\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            normBuffer.grow(maxDoc);\n            final byte[] norms = normBuffer.bytes;\n            reader.norms(fi.name, norms, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(norms, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(norms[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    // get needed buffer size by finding the largest segment\n    int bufferSize = 0;\n    for (IndexReader reader : readers) {\n      bufferSize = Math.max(bufferSize, reader.maxDoc());\n    }\n    \n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          if (normBuffer == null) {\n            normBuffer = new byte[bufferSize];\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            final Bits delDocs = MultiFields.getDeletedDocs(reader);\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    // get needed buffer size by finding the largest segment\n    int bufferSize = 0;\n    for (IndexReader reader : readers) {\n      bufferSize = Math.max(bufferSize, reader.maxDoc());\n    }\n\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          if (normBuffer == null) {\n            normBuffer = new byte[bufferSize];\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) {\n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      int numFieldInfos = fieldInfos.size();\n      for (int i = 0; i < numFieldInfos; i++) {\n        FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for ( IndexReader reader : readers) {\n            int maxDoc = reader.maxDoc();\n            if (normBuffer == null || normBuffer.length < maxDoc) {\n              // the buffer is too small for the current segment\n              normBuffer = new byte[maxDoc];\n            }\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              for (int k = 0; k < maxDoc; k++) {\n                if (!reader.isDeleted(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05","date":1294877328,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    // get needed buffer size by finding the largest segment\n    int bufferSize = 0;\n    for (IndexReader reader : readers) {\n      bufferSize = Math.max(bufferSize, reader.maxDoc());\n    }\n    \n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          if (normBuffer == null) {\n            normBuffer = new byte[bufferSize];\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) {\n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    // get needed buffer size by finding the largest segment\n    int bufferSize = 0;\n    for (IndexReader reader : readers) {\n      bufferSize = Math.max(bufferSize, reader.maxDoc());\n    }\n\n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          if (normBuffer == null) {\n            normBuffer = new byte[bufferSize];\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) {\n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    // get needed buffer size by finding the largest segment\n    int bufferSize = 0;\n    for (IndexReader reader : readers) {\n      bufferSize = Math.max(bufferSize, reader.maxDoc());\n    }\n    \n    byte[] normBuffer = null;\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          if (normBuffer == null) {\n            normBuffer = new byte[bufferSize];\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            reader.norms(fi.name, normBuffer, 0);\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0c7a8f7304b75b1528814c5820fa23a96816c27","date":1298314239,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) {\n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) {\n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14ec33385f6fbb6ce172882d14605790418a5d31","date":1298910796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (int i = 0, numFieldInfos = fieldInfos.size(); i < numFieldInfos; i++) {\n        final FieldInfo fi = fieldInfos.fieldInfo(i);\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) {\n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) {\n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) {\n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) { \n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) { \n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8b76d8c77c9c069618078344054ff4c6a3374f80","date":1306168582,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) {\n        output.close();\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) {\n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"efb7a19703a037c29e30440260d393500febc1f4","date":1306648116,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) {\n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) {\n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(NORMS_HEADER,NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n    } finally {\n      if (output != null) {\n        output.close();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            //nocommit Putting MERGE context here would lead to assert error. What should MergeInfo be initialized with here?\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), new IOContext(Context.DEFAULT));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6f9be74ca7baaef11857ad002cad40419979516","date":1309449808,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            //nocommit Putting MERGE context here would lead to assert error. What should MergeInfo be initialized with here?\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), new IOContext(Context.DEFAULT));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.getLiveDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.getLiveDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.getLiveDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits delDocs = reader.getDeletedDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (!delDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.getLiveDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.getLiveDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.getLiveDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION));\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.getLiveDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0061262413ecc163d6eebba1b5c43ab91a0c2dc5","date":1311195279,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (MergeState.IndexReaderAndLiveDocs reader : readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte normBuffer[] = reader.reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (IndexReader reader : readers) {\n            final int maxDoc = reader.maxDoc();\n            byte normBuffer[] = reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (!reader.hasDeletions()) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.getLiveDocs();\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24230fe54121f9be9d85f2c2067536296785e421","date":1314462346,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (MergeState.IndexReaderAndLiveDocs reader : readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte normBuffer[] = reader.reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(output);\n      } else {\n        IOUtils.closeWhileHandlingException(output);\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (MergeState.IndexReaderAndLiveDocs reader : readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte normBuffer[] = reader.reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      IOUtils.closeSafely(!success, output);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte normBuffer[] = reader.reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(output);\n      } else {\n        IOUtils.closeWhileHandlingException(output);\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (MergeState.IndexReaderAndLiveDocs reader : readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte normBuffer[] = reader.reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(output);\n      } else {\n        IOUtils.closeWhileHandlingException(output);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"143d45d6fe43d56d1f541059577c929fc0ad27a1","date":1323026648,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":"  // TODO: implement merge in normsformat instead.\n  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(Lucene40NormsWriter.NORMS_HEADER, Lucene40NormsWriter.NORMS_HEADER.length);\n          }\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte normBuffer[] = reader.reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(output);\n      } else {\n        IOUtils.closeWhileHandlingException(output);\n      }\n    }\n  }\n\n","sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte normBuffer[] = reader.reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(output);\n      } else {\n        IOUtils.closeWhileHandlingException(output);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6ee314d4978b896d2a804ee60ba6e830624d990","date":1323044870,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":null,"sourceOld":"  // TODO: implement merge in normsformat instead.\n  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(Lucene40NormsWriter.NORMS_HEADER, Lucene40NormsWriter.NORMS_HEADER.length);\n          }\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte normBuffer[] = reader.reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(output);\n      } else {\n        IOUtils.closeWhileHandlingException(output);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":4,"author":"Uwe Schindler","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":null,"sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte normBuffer[] = reader.reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(output);\n      } else {\n        IOUtils.closeWhileHandlingException(output);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#mergeNorms().mjava","sourceNew":null,"sourceOld":"  private void mergeNorms() throws IOException {\n    IndexOutput output = null;\n    boolean success = false;\n    try {\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.isIndexed && !fi.omitNorms) {\n          if (output == null) {\n            output = directory.createOutput(IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.NORMS_EXTENSION), context);\n            output.writeBytes(SegmentNorms.NORMS_HEADER, SegmentNorms.NORMS_HEADER.length);\n          }\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte normBuffer[] = reader.reader.norms(fi.name);\n            if (normBuffer == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n        }\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(output);\n      } else {\n        IOUtils.closeWhileHandlingException(output);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"24230fe54121f9be9d85f2c2067536296785e421":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5"],"8b76d8c77c9c069618078344054ff4c6a3374f80":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["06584e6e98d592b34e1329b384182f368d2025e8","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"efb7a19703a037c29e30440260d393500febc1f4":["8b76d8c77c9c069618078344054ff4c6a3374f80"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["efb7a19703a037c29e30440260d393500febc1f4"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["efb7a19703a037c29e30440260d393500febc1f4"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6","f0c367848b77b4804089201619add2663800223c"],"14ec33385f6fbb6ce172882d14605790418a5d31":["b0c7a8f7304b75b1528814c5820fa23a96816c27"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a3776dccca01c11e7046323cfad46a3b4a471233","efb7a19703a037c29e30440260d393500febc1f4"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["3bb13258feba31ab676502787ab2e1779f129b7a","7a54e23e03b47f3d568ab3020bdd386e4b2f0a05"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b6f9be74ca7baaef11857ad002cad40419979516":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["817d8435e9135b756f08ce6710ab0baac51bdf88","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"406e7055a3e99d3fa6ce49a555a51dd18b321806":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["868da859b43505d9d2a023bfeae6dd0c795f5295","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"1224a4027481acce15495b03bce9b48b93b42722":["14ec33385f6fbb6ce172882d14605790418a5d31"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["135621f3a0670a9394eb563224a3b76cc4dddc0f","efb7a19703a037c29e30440260d393500febc1f4"],"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05":["f0c367848b77b4804089201619add2663800223c"],"06584e6e98d592b34e1329b384182f368d2025e8":["24230fe54121f9be9d85f2c2067536296785e421"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["1224a4027481acce15495b03bce9b48b93b42722","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"f6ee314d4978b896d2a804ee60ba6e830624d990":["143d45d6fe43d56d1f541059577c929fc0ad27a1"],"7f367dfb9086b92a13c77e2d31112c715cd4502c":["406e7055a3e99d3fa6ce49a555a51dd18b321806"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["b6f9be74ca7baaef11857ad002cad40419979516","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"143d45d6fe43d56d1f541059577c929fc0ad27a1":["06584e6e98d592b34e1329b384182f368d2025e8"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["29ef99d61cda9641b6250bf9567329a6e65f901d","1224a4027481acce15495b03bce9b48b93b42722"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["7a54e23e03b47f3d568ab3020bdd386e4b2f0a05"],"a3776dccca01c11e7046323cfad46a3b4a471233":["1224a4027481acce15495b03bce9b48b93b42722","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"f0c367848b77b4804089201619add2663800223c":["7f367dfb9086b92a13c77e2d31112c715cd4502c"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["06584e6e98d592b34e1329b384182f368d2025e8","f6ee314d4978b896d2a804ee60ba6e830624d990"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","7a54e23e03b47f3d568ab3020bdd386e4b2f0a05"],"3bb13258feba31ab676502787ab2e1779f129b7a":["406e7055a3e99d3fa6ce49a555a51dd18b321806","f0c367848b77b4804089201619add2663800223c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3615ce4a1f785ae1b779244de52c6a7d99227e60"]},"commit2Childs":{"24230fe54121f9be9d85f2c2067536296785e421":["06584e6e98d592b34e1329b384182f368d2025e8"],"8b76d8c77c9c069618078344054ff4c6a3374f80":["efb7a19703a037c29e30440260d393500febc1f4"],"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["24230fe54121f9be9d85f2c2067536296785e421"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":[],"efb7a19703a037c29e30440260d393500febc1f4":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","639c36565ce03aed5b0fce7c9e4448e53a1f7efd","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["ddc4c914be86e34b54f70023f45a60fa7f04e929","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["b6f9be74ca7baaef11857ad002cad40419979516"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"14ec33385f6fbb6ce172882d14605790418a5d31":["1224a4027481acce15495b03bce9b48b93b42722"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"b6f9be74ca7baaef11857ad002cad40419979516":["d083e83f225b11e5fdd900e83d26ddb385b6955c"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5","5d004d0e0b3f65bb40da76d476d659d7888270e8"],"406e7055a3e99d3fa6ce49a555a51dd18b321806":["7f367dfb9086b92a13c77e2d31112c715cd4502c","3bb13258feba31ab676502787ab2e1779f129b7a"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"1224a4027481acce15495b03bce9b48b93b42722":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[],"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05":["29ef99d61cda9641b6250bf9567329a6e65f901d","b0c7a8f7304b75b1528814c5820fa23a96816c27","868da859b43505d9d2a023bfeae6dd0c795f5295"],"06584e6e98d592b34e1329b384182f368d2025e8":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","143d45d6fe43d56d1f541059577c929fc0ad27a1","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["2e10cb22a8bdb44339e282925a29182bb2f3174d"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["8b76d8c77c9c069618078344054ff4c6a3374f80","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233"],"f6ee314d4978b896d2a804ee60ba6e830624d990":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"7f367dfb9086b92a13c77e2d31112c715cd4502c":["f0c367848b77b4804089201619add2663800223c"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","406e7055a3e99d3fa6ce49a555a51dd18b321806"],"143d45d6fe43d56d1f541059577c929fc0ad27a1":["f6ee314d4978b896d2a804ee60ba6e830624d990"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["14ec33385f6fbb6ce172882d14605790418a5d31","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"a3776dccca01c11e7046323cfad46a3b4a471233":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"f0c367848b77b4804089201619add2663800223c":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","7a54e23e03b47f3d568ab3020bdd386e4b2f0a05","3bb13258feba31ab676502787ab2e1779f129b7a"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"3bb13258feba31ab676502787ab2e1779f129b7a":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","5d004d0e0b3f65bb40da76d476d659d7888270e8","2e10cb22a8bdb44339e282925a29182bb2f3174d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}