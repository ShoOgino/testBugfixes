{"path":"lucene/core/src/java/org/apache/lucene/codecs/SimpleDVConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","commits":[{"id":"1ed9002c5afac843c7f2d04d88e74b40d627e1af","date":1357602069,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/SimpleDVConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState,List[SortedDocValues]).mjava","sourceNew":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n          if (fixedLength == -2) {\n            fixedLength = lastTerm.length;\n          } else {\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a49d69caa1c0ec8407b5b9138c7f8fb8919c3ff2","date":1358199580,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/SimpleDVConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/SimpleDVConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","sourceNew":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b8acf0807ca5f38beda8e0f7d5ab46ff39f81200","date":1358521790,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/SimpleDVConsumer.SortedBytesMerger#merge(MergeState,List[SortedDocValues]).mjava","sourceNew":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          assert liveDocs != null;\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a49d69caa1c0ec8407b5b9138c7f8fb8919c3ff2":["1ed9002c5afac843c7f2d04d88e74b40d627e1af"],"b8acf0807ca5f38beda8e0f7d5ab46ff39f81200":["a49d69caa1c0ec8407b5b9138c7f8fb8919c3ff2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1ed9002c5afac843c7f2d04d88e74b40d627e1af":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a49d69caa1c0ec8407b5b9138c7f8fb8919c3ff2":["b8acf0807ca5f38beda8e0f7d5ab46ff39f81200"],"b8acf0807ca5f38beda8e0f7d5ab46ff39f81200":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1ed9002c5afac843c7f2d04d88e74b40d627e1af","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1ed9002c5afac843c7f2d04d88e74b40d627e1af":["a49d69caa1c0ec8407b5b9138c7f8fb8919c3ff2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b8acf0807ca5f38beda8e0f7d5ab46ff39f81200","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}