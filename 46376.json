{"path":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","commits":[{"id":"4a7c13535572b8e97cc477fc3388a57321a7751a","date":1427500960,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","pathOld":"/dev/null","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collect(intersection, tt.termNum - startTermIndex);\n          processor.countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (processor.allBucketsSlot >= 0) {\n            processor.collect(intersection, processor.allBucketsSlot);\n            processor.countAcc.incrementCount(processor.allBucketsSlot, collected);\n          }\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReader(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            processor.countAcc.incrementCount(arrIdx, 1);\n            processor.collect(segDoc, arrIdx);\n            if (processor.allBucketsSlot >= 0) {\n              processor.countAcc.incrementCount(processor.allBucketsSlot, 1);\n              processor.collect(segDoc, processor.allBucketsSlot);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx < 0) continue;\n              if (arrIdx >= nTerms) break;\n              processor.countAcc.incrementCount(arrIdx, 1);\n              processor.collect(segDoc, arrIdx);\n              if (processor.allBucketsSlot >= 0) {\n                processor.countAcc.incrementCount(processor.allBucketsSlot, 1);\n                processor.collect(segDoc, processor.allBucketsSlot);\n              }\n\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["550bbe5ffcdb614551686767f4d5a6ee38ef3e85"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":0,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","pathOld":"/dev/null","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collect(intersection, tt.termNum - startTermIndex);\n          processor.countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (processor.allBucketsSlot >= 0) {\n            processor.collect(intersection, processor.allBucketsSlot);\n            processor.countAcc.incrementCount(processor.allBucketsSlot, collected);\n          }\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReader(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            processor.countAcc.incrementCount(arrIdx, 1);\n            processor.collect(segDoc, arrIdx);\n            if (processor.allBucketsSlot >= 0) {\n              processor.countAcc.incrementCount(processor.allBucketsSlot, 1);\n              processor.collect(segDoc, processor.allBucketsSlot);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx < 0) continue;\n              if (arrIdx >= nTerms) break;\n              processor.countAcc.incrementCount(arrIdx, 1);\n              processor.collect(segDoc, arrIdx);\n              if (processor.allBucketsSlot >= 0) {\n                processor.countAcc.incrementCount(processor.allBucketsSlot, 1);\n                processor.collect(segDoc, processor.allBucketsSlot);\n              }\n\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9e13d0d4d8b6dc352cb304974502b9a36c153f78","date":1436492687,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx < 0) continue;\n              if (arrIdx >= nTerms) break;\n              countAcc.incrementCount(arrIdx, 1);\n              processor.collectFirstPhase(segDoc, arrIdx);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collect(intersection, tt.termNum - startTermIndex);\n          processor.countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (processor.allBucketsSlot >= 0) {\n            processor.collect(intersection, processor.allBucketsSlot);\n            processor.countAcc.incrementCount(processor.allBucketsSlot, collected);\n          }\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReader(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            processor.countAcc.incrementCount(arrIdx, 1);\n            processor.collect(segDoc, arrIdx);\n            if (processor.allBucketsSlot >= 0) {\n              processor.countAcc.incrementCount(processor.allBucketsSlot, 1);\n              processor.collect(segDoc, processor.allBucketsSlot);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx < 0) continue;\n              if (arrIdx >= nTerms) break;\n              processor.countAcc.incrementCount(arrIdx, 1);\n              processor.collect(segDoc, arrIdx);\n              if (processor.allBucketsSlot >= 0) {\n                processor.countAcc.incrementCount(processor.allBucketsSlot, 1);\n                processor.collect(segDoc, processor.allBucketsSlot);\n              }\n\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":["550bbe5ffcdb614551686767f4d5a6ee38ef3e85"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"550bbe5ffcdb614551686767f4d5a6ee38ef3e85","date":1457221653,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx < 0) continue;\n              if (arrIdx >= nTerms) break;\n              countAcc.incrementCount(arrIdx, 1);\n              processor.collectFirstPhase(segDoc, arrIdx);\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":["9e13d0d4d8b6dc352cb304974502b9a36c153f78","4a7c13535572b8e97cc477fc3388a57321a7751a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79759974460bc59933cd169acc94f5c6b16368d5","date":1471318443,"type":5,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorByArrayUIF).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","sourceNew":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorByArrayUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/search/facet/UnInvertedField#collectDocsGeneric(FacetFieldProcessorUIF).mjava","sourceNew":null,"sourceOld":"  // called from FieldFacetProcessor\n  // TODO: do a callback version that can be specialized!\n  public void collectDocsGeneric(FacetFieldProcessorUIF processor) throws IOException {\n    use.incrementAndGet();\n\n    int startTermIndex = processor.startTermIndex;\n    int endTermIndex = processor.endTermIndex;\n    int nTerms = processor.nTerms;\n    DocSet docs = processor.fcontext.base;\n\n    int uniqueTerms = 0;\n    final CountSlotAcc countAcc = processor.countAcc;\n\n    for (TopTerm tt : bigTerms.values()) {\n      if (tt.termNum >= startTermIndex && tt.termNum < endTermIndex) {\n        // handle the biggest terms\n        try ( DocSet intersection = searcher.getDocSet(tt.termQuery, docs); )\n        {\n          int collected = processor.collectFirstPhase(intersection, tt.termNum - startTermIndex);\n          countAcc.incrementCount(tt.termNum - startTermIndex, collected);\n          if (collected > 0) {\n            uniqueTerms++;\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n\n      final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n      final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n      LeafReaderContext ctx = null;\n      int segBase = 0;\n      int segMax;\n      int adjustedMax = 0;\n\n\n      // TODO: handle facet.prefix here!!!\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n\n        if (doc >= adjustedMax) {\n          do {\n            ctx = ctxIt.next();\n            if (ctx == null) {\n              // should be impossible\n              throw new RuntimeException(\"INTERNAL FACET ERROR\");\n            }\n            segBase = ctx.docBase;\n            segMax = ctx.reader().maxDoc();\n            adjustedMax = segBase + segMax;\n          } while (doc >= adjustedMax);\n          assert doc >= ctx.docBase;\n          processor.setNextReaderFirstPhase(ctx);\n        }\n        int segDoc = doc - segBase;\n\n\n        int code = index[doc];\n\n        if ((code & 0xff)==1) {\n          int pos = code>>>8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for(;;) {\n            int delta = 0;\n            for(;;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            int arrIdx = tnum - startTermIndex;\n            if (arrIdx < 0) continue;\n            if (arrIdx >= nTerms) break;\n            countAcc.incrementCount(arrIdx, 1);\n            processor.collectFirstPhase(segDoc, arrIdx);\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (;;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80)==0) {\n              if (delta==0) break;\n              tnum += delta - TNUM_OFFSET;\n              int arrIdx = tnum - startTermIndex;\n              if (arrIdx >= 0) {\n                if (arrIdx >= nTerms) break;\n                countAcc.incrementCount(arrIdx, 1);\n                processor.collectFirstPhase(segDoc, arrIdx);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4a7c13535572b8e97cc477fc3388a57321a7751a"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["550bbe5ffcdb614551686767f4d5a6ee38ef3e85","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"9e13d0d4d8b6dc352cb304974502b9a36c153f78":["4a7c13535572b8e97cc477fc3388a57321a7751a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["550bbe5ffcdb614551686767f4d5a6ee38ef3e85","79759974460bc59933cd169acc94f5c6b16368d5"],"79759974460bc59933cd169acc94f5c6b16368d5":["550bbe5ffcdb614551686767f4d5a6ee38ef3e85"],"550bbe5ffcdb614551686767f4d5a6ee38ef3e85":["9e13d0d4d8b6dc352cb304974502b9a36c153f78"],"4a7c13535572b8e97cc477fc3388a57321a7751a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["550bbe5ffcdb614551686767f4d5a6ee38ef3e85","403d05f7f8d69b65659157eff1bc1d2717f04c66"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["403d05f7f8d69b65659157eff1bc1d2717f04c66"]},"commit2Childs":{"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9e13d0d4d8b6dc352cb304974502b9a36c153f78":["550bbe5ffcdb614551686767f4d5a6ee38ef3e85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","4a7c13535572b8e97cc477fc3388a57321a7751a"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"79759974460bc59933cd169acc94f5c6b16368d5":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"4a7c13535572b8e97cc477fc3388a57321a7751a":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","9e13d0d4d8b6dc352cb304974502b9a36c153f78"],"550bbe5ffcdb614551686767f4d5a6ee38ef3e85":["403d05f7f8d69b65659157eff1bc1d2717f04c66","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","79759974460bc59933cd169acc94f5c6b16368d5","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}