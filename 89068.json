{"path":"modules/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","commits":[{"id":"89f15687f60bd49cd3d9de427e85c17fd9397d61","date":1309381327,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"modules/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getClCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          IOException ioEx = new IOException(\n              \"PANIC: Got unexpected exception while trying to get/calculate total counts: \"\n              +e.getMessage());\n          ioEx.initCause(e);\n          throw ioEx;\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      FacetArrays facetArrays = new FacetArrays(intArrayAllocator, floatArrayAllocator);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where\n          // the request maintains the merged result.\n          // In this implementation merges happen after each\n          // partition,\n          // but other impl could merge only at the end.\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n            IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n            IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n            if (oldRes != null) {\n              res4fr = frHndlr.mergeResults(oldRes, res4fr);\n            }\n            fr2tmpRes.put(fr, res4fr);\n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["487b6150786f5145006f5d0d38a5f514b4472319"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getClCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          IOException ioEx = new IOException(\n              \"PANIC: Got unexpected exception while trying to get/calculate total counts: \"\n              +e.getMessage());\n          ioEx.initCause(e);\n          throw ioEx;\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      FacetArrays facetArrays = new FacetArrays(intArrayAllocator, floatArrayAllocator);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where\n          // the request maintains the merged result.\n          // In this implementation merges happen after each\n          // partition,\n          // but other impl could merge only at the end.\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n            IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n            IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n            if (oldRes != null) {\n              res4fr = frHndlr.mergeResults(oldRes, res4fr);\n            }\n            fr2tmpRes.put(fr, res4fr);\n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getClCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          IOException ioEx = new IOException(\n              \"PANIC: Got unexpected exception while trying to get/calculate total counts: \"\n              +e.getMessage());\n          ioEx.initCause(e);\n          throw ioEx;\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      FacetArrays facetArrays = new FacetArrays(intArrayAllocator, floatArrayAllocator);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where\n          // the request maintains the merged result.\n          // In this implementation merges happen after each\n          // partition,\n          // but other impl could merge only at the end.\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n            IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n            IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n            if (oldRes != null) {\n              res4fr = frHndlr.mergeResults(oldRes, res4fr);\n            }\n            fr2tmpRes.put(fr, res4fr);\n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","pathOld":"modules/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator#accumulate(ScoredDocIDs).mjava","sourceNew":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getClCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          IOException ioEx = new IOException(\n              \"PANIC: Got unexpected exception while trying to get/calculate total counts: \"\n              +e.getMessage());\n          ioEx.initCause(e);\n          throw ioEx;\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      FacetArrays facetArrays = new FacetArrays(intArrayAllocator, floatArrayAllocator);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where\n          // the request maintains the merged result.\n          // In this implementation merges happen after each\n          // partition,\n          // but other impl could merge only at the end.\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n            IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n            IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n            if (oldRes != null) {\n              res4fr = frHndlr.mergeResults(oldRes, res4fr);\n            }\n            fr2tmpRes.put(fr, res4fr);\n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","sourceOld":"  @Override\n  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {\n\n    // synchronize to prevent calling two accumulate()'s at the same time.\n    // We decided not to synchronize the method because that might mislead\n    // users to feel encouraged to call this method simultaneously.\n    synchronized (accumulateGuard) {\n\n      // only now we can compute this\n      isUsingComplements = shouldComplement(docids);\n\n      if (isUsingComplements) {\n        try {\n          totalFacetCounts = TotalFacetCountsCache.getSingleton()\n            .getTotalCounts(indexReader, taxonomyReader,\n                searchParams.getFacetIndexingParams(), searchParams.getClCache());\n          if (totalFacetCounts != null) {\n            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);\n          } else {\n            isUsingComplements = false;\n          }\n        } catch (UnsupportedOperationException e) {\n          // TODO (Facet): this exception is thrown from TotalCountsKey if the\n          // IndexReader used does not support getVersion(). We should re-think\n          // this: is this tiny detail worth disabling total counts completely\n          // for such readers? Currently, it's not supported by Parallel and\n          // MultiReader, which might be problematic for several applications.\n          // We could, for example, base our \"isCurrent\" logic on something else\n          // than the reader's version. Need to think more deeply about it.\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"IndexReader used does not support completents: \", e);\n          }\n          isUsingComplements = false;\n        } catch (IOException e) {\n          if (logger.isLoggable(Level.FINEST)) {\n            logger.log(Level.FINEST, \"Failed to load/calculate total counts (complement counting disabled): \", e);\n          }\n          // silently fail if for some reason failed to load/save from/to dir \n          isUsingComplements = false;\n        } catch (Exception e) {\n          // give up: this should not happen!\n          IOException ioEx = new IOException(\n              \"PANIC: Got unexpected exception while trying to get/calculate total counts: \"\n              +e.getMessage());\n          ioEx.initCause(e);\n          throw ioEx;\n        }\n      }\n\n      docids = actualDocsToAccumulate(docids);\n\n      FacetArrays facetArrays = new FacetArrays(intArrayAllocator, floatArrayAllocator);\n\n      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();\n\n      try {\n        for (int part = 0; part < maxPartitions; part++) {\n\n          // fill arrays from category lists\n          fillArraysForPartition(docids, facetArrays, part);\n\n          int offset = part * partitionSize;\n\n          // for each partition we go over all requests and handle\n          // each, where\n          // the request maintains the merged result.\n          // In this implementation merges happen after each\n          // partition,\n          // but other impl could merge only at the end.\n          for (FacetRequest fr : searchParams.getFacetRequests()) {\n            FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n            IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(facetArrays, offset);\n            IntermediateFacetResult oldRes = fr2tmpRes.get(fr);\n            if (oldRes != null) {\n              res4fr = frHndlr.mergeResults(oldRes, res4fr);\n            }\n            fr2tmpRes.put(fr, res4fr);\n          }\n        }\n      } finally {\n        facetArrays.free();\n      }\n\n      // gather results from all requests into a list for returning them\n      List<FacetResult> res = new ArrayList<FacetResult>();\n      for (FacetRequest fr : searchParams.getFacetRequests()) {\n        FacetResultsHandler frHndlr = fr.createFacetResultsHandler(taxonomyReader);\n        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr); \n        if (tmpResult == null) {\n          continue; // do not add a null to the list.\n        }\n        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult); \n        // final labeling if allowed (because labeling is a costly operation)\n        if (isAllowLabeling()) {\n          frHndlr.labelResult(facetRes);\n        }\n        res.add(facetRes);\n      }\n\n      return res;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["89f15687f60bd49cd3d9de427e85c17fd9397d61"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","89f15687f60bd49cd3d9de427e85c17fd9397d61"],"89f15687f60bd49cd3d9de427e85c17fd9397d61":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","89f15687f60bd49cd3d9de427e85c17fd9397d61"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d083e83f225b11e5fdd900e83d26ddb385b6955c","89f15687f60bd49cd3d9de427e85c17fd9397d61","817d8435e9135b756f08ce6710ab0baac51bdf88"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"89f15687f60bd49cd3d9de427e85c17fd9397d61":["b89678825b68eccaf09e6ab71675fc0b0af1e099","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}