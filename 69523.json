{"path":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","commits":[{"id":"3e8715d826e588419327562287d5d6a8040d63d6","date":1427987148,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","pathOld":"/dev/null","sourceNew":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>());\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, false, globalLiveDocs, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random.nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random.nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2638f781be724518ff6c2263d14a48cf6e68017","date":1427989059,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","pathOld":"/dev/null","sourceNew":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>());\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, false, globalLiveDocs, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random.nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random.nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","sourceNew":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>());\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>());\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, false, globalLiveDocs, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    globalLiveDocs = new FixedBitSet(maxDoc);\n    double liveRatio = random.nextDouble();\n    for(int i=0;i<maxDoc;i++) {\n      if (random.nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca792c26af46bd6c4a08d81117c60440cf6a7e3d","date":1445938295,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","sourceNew":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>(),\n                                                0, 0);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>());\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eeba0a4d0845889a402dd225793d62f009d029c9","date":1527938093,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","sourceNew":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>(),\n                                                0, 0, false);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>(),\n                                                0, 0);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab548c8f96022b4780f7500a30b19b4f4a5feeb6","date":1527940044,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","sourceNew":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>(),\n                                                0, 0);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>(),\n                                                0, 0, false);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7e4ca6dc9612ff741d8713743e2bccfae5eadac","date":1528093718,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","sourceNew":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>(),\n                                                0, 0, false);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>(),\n                                                0, 0);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","sourceNew":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>(),\n                                                0, 0, false);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>(),\n                                                0, 0);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","sourceNew":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>(),\n                                                0, 0, false);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>(),\n                                                0, 0);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6652c943595e92c187ee904c382863013eae28f","date":1539042663,"type":3,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#RandomPostingsTester(Random).mjava","sourceNew":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>(),\n                                                0, 0, 0, false);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  public RandomPostingsTester(Random random) throws IOException {\n    fields = new TreeMap<>();\n\n    this.random = random;\n\n    final int numFields = TestUtil.nextInt(random, 1, 5);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n    maxDoc = 0;\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    while (fieldUpto < numFields) {\n      String field = TestUtil.randomSimpleString(random);\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, fieldUpto, false, false, true,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                DocValuesType.NONE, -1, new HashMap<>(),\n                                                0, 0, false);\n      fieldUpto++;\n\n      SortedMap<BytesRef,SeedAndOrd> postings = new TreeMap<>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<>();\n\n      int numTerms;\n      if (random.nextInt(10) == 7) {\n        numTerms = LuceneTestCase.atLeast(random, 50);\n      } else {\n        numTerms = TestUtil.nextInt(random, 2, 20);\n      }\n\n      while (postings.size() < numTerms) {\n        int termUpto = postings.size();\n        // Cannot contain surrogates else default Java string sort order (by UTF16 code unit) is different from Lucene:\n        String term = TestUtil.randomSimpleString(random);\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        if (LuceneTestCase.TEST_NIGHTLY && termUpto == 0 && fieldUpto == 1) {\n          // Make 1 big term:\n          term = \"big_\" + term;\n        } else if (termUpto == 1 && fieldUpto == 1) {\n          // Make 1 medium term:\n          term = \"medium_\" + term;\n        } else if (random.nextBoolean()) {\n          // Low freq term:\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          term = \"verylow_\" + term;\n        }\n\n        long termSeed = random.nextLong();\n        postings.put(new BytesRef(term), new SeedAndOrd(termSeed));\n\n        // NOTE: sort of silly: we enum all the docs just to\n        // get the maxDoc\n        PostingsEnum postingsEnum = getSeedPostings(term, termSeed, IndexOptions.DOCS, true);\n        int doc;\n        int lastDoc = 0;\n        while((doc = postingsEnum.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n          lastDoc = doc;\n        }\n        maxDoc = Math.max(lastDoc, maxDoc);\n      }\n\n      // assign ords\n      long ord = 0;\n      for(SeedAndOrd ent : postings.values()) {\n        ent.ord = ord++;\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    // It's the count, not the last docID:\n    maxDoc++;\n\n    allTerms = new ArrayList<>();\n    for(Map.Entry<String,SortedMap<BytesRef,SeedAndOrd>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      long ord = 0;\n      for(Map.Entry<BytesRef,SeedAndOrd> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey(), ord++));\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: done init postings; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0f4464508ee83288c8c4585b533f9faaa93aa314":["3e8715d826e588419327562287d5d6a8040d63d6"],"f6652c943595e92c187ee904c382863013eae28f":["b7e4ca6dc9612ff741d8713743e2bccfae5eadac"],"ab548c8f96022b4780f7500a30b19b4f4a5feeb6":["eeba0a4d0845889a402dd225793d62f009d029c9"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70042a8a492f7054d480ccdd2be9796510d4327":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d","b7e4ca6dc9612ff741d8713743e2bccfae5eadac"],"d2638f781be724518ff6c2263d14a48cf6e68017":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3e8715d826e588419327562287d5d6a8040d63d6"],"b7e4ca6dc9612ff741d8713743e2bccfae5eadac":["ab548c8f96022b4780f7500a30b19b4f4a5feeb6"],"eeba0a4d0845889a402dd225793d62f009d029c9":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d"],"3e8715d826e588419327562287d5d6a8040d63d6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f592209545c71895260367152601e9200399776d":["ab548c8f96022b4780f7500a30b19b4f4a5feeb6","b7e4ca6dc9612ff741d8713743e2bccfae5eadac"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f6652c943595e92c187ee904c382863013eae28f"]},"commit2Childs":{"0f4464508ee83288c8c4585b533f9faaa93aa314":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d"],"f6652c943595e92c187ee904c382863013eae28f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ab548c8f96022b4780f7500a30b19b4f4a5feeb6":["b7e4ca6dc9612ff741d8713743e2bccfae5eadac","f592209545c71895260367152601e9200399776d"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["b70042a8a492f7054d480ccdd2be9796510d4327","eeba0a4d0845889a402dd225793d62f009d029c9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d2638f781be724518ff6c2263d14a48cf6e68017","3e8715d826e588419327562287d5d6a8040d63d6"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"d2638f781be724518ff6c2263d14a48cf6e68017":[],"b7e4ca6dc9612ff741d8713743e2bccfae5eadac":["f6652c943595e92c187ee904c382863013eae28f","b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d"],"eeba0a4d0845889a402dd225793d62f009d029c9":["ab548c8f96022b4780f7500a30b19b4f4a5feeb6"],"3e8715d826e588419327562287d5d6a8040d63d6":["0f4464508ee83288c8c4585b533f9faaa93aa314","d2638f781be724518ff6c2263d14a48cf6e68017"],"f592209545c71895260367152601e9200399776d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b70042a8a492f7054d480ccdd2be9796510d4327","d2638f781be724518ff6c2263d14a48cf6e68017","f592209545c71895260367152601e9200399776d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}