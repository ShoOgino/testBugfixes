{"path":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#buildIndex().mjava","commits":[{"id":"4a4e2c829188fb99886a64558664d79c9ac0fdf1","date":1431021538,"type":0,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#buildIndex().mjava","pathOld":"/dev/null","sourceNew":"  /** CAUTION: this builds a very large index */\n  public void buildIndex() throws Exception {\n    log.info(\"Building an index of {} docs\", NUM_DOCS);\n\n    // we want a big spread in the long values we use, decrement by BIG_PRIME as we index\n    long longValue = MAX_LONG;\n\n    for (int i = 1; i <= NUM_DOCS; i++) {\n      // with these values, we know that every doc indexed has a unique value in all of the\n      // fields we will compute cardinality against.\n      // which means the number of docs matching a query is the true cardinality for each field\n\n      final String strValue = \"s\"+longValue;\n      indexDoc(sdoc(\"id\",\"\" + i, \n                    \"int_i\", \"\"+i,\n                    \"int_i_prehashed_l\", \"\"+HASHER.hashInt(i).asLong(),\n                    \"long_l\", \"\"+longValue, \n                    \"long_l_prehashed_l\", \"\"+HASHER.hashLong(longValue).asLong(),\n                    \"string_s\", strValue,\n                    // NOTE: renamed hashUnencodedChars starting with guava 15\n                    \"string_s_prehashed_l\", \"\"+HASHER.hashString(strValue).asLong()));\n\n      longValue -= BIG_PRIME;\n    }\n\n    commit();\n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1fcee2c3acd9aa64a73a3b441b3f6cb492af0b87","date":1440409984,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#buildIndex().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#buildIndex().mjava","sourceNew":"  /** CAUTION: this builds a very large index */\n  public void buildIndex() throws Exception {\n    log.info(\"Building an index of {} docs\", NUM_DOCS);\n\n    // we want a big spread in the long values we use, decrement by BIG_PRIME as we index\n    long longValue = MAX_LONG;\n\n    for (int i = 1; i <= NUM_DOCS; i++) {\n      // with these values, we know that every doc indexed has a unique value in all of the\n      // fields we will compute cardinality against.\n      // which means the number of docs matching a query is the true cardinality for each field\n\n      final String strValue = \"s\"+longValue;\n      indexDoc(sdoc(\"id\",\"\" + i, \n                    \"int_i\", \"\"+i,\n                    \"int_i_prehashed_l\", \"\"+HASHER.hashInt(i).asLong(),\n                    \"long_l\", \"\"+longValue, \n                    \"long_l_prehashed_l\", \"\"+HASHER.hashLong(longValue).asLong(),\n                    \"string_s\", strValue,\n                    \"string_s_prehashed_l\", \"\"+HASHER.hashString(strValue).asLong()));\n\n      longValue -= BIG_PRIME;\n    }\n\n    commit();\n    \n  }\n\n","sourceOld":"  /** CAUTION: this builds a very large index */\n  public void buildIndex() throws Exception {\n    log.info(\"Building an index of {} docs\", NUM_DOCS);\n\n    // we want a big spread in the long values we use, decrement by BIG_PRIME as we index\n    long longValue = MAX_LONG;\n\n    for (int i = 1; i <= NUM_DOCS; i++) {\n      // with these values, we know that every doc indexed has a unique value in all of the\n      // fields we will compute cardinality against.\n      // which means the number of docs matching a query is the true cardinality for each field\n\n      final String strValue = \"s\"+longValue;\n      indexDoc(sdoc(\"id\",\"\" + i, \n                    \"int_i\", \"\"+i,\n                    \"int_i_prehashed_l\", \"\"+HASHER.hashInt(i).asLong(),\n                    \"long_l\", \"\"+longValue, \n                    \"long_l_prehashed_l\", \"\"+HASHER.hashLong(longValue).asLong(),\n                    \"string_s\", strValue,\n                    // NOTE: renamed hashUnencodedChars starting with guava 15\n                    \"string_s_prehashed_l\", \"\"+HASHER.hashString(strValue).asLong()));\n\n      longValue -= BIG_PRIME;\n    }\n\n    commit();\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c5ed9a1ca92218962653b439b8551778b5ad7bd","date":1550615278,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#buildIndex().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#buildIndex().mjava","sourceNew":"  /** CAUTION: this builds a very large index */\n  public void buildIndex() throws Exception {\n    log.info(\"Building an index of {} docs\", NUM_DOCS);\n\n    // we want a big spread in the long values we use, decrement by BIG_PRIME as we index\n    long longValue = MAX_LONG;\n\n    for (int i = 1; i <= NUM_DOCS; i++) {\n      // with these values, we know that every doc indexed has a unique value in all of the\n      // fields we will compute cardinality against.\n      // which means the number of docs matching a query is the true cardinality for each field\n\n      final String strValue = \"s\"+longValue;\n      indexDoc(sdoc(\"id\",\"\" + i, \n                    \"int_i\", \"\"+i,\n                    \"int_i_prehashed_l\", \"\"+HASHER.hashInt(i).asLong(),\n                    \"long_l\", \"\"+longValue, \n                    \"long_l_prehashed_l\", \"\"+HASHER.hashLong(longValue).asLong(),\n                    \"string_s\", strValue,\n                    \"string_s_prehashed_l\", \"\"+HASHER.hashString(strValue, StandardCharsets.UTF_8).asLong()));\n\n      longValue -= BIG_PRIME;\n    }\n\n    commit();\n    \n  }\n\n","sourceOld":"  /** CAUTION: this builds a very large index */\n  public void buildIndex() throws Exception {\n    log.info(\"Building an index of {} docs\", NUM_DOCS);\n\n    // we want a big spread in the long values we use, decrement by BIG_PRIME as we index\n    long longValue = MAX_LONG;\n\n    for (int i = 1; i <= NUM_DOCS; i++) {\n      // with these values, we know that every doc indexed has a unique value in all of the\n      // fields we will compute cardinality against.\n      // which means the number of docs matching a query is the true cardinality for each field\n\n      final String strValue = \"s\"+longValue;\n      indexDoc(sdoc(\"id\",\"\" + i, \n                    \"int_i\", \"\"+i,\n                    \"int_i_prehashed_l\", \"\"+HASHER.hashInt(i).asLong(),\n                    \"long_l\", \"\"+longValue, \n                    \"long_l_prehashed_l\", \"\"+HASHER.hashLong(longValue).asLong(),\n                    \"string_s\", strValue,\n                    \"string_s_prehashed_l\", \"\"+HASHER.hashString(strValue).asLong()));\n\n      longValue -= BIG_PRIME;\n    }\n\n    commit();\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1c5ed9a1ca92218962653b439b8551778b5ad7bd":["1fcee2c3acd9aa64a73a3b441b3f6cb492af0b87"],"4a4e2c829188fb99886a64558664d79c9ac0fdf1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1fcee2c3acd9aa64a73a3b441b3f6cb492af0b87":["4a4e2c829188fb99886a64558664d79c9ac0fdf1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1c5ed9a1ca92218962653b439b8551778b5ad7bd"]},"commit2Childs":{"1c5ed9a1ca92218962653b439b8551778b5ad7bd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4a4e2c829188fb99886a64558664d79c9ac0fdf1":["1fcee2c3acd9aa64a73a3b441b3f6cb492af0b87"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4a4e2c829188fb99886a64558664d79c9ac0fdf1"],"1fcee2c3acd9aa64a73a3b441b3f6cb492af0b87":["1c5ed9a1ca92218962653b439b8551778b5ad7bd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}