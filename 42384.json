{"path":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","commits":[{"id":"42579622cc27f9908e64f29fa1130bfc28306009","date":1177874771,"type":0,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"/dev/null","sourceNew":"  public void testCaching() throws IOException {\r\n    Directory dir = new RAMDirectory();\r\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer());\r\n    Document doc = new Document();\r\n    TokenStream stream = new TokenStream() {\r\n      private int index = 0;\r\n      \r\n      public Token next() throws IOException {\r\n        if (index == tokens.length) {\r\n          return null;\r\n        } else {\r\n          return new Token(tokens[index++], 0, 0);\r\n        }        \r\n      }\r\n      \r\n    };\r\n    \r\n    stream = new CachingTokenFilter(stream);\r\n    \r\n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\r\n    \r\n    // 1) we consume all tokens twice before we add the doc to the index\r\n    checkTokens(stream);\r\n    stream.reset();  \r\n    checkTokens(stream);\r\n    \r\n    // 2) now add the document to the index and verify if all tokens are indexed\r\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\r\n    writer.addDocument(doc);\r\n    writer.close();\r\n    \r\n    IndexReader reader = IndexReader.open(dir);\r\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\r\n    assertTrue(termPositions.next());\r\n    assertEquals(1, termPositions.freq());\r\n    assertEquals(0, termPositions.nextPosition());\r\n\r\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\r\n    assertTrue(termPositions.next());\r\n    assertEquals(2, termPositions.freq());\r\n    assertEquals(1, termPositions.nextPosition());\r\n    assertEquals(3, termPositions.nextPosition());\r\n    \r\n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\r\n    assertTrue(termPositions.next());\r\n    assertEquals(1, termPositions.freq());\r\n    assertEquals(2, termPositions.nextPosition());\r\n    reader.close();\r\n    \r\n    // 3) reset stream and consume tokens again\r\n    stream.reset();\r\n    checkTokens(stream);\r\n  }\r\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"235efcba838a273934c5dd0ef66bb07c7fb0d718","date":1201256475,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\r\n    Directory dir = new RAMDirectory();\r\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\r\n    Document doc = new Document();\r\n    TokenStream stream = new TokenStream() {\r\n      private int index = 0;\r\n      \r\n      public Token next() throws IOException {\r\n        if (index == tokens.length) {\r\n          return null;\r\n        } else {\r\n          return new Token(tokens[index++], 0, 0);\r\n        }        \r\n      }\r\n      \r\n    };\r\n    \r\n    stream = new CachingTokenFilter(stream);\r\n    \r\n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\r\n    \r\n    // 1) we consume all tokens twice before we add the doc to the index\r\n    checkTokens(stream);\r\n    stream.reset();  \r\n    checkTokens(stream);\r\n    \r\n    // 2) now add the document to the index and verify if all tokens are indexed\r\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\r\n    writer.addDocument(doc);\r\n    writer.close();\r\n    \r\n    IndexReader reader = IndexReader.open(dir);\r\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\r\n    assertTrue(termPositions.next());\r\n    assertEquals(1, termPositions.freq());\r\n    assertEquals(0, termPositions.nextPosition());\r\n\r\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\r\n    assertTrue(termPositions.next());\r\n    assertEquals(2, termPositions.freq());\r\n    assertEquals(1, termPositions.nextPosition());\r\n    assertEquals(3, termPositions.nextPosition());\r\n    \r\n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\r\n    assertTrue(termPositions.next());\r\n    assertEquals(1, termPositions.freq());\r\n    assertEquals(2, termPositions.nextPosition());\r\n    reader.close();\r\n    \r\n    // 3) reset stream and consume tokens again\r\n    stream.reset();\r\n    checkTokens(stream);\r\n  }\r\n\n","sourceOld":"  public void testCaching() throws IOException {\r\n    Directory dir = new RAMDirectory();\r\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer());\r\n    Document doc = new Document();\r\n    TokenStream stream = new TokenStream() {\r\n      private int index = 0;\r\n      \r\n      public Token next() throws IOException {\r\n        if (index == tokens.length) {\r\n          return null;\r\n        } else {\r\n          return new Token(tokens[index++], 0, 0);\r\n        }        \r\n      }\r\n      \r\n    };\r\n    \r\n    stream = new CachingTokenFilter(stream);\r\n    \r\n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\r\n    \r\n    // 1) we consume all tokens twice before we add the doc to the index\r\n    checkTokens(stream);\r\n    stream.reset();  \r\n    checkTokens(stream);\r\n    \r\n    // 2) now add the document to the index and verify if all tokens are indexed\r\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\r\n    writer.addDocument(doc);\r\n    writer.close();\r\n    \r\n    IndexReader reader = IndexReader.open(dir);\r\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\r\n    assertTrue(termPositions.next());\r\n    assertEquals(1, termPositions.freq());\r\n    assertEquals(0, termPositions.nextPosition());\r\n\r\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\r\n    assertTrue(termPositions.next());\r\n    assertEquals(2, termPositions.freq());\r\n    assertEquals(1, termPositions.nextPosition());\r\n    assertEquals(3, termPositions.nextPosition());\r\n    \r\n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\r\n    assertTrue(termPositions.next());\r\n    assertEquals(1, termPositions.freq());\r\n    assertEquals(2, termPositions.nextPosition());\r\n    reader.close();\r\n    \r\n    // 3) reset stream and consume tokens again\r\n    stream.reset();\r\n    checkTokens(stream);\r\n  }\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2123bddbd65dea198cac380540636ce43a880403","date":1211269254,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      \n      public Token next() throws IOException {\n        if (index == tokens.length) {\n          return null;\n        } else {\n          return new Token(tokens[index++], 0, 0);\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\r\n    Directory dir = new RAMDirectory();\r\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\r\n    Document doc = new Document();\r\n    TokenStream stream = new TokenStream() {\r\n      private int index = 0;\r\n      \r\n      public Token next() throws IOException {\r\n        if (index == tokens.length) {\r\n          return null;\r\n        } else {\r\n          return new Token(tokens[index++], 0, 0);\r\n        }        \r\n      }\r\n      \r\n    };\r\n    \r\n    stream = new CachingTokenFilter(stream);\r\n    \r\n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\r\n    \r\n    // 1) we consume all tokens twice before we add the doc to the index\r\n    checkTokens(stream);\r\n    stream.reset();  \r\n    checkTokens(stream);\r\n    \r\n    // 2) now add the document to the index and verify if all tokens are indexed\r\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\r\n    writer.addDocument(doc);\r\n    writer.close();\r\n    \r\n    IndexReader reader = IndexReader.open(dir);\r\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\r\n    assertTrue(termPositions.next());\r\n    assertEquals(1, termPositions.freq());\r\n    assertEquals(0, termPositions.nextPosition());\r\n\r\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\r\n    assertTrue(termPositions.next());\r\n    assertEquals(2, termPositions.freq());\r\n    assertEquals(1, termPositions.nextPosition());\r\n    assertEquals(3, termPositions.nextPosition());\r\n    \r\n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\r\n    assertTrue(termPositions.next());\r\n    assertEquals(1, termPositions.freq());\r\n    assertEquals(2, termPositions.nextPosition());\r\n    reader.close();\r\n    \r\n    // 3) reset stream and consume tokens again\r\n    stream.reset();\r\n    checkTokens(stream);\r\n  }\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      \n      public Token next(final Token reusableToken) throws IOException {\n        assert reusableToken != null;\n        if (index == tokens.length) {\n          return null;\n        } else {\n          return reusableToken.reinit(tokens[index++], 0, 0);\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      \n      public Token next() throws IOException {\n        if (index == tokens.length) {\n          return null;\n        } else {\n          return new Token(tokens[index++], 0, 0);\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223","date":1227051709,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setStartOffset(0);\n          offsetAtt.setEndOffset(0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      \n      public Token next(final Token reusableToken) throws IOException {\n        assert reusableToken != null;\n        if (index == tokens.length) {\n          return null;\n        } else {\n          return reusableToken.reinit(tokens[index++], 0, 0);\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"10855b393afd8884613d82de3a4fff773d4e5334","date":1240953458,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setStartOffset(0);\n          offsetAtt.setEndOffset(0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8d78f014fded44fbde905f4f84cdc21907b371e8","date":1254383623,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e8d1458a2543cbd30cbfe7929be4dcb5c5251659","date":1254582241,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = (TermAttribute) addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = (OffsetAttribute) addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0a046c0c310bc77931fc8441bd920053b607dd14","date":1254584734,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1326054a8d3aa66382d49decc7f330955c9c6f71","date":1257386139,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"360d15dc189fb48153cb62234f7d20819e4e292e","date":1263562938,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fe0932c1d340f83fb0a611e5829b3046a1cc1152","date":1264946739,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6","date":1265808957,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1cedb00d2dd44640194401179358a2e3ba6051bf","date":1268243626,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e52fea2c4081a1e552b98506691990be59503168","date":1268250331,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8","date":1268494368,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","pathOld":"src/test/org/apache/lucene/analysis/TestCachingTokenFilter#testCaching().mjava","sourceNew":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","sourceOld":"  public void testCaching() throws IOException {\n    Directory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));\n    Document doc = new Document();\n    TokenStream stream = new TokenStream() {\n      private int index = 0;\n      private TermAttribute termAtt = addAttribute(TermAttribute.class);\n      private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);\n      \n      @Override\n      public boolean incrementToken() throws IOException {\n        if (index == tokens.length) {\n          return false;\n        } else {\n          clearAttributes();\n          termAtt.setTermBuffer(tokens[index++]);\n          offsetAtt.setOffset(0,0);\n          return true;\n        }        \n      }\n      \n    };\n    \n    stream = new CachingTokenFilter(stream);\n    \n    doc.add(new Field(\"preanalyzed\", stream, TermVector.NO));\n    \n    // 1) we consume all tokens twice before we add the doc to the index\n    checkTokens(stream);\n    stream.reset();  \n    checkTokens(stream);\n    \n    // 2) now add the document to the index and verify if all tokens are indexed\n    //    don't reset the stream here, the DocumentWriter should do that implicitly\n    writer.addDocument(doc);\n    writer.close();\n    \n    IndexReader reader = IndexReader.open(dir, true);\n    TermPositions termPositions = reader.termPositions(new Term(\"preanalyzed\", \"term1\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(0, termPositions.nextPosition());\n\n    termPositions.seek(new Term(\"preanalyzed\", \"term2\"));\n    assertTrue(termPositions.next());\n    assertEquals(2, termPositions.freq());\n    assertEquals(1, termPositions.nextPosition());\n    assertEquals(3, termPositions.nextPosition());\n    \n    termPositions.seek(new Term(\"preanalyzed\", \"term3\"));\n    assertTrue(termPositions.next());\n    assertEquals(1, termPositions.freq());\n    assertEquals(2, termPositions.nextPosition());\n    reader.close();\n    \n    // 3) reset stream and consume tokens again\n    stream.reset();\n    checkTokens(stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["2123bddbd65dea198cac380540636ce43a880403"],"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"235efcba838a273934c5dd0ef66bb07c7fb0d718":["42579622cc27f9908e64f29fa1130bfc28306009"],"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6":["fe0932c1d340f83fb0a611e5829b3046a1cc1152"],"0a046c0c310bc77931fc8441bd920053b607dd14":["8d78f014fded44fbde905f4f84cdc21907b371e8","e8d1458a2543cbd30cbfe7929be4dcb5c5251659"],"2123bddbd65dea198cac380540636ce43a880403":["235efcba838a273934c5dd0ef66bb07c7fb0d718"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["55f083e91bb056b57de136da1dfc3b9b6ecc4ef6"],"e52fea2c4081a1e552b98506691990be59503168":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"10855b393afd8884613d82de3a4fff773d4e5334":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"],"360d15dc189fb48153cb62234f7d20819e4e292e":["1326054a8d3aa66382d49decc7f330955c9c6f71"],"1326054a8d3aa66382d49decc7f330955c9c6f71":["0a046c0c310bc77931fc8441bd920053b607dd14"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["e52fea2c4081a1e552b98506691990be59503168"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["10855b393afd8884613d82de3a4fff773d4e5334"],"fe0932c1d340f83fb0a611e5829b3046a1cc1152":["360d15dc189fb48153cb62234f7d20819e4e292e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8d78f014fded44fbde905f4f84cdc21907b371e8":["10855b393afd8884613d82de3a4fff773d4e5334"],"42579622cc27f9908e64f29fa1130bfc28306009":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"]},"commit2Childs":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"],"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["10855b393afd8884613d82de3a4fff773d4e5334"],"235efcba838a273934c5dd0ef66bb07c7fb0d718":["2123bddbd65dea198cac380540636ce43a880403"],"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"0a046c0c310bc77931fc8441bd920053b607dd14":["1326054a8d3aa66382d49decc7f330955c9c6f71"],"2123bddbd65dea198cac380540636ce43a880403":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["e52fea2c4081a1e552b98506691990be59503168"],"e52fea2c4081a1e552b98506691990be59503168":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"],"10855b393afd8884613d82de3a4fff773d4e5334":["e8d1458a2543cbd30cbfe7929be4dcb5c5251659","8d78f014fded44fbde905f4f84cdc21907b371e8"],"360d15dc189fb48153cb62234f7d20819e4e292e":["fe0932c1d340f83fb0a611e5829b3046a1cc1152"],"1326054a8d3aa66382d49decc7f330955c9c6f71":["360d15dc189fb48153cb62234f7d20819e4e292e"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["0a046c0c310bc77931fc8441bd920053b607dd14"],"fe0932c1d340f83fb0a611e5829b3046a1cc1152":["55f083e91bb056b57de136da1dfc3b9b6ecc4ef6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["42579622cc27f9908e64f29fa1130bfc28306009"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["0a046c0c310bc77931fc8441bd920053b607dd14"],"42579622cc27f9908e64f29fa1130bfc28306009":["235efcba838a273934c5dd0ef66bb07c7fb0d718"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}