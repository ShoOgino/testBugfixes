{"path":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","commits":[{"id":"4cc45c615dbb82bf79d5f9550286098367874fbf","date":1409571423,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","sourceNew":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(analyzer).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random());\n    writer.commit();\n\n    DirectoryReader firstReader = DirectoryReader.open(dir);\n    DirectoryReader secondReader = DirectoryReader.open(dir);\n    List<AtomicReaderContext> leaves = firstReader.leaves();\n    List<AtomicReaderContext> leaves2 = secondReader.leaves();\n    \n    for (AtomicReaderContext ctx : leaves) {\n      Terms terms = ctx.reader().terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n    }\n    writer.close();\n    IOUtils.close(firstReader, secondReader, dir);\n  }\n\n","sourceOld":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(analyzer).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random());\n    writer.commit();\n\n    DirectoryReader firstReader = DirectoryReader.open(dir);\n    DirectoryReader secondReader = DirectoryReader.open(dir);\n    List<AtomicReaderContext> leaves = firstReader.leaves();\n    List<AtomicReaderContext> leaves2 = secondReader.leaves();\n    \n    for (AtomicReaderContext ctx : leaves) {\n      Terms terms = ctx.reader().terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n    }\n    writer.close();\n    IOUtils.close(firstReader, secondReader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"402ad3ddc9da7b70da1b167667a60ece6a1381fb","date":1409656478,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","sourceNew":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(analyzer).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random());\n    writer.commit();\n\n    DirectoryReader firstReader = DirectoryReader.open(dir);\n    DirectoryReader secondReader = DirectoryReader.open(dir);\n    List<AtomicReaderContext> leaves = firstReader.leaves();\n    List<AtomicReaderContext> leaves2 = secondReader.leaves();\n    \n    for (AtomicReaderContext ctx : leaves) {\n      Terms terms = ctx.reader().terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n    }\n    writer.close();\n    IOUtils.close(firstReader, secondReader, dir);\n  }\n\n","sourceOld":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(analyzer).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random());\n    writer.commit();\n\n    DirectoryReader firstReader = DirectoryReader.open(dir);\n    DirectoryReader secondReader = DirectoryReader.open(dir);\n    List<AtomicReaderContext> leaves = firstReader.leaves();\n    List<AtomicReaderContext> leaves2 = secondReader.leaves();\n    \n    for (AtomicReaderContext ctx : leaves) {\n      Terms terms = ctx.reader().terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n    }\n    writer.close();\n    IOUtils.close(firstReader, secondReader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","pathOld":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","sourceNew":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(analyzer).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random());\n    writer.commit();\n\n    DirectoryReader firstReader = DirectoryReader.open(dir);\n    DirectoryReader secondReader = DirectoryReader.open(dir);\n    List<LeafReaderContext> leaves = firstReader.leaves();\n    List<LeafReaderContext> leaves2 = secondReader.leaves();\n    \n    for (LeafReaderContext ctx : leaves) {\n      Terms terms = ctx.reader().terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n    }\n    writer.close();\n    IOUtils.close(firstReader, secondReader, dir);\n  }\n\n","sourceOld":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(analyzer).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random());\n    writer.commit();\n\n    DirectoryReader firstReader = DirectoryReader.open(dir);\n    DirectoryReader secondReader = DirectoryReader.open(dir);\n    List<AtomicReaderContext> leaves = firstReader.leaves();\n    List<AtomicReaderContext> leaves2 = secondReader.leaves();\n    \n    for (AtomicReaderContext ctx : leaves) {\n      Terms terms = ctx.reader().terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n    }\n    writer.close();\n    IOUtils.close(firstReader, secondReader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71387d8cb6923eb831b17a8b734608ba2e21c653","date":1414126093,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum#testReuseDocsEnumDifferentReader().mjava","sourceNew":null,"sourceOld":"  // make sure we never reuse from another reader even if it is the same field & codec etc\n  public void testReuseDocsEnumDifferentReader() throws IOException {\n    Directory dir = newDirectory();\n    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(analyzer).setCodec(cp));\n    int numdocs = atLeast(20);\n    createRandomIndex(numdocs, writer, random());\n    writer.commit();\n\n    DirectoryReader firstReader = DirectoryReader.open(dir);\n    DirectoryReader secondReader = DirectoryReader.open(dir);\n    List<LeafReaderContext> leaves = firstReader.leaves();\n    List<LeafReaderContext> leaves2 = secondReader.leaves();\n    \n    for (LeafReaderContext ctx : leaves) {\n      Terms terms = ctx.reader().terms(\"body\");\n      TermsEnum iterator = terms.iterator(null);\n      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();\n      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());\n      iterator = terms.iterator(null);\n      DocsEnum docs = null;\n      BytesRef term = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(null, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n      \n      iterator = terms.iterator(null);\n      enums.clear();\n      docs = null;\n      while ((term = iterator.next()) != null) {\n        docs = iterator.docs(bits, randomDocsEnum(\"body\", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);\n        enums.put(docs, true);\n      }\n      assertEquals(terms.size(), enums.size());\n    }\n    writer.close();\n    IOUtils.close(firstReader, secondReader, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4cc45c615dbb82bf79d5f9550286098367874fbf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71387d8cb6923eb831b17a8b734608ba2e21c653"],"402ad3ddc9da7b70da1b167667a60ece6a1381fb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4cc45c615dbb82bf79d5f9550286098367874fbf"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4cc45c615dbb82bf79d5f9550286098367874fbf","402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"4cc45c615dbb82bf79d5f9550286098367874fbf":["402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["71387d8cb6923eb831b17a8b734608ba2e21c653"],"402ad3ddc9da7b70da1b167667a60ece6a1381fb":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}