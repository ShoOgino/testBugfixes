{"path":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","commits":[{"id":"878eedeaae8b281cc57edbb48be7876469cec585","date":1205050740,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"/dev/null","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true, false);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["8cac9bbcf5acbef2d0d83f6e9e32a22d71301db5","0f44610301174bfb430443d89a88dc1c502feea1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5350389bf83287111f7760b9e3db3af8e3648474","date":1216372812,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      //flush(false, true, false);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true, false);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"902ba79f4590a41c663c447756d2e5041cbbdda9","date":1217956662,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      //flush(false, true, false);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      //flush(false, true, false);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0f44610301174bfb430443d89a88dc1c502feea1","date":1231194664,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      //flush(false, true, false);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    deleter.incRef(merge.segmentsClone, false);\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      //flush(false, true, false);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":["878eedeaae8b281cc57edbb48be7876469cec585"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c4ff8864209d2e972cb4393600c26082f9a6533d","date":1239297466,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n      //flush(false, true, false);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    deleter.incRef(merge.segmentsClone, false);\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cd488f50316362b01a7f67b11a96796b9652e3e5","date":1241121034,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3ab91f3bb602daf6393fa7f78b11afd3400d669","date":1243282044,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map details = new HashMap();\n    details.put(\"optimize\", merge.optimize+\"\");\n    details.put(\"mergeFactor\", end+\"\");\n    details.put(\"mergeDocStores\", mergeDocStores+\"\");\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ce99e2b80b5a9cb2b9b59c01219e5397b081dcd8","date":1255049357,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map details = new HashMap();\n    details.put(\"optimize\", merge.optimize+\"\");\n    details.put(\"mergeFactor\", end+\"\");\n    details.put(\"mergeDocStores\", mergeDocStores+\"\");\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map details = new HashMap();\n    details.put(\"optimize\", merge.optimize+\"\");\n    details.put(\"mergeFactor\", end+\"\");\n    details.put(\"mergeDocStores\", mergeDocStores+\"\");\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ef82ff03e4016c705811b2658e81471a645c0e49","date":1255900293,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", merge.optimize+\"\");\n    details.put(\"mergeFactor\", end+\"\");\n    details.put(\"mergeDocStores\", mergeDocStores+\"\");\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map details = new HashMap();\n    details.put(\"optimize\", merge.optimize+\"\");\n    details.put(\"mergeFactor\", end+\"\");\n    details.put(\"mergeDocStores\", mergeDocStores+\"\");\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5b3a37e3d7ae8e41898848694a885910f54c5980","date":1267376360,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", merge.optimize+\"\");\n    details.put(\"mergeFactor\", end+\"\");\n    details.put(\"mergeDocStores\", mergeDocStores+\"\");\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", Boolean.toString(merge.optimize));\n    details.put(\"mergeFactor\", Integer.toString(end));\n    details.put(\"mergeDocStores\", Boolean.toString(mergeDocStores));\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"ce99e2b80b5a9cb2b9b59c01219e5397b081dcd8":["d3ab91f3bb602daf6393fa7f78b11afd3400d669"],"c4ff8864209d2e972cb4393600c26082f9a6533d":["0f44610301174bfb430443d89a88dc1c502feea1"],"902ba79f4590a41c663c447756d2e5041cbbdda9":["5350389bf83287111f7760b9e3db3af8e3648474"],"878eedeaae8b281cc57edbb48be7876469cec585":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd488f50316362b01a7f67b11a96796b9652e3e5":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"0f44610301174bfb430443d89a88dc1c502feea1":["902ba79f4590a41c663c447756d2e5041cbbdda9"],"ef82ff03e4016c705811b2658e81471a645c0e49":["ce99e2b80b5a9cb2b9b59c01219e5397b081dcd8"],"d3ab91f3bb602daf6393fa7f78b11afd3400d669":["cd488f50316362b01a7f67b11a96796b9652e3e5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5b3a37e3d7ae8e41898848694a885910f54c5980":["ef82ff03e4016c705811b2658e81471a645c0e49"],"5350389bf83287111f7760b9e3db3af8e3648474":["878eedeaae8b281cc57edbb48be7876469cec585"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["5b3a37e3d7ae8e41898848694a885910f54c5980"]},"commit2Childs":{"ce99e2b80b5a9cb2b9b59c01219e5397b081dcd8":["ef82ff03e4016c705811b2658e81471a645c0e49"],"c4ff8864209d2e972cb4393600c26082f9a6533d":["cd488f50316362b01a7f67b11a96796b9652e3e5"],"902ba79f4590a41c663c447756d2e5041cbbdda9":["0f44610301174bfb430443d89a88dc1c502feea1"],"878eedeaae8b281cc57edbb48be7876469cec585":["5350389bf83287111f7760b9e3db3af8e3648474"],"0f44610301174bfb430443d89a88dc1c502feea1":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"cd488f50316362b01a7f67b11a96796b9652e3e5":["d3ab91f3bb602daf6393fa7f78b11afd3400d669"],"d3ab91f3bb602daf6393fa7f78b11afd3400d669":["ce99e2b80b5a9cb2b9b59c01219e5397b081dcd8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["878eedeaae8b281cc57edbb48be7876469cec585"],"ef82ff03e4016c705811b2658e81471a645c0e49":["5b3a37e3d7ae8e41898848694a885910f54c5980"],"5b3a37e3d7ae8e41898848694a885910f54c5980":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"5350389bf83287111f7760b9e3db3af8e3648474":["902ba79f4590a41c663c447756d2e5041cbbdda9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}