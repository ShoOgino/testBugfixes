{"path":"solr/core/src/java/org/apache/solr/util/stats/ExponentiallyDecayingSample#rescale(long,long).mjava","commits":[{"id":"ce58244be71dcade436d9313669202ebc249466b","date":1357224775,"type":0,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/stats/ExponentiallyDecayingSample#rescale(long,long).mjava","pathOld":"/dev/null","sourceNew":"  /* \"A common feature of the above techniques—indeed, the key technique that\n   * allows us to track the decayed weights efficiently—is that they maintain\n   * counts and other quantities based on g(ti − L), and only scale by g(t − L)\n   * at query time. But while g(ti −L)/g(t−L) is guaranteed to lie between zero\n   * and one, the intermediate values of g(ti − L) could become very large. For\n   * polynomial functions, these values should not grow too large, and should be\n   * effectively represented in practice by floating point values without loss of\n   * precision. For exponential functions, these values could grow quite large as\n   * new values of (ti − L) become large, and potentially exceed the capacity of\n   * common floating point types. However, since the values stored by the\n   * algorithms are linear combinations of g values (scaled sums), they can be\n   * rescaled relative to a new landmark. That is, by the analysis of exponential\n   * decay in Section III-A, the choice of L does not affect the final result. We\n   * can therefore multiply each value based on L by a factor of exp(−α(L′ − L)),\n   * and obtain the correct value as if we had instead computed relative to a new\n   * landmark L′ (and then use this new L′ at query time). This can be done with\n   * a linear pass over whatever data structure is being used.\"\n   */\n  private void rescale(long now, long next) {\n    if (nextScaleTime.compareAndSet(next, now + RESCALE_THRESHOLD)) {\n      lockForRescale();\n      try {\n        final long oldStartTime = startTime;\n        this.startTime = currentTimeInSeconds();\n        final ArrayList<Double> keys = new ArrayList<Double>(values.keySet());\n        for (Double key : keys) {\n          final Long value = values.remove(key);\n          values.put(key * exp(-alpha * (startTime - oldStartTime)), value);\n        }\n\n        // make sure the counter is in sync with the number of stored samples.\n        count.set(values.size());\n      } finally {\n        unlockForRescale();\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/util/stats/ExponentiallyDecayingSample#rescale(long,long).mjava","pathOld":"/dev/null","sourceNew":"  /* \"A common feature of the above techniques—indeed, the key technique that\n   * allows us to track the decayed weights efficiently—is that they maintain\n   * counts and other quantities based on g(ti − L), and only scale by g(t − L)\n   * at query time. But while g(ti −L)/g(t−L) is guaranteed to lie between zero\n   * and one, the intermediate values of g(ti − L) could become very large. For\n   * polynomial functions, these values should not grow too large, and should be\n   * effectively represented in practice by floating point values without loss of\n   * precision. For exponential functions, these values could grow quite large as\n   * new values of (ti − L) become large, and potentially exceed the capacity of\n   * common floating point types. However, since the values stored by the\n   * algorithms are linear combinations of g values (scaled sums), they can be\n   * rescaled relative to a new landmark. That is, by the analysis of exponential\n   * decay in Section III-A, the choice of L does not affect the final result. We\n   * can therefore multiply each value based on L by a factor of exp(−α(L′ − L)),\n   * and obtain the correct value as if we had instead computed relative to a new\n   * landmark L′ (and then use this new L′ at query time). This can be done with\n   * a linear pass over whatever data structure is being used.\"\n   */\n  private void rescale(long now, long next) {\n    if (nextScaleTime.compareAndSet(next, now + RESCALE_THRESHOLD)) {\n      lockForRescale();\n      try {\n        final long oldStartTime = startTime;\n        this.startTime = currentTimeInSeconds();\n        final ArrayList<Double> keys = new ArrayList<Double>(values.keySet());\n        for (Double key : keys) {\n          final Long value = values.remove(key);\n          values.put(key * exp(-alpha * (startTime - oldStartTime)), value);\n        }\n\n        // make sure the counter is in sync with the number of stored samples.\n        count.set(values.size());\n      } finally {\n        unlockForRescale();\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/stats/ExponentiallyDecayingSample#rescale(long,long).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/stats/ExponentiallyDecayingSample#rescale(long,long).mjava","sourceNew":"  /* \"A common feature of the above techniques—indeed, the key technique that\n   * allows us to track the decayed weights efficiently—is that they maintain\n   * counts and other quantities based on g(ti − L), and only scale by g(t − L)\n   * at query time. But while g(ti −L)/g(t−L) is guaranteed to lie between zero\n   * and one, the intermediate values of g(ti − L) could become very large. For\n   * polynomial functions, these values should not grow too large, and should be\n   * effectively represented in practice by floating point values without loss of\n   * precision. For exponential functions, these values could grow quite large as\n   * new values of (ti − L) become large, and potentially exceed the capacity of\n   * common floating point types. However, since the values stored by the\n   * algorithms are linear combinations of g values (scaled sums), they can be\n   * rescaled relative to a new landmark. That is, by the analysis of exponential\n   * decay in Section III-A, the choice of L does not affect the final result. We\n   * can therefore multiply each value based on L by a factor of exp(−α(L′ − L)),\n   * and obtain the correct value as if we had instead computed relative to a new\n   * landmark L′ (and then use this new L′ at query time). This can be done with\n   * a linear pass over whatever data structure is being used.\"\n   */\n  private void rescale(long now, long next) {\n    if (nextScaleTime.compareAndSet(next, now + RESCALE_THRESHOLD)) {\n      lockForRescale();\n      try {\n        final long oldStartTime = startTime;\n        this.startTime = currentTimeInSeconds();\n        final ArrayList<Double> keys = new ArrayList<>(values.keySet());\n        for (Double key : keys) {\n          final Long value = values.remove(key);\n          values.put(key * exp(-alpha * (startTime - oldStartTime)), value);\n        }\n\n        // make sure the counter is in sync with the number of stored samples.\n        count.set(values.size());\n      } finally {\n        unlockForRescale();\n      }\n    }\n  }\n\n","sourceOld":"  /* \"A common feature of the above techniques—indeed, the key technique that\n   * allows us to track the decayed weights efficiently—is that they maintain\n   * counts and other quantities based on g(ti − L), and only scale by g(t − L)\n   * at query time. But while g(ti −L)/g(t−L) is guaranteed to lie between zero\n   * and one, the intermediate values of g(ti − L) could become very large. For\n   * polynomial functions, these values should not grow too large, and should be\n   * effectively represented in practice by floating point values without loss of\n   * precision. For exponential functions, these values could grow quite large as\n   * new values of (ti − L) become large, and potentially exceed the capacity of\n   * common floating point types. However, since the values stored by the\n   * algorithms are linear combinations of g values (scaled sums), they can be\n   * rescaled relative to a new landmark. That is, by the analysis of exponential\n   * decay in Section III-A, the choice of L does not affect the final result. We\n   * can therefore multiply each value based on L by a factor of exp(−α(L′ − L)),\n   * and obtain the correct value as if we had instead computed relative to a new\n   * landmark L′ (and then use this new L′ at query time). This can be done with\n   * a linear pass over whatever data structure is being used.\"\n   */\n  private void rescale(long now, long next) {\n    if (nextScaleTime.compareAndSet(next, now + RESCALE_THRESHOLD)) {\n      lockForRescale();\n      try {\n        final long oldStartTime = startTime;\n        this.startTime = currentTimeInSeconds();\n        final ArrayList<Double> keys = new ArrayList<Double>(values.keySet());\n        for (Double key : keys) {\n          final Long value = values.remove(key);\n          values.put(key * exp(-alpha * (startTime - oldStartTime)), value);\n        }\n\n        // make sure the counter is in sync with the number of stored samples.\n        count.set(values.size());\n      } finally {\n        unlockForRescale();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f3e7752e8a5e963c6939aaa99c36134147a5ece","date":1479805653,"type":4,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/util/stats/ExponentiallyDecayingSample#rescale(long,long).mjava","sourceNew":null,"sourceOld":"  /* \"A common feature of the above techniques—indeed, the key technique that\n   * allows us to track the decayed weights efficiently—is that they maintain\n   * counts and other quantities based on g(ti − L), and only scale by g(t − L)\n   * at query time. But while g(ti −L)/g(t−L) is guaranteed to lie between zero\n   * and one, the intermediate values of g(ti − L) could become very large. For\n   * polynomial functions, these values should not grow too large, and should be\n   * effectively represented in practice by floating point values without loss of\n   * precision. For exponential functions, these values could grow quite large as\n   * new values of (ti − L) become large, and potentially exceed the capacity of\n   * common floating point types. However, since the values stored by the\n   * algorithms are linear combinations of g values (scaled sums), they can be\n   * rescaled relative to a new landmark. That is, by the analysis of exponential\n   * decay in Section III-A, the choice of L does not affect the final result. We\n   * can therefore multiply each value based on L by a factor of exp(−α(L′ − L)),\n   * and obtain the correct value as if we had instead computed relative to a new\n   * landmark L′ (and then use this new L′ at query time). This can be done with\n   * a linear pass over whatever data structure is being used.\"\n   */\n  private void rescale(long now, long next) {\n    if (nextScaleTime.compareAndSet(next, now + RESCALE_THRESHOLD)) {\n      lockForRescale();\n      try {\n        final long oldStartTime = startTime;\n        this.startTime = currentTimeInSeconds();\n        final ArrayList<Double> keys = new ArrayList<>(values.keySet());\n        for (Double key : keys) {\n          final Long value = values.remove(key);\n          values.put(key * exp(-alpha * (startTime - oldStartTime)), value);\n        }\n\n        // make sure the counter is in sync with the number of stored samples.\n        count.set(values.size());\n      } finally {\n        unlockForRescale();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"727bb765ff2542275f6d31f67be18d7104bae148","date":1480353976,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/util/stats/ExponentiallyDecayingSample#rescale(long,long).mjava","sourceNew":null,"sourceOld":"  /* \"A common feature of the above techniques—indeed, the key technique that\n   * allows us to track the decayed weights efficiently—is that they maintain\n   * counts and other quantities based on g(ti − L), and only scale by g(t − L)\n   * at query time. But while g(ti −L)/g(t−L) is guaranteed to lie between zero\n   * and one, the intermediate values of g(ti − L) could become very large. For\n   * polynomial functions, these values should not grow too large, and should be\n   * effectively represented in practice by floating point values without loss of\n   * precision. For exponential functions, these values could grow quite large as\n   * new values of (ti − L) become large, and potentially exceed the capacity of\n   * common floating point types. However, since the values stored by the\n   * algorithms are linear combinations of g values (scaled sums), they can be\n   * rescaled relative to a new landmark. That is, by the analysis of exponential\n   * decay in Section III-A, the choice of L does not affect the final result. We\n   * can therefore multiply each value based on L by a factor of exp(−α(L′ − L)),\n   * and obtain the correct value as if we had instead computed relative to a new\n   * landmark L′ (and then use this new L′ at query time). This can be done with\n   * a linear pass over whatever data structure is being used.\"\n   */\n  private void rescale(long now, long next) {\n    if (nextScaleTime.compareAndSet(next, now + RESCALE_THRESHOLD)) {\n      lockForRescale();\n      try {\n        final long oldStartTime = startTime;\n        this.startTime = currentTimeInSeconds();\n        final ArrayList<Double> keys = new ArrayList<>(values.keySet());\n        for (Double key : keys) {\n          final Long value = values.remove(key);\n          values.put(key * exp(-alpha * (startTime - oldStartTime)), value);\n        }\n\n        // make sure the counter is in sync with the number of stored samples.\n        count.set(values.size());\n      } finally {\n        unlockForRescale();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ce58244be71dcade436d9313669202ebc249466b"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ce58244be71dcade436d9313669202ebc249466b"],"0f3e7752e8a5e963c6939aaa99c36134147a5ece":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"727bb765ff2542275f6d31f67be18d7104bae148":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","0f3e7752e8a5e963c6939aaa99c36134147a5ece"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ce58244be71dcade436d9313669202ebc249466b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f3e7752e8a5e963c6939aaa99c36134147a5ece"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["0f3e7752e8a5e963c6939aaa99c36134147a5ece","727bb765ff2542275f6d31f67be18d7104bae148"],"0f3e7752e8a5e963c6939aaa99c36134147a5ece":["727bb765ff2542275f6d31f67be18d7104bae148","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"727bb765ff2542275f6d31f67be18d7104bae148":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","ce58244be71dcade436d9313669202ebc249466b"],"ce58244be71dcade436d9313669202ebc249466b":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","727bb765ff2542275f6d31f67be18d7104bae148","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}