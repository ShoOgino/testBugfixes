{"path":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#test().mjava","commits":[{"id":"4a4e2c829188fb99886a64558664d79c9ac0fdf1","date":1431021538,"type":0,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#test().mjava","pathOld":"/dev/null","sourceNew":"  public void test() throws Exception {\n    buildIndex();\n    \n    { // simple sanity checks - don't leak variables\n      QueryResponse rsp = null;\n      rsp = query(params(\"rows\", \"0\", \"q\", \"id:42\")); \n      assertEquals(1, rsp.getResults().getNumFound());\n      \n      rsp = query(params(\"rows\", \"0\", \"q\", \"*:*\", \n                         \"stats\",\"true\", \"stats.field\", \"{!min=true max=true}long_l\"));\n      assertEquals(NUM_DOCS, rsp.getResults().getNumFound());\n      assertEquals(MIN_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMin()));\n      assertEquals(MAX_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMax()));\n    }\n\n    final int NUM_QUERIES = atLeast(100);\n\n    // Some Randomized queries with randomized log2m and max regwidth\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      // testing shows that on random data, at the size we're dealing with, \n      // MINIMUM_LOG2M_PARAM is just too absurdly small to give anything remotely close the \n      // the theoretically expected relative error.\n      //\n      // So we have to use a slightly higher lower bound on what log2m values we randomly test\n      final int log2m = TestUtil.nextInt(random(), \n                                         2 + HLL.MINIMUM_LOG2M_PARAM, \n                                         HLL.MAXIMUM_LOG2M_PARAM);\n\n      // use max regwidth to try and prevent hash collisions from introducing problems\n      final int regwidth = HLL.MAXIMUM_REGWIDTH_PARAM;\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      SolrParams p = buildCardinalityQ(lowId, highId, log2m, regwidth);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      for (String f : STAT_FIELDS) {\n        // regardless of log2m and regwidth, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed, real=\"+ numMatches + \", p=\" + p,\n                     stats.get(f).getCardinality().longValue(),\n                     stats.get(f+\"_prehashed_l\").getCardinality().longValue());\n      }\n\n      for (String f : STAT_FIELDS) {\n        // check the relative error of the estimate returned against the known truth\n\n        final double relErr = expectedRelativeError(log2m);\n        final long estimate = stats.get(f).getCardinality().longValue();\n        assertTrue(f + \": relativeErr=\"+relErr+\", estimate=\"+estimate+\", real=\"+numMatches+\", p=\" + p,\n                   (Math.abs(numMatches - estimate) / numMatches) < relErr);\n        \n      }\n    }\n    \n    // Some Randomized queries with both low and high accuracy options\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      // WTF? - https://github.com/aggregateknowledge/java-hll/issues/15\n      // \n      // aparently we can't rely on estimates always being more accurate with higher log2m values?\n      // so for now, just try testing accuracy values that differ by at least 0.5\n      //\n      // (that should give us a significant enough log2m diff that the \"highAccuracy\" is always\n      // more accurate -- if, not then the entire premise of the float value is fundementally bogus)\n      // \n      final double lowAccuracy = random().nextDouble() / 2;\n      // final double highAccuracy = Math.min(1.0D, lowAccuracy + (random().nextDouble() / 2));\n      final double highAccuracy = Math.min(1.0D, lowAccuracy + 0.5D);\n\n      SolrParams p = buildCardinalityQ(lowId, highId, lowAccuracy, highAccuracy);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      // can't use STAT_FIELDS here ...\n      //\n      // hueristic differences for regwidth on 32 bit values mean we get differences \n      // between estimates for the normal field vs the prehashed (long) field\n      //\n      // so we settle for only testing things where the regwidth is consistent \n      // w/the prehashed long...\n      for (String f : new String[] { \"long_l\", \"string_s\" }) {\n\n        // regardless of accuracy, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed (low), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"low_\"+f).getCardinality().longValue(),\n                     stats.get(\"low_\"+f+\"_prehashed_l\").getCardinality().longValue());\n        assertEquals(f + \": hashed vs prehashed (high), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"high_\"+f).getCardinality().longValue(),\n                     stats.get(\"high_\"+f+\"_prehashed_l\").getCardinality().longValue());\n      }\n      \n      for (String f : STAT_FIELDS) {\n        for (String ff : new String[] { f, f+\"_prehashed_l\"}) {\n          // for both the prehashed and regular fields, the high accuracy option \n          // should always produce an estimate at least as good as the low accuracy option\n          \n          long poorEst = stats.get(\"low_\"+ff).getCardinality();\n          long goodEst = stats.get(\"high_\"+ff).getCardinality();\n          assertTrue(ff + \": goodEst=\"+goodEst+\", poorEst=\"+poorEst+\", real=\"+numMatches+\", p=\" + p,\n                     Math.abs(numMatches - goodEst) <= Math.abs(numMatches - poorEst));\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"351efe6fdecf9af62134d37ec2582e4a0331a4dc","date":1498149096,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#test().mjava","sourceNew":"  public void test() throws Exception {\n    buildIndex();\n    \n    { // simple sanity checks - don't leak variables\n      QueryResponse rsp = null;\n      rsp = query(params(\"rows\", \"0\", \"q\", \"id:42\")); \n      assertEquals(1, rsp.getResults().getNumFound());\n      \n      rsp = query(params(\"rows\", \"0\", \"q\", \"*:*\", \n                         \"stats\",\"true\", \"stats.field\", \"{!min=true max=true}long_l\"));\n      assertEquals(NUM_DOCS, rsp.getResults().getNumFound());\n      assertEquals(MIN_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMin()));\n      assertEquals(MAX_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMax()));\n    }\n\n    final int NUM_QUERIES = atLeast(100);\n\n    // Some Randomized queries with randomized log2m and max regwidth\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      // testing shows that on random data, at the size we're dealing with, \n      // MINIMUM_LOG2M_PARAM is just too absurdly small to give anything remotely close the \n      // the theoretically expected relative error.\n      //\n      // So we have to use a slightly higher lower bound on what log2m values we randomly test\n      final int log2m = TestUtil.nextInt(random(), \n                                         2 + HLL.MINIMUM_LOG2M_PARAM, \n                                         HLL.MAXIMUM_LOG2M_PARAM);\n\n      // use max regwidth to try and prevent hash collisions from introducing problems\n      final int regwidth = HLL.MAXIMUM_REGWIDTH_PARAM;\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      SolrParams p = buildCardinalityQ(lowId, highId, log2m, regwidth);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      if (Boolean.getBoolean(NUMERIC_POINTS_SYSPROP)) {\n        log.warn(\"SOLR-10918: can't relying on exact match with pre-hashed values when using points\");\n      } else {\n        for (String f : STAT_FIELDS) {\n          // regardless of log2m and regwidth, the estimated cardinality of the \n          // hashed vs prehashed values should be exactly the same for each field\n          \n          assertEquals(f + \": hashed vs prehashed, real=\"+ numMatches + \", p=\" + p,\n                       stats.get(f).getCardinality().longValue(),\n                       stats.get(f+\"_prehashed_l\").getCardinality().longValue());\n        }\n      }\n\n      for (String f : STAT_FIELDS) {\n        // check the relative error of the estimate returned against the known truth\n\n        final double relErr = expectedRelativeError(log2m);\n        final long estimate = stats.get(f).getCardinality().longValue();\n        assertTrue(f + \": relativeErr=\"+relErr+\", estimate=\"+estimate+\", real=\"+numMatches+\", p=\" + p,\n                   (Math.abs(numMatches - estimate) / numMatches) < relErr);\n        \n      }\n    }\n    \n    // Some Randomized queries with both low and high accuracy options\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      // WTF? - https://github.com/aggregateknowledge/java-hll/issues/15\n      // \n      // aparently we can't rely on estimates always being more accurate with higher log2m values?\n      // so for now, just try testing accuracy values that differ by at least 0.5\n      //\n      // (that should give us a significant enough log2m diff that the \"highAccuracy\" is always\n      // more accurate -- if, not then the entire premise of the float value is fundementally bogus)\n      // \n      final double lowAccuracy = random().nextDouble() / 2;\n      // final double highAccuracy = Math.min(1.0D, lowAccuracy + (random().nextDouble() / 2));\n      final double highAccuracy = Math.min(1.0D, lowAccuracy + 0.5D);\n\n      SolrParams p = buildCardinalityQ(lowId, highId, lowAccuracy, highAccuracy);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      // can't use STAT_FIELDS here ...\n      //\n      // hueristic differences for regwidth on 32 bit values mean we get differences \n      // between estimates for the normal field vs the prehashed (long) field\n      //\n      // so we settle for only testing things where the regwidth is consistent \n      // w/the prehashed long...\n      for (String f : new String[] { \"long_l\", \"string_s\" }) {\n\n        // regardless of accuracy, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed (low), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"low_\"+f).getCardinality().longValue(),\n                     stats.get(\"low_\"+f+\"_prehashed_l\").getCardinality().longValue());\n        assertEquals(f + \": hashed vs prehashed (high), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"high_\"+f).getCardinality().longValue(),\n                     stats.get(\"high_\"+f+\"_prehashed_l\").getCardinality().longValue());\n      }\n      \n      for (String f : STAT_FIELDS) {\n        for (String ff : new String[] { f, f+\"_prehashed_l\"}) {\n          // for both the prehashed and regular fields, the high accuracy option \n          // should always produce an estimate at least as good as the low accuracy option\n          \n          long poorEst = stats.get(\"low_\"+ff).getCardinality();\n          long goodEst = stats.get(\"high_\"+ff).getCardinality();\n          assertTrue(ff + \": goodEst=\"+goodEst+\", poorEst=\"+poorEst+\", real=\"+numMatches+\", p=\" + p,\n                     Math.abs(numMatches - goodEst) <= Math.abs(numMatches - poorEst));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    buildIndex();\n    \n    { // simple sanity checks - don't leak variables\n      QueryResponse rsp = null;\n      rsp = query(params(\"rows\", \"0\", \"q\", \"id:42\")); \n      assertEquals(1, rsp.getResults().getNumFound());\n      \n      rsp = query(params(\"rows\", \"0\", \"q\", \"*:*\", \n                         \"stats\",\"true\", \"stats.field\", \"{!min=true max=true}long_l\"));\n      assertEquals(NUM_DOCS, rsp.getResults().getNumFound());\n      assertEquals(MIN_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMin()));\n      assertEquals(MAX_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMax()));\n    }\n\n    final int NUM_QUERIES = atLeast(100);\n\n    // Some Randomized queries with randomized log2m and max regwidth\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      // testing shows that on random data, at the size we're dealing with, \n      // MINIMUM_LOG2M_PARAM is just too absurdly small to give anything remotely close the \n      // the theoretically expected relative error.\n      //\n      // So we have to use a slightly higher lower bound on what log2m values we randomly test\n      final int log2m = TestUtil.nextInt(random(), \n                                         2 + HLL.MINIMUM_LOG2M_PARAM, \n                                         HLL.MAXIMUM_LOG2M_PARAM);\n\n      // use max regwidth to try and prevent hash collisions from introducing problems\n      final int regwidth = HLL.MAXIMUM_REGWIDTH_PARAM;\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      SolrParams p = buildCardinalityQ(lowId, highId, log2m, regwidth);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      for (String f : STAT_FIELDS) {\n        // regardless of log2m and regwidth, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed, real=\"+ numMatches + \", p=\" + p,\n                     stats.get(f).getCardinality().longValue(),\n                     stats.get(f+\"_prehashed_l\").getCardinality().longValue());\n      }\n\n      for (String f : STAT_FIELDS) {\n        // check the relative error of the estimate returned against the known truth\n\n        final double relErr = expectedRelativeError(log2m);\n        final long estimate = stats.get(f).getCardinality().longValue();\n        assertTrue(f + \": relativeErr=\"+relErr+\", estimate=\"+estimate+\", real=\"+numMatches+\", p=\" + p,\n                   (Math.abs(numMatches - estimate) / numMatches) < relErr);\n        \n      }\n    }\n    \n    // Some Randomized queries with both low and high accuracy options\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      // WTF? - https://github.com/aggregateknowledge/java-hll/issues/15\n      // \n      // aparently we can't rely on estimates always being more accurate with higher log2m values?\n      // so for now, just try testing accuracy values that differ by at least 0.5\n      //\n      // (that should give us a significant enough log2m diff that the \"highAccuracy\" is always\n      // more accurate -- if, not then the entire premise of the float value is fundementally bogus)\n      // \n      final double lowAccuracy = random().nextDouble() / 2;\n      // final double highAccuracy = Math.min(1.0D, lowAccuracy + (random().nextDouble() / 2));\n      final double highAccuracy = Math.min(1.0D, lowAccuracy + 0.5D);\n\n      SolrParams p = buildCardinalityQ(lowId, highId, lowAccuracy, highAccuracy);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      // can't use STAT_FIELDS here ...\n      //\n      // hueristic differences for regwidth on 32 bit values mean we get differences \n      // between estimates for the normal field vs the prehashed (long) field\n      //\n      // so we settle for only testing things where the regwidth is consistent \n      // w/the prehashed long...\n      for (String f : new String[] { \"long_l\", \"string_s\" }) {\n\n        // regardless of accuracy, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed (low), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"low_\"+f).getCardinality().longValue(),\n                     stats.get(\"low_\"+f+\"_prehashed_l\").getCardinality().longValue());\n        assertEquals(f + \": hashed vs prehashed (high), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"high_\"+f).getCardinality().longValue(),\n                     stats.get(\"high_\"+f+\"_prehashed_l\").getCardinality().longValue());\n      }\n      \n      for (String f : STAT_FIELDS) {\n        for (String ff : new String[] { f, f+\"_prehashed_l\"}) {\n          // for both the prehashed and regular fields, the high accuracy option \n          // should always produce an estimate at least as good as the low accuracy option\n          \n          long poorEst = stats.get(\"low_\"+ff).getCardinality();\n          long goodEst = stats.get(\"high_\"+ff).getCardinality();\n          assertTrue(ff + \": goodEst=\"+goodEst+\", poorEst=\"+poorEst+\", real=\"+numMatches+\", p=\" + p,\n                     Math.abs(numMatches - goodEst) <= Math.abs(numMatches - poorEst));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#test().mjava","sourceNew":"  public void test() throws Exception {\n    buildIndex();\n    \n    { // simple sanity checks - don't leak variables\n      QueryResponse rsp = null;\n      rsp = query(params(\"rows\", \"0\", \"q\", \"id:42\")); \n      assertEquals(1, rsp.getResults().getNumFound());\n      \n      rsp = query(params(\"rows\", \"0\", \"q\", \"*:*\", \n                         \"stats\",\"true\", \"stats.field\", \"{!min=true max=true}long_l\"));\n      assertEquals(NUM_DOCS, rsp.getResults().getNumFound());\n      assertEquals(MIN_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMin()));\n      assertEquals(MAX_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMax()));\n    }\n\n    final int NUM_QUERIES = atLeast(100);\n\n    // Some Randomized queries with randomized log2m and max regwidth\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      // testing shows that on random data, at the size we're dealing with, \n      // MINIMUM_LOG2M_PARAM is just too absurdly small to give anything remotely close the \n      // the theoretically expected relative error.\n      //\n      // So we have to use a slightly higher lower bound on what log2m values we randomly test\n      final int log2m = TestUtil.nextInt(random(), \n                                         2 + HLL.MINIMUM_LOG2M_PARAM, \n                                         HLL.MAXIMUM_LOG2M_PARAM);\n\n      // use max regwidth to try and prevent hash collisions from introducing problems\n      final int regwidth = HLL.MAXIMUM_REGWIDTH_PARAM;\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      SolrParams p = buildCardinalityQ(lowId, highId, log2m, regwidth);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      if (Boolean.getBoolean(NUMERIC_POINTS_SYSPROP)) {\n        log.warn(\"SOLR-10918: can't relying on exact match with pre-hashed values when using points\");\n      } else {\n        for (String f : STAT_FIELDS) {\n          // regardless of log2m and regwidth, the estimated cardinality of the \n          // hashed vs prehashed values should be exactly the same for each field\n          \n          assertEquals(f + \": hashed vs prehashed, real=\"+ numMatches + \", p=\" + p,\n                       stats.get(f).getCardinality().longValue(),\n                       stats.get(f+\"_prehashed_l\").getCardinality().longValue());\n        }\n      }\n\n      for (String f : STAT_FIELDS) {\n        // check the relative error of the estimate returned against the known truth\n\n        final double relErr = expectedRelativeError(log2m);\n        final long estimate = stats.get(f).getCardinality().longValue();\n        assertTrue(f + \": relativeErr=\"+relErr+\", estimate=\"+estimate+\", real=\"+numMatches+\", p=\" + p,\n                   (Math.abs(numMatches - estimate) / numMatches) < relErr);\n        \n      }\n    }\n    \n    // Some Randomized queries with both low and high accuracy options\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      // WTF? - https://github.com/aggregateknowledge/java-hll/issues/15\n      // \n      // aparently we can't rely on estimates always being more accurate with higher log2m values?\n      // so for now, just try testing accuracy values that differ by at least 0.5\n      //\n      // (that should give us a significant enough log2m diff that the \"highAccuracy\" is always\n      // more accurate -- if, not then the entire premise of the float value is fundementally bogus)\n      // \n      final double lowAccuracy = random().nextDouble() / 2;\n      // final double highAccuracy = Math.min(1.0D, lowAccuracy + (random().nextDouble() / 2));\n      final double highAccuracy = Math.min(1.0D, lowAccuracy + 0.5D);\n\n      SolrParams p = buildCardinalityQ(lowId, highId, lowAccuracy, highAccuracy);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      // can't use STAT_FIELDS here ...\n      //\n      // hueristic differences for regwidth on 32 bit values mean we get differences \n      // between estimates for the normal field vs the prehashed (long) field\n      //\n      // so we settle for only testing things where the regwidth is consistent \n      // w/the prehashed long...\n      for (String f : new String[] { \"long_l\", \"string_s\" }) {\n\n        // regardless of accuracy, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed (low), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"low_\"+f).getCardinality().longValue(),\n                     stats.get(\"low_\"+f+\"_prehashed_l\").getCardinality().longValue());\n        assertEquals(f + \": hashed vs prehashed (high), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"high_\"+f).getCardinality().longValue(),\n                     stats.get(\"high_\"+f+\"_prehashed_l\").getCardinality().longValue());\n      }\n      \n      for (String f : STAT_FIELDS) {\n        for (String ff : new String[] { f, f+\"_prehashed_l\"}) {\n          // for both the prehashed and regular fields, the high accuracy option \n          // should always produce an estimate at least as good as the low accuracy option\n          \n          long poorEst = stats.get(\"low_\"+ff).getCardinality();\n          long goodEst = stats.get(\"high_\"+ff).getCardinality();\n          assertTrue(ff + \": goodEst=\"+goodEst+\", poorEst=\"+poorEst+\", real=\"+numMatches+\", p=\" + p,\n                     Math.abs(numMatches - goodEst) <= Math.abs(numMatches - poorEst));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    buildIndex();\n    \n    { // simple sanity checks - don't leak variables\n      QueryResponse rsp = null;\n      rsp = query(params(\"rows\", \"0\", \"q\", \"id:42\")); \n      assertEquals(1, rsp.getResults().getNumFound());\n      \n      rsp = query(params(\"rows\", \"0\", \"q\", \"*:*\", \n                         \"stats\",\"true\", \"stats.field\", \"{!min=true max=true}long_l\"));\n      assertEquals(NUM_DOCS, rsp.getResults().getNumFound());\n      assertEquals(MIN_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMin()));\n      assertEquals(MAX_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMax()));\n    }\n\n    final int NUM_QUERIES = atLeast(100);\n\n    // Some Randomized queries with randomized log2m and max regwidth\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      // testing shows that on random data, at the size we're dealing with, \n      // MINIMUM_LOG2M_PARAM is just too absurdly small to give anything remotely close the \n      // the theoretically expected relative error.\n      //\n      // So we have to use a slightly higher lower bound on what log2m values we randomly test\n      final int log2m = TestUtil.nextInt(random(), \n                                         2 + HLL.MINIMUM_LOG2M_PARAM, \n                                         HLL.MAXIMUM_LOG2M_PARAM);\n\n      // use max regwidth to try and prevent hash collisions from introducing problems\n      final int regwidth = HLL.MAXIMUM_REGWIDTH_PARAM;\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      SolrParams p = buildCardinalityQ(lowId, highId, log2m, regwidth);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      for (String f : STAT_FIELDS) {\n        // regardless of log2m and regwidth, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed, real=\"+ numMatches + \", p=\" + p,\n                     stats.get(f).getCardinality().longValue(),\n                     stats.get(f+\"_prehashed_l\").getCardinality().longValue());\n      }\n\n      for (String f : STAT_FIELDS) {\n        // check the relative error of the estimate returned against the known truth\n\n        final double relErr = expectedRelativeError(log2m);\n        final long estimate = stats.get(f).getCardinality().longValue();\n        assertTrue(f + \": relativeErr=\"+relErr+\", estimate=\"+estimate+\", real=\"+numMatches+\", p=\" + p,\n                   (Math.abs(numMatches - estimate) / numMatches) < relErr);\n        \n      }\n    }\n    \n    // Some Randomized queries with both low and high accuracy options\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      // WTF? - https://github.com/aggregateknowledge/java-hll/issues/15\n      // \n      // aparently we can't rely on estimates always being more accurate with higher log2m values?\n      // so for now, just try testing accuracy values that differ by at least 0.5\n      //\n      // (that should give us a significant enough log2m diff that the \"highAccuracy\" is always\n      // more accurate -- if, not then the entire premise of the float value is fundementally bogus)\n      // \n      final double lowAccuracy = random().nextDouble() / 2;\n      // final double highAccuracy = Math.min(1.0D, lowAccuracy + (random().nextDouble() / 2));\n      final double highAccuracy = Math.min(1.0D, lowAccuracy + 0.5D);\n\n      SolrParams p = buildCardinalityQ(lowId, highId, lowAccuracy, highAccuracy);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      // can't use STAT_FIELDS here ...\n      //\n      // hueristic differences for regwidth on 32 bit values mean we get differences \n      // between estimates for the normal field vs the prehashed (long) field\n      //\n      // so we settle for only testing things where the regwidth is consistent \n      // w/the prehashed long...\n      for (String f : new String[] { \"long_l\", \"string_s\" }) {\n\n        // regardless of accuracy, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed (low), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"low_\"+f).getCardinality().longValue(),\n                     stats.get(\"low_\"+f+\"_prehashed_l\").getCardinality().longValue());\n        assertEquals(f + \": hashed vs prehashed (high), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"high_\"+f).getCardinality().longValue(),\n                     stats.get(\"high_\"+f+\"_prehashed_l\").getCardinality().longValue());\n      }\n      \n      for (String f : STAT_FIELDS) {\n        for (String ff : new String[] { f, f+\"_prehashed_l\"}) {\n          // for both the prehashed and regular fields, the high accuracy option \n          // should always produce an estimate at least as good as the low accuracy option\n          \n          long poorEst = stats.get(\"low_\"+ff).getCardinality();\n          long goodEst = stats.get(\"high_\"+ff).getCardinality();\n          assertTrue(ff + \": goodEst=\"+goodEst+\", poorEst=\"+poorEst+\", real=\"+numMatches+\", p=\" + p,\n                     Math.abs(numMatches - goodEst) <= Math.abs(numMatches - poorEst));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#test().mjava","sourceNew":"  public void test() throws Exception {\n    buildIndex();\n    \n    { // simple sanity checks - don't leak variables\n      QueryResponse rsp = null;\n      rsp = query(params(\"rows\", \"0\", \"q\", \"id:42\")); \n      assertEquals(1, rsp.getResults().getNumFound());\n      \n      rsp = query(params(\"rows\", \"0\", \"q\", \"*:*\", \n                         \"stats\",\"true\", \"stats.field\", \"{!min=true max=true}long_l\"));\n      assertEquals(NUM_DOCS, rsp.getResults().getNumFound());\n      assertEquals(MIN_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMin()));\n      assertEquals(MAX_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMax()));\n    }\n\n    final int NUM_QUERIES = atLeast(100);\n\n    // Some Randomized queries with randomized log2m and max regwidth\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      // testing shows that on random data, at the size we're dealing with, \n      // MINIMUM_LOG2M_PARAM is just too absurdly small to give anything remotely close the \n      // the theoretically expected relative error.\n      //\n      // So we have to use a slightly higher lower bound on what log2m values we randomly test\n      final int log2m = TestUtil.nextInt(random(), \n                                         2 + HLL.MINIMUM_LOG2M_PARAM, \n                                         HLL.MAXIMUM_LOG2M_PARAM);\n\n      // use max regwidth to try and prevent hash collisions from introducing problems\n      final int regwidth = HLL.MAXIMUM_REGWIDTH_PARAM;\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      SolrParams p = buildCardinalityQ(lowId, highId, log2m, regwidth);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      if (Boolean.getBoolean(NUMERIC_POINTS_SYSPROP)) {\n        log.warn(\"SOLR-10918: can't relying on exact match with pre-hashed values when using points\");\n      } else {\n        for (String f : STAT_FIELDS) {\n          // regardless of log2m and regwidth, the estimated cardinality of the \n          // hashed vs prehashed values should be exactly the same for each field\n          \n          assertEquals(f + \": hashed vs prehashed, real=\"+ numMatches + \", p=\" + p,\n                       stats.get(f).getCardinality().longValue(),\n                       stats.get(f+\"_prehashed_l\").getCardinality().longValue());\n        }\n      }\n\n      for (String f : STAT_FIELDS) {\n        // check the relative error of the estimate returned against the known truth\n\n        final double relErr = expectedRelativeError(log2m);\n        final long estimate = stats.get(f).getCardinality().longValue();\n        assertTrue(f + \": relativeErr=\"+relErr+\", estimate=\"+estimate+\", real=\"+numMatches+\", p=\" + p,\n                   (Math.abs(numMatches - estimate) / numMatches) < relErr);\n        \n      }\n    }\n    \n    // Some Randomized queries with both low and high accuracy options\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      // WTF? - https://github.com/aggregateknowledge/java-hll/issues/15\n      // \n      // aparently we can't rely on estimates always being more accurate with higher log2m values?\n      // so for now, just try testing accuracy values that differ by at least 0.5\n      //\n      // (that should give us a significant enough log2m diff that the \"highAccuracy\" is always\n      // more accurate -- if, not then the entire premise of the float value is fundementally bogus)\n      // \n      final double lowAccuracy = random().nextDouble() / 2;\n      // final double highAccuracy = Math.min(1.0D, lowAccuracy + (random().nextDouble() / 2));\n      final double highAccuracy = Math.min(1.0D, lowAccuracy + 0.5D);\n\n      SolrParams p = buildCardinalityQ(lowId, highId, lowAccuracy, highAccuracy);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      // can't use STAT_FIELDS here ...\n      //\n      // hueristic differences for regwidth on 32 bit values mean we get differences \n      // between estimates for the normal field vs the prehashed (long) field\n      //\n      // so we settle for only testing things where the regwidth is consistent \n      // w/the prehashed long...\n      for (String f : new String[] { \"long_l\", \"string_s\" }) {\n\n        // regardless of accuracy, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed (low), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"low_\"+f).getCardinality().longValue(),\n                     stats.get(\"low_\"+f+\"_prehashed_l\").getCardinality().longValue());\n        assertEquals(f + \": hashed vs prehashed (high), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"high_\"+f).getCardinality().longValue(),\n                     stats.get(\"high_\"+f+\"_prehashed_l\").getCardinality().longValue());\n      }\n      \n      for (String f : STAT_FIELDS) {\n        for (String ff : new String[] { f, f+\"_prehashed_l\"}) {\n          // for both the prehashed and regular fields, the high accuracy option \n          // should always produce an estimate at least as good as the low accuracy option\n          \n          long poorEst = stats.get(\"low_\"+ff).getCardinality();\n          long goodEst = stats.get(\"high_\"+ff).getCardinality();\n          assertTrue(ff + \": goodEst=\"+goodEst+\", poorEst=\"+poorEst+\", real=\"+numMatches+\", p=\" + p,\n                     Math.abs(numMatches - goodEst) <= Math.abs(numMatches - poorEst));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    buildIndex();\n    \n    { // simple sanity checks - don't leak variables\n      QueryResponse rsp = null;\n      rsp = query(params(\"rows\", \"0\", \"q\", \"id:42\")); \n      assertEquals(1, rsp.getResults().getNumFound());\n      \n      rsp = query(params(\"rows\", \"0\", \"q\", \"*:*\", \n                         \"stats\",\"true\", \"stats.field\", \"{!min=true max=true}long_l\"));\n      assertEquals(NUM_DOCS, rsp.getResults().getNumFound());\n      assertEquals(MIN_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMin()));\n      assertEquals(MAX_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMax()));\n    }\n\n    final int NUM_QUERIES = atLeast(100);\n\n    // Some Randomized queries with randomized log2m and max regwidth\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      // testing shows that on random data, at the size we're dealing with, \n      // MINIMUM_LOG2M_PARAM is just too absurdly small to give anything remotely close the \n      // the theoretically expected relative error.\n      //\n      // So we have to use a slightly higher lower bound on what log2m values we randomly test\n      final int log2m = TestUtil.nextInt(random(), \n                                         2 + HLL.MINIMUM_LOG2M_PARAM, \n                                         HLL.MAXIMUM_LOG2M_PARAM);\n\n      // use max regwidth to try and prevent hash collisions from introducing problems\n      final int regwidth = HLL.MAXIMUM_REGWIDTH_PARAM;\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      SolrParams p = buildCardinalityQ(lowId, highId, log2m, regwidth);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      for (String f : STAT_FIELDS) {\n        // regardless of log2m and regwidth, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed, real=\"+ numMatches + \", p=\" + p,\n                     stats.get(f).getCardinality().longValue(),\n                     stats.get(f+\"_prehashed_l\").getCardinality().longValue());\n      }\n\n      for (String f : STAT_FIELDS) {\n        // check the relative error of the estimate returned against the known truth\n\n        final double relErr = expectedRelativeError(log2m);\n        final long estimate = stats.get(f).getCardinality().longValue();\n        assertTrue(f + \": relativeErr=\"+relErr+\", estimate=\"+estimate+\", real=\"+numMatches+\", p=\" + p,\n                   (Math.abs(numMatches - estimate) / numMatches) < relErr);\n        \n      }\n    }\n    \n    // Some Randomized queries with both low and high accuracy options\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      // WTF? - https://github.com/aggregateknowledge/java-hll/issues/15\n      // \n      // aparently we can't rely on estimates always being more accurate with higher log2m values?\n      // so for now, just try testing accuracy values that differ by at least 0.5\n      //\n      // (that should give us a significant enough log2m diff that the \"highAccuracy\" is always\n      // more accurate -- if, not then the entire premise of the float value is fundementally bogus)\n      // \n      final double lowAccuracy = random().nextDouble() / 2;\n      // final double highAccuracy = Math.min(1.0D, lowAccuracy + (random().nextDouble() / 2));\n      final double highAccuracy = Math.min(1.0D, lowAccuracy + 0.5D);\n\n      SolrParams p = buildCardinalityQ(lowId, highId, lowAccuracy, highAccuracy);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      // can't use STAT_FIELDS here ...\n      //\n      // hueristic differences for regwidth on 32 bit values mean we get differences \n      // between estimates for the normal field vs the prehashed (long) field\n      //\n      // so we settle for only testing things where the regwidth is consistent \n      // w/the prehashed long...\n      for (String f : new String[] { \"long_l\", \"string_s\" }) {\n\n        // regardless of accuracy, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed (low), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"low_\"+f).getCardinality().longValue(),\n                     stats.get(\"low_\"+f+\"_prehashed_l\").getCardinality().longValue());\n        assertEquals(f + \": hashed vs prehashed (high), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"high_\"+f).getCardinality().longValue(),\n                     stats.get(\"high_\"+f+\"_prehashed_l\").getCardinality().longValue());\n      }\n      \n      for (String f : STAT_FIELDS) {\n        for (String ff : new String[] { f, f+\"_prehashed_l\"}) {\n          // for both the prehashed and regular fields, the high accuracy option \n          // should always produce an estimate at least as good as the low accuracy option\n          \n          long poorEst = stats.get(\"low_\"+ff).getCardinality();\n          long goodEst = stats.get(\"high_\"+ff).getCardinality();\n          assertTrue(ff + \": goodEst=\"+goodEst+\", poorEst=\"+poorEst+\", real=\"+numMatches+\", p=\" + p,\n                     Math.abs(numMatches - goodEst) <= Math.abs(numMatches - poorEst));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"44dd40f6c2c1465aebf4677bab10f696c7ea18d8","date":1539566013,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#test().mjava","sourceNew":"  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 14-Oct-2018\n  public void test() throws Exception {\n    buildIndex();\n    \n    { // simple sanity checks - don't leak variables\n      QueryResponse rsp = null;\n      rsp = query(params(\"rows\", \"0\", \"q\", \"id:42\")); \n      assertEquals(1, rsp.getResults().getNumFound());\n      \n      rsp = query(params(\"rows\", \"0\", \"q\", \"*:*\", \n                         \"stats\",\"true\", \"stats.field\", \"{!min=true max=true}long_l\"));\n      assertEquals(NUM_DOCS, rsp.getResults().getNumFound());\n      assertEquals(MIN_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMin()));\n      assertEquals(MAX_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMax()));\n    }\n\n    final int NUM_QUERIES = atLeast(100);\n\n    // Some Randomized queries with randomized log2m and max regwidth\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      // testing shows that on random data, at the size we're dealing with, \n      // MINIMUM_LOG2M_PARAM is just too absurdly small to give anything remotely close the \n      // the theoretically expected relative error.\n      //\n      // So we have to use a slightly higher lower bound on what log2m values we randomly test\n      final int log2m = TestUtil.nextInt(random(), \n                                         2 + HLL.MINIMUM_LOG2M_PARAM, \n                                         HLL.MAXIMUM_LOG2M_PARAM);\n\n      // use max regwidth to try and prevent hash collisions from introducing problems\n      final int regwidth = HLL.MAXIMUM_REGWIDTH_PARAM;\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      SolrParams p = buildCardinalityQ(lowId, highId, log2m, regwidth);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      if (Boolean.getBoolean(NUMERIC_POINTS_SYSPROP)) {\n        log.warn(\"SOLR-10918: can't relying on exact match with pre-hashed values when using points\");\n      } else {\n        for (String f : STAT_FIELDS) {\n          // regardless of log2m and regwidth, the estimated cardinality of the \n          // hashed vs prehashed values should be exactly the same for each field\n          \n          assertEquals(f + \": hashed vs prehashed, real=\"+ numMatches + \", p=\" + p,\n                       stats.get(f).getCardinality().longValue(),\n                       stats.get(f+\"_prehashed_l\").getCardinality().longValue());\n        }\n      }\n\n      for (String f : STAT_FIELDS) {\n        // check the relative error of the estimate returned against the known truth\n\n        final double relErr = expectedRelativeError(log2m);\n        final long estimate = stats.get(f).getCardinality().longValue();\n        assertTrue(f + \": relativeErr=\"+relErr+\", estimate=\"+estimate+\", real=\"+numMatches+\", p=\" + p,\n                   (Math.abs(numMatches - estimate) / numMatches) < relErr);\n        \n      }\n    }\n    \n    // Some Randomized queries with both low and high accuracy options\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      // WTF? - https://github.com/aggregateknowledge/java-hll/issues/15\n      // \n      // aparently we can't rely on estimates always being more accurate with higher log2m values?\n      // so for now, just try testing accuracy values that differ by at least 0.5\n      //\n      // (that should give us a significant enough log2m diff that the \"highAccuracy\" is always\n      // more accurate -- if, not then the entire premise of the float value is fundementally bogus)\n      // \n      final double lowAccuracy = random().nextDouble() / 2;\n      // final double highAccuracy = Math.min(1.0D, lowAccuracy + (random().nextDouble() / 2));\n      final double highAccuracy = Math.min(1.0D, lowAccuracy + 0.5D);\n\n      SolrParams p = buildCardinalityQ(lowId, highId, lowAccuracy, highAccuracy);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      // can't use STAT_FIELDS here ...\n      //\n      // hueristic differences for regwidth on 32 bit values mean we get differences \n      // between estimates for the normal field vs the prehashed (long) field\n      //\n      // so we settle for only testing things where the regwidth is consistent \n      // w/the prehashed long...\n      for (String f : new String[] { \"long_l\", \"string_s\" }) {\n\n        // regardless of accuracy, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed (low), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"low_\"+f).getCardinality().longValue(),\n                     stats.get(\"low_\"+f+\"_prehashed_l\").getCardinality().longValue());\n        assertEquals(f + \": hashed vs prehashed (high), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"high_\"+f).getCardinality().longValue(),\n                     stats.get(\"high_\"+f+\"_prehashed_l\").getCardinality().longValue());\n      }\n      \n      for (String f : STAT_FIELDS) {\n        for (String ff : new String[] { f, f+\"_prehashed_l\"}) {\n          // for both the prehashed and regular fields, the high accuracy option \n          // should always produce an estimate at least as good as the low accuracy option\n          \n          long poorEst = stats.get(\"low_\"+ff).getCardinality();\n          long goodEst = stats.get(\"high_\"+ff).getCardinality();\n          assertTrue(ff + \": goodEst=\"+goodEst+\", poorEst=\"+poorEst+\", real=\"+numMatches+\", p=\" + p,\n                     Math.abs(numMatches - goodEst) <= Math.abs(numMatches - poorEst));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    buildIndex();\n    \n    { // simple sanity checks - don't leak variables\n      QueryResponse rsp = null;\n      rsp = query(params(\"rows\", \"0\", \"q\", \"id:42\")); \n      assertEquals(1, rsp.getResults().getNumFound());\n      \n      rsp = query(params(\"rows\", \"0\", \"q\", \"*:*\", \n                         \"stats\",\"true\", \"stats.field\", \"{!min=true max=true}long_l\"));\n      assertEquals(NUM_DOCS, rsp.getResults().getNumFound());\n      assertEquals(MIN_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMin()));\n      assertEquals(MAX_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMax()));\n    }\n\n    final int NUM_QUERIES = atLeast(100);\n\n    // Some Randomized queries with randomized log2m and max regwidth\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      // testing shows that on random data, at the size we're dealing with, \n      // MINIMUM_LOG2M_PARAM is just too absurdly small to give anything remotely close the \n      // the theoretically expected relative error.\n      //\n      // So we have to use a slightly higher lower bound on what log2m values we randomly test\n      final int log2m = TestUtil.nextInt(random(), \n                                         2 + HLL.MINIMUM_LOG2M_PARAM, \n                                         HLL.MAXIMUM_LOG2M_PARAM);\n\n      // use max regwidth to try and prevent hash collisions from introducing problems\n      final int regwidth = HLL.MAXIMUM_REGWIDTH_PARAM;\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      SolrParams p = buildCardinalityQ(lowId, highId, log2m, regwidth);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      if (Boolean.getBoolean(NUMERIC_POINTS_SYSPROP)) {\n        log.warn(\"SOLR-10918: can't relying on exact match with pre-hashed values when using points\");\n      } else {\n        for (String f : STAT_FIELDS) {\n          // regardless of log2m and regwidth, the estimated cardinality of the \n          // hashed vs prehashed values should be exactly the same for each field\n          \n          assertEquals(f + \": hashed vs prehashed, real=\"+ numMatches + \", p=\" + p,\n                       stats.get(f).getCardinality().longValue(),\n                       stats.get(f+\"_prehashed_l\").getCardinality().longValue());\n        }\n      }\n\n      for (String f : STAT_FIELDS) {\n        // check the relative error of the estimate returned against the known truth\n\n        final double relErr = expectedRelativeError(log2m);\n        final long estimate = stats.get(f).getCardinality().longValue();\n        assertTrue(f + \": relativeErr=\"+relErr+\", estimate=\"+estimate+\", real=\"+numMatches+\", p=\" + p,\n                   (Math.abs(numMatches - estimate) / numMatches) < relErr);\n        \n      }\n    }\n    \n    // Some Randomized queries with both low and high accuracy options\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      // WTF? - https://github.com/aggregateknowledge/java-hll/issues/15\n      // \n      // aparently we can't rely on estimates always being more accurate with higher log2m values?\n      // so for now, just try testing accuracy values that differ by at least 0.5\n      //\n      // (that should give us a significant enough log2m diff that the \"highAccuracy\" is always\n      // more accurate -- if, not then the entire premise of the float value is fundementally bogus)\n      // \n      final double lowAccuracy = random().nextDouble() / 2;\n      // final double highAccuracy = Math.min(1.0D, lowAccuracy + (random().nextDouble() / 2));\n      final double highAccuracy = Math.min(1.0D, lowAccuracy + 0.5D);\n\n      SolrParams p = buildCardinalityQ(lowId, highId, lowAccuracy, highAccuracy);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      // can't use STAT_FIELDS here ...\n      //\n      // hueristic differences for regwidth on 32 bit values mean we get differences \n      // between estimates for the normal field vs the prehashed (long) field\n      //\n      // so we settle for only testing things where the regwidth is consistent \n      // w/the prehashed long...\n      for (String f : new String[] { \"long_l\", \"string_s\" }) {\n\n        // regardless of accuracy, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed (low), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"low_\"+f).getCardinality().longValue(),\n                     stats.get(\"low_\"+f+\"_prehashed_l\").getCardinality().longValue());\n        assertEquals(f + \": hashed vs prehashed (high), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"high_\"+f).getCardinality().longValue(),\n                     stats.get(\"high_\"+f+\"_prehashed_l\").getCardinality().longValue());\n      }\n      \n      for (String f : STAT_FIELDS) {\n        for (String ff : new String[] { f, f+\"_prehashed_l\"}) {\n          // for both the prehashed and regular fields, the high accuracy option \n          // should always produce an estimate at least as good as the low accuracy option\n          \n          long poorEst = stats.get(\"low_\"+ff).getCardinality();\n          long goodEst = stats.get(\"high_\"+ff).getCardinality();\n          assertTrue(ff + \": goodEst=\"+goodEst+\", poorEst=\"+poorEst+\", real=\"+numMatches+\", p=\" + p,\n                     Math.abs(numMatches - goodEst) <= Math.abs(numMatches - poorEst));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"832396a97c45687ccbe41d05936969e4996dc701","date":1573881939,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/component/TestDistributedStatsComponentCardinality#test().mjava","sourceNew":"  public void test() throws Exception {\n    buildIndex();\n    \n    { // simple sanity checks - don't leak variables\n      QueryResponse rsp = null;\n      rsp = query(params(\"rows\", \"0\", \"q\", \"id:42\")); \n      assertEquals(1, rsp.getResults().getNumFound());\n      \n      rsp = query(params(\"rows\", \"0\", \"q\", \"*:*\", \n                         \"stats\",\"true\", \"stats.field\", \"{!min=true max=true}long_l\"));\n      assertEquals(NUM_DOCS, rsp.getResults().getNumFound());\n      assertEquals(MIN_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMin()));\n      assertEquals(MAX_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMax()));\n    }\n\n    final int NUM_QUERIES = atLeast(100);\n\n    // Some Randomized queries with randomized log2m and max regwidth\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      // testing shows that on random data, at the size we're dealing with, \n      // MINIMUM_LOG2M_PARAM is just too absurdly small to give anything remotely close the \n      // the theoretically expected relative error.\n      //\n      // So we have to use a slightly higher lower bound on what log2m values we randomly test\n      final int log2m = TestUtil.nextInt(random(), \n                                         2 + HLL.MINIMUM_LOG2M_PARAM, \n                                         HLL.MAXIMUM_LOG2M_PARAM);\n\n      // use max regwidth to try and prevent hash collisions from introducing problems\n      final int regwidth = HLL.MAXIMUM_REGWIDTH_PARAM;\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      SolrParams p = buildCardinalityQ(lowId, highId, log2m, regwidth);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      if (Boolean.getBoolean(NUMERIC_POINTS_SYSPROP)) {\n        log.warn(\"SOLR-10918: can't relying on exact match with pre-hashed values when using points\");\n      } else {\n        for (String f : STAT_FIELDS) {\n          // regardless of log2m and regwidth, the estimated cardinality of the \n          // hashed vs prehashed values should be exactly the same for each field\n          \n          assertEquals(f + \": hashed vs prehashed, real=\"+ numMatches + \", p=\" + p,\n                       stats.get(f).getCardinality().longValue(),\n                       stats.get(f+\"_prehashed_l\").getCardinality().longValue());\n        }\n      }\n\n      for (String f : STAT_FIELDS) {\n        // check the relative error of the estimate returned against the known truth\n\n        final double relErr = expectedRelativeError(log2m);\n        final long estimate = stats.get(f).getCardinality().longValue();\n        assertTrue(f + \": relativeErr=\"+relErr+\", estimate=\"+estimate+\", real=\"+numMatches+\", p=\" + p,\n                   (Math.abs(numMatches - estimate) / numMatches) < relErr);\n        \n      }\n    }\n    \n    // Some Randomized queries with both low and high accuracy options\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      // WTF? - https://github.com/aggregateknowledge/java-hll/issues/15\n      // \n      // aparently we can't rely on estimates always being more accurate with higher log2m values?\n      // so for now, just try testing accuracy values that differ by at least 0.5\n      //\n      // (that should give us a significant enough log2m diff that the \"highAccuracy\" is always\n      // more accurate -- if, not then the entire premise of the float value is fundementally bogus)\n      // \n      final double lowAccuracy = random().nextDouble() / 2;\n      // final double highAccuracy = Math.min(1.0D, lowAccuracy + (random().nextDouble() / 2));\n      final double highAccuracy = Math.min(1.0D, lowAccuracy + 0.5D);\n\n      SolrParams p = buildCardinalityQ(lowId, highId, lowAccuracy, highAccuracy);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      // can't use STAT_FIELDS here ...\n      //\n      // hueristic differences for regwidth on 32 bit values mean we get differences \n      // between estimates for the normal field vs the prehashed (long) field\n      //\n      // so we settle for only testing things where the regwidth is consistent \n      // w/the prehashed long...\n      for (String f : new String[] { \"long_l\", \"string_s\" }) {\n\n        // regardless of accuracy, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed (low), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"low_\"+f).getCardinality().longValue(),\n                     stats.get(\"low_\"+f+\"_prehashed_l\").getCardinality().longValue());\n        assertEquals(f + \": hashed vs prehashed (high), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"high_\"+f).getCardinality().longValue(),\n                     stats.get(\"high_\"+f+\"_prehashed_l\").getCardinality().longValue());\n      }\n      \n      for (String f : STAT_FIELDS) {\n        for (String ff : new String[] { f, f+\"_prehashed_l\"}) {\n          // for both the prehashed and regular fields, the high accuracy option \n          // should always produce an estimate at least as good as the low accuracy option\n          \n          long poorEst = stats.get(\"low_\"+ff).getCardinality();\n          long goodEst = stats.get(\"high_\"+ff).getCardinality();\n          assertTrue(ff + \": goodEst=\"+goodEst+\", poorEst=\"+poorEst+\", real=\"+numMatches+\", p=\" + p,\n                     Math.abs(numMatches - goodEst) <= Math.abs(numMatches - poorEst));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 14-Oct-2018\n  public void test() throws Exception {\n    buildIndex();\n    \n    { // simple sanity checks - don't leak variables\n      QueryResponse rsp = null;\n      rsp = query(params(\"rows\", \"0\", \"q\", \"id:42\")); \n      assertEquals(1, rsp.getResults().getNumFound());\n      \n      rsp = query(params(\"rows\", \"0\", \"q\", \"*:*\", \n                         \"stats\",\"true\", \"stats.field\", \"{!min=true max=true}long_l\"));\n      assertEquals(NUM_DOCS, rsp.getResults().getNumFound());\n      assertEquals(MIN_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMin()));\n      assertEquals(MAX_LONG, Math.round((double) rsp.getFieldStatsInfo().get(\"long_l\").getMax()));\n    }\n\n    final int NUM_QUERIES = atLeast(100);\n\n    // Some Randomized queries with randomized log2m and max regwidth\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      // testing shows that on random data, at the size we're dealing with, \n      // MINIMUM_LOG2M_PARAM is just too absurdly small to give anything remotely close the \n      // the theoretically expected relative error.\n      //\n      // So we have to use a slightly higher lower bound on what log2m values we randomly test\n      final int log2m = TestUtil.nextInt(random(), \n                                         2 + HLL.MINIMUM_LOG2M_PARAM, \n                                         HLL.MAXIMUM_LOG2M_PARAM);\n\n      // use max regwidth to try and prevent hash collisions from introducing problems\n      final int regwidth = HLL.MAXIMUM_REGWIDTH_PARAM;\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      SolrParams p = buildCardinalityQ(lowId, highId, log2m, regwidth);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      if (Boolean.getBoolean(NUMERIC_POINTS_SYSPROP)) {\n        log.warn(\"SOLR-10918: can't relying on exact match with pre-hashed values when using points\");\n      } else {\n        for (String f : STAT_FIELDS) {\n          // regardless of log2m and regwidth, the estimated cardinality of the \n          // hashed vs prehashed values should be exactly the same for each field\n          \n          assertEquals(f + \": hashed vs prehashed, real=\"+ numMatches + \", p=\" + p,\n                       stats.get(f).getCardinality().longValue(),\n                       stats.get(f+\"_prehashed_l\").getCardinality().longValue());\n        }\n      }\n\n      for (String f : STAT_FIELDS) {\n        // check the relative error of the estimate returned against the known truth\n\n        final double relErr = expectedRelativeError(log2m);\n        final long estimate = stats.get(f).getCardinality().longValue();\n        assertTrue(f + \": relativeErr=\"+relErr+\", estimate=\"+estimate+\", real=\"+numMatches+\", p=\" + p,\n                   (Math.abs(numMatches - estimate) / numMatches) < relErr);\n        \n      }\n    }\n    \n    // Some Randomized queries with both low and high accuracy options\n    for (int i = 0; i < NUM_QUERIES; i++) {\n\n      final int lowId = TestUtil.nextInt(random(), 1, NUM_DOCS-2000);\n      final int highId = TestUtil.nextInt(random(), lowId+1000, NUM_DOCS);\n      final int numMatches = 1+highId-lowId;\n\n      // WTF? - https://github.com/aggregateknowledge/java-hll/issues/15\n      // \n      // aparently we can't rely on estimates always being more accurate with higher log2m values?\n      // so for now, just try testing accuracy values that differ by at least 0.5\n      //\n      // (that should give us a significant enough log2m diff that the \"highAccuracy\" is always\n      // more accurate -- if, not then the entire premise of the float value is fundementally bogus)\n      // \n      final double lowAccuracy = random().nextDouble() / 2;\n      // final double highAccuracy = Math.min(1.0D, lowAccuracy + (random().nextDouble() / 2));\n      final double highAccuracy = Math.min(1.0D, lowAccuracy + 0.5D);\n\n      SolrParams p = buildCardinalityQ(lowId, highId, lowAccuracy, highAccuracy);\n      QueryResponse rsp = query(p);\n      assertEquals(\"sanity check num matches, p=\"+p, numMatches, rsp.getResults().getNumFound());\n\n      Map<String,FieldStatsInfo> stats = rsp.getFieldStatsInfo();\n\n      // can't use STAT_FIELDS here ...\n      //\n      // hueristic differences for regwidth on 32 bit values mean we get differences \n      // between estimates for the normal field vs the prehashed (long) field\n      //\n      // so we settle for only testing things where the regwidth is consistent \n      // w/the prehashed long...\n      for (String f : new String[] { \"long_l\", \"string_s\" }) {\n\n        // regardless of accuracy, the estimated cardinality of the \n        // hashed vs prehashed values should be exactly the same for each field\n\n        assertEquals(f + \": hashed vs prehashed (low), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"low_\"+f).getCardinality().longValue(),\n                     stats.get(\"low_\"+f+\"_prehashed_l\").getCardinality().longValue());\n        assertEquals(f + \": hashed vs prehashed (high), real=\"+ numMatches + \", p=\" + p,\n                     stats.get(\"high_\"+f).getCardinality().longValue(),\n                     stats.get(\"high_\"+f+\"_prehashed_l\").getCardinality().longValue());\n      }\n      \n      for (String f : STAT_FIELDS) {\n        for (String ff : new String[] { f, f+\"_prehashed_l\"}) {\n          // for both the prehashed and regular fields, the high accuracy option \n          // should always produce an estimate at least as good as the low accuracy option\n          \n          long poorEst = stats.get(\"low_\"+ff).getCardinality();\n          long goodEst = stats.get(\"high_\"+ff).getCardinality();\n          assertTrue(ff + \": goodEst=\"+goodEst+\", poorEst=\"+poorEst+\", real=\"+numMatches+\", p=\" + p,\n                     Math.abs(numMatches - goodEst) <= Math.abs(numMatches - poorEst));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"44dd40f6c2c1465aebf4677bab10f696c7ea18d8":["28288370235ed02234a64753cdbf0c6ec096304a"],"351efe6fdecf9af62134d37ec2582e4a0331a4dc":["4a4e2c829188fb99886a64558664d79c9ac0fdf1"],"4a4e2c829188fb99886a64558664d79c9ac0fdf1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"832396a97c45687ccbe41d05936969e4996dc701":["44dd40f6c2c1465aebf4677bab10f696c7ea18d8"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["4a4e2c829188fb99886a64558664d79c9ac0fdf1","351efe6fdecf9af62134d37ec2582e4a0331a4dc"],"28288370235ed02234a64753cdbf0c6ec096304a":["4a4e2c829188fb99886a64558664d79c9ac0fdf1","351efe6fdecf9af62134d37ec2582e4a0331a4dc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["832396a97c45687ccbe41d05936969e4996dc701"]},"commit2Childs":{"44dd40f6c2c1465aebf4677bab10f696c7ea18d8":["832396a97c45687ccbe41d05936969e4996dc701"],"351efe6fdecf9af62134d37ec2582e4a0331a4dc":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"4a4e2c829188fb99886a64558664d79c9ac0fdf1":["351efe6fdecf9af62134d37ec2582e4a0331a4dc","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4a4e2c829188fb99886a64558664d79c9ac0fdf1"],"832396a97c45687ccbe41d05936969e4996dc701":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["44dd40f6c2c1465aebf4677bab10f696c7ea18d8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}