{"path":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","pathOld":"modules/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6b1e6a1c51433ebaa34dc0b76d7ab1876072be39","date":1370266419,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","pathOld":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","pathOld":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<String[]>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<String>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<int[]>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","date":1407854805,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","pathOld":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.setLength(token.length());\n      for (int i = 0; i < token.length(); i++) {\n        scratch.setIntAt(i, (int) token.charAt(i));\n      }\n      fstBuilder.add(scratch.get(), ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.length = token.length();\n      for (int i = 0; i < token.length(); i++) {\n        scratch.ints[i] = (int) token.charAt(i);\n      }\n      fstBuilder.add(scratch, ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":["b3be20ca1091c0b7cdb2308b9023606a5e451cec","6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","date":1419400138,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","pathOld":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if it's needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.setLength(token.length());\n      for (int i = 0; i < token.length(); i++) {\n        scratch.setIntAt(i, (int) token.charAt(i));\n      }\n      fstBuilder.add(scratch.get(), ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if its needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.setLength(token.length());\n      for (int i = 0; i < token.length(); i++) {\n        scratch.setIntAt(i, (int) token.charAt(i));\n      }\n      fstBuilder.add(scratch.get(), ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e0e5dacb8158de7670b41d1a749a4b7487e6acf","date":1431331436,"type":5,"author":"Christian Moen","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(List[String[]]).mjava","pathOld":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary#UserDictionary(Reader).mjava","sourceNew":"  private UserDictionary(List<String[]> featureEntries) throws IOException {\n\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if it's needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.setLength(token.length());\n      for (int i = 0; i < token.length(); i++) {\n        scratch.setIntAt(i, (int) token.charAt(i));\n      }\n      fstBuilder.add(scratch.get(), ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","sourceOld":"  public UserDictionary(Reader reader) throws IOException {\n    BufferedReader br = new BufferedReader(reader);\n    String line = null;\n    int wordId = CUSTOM_DICTIONARY_WORD_ID_OFFSET;\n    List<String[]> featureEntries = new ArrayList<>();\n \n    // text, segmentation, readings, POS\n    while ((line = br.readLine()) != null) {\n      // Remove comments\n      line = line.replaceAll(\"#.*$\", \"\");\n      \n      // Skip empty lines or comment lines\n      if (line.trim().length() == 0) {\n        continue;\n      }\n      String[] values = CSVUtil.parse(line);\n      featureEntries.add(values);\n    }\n    \n    // TODO: should we allow multiple segmentations per input 'phrase'?\n    // the old treemap didn't support this either, and i'm not sure if it's needed/useful?\n\n    Collections.sort(featureEntries, new Comparator<String[]>() {\n      @Override\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n     }\n    });\n    \n    List<String> data = new ArrayList<>(featureEntries.size());\n    List<int[]> segmentations = new ArrayList<>(featureEntries.size());\n    \n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = 0;\n    \n    for (String[] values : featureEntries) {\n      String[] segmentation = values[1].replaceAll(\"  *\", \" \").split(\" \");\n      String[] readings = values[2].replaceAll(\"  *\", \" \").split(\" \");\n      String pos = values[3];\n      \n      if (segmentation.length != readings.length) {\n        throw new RuntimeException(\"Illegal user dictionary entry \" + values[0] +\n                                   \" - the number of segmentations (\" + segmentation.length + \")\" +\n                                   \" does not the match number of readings (\" + readings.length + \")\");\n      }\n      \n      int[] wordIdAndLength = new int[segmentation.length + 1]; // wordId offset, length, length....\n      wordIdAndLength[0] = wordId;\n      for (int i = 0; i < segmentation.length; i++) {\n        wordIdAndLength[i + 1] = segmentation[i].length();\n        data.add(readings[i] + INTERNAL_SEPARATOR + pos);\n        wordId++;\n      }\n      // add mapping to FST\n      String token = values[0];\n      scratch.grow(token.length());\n      scratch.setLength(token.length());\n      for (int i = 0; i < token.length(); i++) {\n        scratch.setIntAt(i, (int) token.charAt(i));\n      }\n      fstBuilder.add(scratch.get(), ord);\n      segmentations.add(wordIdAndLength);\n      ord++;\n    }\n    this.fst = new TokenInfoFST(fstBuilder.finish(), false);\n    this.data = data.toArray(new String[data.size()]);\n    this.segmentations = segmentations.toArray(new int[segmentations.size()][]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3e0e5dacb8158de7670b41d1a749a4b7487e6acf":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6b1e6a1c51433ebaa34dc0b76d7ab1876072be39"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3e0e5dacb8158de7670b41d1a749a4b7487e6acf"],"6b1e6a1c51433ebaa34dc0b76d7ab1876072be39":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"3e0e5dacb8158de7670b41d1a749a4b7487e6acf":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["3e0e5dacb8158de7670b41d1a749a4b7487e6acf"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["6b1e6a1c51433ebaa34dc0b76d7ab1876072be39"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"6b1e6a1c51433ebaa34dc0b76d7ab1876072be39":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}