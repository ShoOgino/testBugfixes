{"path":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","commits":[{"id":"75f521a62662f1ec69a2107e8ca6d1e37d54c033","date":1438786323,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(Integer).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    mergePolicy = TestSortingMergePolicy.newSortingMergePolicy(sort);\n    iwc.setMergePolicy(mergePolicy);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(Integer maxSegmentCount) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    mergePolicy = TestSortingMergePolicy.newSortingMergePolicy(sort);\n    iwc.setMergePolicy(mergePolicy);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (maxSegmentCount != null) {\n      iw.forceMerge(maxSegmentCount.intValue());\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"40bac33458024322ad072c1e9ec203323cc61646","date":1438978400,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    mergePolicy = TestSortingMergePolicy.newSortingMergePolicy(sort);\n    iwc.setMergePolicy(mergePolicy);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    mergePolicy = TestSortingMergePolicy.newSortingMergePolicy(sort);\n    iwc.setMergePolicy(mergePolicy);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ceaef6cfc68c8ab22a684192e469a8280f9e6e70","date":1462354657,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    // nocommit:\n    iwc.setCodec(Codec.forName(\"SimpleText\"));\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    mergePolicy = TestSortingMergePolicy.newSortingMergePolicy(sort);\n    iwc.setMergePolicy(mergePolicy);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb0345a2d45479f891041f8b3ce351bc975e64ac","date":1462708700,"type":5,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    // nocommit:\n    iwc.setCodec(Codec.forName(\"SimpleText\"));\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    // nocommit:\n    iwc.setCodec(Codec.forName(\"SimpleText\"));\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":5,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    mergePolicy = TestSortingMergePolicy.newSortingMergePolicy(sort);\n    iwc.setMergePolicy(mergePolicy);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    mergePolicy = TestSortingMergePolicy.newSortingMergePolicy(sort);\n    iwc.setMergePolicy(mergePolicy);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    mergePolicy = TestSortingMergePolicy.newSortingMergePolicy(sort);\n    iwc.setMergePolicy(mergePolicy);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":null,"sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    mergePolicy = TestSortingMergePolicy.newSortingMergePolicy(sort);\n    iwc.setMergePolicy(mergePolicy);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["40bac33458024322ad072c1e9ec203323cc61646"],"40bac33458024322ad072c1e9ec203323cc61646":["75f521a62662f1ec69a2107e8ca6d1e37d54c033"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"75f521a62662f1ec69a2107e8ca6d1e37d54c033":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0ad30c6a479e764150a3316e57263319775f1df2":["40bac33458024322ad072c1e9ec203323cc61646","3d33e731a93d4b57e662ff094f64f94a745422d4"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["40bac33458024322ad072c1e9ec203323cc61646","fb0345a2d45479f891041f8b3ce351bc975e64ac"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["40bac33458024322ad072c1e9ec203323cc61646","d470c8182e92b264680e34081b75e70a9f2b3c89"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["40bac33458024322ad072c1e9ec203323cc61646","0ad30c6a479e764150a3316e57263319775f1df2"],"fb0345a2d45479f891041f8b3ce351bc975e64ac":["ceaef6cfc68c8ab22a684192e469a8280f9e6e70"]},"commit2Childs":{"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["fb0345a2d45479f891041f8b3ce351bc975e64ac"],"40bac33458024322ad072c1e9ec203323cc61646":["ceaef6cfc68c8ab22a684192e469a8280f9e6e70","0ad30c6a479e764150a3316e57263319775f1df2","3d33e731a93d4b57e662ff094f64f94a745422d4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["75f521a62662f1ec69a2107e8ca6d1e37d54c033"],"75f521a62662f1ec69a2107e8ca6d1e37d54c033":["40bac33458024322ad072c1e9ec203323cc61646"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"fb0345a2d45479f891041f8b3ce351bc975e64ac":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}