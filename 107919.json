{"path":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","commits":[{"id":"d03062c59d1dcb53f10a483e2259b28499f94b3e","date":1220450371,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.getDirectory(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        w.setMergeFactor(5);\n        w.setMergeScheduler(new SerialMergeScheduler());\n        Document doc = new Document();\n        doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n        doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n        doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.TOKENIZED));\n        w.addDocument(doc);\n        w.close();\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      for(int i=0;i<5;i++) {\n        Document doc = r.document(i);\n        assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n        byte[] b = doc.getField(\"test2\").binaryValue();\n        assertTrue(Arrays.equals(b, cmp));\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["9e65386affc8075b9857e4ea9dc8e03ab5bf543b","9e65386affc8075b9857e4ea9dc8e03ab5bf543b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9e65386affc8075b9857e4ea9dc8e03ab5bf543b","date":1220463269,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","sourceNew":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.getDirectory(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.TOKENIZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","sourceOld":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.getDirectory(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        w.setMergeFactor(5);\n        w.setMergeScheduler(new SerialMergeScheduler());\n        Document doc = new Document();\n        doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n        doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n        doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.TOKENIZED));\n        w.addDocument(doc);\n        w.close();\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      for(int i=0;i<5;i++) {\n        Document doc = r.document(i);\n        assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n        byte[] b = doc.getField(\"test2\").binaryValue();\n        assertTrue(Arrays.equals(b, cmp));\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","bugFix":["d03062c59d1dcb53f10a483e2259b28499f94b3e"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a","date":1221082732,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","sourceNew":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.getDirectory(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.ANALYZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","sourceOld":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.getDirectory(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.TOKENIZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3215ae1377fc1ca1790921d75dd39cb764743b85","date":1237371771,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#xxxtestMergeCompressedFields().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","sourceNew":"  // LUCENE-1374\n  public void xxxtestMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.getDirectory(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.ANALYZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","sourceOld":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.getDirectory(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.ANALYZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"09c482d1e63332617181729a225b215c452d8a79","date":1237396006,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#xxxtestMergeCompressedFields().mjava","sourceNew":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.getDirectory(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.ANALYZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","sourceOld":"  // LUCENE-1374\n  public void xxxtestMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.getDirectory(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.ANALYZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5309ea37b2a7ec9c5f21c9eeacc9d9fb808cdb02","date":1243677645,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","sourceNew":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.open(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.ANALYZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","sourceOld":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.getDirectory(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.ANALYZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e8d1458a2543cbd30cbfe7929be4dcb5c5251659","date":1254582241,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","sourceNew":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.open(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.ANALYZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir, true);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","sourceOld":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.open(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.ANALYZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a046c0c310bc77931fc8441bd920053b607dd14","date":1254584734,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","sourceNew":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.open(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.ANALYZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir, true);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","sourceOld":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.open(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.ANALYZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e0c804f7aa477229414a7e12882af490c241f64d","date":1254963299,"type":4,"author":"Michael Busch","isMerge":false,"pathNew":"/dev/null","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testMergeCompressedFields().mjava","sourceNew":null,"sourceOld":"  // LUCENE-1374\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.open(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        try {\n          w.setMergeFactor(5);\n          w.setMergeScheduler(new SerialMergeScheduler());\n          Document doc = new Document();\n          doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n          doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n          doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.ANALYZED));\n          w.addDocument(doc);\n        } finally {\n          w.close();\n        }\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir, true);\n      try {\n        for(int i=0;i<5;i++) {\n          Document doc = r.document(i);\n          assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n          byte[] b = doc.getField(\"test2\").binaryValue();\n          assertTrue(Arrays.equals(b, cmp));\n        }\n      } finally {\n        r.close();\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["5309ea37b2a7ec9c5f21c9eeacc9d9fb808cdb02"],"5309ea37b2a7ec9c5f21c9eeacc9d9fb808cdb02":["09c482d1e63332617181729a225b215c452d8a79"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0a046c0c310bc77931fc8441bd920053b607dd14":["5309ea37b2a7ec9c5f21c9eeacc9d9fb808cdb02","e8d1458a2543cbd30cbfe7929be4dcb5c5251659"],"3215ae1377fc1ca1790921d75dd39cb764743b85":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["9e65386affc8075b9857e4ea9dc8e03ab5bf543b"],"e0c804f7aa477229414a7e12882af490c241f64d":["0a046c0c310bc77931fc8441bd920053b607dd14"],"09c482d1e63332617181729a225b215c452d8a79":["3215ae1377fc1ca1790921d75dd39cb764743b85"],"9e65386affc8075b9857e4ea9dc8e03ab5bf543b":["d03062c59d1dcb53f10a483e2259b28499f94b3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e0c804f7aa477229414a7e12882af490c241f64d"],"d03062c59d1dcb53f10a483e2259b28499f94b3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["0a046c0c310bc77931fc8441bd920053b607dd14"],"5309ea37b2a7ec9c5f21c9eeacc9d9fb808cdb02":["e8d1458a2543cbd30cbfe7929be4dcb5c5251659","0a046c0c310bc77931fc8441bd920053b607dd14"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d03062c59d1dcb53f10a483e2259b28499f94b3e"],"0a046c0c310bc77931fc8441bd920053b607dd14":["e0c804f7aa477229414a7e12882af490c241f64d"],"3215ae1377fc1ca1790921d75dd39cb764743b85":["09c482d1e63332617181729a225b215c452d8a79"],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["3215ae1377fc1ca1790921d75dd39cb764743b85"],"09c482d1e63332617181729a225b215c452d8a79":["5309ea37b2a7ec9c5f21c9eeacc9d9fb808cdb02"],"9e65386affc8075b9857e4ea9dc8e03ab5bf543b":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"e0c804f7aa477229414a7e12882af490c241f64d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d03062c59d1dcb53f10a483e2259b28499f94b3e":["9e65386affc8075b9857e4ea9dc8e03ab5bf543b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}