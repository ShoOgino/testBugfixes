{"path":"lucene/facet/src/test/org/apache/lucene/facet/index/TestFacetsPayloadMigrationReader#createIndex(Directory,Directory,FacetIndexingParams).mjava","commits":[{"id":"6c2cd18c7da6f499a33f06fc89c07a463ec074c0","date":1358329431,"type":0,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/index/TestFacetsPayloadMigrationReader#createIndex(Directory,Directory,FacetIndexingParams).mjava","pathOld":"/dev/null","sourceNew":"  private HashMap<String,Integer> createIndex(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) \n      throws Exception {\n    Random random = random();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMaxBufferedDocs(2); // force few segments\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // avoid merges so that we're left with few segments\n    IndexWriter indexWriter = new IndexWriter(indexDir, conf);\n    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);\n    \n    FacetFields facetFields = new PayloadFacetFields(taxoWriter, fip);\n    \n    HashMap<String,Integer> expectedCounts = new HashMap<String,Integer>(DIMENSIONS.length);\n    int numDocs = atLeast(10);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numCategories = random.nextInt(3) + 1;\n      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>(numCategories);\n      HashSet<String> docDimensions = new HashSet<String>();\n      while (numCategories-- > 0) {\n        String dim = DIMENSIONS[random.nextInt(DIMENSIONS.length)];\n        // we should only increment the expected count by 1 per document\n        docDimensions.add(dim);\n        categories.add(new CategoryPath(dim, Integer.toString(i), Integer.toString(numCategories)));\n      }\n      facetFields.addFields(doc, categories);\n      doc.add(new StringField(\"docid\", Integer.toString(i), Store.YES));\n      doc.add(new TextField(\"foo\", \"content\" + i, Store.YES));\n      indexWriter.addDocument(doc);\n\n      // update expected count per dimension\n      for (String dim : docDimensions) {\n        Integer val = expectedCounts.get(dim);\n        if (val == null) {\n          expectedCounts.put(dim, Integer.valueOf(1));\n        } else {\n          expectedCounts.put(dim, Integer.valueOf(val.intValue() + 1));\n        }\n      }\n      \n      if (random.nextDouble() < 0.2) { // add some documents that will be deleted\n        doc = new Document();\n        doc.add(new StringField(\"del\", \"key\", Store.NO));\n        facetFields.addFields(doc, Collections.singletonList(new CategoryPath(\"dummy\")));\n        indexWriter.addDocument(doc);\n      }\n    }\n    \n    indexWriter.commit();\n    taxoWriter.commit();\n\n    // delete the docs that were marked for deletion. note that the 'dummy'\n    // category is not removed from the taxonomy, so must account for it when we\n    // verify the migrated index.\n    indexWriter.deleteDocuments(new Term(\"del\", \"key\"));\n    indexWriter.commit();\n    \n    IOUtils.close(indexWriter, taxoWriter);\n    \n    return expectedCounts;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c4015cd39dff8d4dec562d909f9766debac53aa6","date":1358548736,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/index/TestFacetsPayloadMigrationReader#createIndex(Directory,Directory,FacetIndexingParams).mjava","pathOld":"/dev/null","sourceNew":"  private HashMap<String,Integer> createIndex(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) \n      throws Exception {\n    Random random = random();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMaxBufferedDocs(2); // force few segments\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // avoid merges so that we're left with few segments\n    IndexWriter indexWriter = new IndexWriter(indexDir, conf);\n    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);\n    \n    FacetFields facetFields = new PayloadFacetFields(taxoWriter, fip);\n    \n    HashMap<String,Integer> expectedCounts = new HashMap<String,Integer>(DIMENSIONS.length);\n    int numDocs = atLeast(10);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numCategories = random.nextInt(3) + 1;\n      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>(numCategories);\n      HashSet<String> docDimensions = new HashSet<String>();\n      while (numCategories-- > 0) {\n        String dim = DIMENSIONS[random.nextInt(DIMENSIONS.length)];\n        // we should only increment the expected count by 1 per document\n        docDimensions.add(dim);\n        categories.add(new CategoryPath(dim, Integer.toString(i), Integer.toString(numCategories)));\n      }\n      facetFields.addFields(doc, categories);\n      doc.add(new StringField(\"docid\", Integer.toString(i), Store.YES));\n      doc.add(new TextField(\"foo\", \"content\" + i, Store.YES));\n      indexWriter.addDocument(doc);\n\n      // update expected count per dimension\n      for (String dim : docDimensions) {\n        Integer val = expectedCounts.get(dim);\n        if (val == null) {\n          expectedCounts.put(dim, Integer.valueOf(1));\n        } else {\n          expectedCounts.put(dim, Integer.valueOf(val.intValue() + 1));\n        }\n      }\n      \n      if (random.nextDouble() < 0.2) { // add some documents that will be deleted\n        doc = new Document();\n        doc.add(new StringField(\"del\", \"key\", Store.NO));\n        facetFields.addFields(doc, Collections.singletonList(new CategoryPath(\"dummy\")));\n        indexWriter.addDocument(doc);\n      }\n    }\n    \n    indexWriter.commit();\n    taxoWriter.commit();\n\n    // delete the docs that were marked for deletion. note that the 'dummy'\n    // category is not removed from the taxonomy, so must account for it when we\n    // verify the migrated index.\n    indexWriter.deleteDocuments(new Term(\"del\", \"key\"));\n    indexWriter.commit();\n    \n    IOUtils.close(indexWriter, taxoWriter);\n    \n    return expectedCounts;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"607428da722dcb3e86bbd11c63de8986e6275c36","date":1360334150,"type":5,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader#createIndex(Directory,Directory,FacetIndexingParams).mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/index/TestFacetsPayloadMigrationReader#createIndex(Directory,Directory,FacetIndexingParams).mjava","sourceNew":"  private HashMap<String,Integer> createIndex(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) \n      throws Exception {\n    Random random = random();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMaxBufferedDocs(2); // force few segments\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // avoid merges so that we're left with few segments\n    IndexWriter indexWriter = new IndexWriter(indexDir, conf);\n    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);\n    \n    FacetFields facetFields = new PayloadFacetFields(taxoWriter, fip);\n    \n    HashMap<String,Integer> expectedCounts = new HashMap<String,Integer>(DIMENSIONS.length);\n    int numDocs = atLeast(10);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numCategories = random.nextInt(3) + 1;\n      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>(numCategories);\n      HashSet<String> docDimensions = new HashSet<String>();\n      while (numCategories-- > 0) {\n        String dim = DIMENSIONS[random.nextInt(DIMENSIONS.length)];\n        // we should only increment the expected count by 1 per document\n        docDimensions.add(dim);\n        categories.add(new CategoryPath(dim, Integer.toString(i), Integer.toString(numCategories)));\n      }\n      facetFields.addFields(doc, categories);\n      doc.add(new StringField(\"docid\", Integer.toString(i), Store.YES));\n      doc.add(new TextField(\"foo\", \"content\" + i, Store.YES));\n      indexWriter.addDocument(doc);\n\n      // update expected count per dimension\n      for (String dim : docDimensions) {\n        Integer val = expectedCounts.get(dim);\n        if (val == null) {\n          expectedCounts.put(dim, Integer.valueOf(1));\n        } else {\n          expectedCounts.put(dim, Integer.valueOf(val.intValue() + 1));\n        }\n      }\n      \n      if (random.nextDouble() < 0.2) { // add some documents that will be deleted\n        doc = new Document();\n        doc.add(new StringField(\"del\", \"key\", Store.NO));\n        facetFields.addFields(doc, Collections.singletonList(new CategoryPath(\"dummy\")));\n        indexWriter.addDocument(doc);\n      }\n    }\n    \n    indexWriter.commit();\n    taxoWriter.commit();\n\n    // delete the docs that were marked for deletion. note that the 'dummy'\n    // category is not removed from the taxonomy, so must account for it when we\n    // verify the migrated index.\n    indexWriter.deleteDocuments(new Term(\"del\", \"key\"));\n    indexWriter.commit();\n    \n    IOUtils.close(indexWriter, taxoWriter);\n    \n    return expectedCounts;\n  }\n\n","sourceOld":"  private HashMap<String,Integer> createIndex(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) \n      throws Exception {\n    Random random = random();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMaxBufferedDocs(2); // force few segments\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // avoid merges so that we're left with few segments\n    IndexWriter indexWriter = new IndexWriter(indexDir, conf);\n    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);\n    \n    FacetFields facetFields = new PayloadFacetFields(taxoWriter, fip);\n    \n    HashMap<String,Integer> expectedCounts = new HashMap<String,Integer>(DIMENSIONS.length);\n    int numDocs = atLeast(10);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numCategories = random.nextInt(3) + 1;\n      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>(numCategories);\n      HashSet<String> docDimensions = new HashSet<String>();\n      while (numCategories-- > 0) {\n        String dim = DIMENSIONS[random.nextInt(DIMENSIONS.length)];\n        // we should only increment the expected count by 1 per document\n        docDimensions.add(dim);\n        categories.add(new CategoryPath(dim, Integer.toString(i), Integer.toString(numCategories)));\n      }\n      facetFields.addFields(doc, categories);\n      doc.add(new StringField(\"docid\", Integer.toString(i), Store.YES));\n      doc.add(new TextField(\"foo\", \"content\" + i, Store.YES));\n      indexWriter.addDocument(doc);\n\n      // update expected count per dimension\n      for (String dim : docDimensions) {\n        Integer val = expectedCounts.get(dim);\n        if (val == null) {\n          expectedCounts.put(dim, Integer.valueOf(1));\n        } else {\n          expectedCounts.put(dim, Integer.valueOf(val.intValue() + 1));\n        }\n      }\n      \n      if (random.nextDouble() < 0.2) { // add some documents that will be deleted\n        doc = new Document();\n        doc.add(new StringField(\"del\", \"key\", Store.NO));\n        facetFields.addFields(doc, Collections.singletonList(new CategoryPath(\"dummy\")));\n        indexWriter.addDocument(doc);\n      }\n    }\n    \n    indexWriter.commit();\n    taxoWriter.commit();\n\n    // delete the docs that were marked for deletion. note that the 'dummy'\n    // category is not removed from the taxonomy, so must account for it when we\n    // verify the migrated index.\n    indexWriter.deleteDocuments(new Term(\"del\", \"key\"));\n    indexWriter.commit();\n    \n    IOUtils.close(indexWriter, taxoWriter);\n    \n    return expectedCounts;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"607428da722dcb3e86bbd11c63de8986e6275c36":["6c2cd18c7da6f499a33f06fc89c07a463ec074c0"],"c4015cd39dff8d4dec562d909f9766debac53aa6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","6c2cd18c7da6f499a33f06fc89c07a463ec074c0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6c2cd18c7da6f499a33f06fc89c07a463ec074c0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["607428da722dcb3e86bbd11c63de8986e6275c36"]},"commit2Childs":{"607428da722dcb3e86bbd11c63de8986e6275c36":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c4015cd39dff8d4dec562d909f9766debac53aa6":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c4015cd39dff8d4dec562d909f9766debac53aa6","6c2cd18c7da6f499a33f06fc89c07a463ec074c0"],"6c2cd18c7da6f499a33f06fc89c07a463ec074c0":["607428da722dcb3e86bbd11c63de8986e6275c36","c4015cd39dff8d4dec562d909f9766debac53aa6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c4015cd39dff8d4dec562d909f9766debac53aa6","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}