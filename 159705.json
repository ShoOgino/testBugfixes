{"path":"lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","commits":[{"id":"77d4998e63ada9336818d1ebaacc362168f473e8","date":1318620209,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"/dev/null","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    String codec = CodecProvider.getDefault().getFieldCodec(\"field\");\n    // TODO: once LUCENE-3517 is fixed, remove this:\n    assumeFalse(\"PulsingCodec fails this test because of over-cloning\", codec.equals(\"Pulsing\") || codec.equals(\"MockRandom\"));\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    s.close();\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2","d19974432be9aed28ee7dca73bdf01d139e763a9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"905f6760f432211e868cf7d229c8797382853a7a","date":1318620791,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    s.close();\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    String codec = CodecProvider.getDefault().getFieldCodec(\"field\");\n    // TODO: once LUCENE-3517 is fixed, remove this:\n    assumeFalse(\"PulsingCodec fails this test because of over-cloning\", codec.equals(\"Pulsing\") || codec.equals(\"MockRandom\"));\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    s.close();\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    s.close();\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    s.close();\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0e7c2454a6a8237bfd0e953f5b940838408c9055","date":1323649300,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    s.close();\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","date":1323720782,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    s.close();\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"7b91922b55d15444d554721b352861d028eb8278":["905f6760f432211e868cf7d229c8797382853a7a"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["7b91922b55d15444d554721b352861d028eb8278"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["0e7c2454a6a8237bfd0e953f5b940838408c9055"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["7b91922b55d15444d554721b352861d028eb8278","0e7c2454a6a8237bfd0e953f5b940838408c9055"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"905f6760f432211e868cf7d229c8797382853a7a":["77d4998e63ada9336818d1ebaacc362168f473e8"],"77d4998e63ada9336818d1ebaacc362168f473e8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"7b91922b55d15444d554721b352861d028eb8278":["0e7c2454a6a8237bfd0e953f5b940838408c9055","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["77d4998e63ada9336818d1ebaacc362168f473e8"],"905f6760f432211e868cf7d229c8797382853a7a":["7b91922b55d15444d554721b352861d028eb8278"],"77d4998e63ada9336818d1ebaacc362168f473e8":["905f6760f432211e868cf7d229c8797382853a7a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}