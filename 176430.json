{"path":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","commits":[{"id":"3cdad2c6b6234338031bcc1f24c001a5ad66f714","date":1296866109,"type":1,"author":"Doron Cohen","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"/dev/null","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":1,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(String,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(String dirName, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + dirName, tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer());\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (!delDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<Fieldable> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          Field f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3a0403b45dfe384fae4a1b6e96c3265d000c498","date":1321445981,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexReader reader = IndexReader.open(dir);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexSearcher searcher = new IndexSearcher(dir, true);\n    IndexReader reader = searcher.getIndexReader();\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexReader reader = IndexReader.open(dir);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        Terms tfv = reader.getTermVectors(i).terms(\"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexReader reader = IndexReader.open(dir);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        TermFreqVector tfv = reader.getTermFreqVector(i, \"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n        assertTrue(tfv instanceof TermPositionVector);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0e7c2454a6a8237bfd0e953f5b940838408c9055","date":1323649300,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexReader reader = IndexReader.open(dir);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        Terms tfv = reader.getTermVectors(i).terms(\"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexReader reader = IndexReader.open(dir);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        Terms tfv = reader.getTermVectors(i).terms(\"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","date":1323720782,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexReader reader = IndexReader.open(dir);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        Terms tfv = reader.getTermVectors(i).terms(\"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexReader reader = IndexReader.open(dir);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        Terms tfv = reader.getTermVectors(i).terms(\"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    searcher.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8be580b58bcc650d428f3f22de81cadcf51d650a","date":1325279655,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(Directory,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(Directory dir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    IndexReader reader = IndexReader.open(dir);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        Terms tfv = reader.getTermVectors(i).terms(\"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + oldName, tfv);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    reader.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexReader reader = IndexReader.open(dir);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        Terms tfv = reader.getTermVectors(i).terms(\"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["3cc749c053615f5871f3b95715fe292f34e70a53","0e7c2454a6a8237bfd0e953f5b940838408c9055"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"3cdad2c6b6234338031bcc1f24c001a5ad66f714":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3cc749c053615f5871f3b95715fe292f34e70a53":["a3a0403b45dfe384fae4a1b6e96c3265d000c498"],"8be580b58bcc650d428f3f22de81cadcf51d650a":["0e7c2454a6a8237bfd0e953f5b940838408c9055"],"962d04139994fce5193143ef35615499a9a96d78":["bde51b089eb7f86171eb3406e38a274743f9b7ac","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["f2c5f0cb44df114db4228c8f77861714b5cabaea","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a3776dccca01c11e7046323cfad46a3b4a471233","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["3cdad2c6b6234338031bcc1f24c001a5ad66f714"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["3cc749c053615f5871f3b95715fe292f34e70a53"],"a3776dccca01c11e7046323cfad46a3b4a471233":["3cdad2c6b6234338031bcc1f24c001a5ad66f714","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdad2c6b6234338031bcc1f24c001a5ad66f714"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a3a0403b45dfe384fae4a1b6e96c3265d000c498":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdad2c6b6234338031bcc1f24c001a5ad66f714"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8be580b58bcc650d428f3f22de81cadcf51d650a"]},"commit2Childs":{"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":[],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"3cdad2c6b6234338031bcc1f24c001a5ad66f714":["f2c5f0cb44df114db4228c8f77861714b5cabaea","a3776dccca01c11e7046323cfad46a3b4a471233","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"3cc749c053615f5871f3b95715fe292f34e70a53":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","0e7c2454a6a8237bfd0e953f5b940838408c9055"],"8be580b58bcc650d428f3f22de81cadcf51d650a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"962d04139994fce5193143ef35615499a9a96d78":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","d083e83f225b11e5fdd900e83d26ddb385b6955c","a3776dccca01c11e7046323cfad46a3b4a471233"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","8be580b58bcc650d428f3f22de81cadcf51d650a"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3cdad2c6b6234338031bcc1f24c001a5ad66f714","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"a3a0403b45dfe384fae4a1b6e96c3265d000c498":["3cc749c053615f5871f3b95715fe292f34e70a53"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["a3a0403b45dfe384fae4a1b6e96c3265d000c498"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["962d04139994fce5193143ef35615499a9a96d78"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}