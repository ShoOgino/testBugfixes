{"path":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyTermDeletes(BufferedUpdatesStream.SegmentState[]).mjava","commits":[{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyTermDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"/dev/null","sourceNew":"  private long applyTermDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteTerms.size() == 0) {\n      return 0;\n    }\n\n    // We apply segment-private deletes on flush:\n    assert privateSegment == null;\n\n    try {\n      long startNS = System.nanoTime();\n\n      long delCount = 0;\n\n      for (BufferedUpdatesStream.SegmentState segState : segStates) {\n        assert segState.delGen != delGen: \"segState.delGen=\" + segState.delGen + \" vs this.gen=\" + delGen;\n        if (segState.delGen > delGen) {\n          // our deletes don't apply to this segment\n          continue;\n        }\n        if (segState.rld.refCount() == 1) {\n          // This means we are the only remaining reference to this segment, meaning\n          // it was merged away while we were running, so we can safely skip running\n          // because we will run on the newly merged segment next:\n          continue;\n        }\n\n        FieldTermIterator iter = deleteTerms.iterator();\n\n        BytesRef delTerm;\n        String field = null;\n        TermsEnum termsEnum = null;\n        BytesRef readerTerm = null;\n        PostingsEnum postingsEnum = null;\n        while ((delTerm = iter.next()) != null) {\n\n          if (iter.field() != field) {\n            // field changed\n            field = iter.field();\n            Terms terms = segState.reader.terms(field);\n            if (terms != null) {\n              termsEnum = terms.iterator();\n              readerTerm = termsEnum.next();\n            } else {\n              termsEnum = null;\n            }\n          }\n\n          if (termsEnum != null) {\n            int cmp = delTerm.compareTo(readerTerm);\n            if (cmp < 0) {\n              // TODO: can we advance across del terms here?\n              // move to next del term\n              continue;\n            } else if (cmp == 0) {\n              // fall through\n            } else if (cmp > 0) {\n              TermsEnum.SeekStatus status = termsEnum.seekCeil(delTerm);\n              if (status == TermsEnum.SeekStatus.FOUND) {\n                // fall through\n              } else if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n                readerTerm = termsEnum.term();\n                continue;\n              } else {\n                // TODO: can we advance to next field in deleted terms?\n                // no more terms in this segment\n                termsEnum = null;\n                continue;\n              }\n            }\n\n            // we don't need term frequencies for this\n            postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n            assert postingsEnum != null;\n\n            int docID;\n            while ((docID = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n\n              // NOTE: there is no limit check on the docID\n              // when deleting by Term (unlike by Query)\n              // because on flush we apply all Term deletes to\n              // each segment.  So all Term deleting here is\n              // against prior segments:\n              if (segState.rld.delete(docID)) {\n                delCount++;\n              }\n            }\n          }\n        }\n      }\n\n      if (infoStream.isEnabled(\"BD\")) {\n        infoStream.message(\"BD\",\n                           String.format(Locale.ROOT, \"applyTermDeletes took %.2f msec for %d segments and %d del terms; %d new deletions\",\n                                         (System.nanoTime()-startNS)/1000000.,\n                                         segStates.length,\n                                         deleteTerms.size(),\n                                         delCount));\n      }\n\n      return delCount;\n\n    } catch (Throwable t) {\n      throw IOUtils.rethrowAlways(t);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyTermDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"/dev/null","sourceNew":"  private long applyTermDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteTerms.size() == 0) {\n      return 0;\n    }\n\n    // We apply segment-private deletes on flush:\n    assert privateSegment == null;\n\n    try {\n      long startNS = System.nanoTime();\n\n      long delCount = 0;\n\n      for (BufferedUpdatesStream.SegmentState segState : segStates) {\n        assert segState.delGen != delGen: \"segState.delGen=\" + segState.delGen + \" vs this.gen=\" + delGen;\n        if (segState.delGen > delGen) {\n          // our deletes don't apply to this segment\n          continue;\n        }\n        if (segState.rld.refCount() == 1) {\n          // This means we are the only remaining reference to this segment, meaning\n          // it was merged away while we were running, so we can safely skip running\n          // because we will run on the newly merged segment next:\n          continue;\n        }\n\n        FieldTermIterator iter = deleteTerms.iterator();\n\n        BytesRef delTerm;\n        String field = null;\n        TermsEnum termsEnum = null;\n        BytesRef readerTerm = null;\n        PostingsEnum postingsEnum = null;\n        while ((delTerm = iter.next()) != null) {\n\n          if (iter.field() != field) {\n            // field changed\n            field = iter.field();\n            Terms terms = segState.reader.terms(field);\n            if (terms != null) {\n              termsEnum = terms.iterator();\n              readerTerm = termsEnum.next();\n            } else {\n              termsEnum = null;\n            }\n          }\n\n          if (termsEnum != null) {\n            int cmp = delTerm.compareTo(readerTerm);\n            if (cmp < 0) {\n              // TODO: can we advance across del terms here?\n              // move to next del term\n              continue;\n            } else if (cmp == 0) {\n              // fall through\n            } else if (cmp > 0) {\n              TermsEnum.SeekStatus status = termsEnum.seekCeil(delTerm);\n              if (status == TermsEnum.SeekStatus.FOUND) {\n                // fall through\n              } else if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n                readerTerm = termsEnum.term();\n                continue;\n              } else {\n                // TODO: can we advance to next field in deleted terms?\n                // no more terms in this segment\n                termsEnum = null;\n                continue;\n              }\n            }\n\n            // we don't need term frequencies for this\n            postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n            assert postingsEnum != null;\n\n            int docID;\n            while ((docID = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n\n              // NOTE: there is no limit check on the docID\n              // when deleting by Term (unlike by Query)\n              // because on flush we apply all Term deletes to\n              // each segment.  So all Term deleting here is\n              // against prior segments:\n              if (segState.rld.delete(docID)) {\n                delCount++;\n              }\n            }\n          }\n        }\n      }\n\n      if (infoStream.isEnabled(\"BD\")) {\n        infoStream.message(\"BD\",\n                           String.format(Locale.ROOT, \"applyTermDeletes took %.2f msec for %d segments and %d del terms; %d new deletions\",\n                                         (System.nanoTime()-startNS)/1000000.,\n                                         segStates.length,\n                                         deleteTerms.size(),\n                                         delCount));\n      }\n\n      return delCount;\n\n    } catch (Throwable t) {\n      throw IOUtils.rethrowAlways(t);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyTermDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"/dev/null","sourceNew":"  private long applyTermDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteTerms.size() == 0) {\n      return 0;\n    }\n\n    // We apply segment-private deletes on flush:\n    assert privateSegment == null;\n\n    try {\n      long startNS = System.nanoTime();\n\n      long delCount = 0;\n\n      for (BufferedUpdatesStream.SegmentState segState : segStates) {\n        assert segState.delGen != delGen: \"segState.delGen=\" + segState.delGen + \" vs this.gen=\" + delGen;\n        if (segState.delGen > delGen) {\n          // our deletes don't apply to this segment\n          continue;\n        }\n        if (segState.rld.refCount() == 1) {\n          // This means we are the only remaining reference to this segment, meaning\n          // it was merged away while we were running, so we can safely skip running\n          // because we will run on the newly merged segment next:\n          continue;\n        }\n\n        FieldTermIterator iter = deleteTerms.iterator();\n\n        BytesRef delTerm;\n        String field = null;\n        TermsEnum termsEnum = null;\n        BytesRef readerTerm = null;\n        PostingsEnum postingsEnum = null;\n        while ((delTerm = iter.next()) != null) {\n\n          if (iter.field() != field) {\n            // field changed\n            field = iter.field();\n            Terms terms = segState.reader.terms(field);\n            if (terms != null) {\n              termsEnum = terms.iterator();\n              readerTerm = termsEnum.next();\n            } else {\n              termsEnum = null;\n            }\n          }\n\n          if (termsEnum != null) {\n            int cmp = delTerm.compareTo(readerTerm);\n            if (cmp < 0) {\n              // TODO: can we advance across del terms here?\n              // move to next del term\n              continue;\n            } else if (cmp == 0) {\n              // fall through\n            } else if (cmp > 0) {\n              TermsEnum.SeekStatus status = termsEnum.seekCeil(delTerm);\n              if (status == TermsEnum.SeekStatus.FOUND) {\n                // fall through\n              } else if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n                readerTerm = termsEnum.term();\n                continue;\n              } else {\n                // TODO: can we advance to next field in deleted terms?\n                // no more terms in this segment\n                termsEnum = null;\n                continue;\n              }\n            }\n\n            // we don't need term frequencies for this\n            postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n            assert postingsEnum != null;\n\n            int docID;\n            while ((docID = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n\n              // NOTE: there is no limit check on the docID\n              // when deleting by Term (unlike by Query)\n              // because on flush we apply all Term deletes to\n              // each segment.  So all Term deleting here is\n              // against prior segments:\n              if (segState.rld.delete(docID)) {\n                delCount++;\n              }\n            }\n          }\n        }\n      }\n\n      if (infoStream.isEnabled(\"BD\")) {\n        infoStream.message(\"BD\",\n                           String.format(Locale.ROOT, \"applyTermDeletes took %.2f msec for %d segments and %d del terms; %d new deletions\",\n                                         (System.nanoTime()-startNS)/1000000.,\n                                         segStates.length,\n                                         deleteTerms.size(),\n                                         delCount));\n      }\n\n      return delCount;\n\n    } catch (Throwable t) {\n      throw IOUtils.rethrowAlways(t);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14d66d86a8b184a86bcaebcf6e15fcef486e0876","date":1521539412,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyTermDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyTermDeletes(BufferedUpdatesStream.SegmentState[]).mjava","sourceNew":"  private long applyTermDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteTerms.size() == 0) {\n      return 0;\n    }\n\n    // We apply segment-private deletes on flush:\n    assert privateSegment == null;\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n      assert segState.delGen != delGen: \"segState.delGen=\" + segState.delGen + \" vs this.gen=\" + delGen;\n      if (segState.delGen > delGen) {\n        // our deletes don't apply to this segment\n        continue;\n      }\n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      FieldTermIterator iter = deleteTerms.iterator();\n\n      BytesRef delTerm;\n      String field = null;\n      TermsEnum termsEnum = null;\n      BytesRef readerTerm = null;\n      PostingsEnum postingsEnum = null;\n      while ((delTerm = iter.next()) != null) {\n\n        if (iter.field() != field) {\n          // field changed\n          field = iter.field();\n          Terms terms = segState.reader.terms(field);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n            readerTerm = termsEnum.next();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null) {\n          int cmp = delTerm.compareTo(readerTerm);\n          if (cmp < 0) {\n            // TODO: can we advance across del terms here?\n            // move to next del term\n            continue;\n          } else if (cmp == 0) {\n            // fall through\n          } else if (cmp > 0) {\n            TermsEnum.SeekStatus status = termsEnum.seekCeil(delTerm);\n            if (status == TermsEnum.SeekStatus.FOUND) {\n              // fall through\n            } else if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              readerTerm = termsEnum.term();\n              continue;\n            } else {\n              // TODO: can we advance to next field in deleted terms?\n              // no more terms in this segment\n              termsEnum = null;\n              continue;\n            }\n          }\n\n          // we don't need term frequencies for this\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          assert postingsEnum != null;\n\n          int docID;\n          while ((docID = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.2f msec for %d segments and %d del terms; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteTerms.size(),\n                                       delCount));\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  private long applyTermDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteTerms.size() == 0) {\n      return 0;\n    }\n\n    // We apply segment-private deletes on flush:\n    assert privateSegment == null;\n\n    try {\n      long startNS = System.nanoTime();\n\n      long delCount = 0;\n\n      for (BufferedUpdatesStream.SegmentState segState : segStates) {\n        assert segState.delGen != delGen: \"segState.delGen=\" + segState.delGen + \" vs this.gen=\" + delGen;\n        if (segState.delGen > delGen) {\n          // our deletes don't apply to this segment\n          continue;\n        }\n        if (segState.rld.refCount() == 1) {\n          // This means we are the only remaining reference to this segment, meaning\n          // it was merged away while we were running, so we can safely skip running\n          // because we will run on the newly merged segment next:\n          continue;\n        }\n\n        FieldTermIterator iter = deleteTerms.iterator();\n\n        BytesRef delTerm;\n        String field = null;\n        TermsEnum termsEnum = null;\n        BytesRef readerTerm = null;\n        PostingsEnum postingsEnum = null;\n        while ((delTerm = iter.next()) != null) {\n\n          if (iter.field() != field) {\n            // field changed\n            field = iter.field();\n            Terms terms = segState.reader.terms(field);\n            if (terms != null) {\n              termsEnum = terms.iterator();\n              readerTerm = termsEnum.next();\n            } else {\n              termsEnum = null;\n            }\n          }\n\n          if (termsEnum != null) {\n            int cmp = delTerm.compareTo(readerTerm);\n            if (cmp < 0) {\n              // TODO: can we advance across del terms here?\n              // move to next del term\n              continue;\n            } else if (cmp == 0) {\n              // fall through\n            } else if (cmp > 0) {\n              TermsEnum.SeekStatus status = termsEnum.seekCeil(delTerm);\n              if (status == TermsEnum.SeekStatus.FOUND) {\n                // fall through\n              } else if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n                readerTerm = termsEnum.term();\n                continue;\n              } else {\n                // TODO: can we advance to next field in deleted terms?\n                // no more terms in this segment\n                termsEnum = null;\n                continue;\n              }\n            }\n\n            // we don't need term frequencies for this\n            postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n            assert postingsEnum != null;\n\n            int docID;\n            while ((docID = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n\n              // NOTE: there is no limit check on the docID\n              // when deleting by Term (unlike by Query)\n              // because on flush we apply all Term deletes to\n              // each segment.  So all Term deleting here is\n              // against prior segments:\n              if (segState.rld.delete(docID)) {\n                delCount++;\n              }\n            }\n          }\n        }\n      }\n\n      if (infoStream.isEnabled(\"BD\")) {\n        infoStream.message(\"BD\",\n                           String.format(Locale.ROOT, \"applyTermDeletes took %.2f msec for %d segments and %d del terms; %d new deletions\",\n                                         (System.nanoTime()-startNS)/1000000.,\n                                         segStates.length,\n                                         deleteTerms.size(),\n                                         delCount));\n      }\n\n      return delCount;\n\n    } catch (Throwable t) {\n      throw IOUtils.rethrowAlways(t);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","date":1521731438,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyTermDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyTermDeletes(BufferedUpdatesStream.SegmentState[]).mjava","sourceNew":"  private long applyTermDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteTerms.size() == 0) {\n      return 0;\n    }\n\n    // We apply segment-private deletes on flush:\n    assert privateSegment == null;\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n      assert segState.delGen != delGen: \"segState.delGen=\" + segState.delGen + \" vs this.gen=\" + delGen;\n      if (segState.delGen > delGen) {\n        // our deletes don't apply to this segment\n        continue;\n      }\n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      FieldTermIterator iter = deleteTerms.iterator();\n\n      BytesRef delTerm;\n      String field = null;\n      TermsEnum termsEnum = null;\n      BytesRef readerTerm = null;\n      PostingsEnum postingsEnum = null;\n      while ((delTerm = iter.next()) != null) {\n\n        if (iter.field() != field) {\n          // field changed\n          field = iter.field();\n          Terms terms = segState.reader.terms(field);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n            readerTerm = termsEnum.next();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null) {\n          int cmp = delTerm.compareTo(readerTerm);\n          if (cmp < 0) {\n            // TODO: can we advance across del terms here?\n            // move to next del term\n            continue;\n          } else if (cmp == 0) {\n            // fall through\n          } else if (cmp > 0) {\n            TermsEnum.SeekStatus status = termsEnum.seekCeil(delTerm);\n            if (status == TermsEnum.SeekStatus.FOUND) {\n              // fall through\n            } else if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              readerTerm = termsEnum.term();\n              continue;\n            } else {\n              // TODO: can we advance to next field in deleted terms?\n              // no more terms in this segment\n              termsEnum = null;\n              continue;\n            }\n          }\n\n          // we don't need term frequencies for this\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          assert postingsEnum != null;\n\n          int docID;\n          while ((docID = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.2f msec for %d segments and %d del terms; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteTerms.size(),\n                                       delCount));\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  private long applyTermDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteTerms.size() == 0) {\n      return 0;\n    }\n\n    // We apply segment-private deletes on flush:\n    assert privateSegment == null;\n\n    try {\n      long startNS = System.nanoTime();\n\n      long delCount = 0;\n\n      for (BufferedUpdatesStream.SegmentState segState : segStates) {\n        assert segState.delGen != delGen: \"segState.delGen=\" + segState.delGen + \" vs this.gen=\" + delGen;\n        if (segState.delGen > delGen) {\n          // our deletes don't apply to this segment\n          continue;\n        }\n        if (segState.rld.refCount() == 1) {\n          // This means we are the only remaining reference to this segment, meaning\n          // it was merged away while we were running, so we can safely skip running\n          // because we will run on the newly merged segment next:\n          continue;\n        }\n\n        FieldTermIterator iter = deleteTerms.iterator();\n\n        BytesRef delTerm;\n        String field = null;\n        TermsEnum termsEnum = null;\n        BytesRef readerTerm = null;\n        PostingsEnum postingsEnum = null;\n        while ((delTerm = iter.next()) != null) {\n\n          if (iter.field() != field) {\n            // field changed\n            field = iter.field();\n            Terms terms = segState.reader.terms(field);\n            if (terms != null) {\n              termsEnum = terms.iterator();\n              readerTerm = termsEnum.next();\n            } else {\n              termsEnum = null;\n            }\n          }\n\n          if (termsEnum != null) {\n            int cmp = delTerm.compareTo(readerTerm);\n            if (cmp < 0) {\n              // TODO: can we advance across del terms here?\n              // move to next del term\n              continue;\n            } else if (cmp == 0) {\n              // fall through\n            } else if (cmp > 0) {\n              TermsEnum.SeekStatus status = termsEnum.seekCeil(delTerm);\n              if (status == TermsEnum.SeekStatus.FOUND) {\n                // fall through\n              } else if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n                readerTerm = termsEnum.term();\n                continue;\n              } else {\n                // TODO: can we advance to next field in deleted terms?\n                // no more terms in this segment\n                termsEnum = null;\n                continue;\n              }\n            }\n\n            // we don't need term frequencies for this\n            postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n            assert postingsEnum != null;\n\n            int docID;\n            while ((docID = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n\n              // NOTE: there is no limit check on the docID\n              // when deleting by Term (unlike by Query)\n              // because on flush we apply all Term deletes to\n              // each segment.  So all Term deleting here is\n              // against prior segments:\n              if (segState.rld.delete(docID)) {\n                delCount++;\n              }\n            }\n          }\n        }\n      }\n\n      if (infoStream.isEnabled(\"BD\")) {\n        infoStream.message(\"BD\",\n                           String.format(Locale.ROOT, \"applyTermDeletes took %.2f msec for %d segments and %d del terms; %d new deletions\",\n                                         (System.nanoTime()-startNS)/1000000.,\n                                         segStates.length,\n                                         deleteTerms.size(),\n                                         delCount));\n      }\n\n      return delCount;\n\n    } catch (Throwable t) {\n      throw IOUtils.rethrowAlways(t);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83b6ce113ec151d7bf9175578d92d5320f91ab2e","date":1544711434,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyTermDeletes(BufferedUpdatesStream.SegmentState[]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyTermDeletes(BufferedUpdatesStream.SegmentState[]).mjava","sourceNew":"  private long applyTermDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteTerms.size() == 0) {\n      return 0;\n    }\n\n    // We apply segment-private deletes on flush:\n    assert privateSegment == null;\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n      assert segState.delGen != delGen: \"segState.delGen=\" + segState.delGen + \" vs this.gen=\" + delGen;\n      if (segState.delGen > delGen) {\n        // our deletes don't apply to this segment\n        continue;\n      }\n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      FieldTermIterator iter = deleteTerms.iterator();\n      BytesRef delTerm;\n      TermDocsIterator termDocsIterator = new TermDocsIterator(segState.reader, true);\n      while ((delTerm = iter.next()) != null) {\n        final DocIdSetIterator iterator = termDocsIterator.nextTerm(iter.field(), delTerm);\n        if (iterator != null) {\n          int docID;\n          while ((docID = iterator.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.2f msec for %d segments and %d del terms; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteTerms.size(),\n                                       delCount));\n    }\n\n    return delCount;\n  }\n\n","sourceOld":"  private long applyTermDeletes(BufferedUpdatesStream.SegmentState[] segStates) throws IOException {\n\n    if (deleteTerms.size() == 0) {\n      return 0;\n    }\n\n    // We apply segment-private deletes on flush:\n    assert privateSegment == null;\n\n    long startNS = System.nanoTime();\n\n    long delCount = 0;\n\n    for (BufferedUpdatesStream.SegmentState segState : segStates) {\n      assert segState.delGen != delGen: \"segState.delGen=\" + segState.delGen + \" vs this.gen=\" + delGen;\n      if (segState.delGen > delGen) {\n        // our deletes don't apply to this segment\n        continue;\n      }\n      if (segState.rld.refCount() == 1) {\n        // This means we are the only remaining reference to this segment, meaning\n        // it was merged away while we were running, so we can safely skip running\n        // because we will run on the newly merged segment next:\n        continue;\n      }\n\n      FieldTermIterator iter = deleteTerms.iterator();\n\n      BytesRef delTerm;\n      String field = null;\n      TermsEnum termsEnum = null;\n      BytesRef readerTerm = null;\n      PostingsEnum postingsEnum = null;\n      while ((delTerm = iter.next()) != null) {\n\n        if (iter.field() != field) {\n          // field changed\n          field = iter.field();\n          Terms terms = segState.reader.terms(field);\n          if (terms != null) {\n            termsEnum = terms.iterator();\n            readerTerm = termsEnum.next();\n          } else {\n            termsEnum = null;\n          }\n        }\n\n        if (termsEnum != null) {\n          int cmp = delTerm.compareTo(readerTerm);\n          if (cmp < 0) {\n            // TODO: can we advance across del terms here?\n            // move to next del term\n            continue;\n          } else if (cmp == 0) {\n            // fall through\n          } else if (cmp > 0) {\n            TermsEnum.SeekStatus status = termsEnum.seekCeil(delTerm);\n            if (status == TermsEnum.SeekStatus.FOUND) {\n              // fall through\n            } else if (status == TermsEnum.SeekStatus.NOT_FOUND) {\n              readerTerm = termsEnum.term();\n              continue;\n            } else {\n              // TODO: can we advance to next field in deleted terms?\n              // no more terms in this segment\n              termsEnum = null;\n              continue;\n            }\n          }\n\n          // we don't need term frequencies for this\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          assert postingsEnum != null;\n\n          int docID;\n          while ((docID = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n\n            // NOTE: there is no limit check on the docID\n            // when deleting by Term (unlike by Query)\n            // because on flush we apply all Term deletes to\n            // each segment.  So all Term deleting here is\n            // against prior segments:\n            if (segState.rld.delete(docID)) {\n              delCount++;\n            }\n          }\n        }\n      }\n    }\n\n    if (infoStream.isEnabled(\"BD\")) {\n      infoStream.message(\"BD\",\n                         String.format(Locale.ROOT, \"applyTermDeletes took %.2f msec for %d segments and %d del terms; %d new deletions\",\n                                       (System.nanoTime()-startNS)/1000000.,\n                                       segStates.length,\n                                       deleteTerms.size(),\n                                       delCount));\n    }\n\n    return delCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"83b6ce113ec151d7bf9175578d92d5320f91ab2e":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["28288370235ed02234a64753cdbf0c6ec096304a","14d66d86a8b184a86bcaebcf6e15fcef486e0876"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["83b6ce113ec151d7bf9175578d92d5320f91ab2e"],"14d66d86a8b184a86bcaebcf6e15fcef486e0876":["28288370235ed02234a64753cdbf0c6ec096304a"]},"commit2Childs":{"83b6ce113ec151d7bf9175578d92d5320f91ab2e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["83b6ce113ec151d7bf9175578d92d5320f91ab2e"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","14d66d86a8b184a86bcaebcf6e15fcef486e0876"],"14d66d86a8b184a86bcaebcf6e15fcef486e0876":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}