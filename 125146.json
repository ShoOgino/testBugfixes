{"path":"src/java/org/apache/lucene/index/DocumentsWriter#writeSegment().mjava","commits":[{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#writeSegment().mjava","pathOld":"/dev/null","sourceNew":"  /** Creates a segment from all Postings in the Postings\n   *  hashes across all ThreadStates & FieldDatas. */\n  private List writeSegment() throws IOException {\n\n    assert allThreadsIdle();\n\n    assert nextDocID == numDocsInRAM;\n\n    final String segmentName;\n\n    segmentName = segment;\n\n    TermInfosWriter termsOut = new TermInfosWriter(directory, segmentName, fieldInfos,\n                                                   writer.getTermIndexInterval());\n\n    IndexOutput freqOut = directory.createOutput(segmentName + \".frq\");\n    IndexOutput proxOut = directory.createOutput(segmentName + \".prx\");\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    ArrayList allFields = new ArrayList();\n    assert allThreadsIdle();\n    for(int i=0;i<threadStates.length;i++) {\n      ThreadState state = threadStates[i];\n      state.trimFields();\n      final int numFields = state.numAllFieldData;\n      for(int j=0;j<numFields;j++) {\n        ThreadState.FieldData fp = state.allFieldDataArray[j];\n        if (fp.numPostings > 0)\n          allFields.add(fp);\n      }\n    }\n\n    // Sort by field name\n    Collections.sort(allFields);\n    final int numAllFields = allFields.size();\n\n    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,\n                                               termsOut.maxSkipLevels,\n                                               numDocsInRAM, freqOut, proxOut);\n\n    int start = 0;\n    while(start < numAllFields) {\n\n      final String fieldName = ((ThreadState.FieldData) allFields.get(start)).fieldInfo.name;\n\n      int end = start+1;\n      while(end < numAllFields && ((ThreadState.FieldData) allFields.get(end)).fieldInfo.name.equals(fieldName))\n        end++;\n      \n      ThreadState.FieldData[] fields = new ThreadState.FieldData[end-start];\n      for(int i=start;i<end;i++)\n        fields[i-start] = (ThreadState.FieldData) allFields.get(i);\n\n      // If this field has postings then add them to the\n      // segment\n      appendPostings(fields, termsOut, freqOut, proxOut);\n\n      for(int i=0;i<fields.length;i++)\n        fields[i].resetPostingArrays();\n\n      start = end;\n    }\n\n    freqOut.close();\n    proxOut.close();\n    termsOut.close();\n    \n    // Record all files we have flushed\n    List flushedFiles = new ArrayList();\n    flushedFiles.add(segmentFileName(IndexFileNames.FIELD_INFOS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.FREQ_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.PROX_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_INDEX_EXTENSION));\n\n    if (hasNorms) {\n      writeNorms(segmentName, numDocsInRAM);\n      hasNorms = false;\n      flushedFiles.add(segmentFileName(IndexFileNames.NORMS_EXTENSION));\n    }\n\n    if (infoStream != null) {\n      final long newSegmentSize = segmentSize(segmentName);\n      String message = \"  oldRAMSize=\" + numBytesUsed + \" newFlushedSize=\" + newSegmentSize + \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) + \" new/old=\" + nf.format(100.0*newSegmentSize/numBytesUsed) + \"%\";\n      infoStream.println(message);\n    }\n\n    resetPostingsData();\n\n    nextDocID = 0;\n    nextWriteDocID = 0;\n    numDocsInRAM = 0;\n    files = null;\n\n    // Maybe downsize postingsFreeList array\n    if (postingsFreeList.length > 1.5*postingsFreeCount) {\n      int newSize = postingsFreeList.length;\n      while(newSize > 1.25*postingsFreeCount) {\n        newSize = (int) (newSize*0.8);\n      }\n      Posting[] newArray = new Posting[newSize];\n      System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);\n      postingsFreeList = newArray;\n    }\n\n    return flushedFiles;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6754b3085e8633adf1ed1ac8bac35fb34a06cf97","date":1183575801,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#writeSegment().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#writeSegment().mjava","sourceNew":"  /** Creates a segment from all Postings in the Postings\n   *  hashes across all ThreadStates & FieldDatas. */\n  private List writeSegment() throws IOException {\n\n    assert allThreadsIdle();\n\n    assert nextDocID == numDocsInRAM;\n\n    final String segmentName;\n\n    segmentName = segment;\n\n    TermInfosWriter termsOut = new TermInfosWriter(directory, segmentName, fieldInfos,\n                                                   writer.getTermIndexInterval());\n\n    IndexOutput freqOut = directory.createOutput(segmentName + \".frq\");\n    IndexOutput proxOut = directory.createOutput(segmentName + \".prx\");\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    ArrayList allFields = new ArrayList();\n    assert allThreadsIdle();\n    for(int i=0;i<threadStates.length;i++) {\n      ThreadState state = threadStates[i];\n      state.trimFields();\n      final int numFields = state.numAllFieldData;\n      for(int j=0;j<numFields;j++) {\n        ThreadState.FieldData fp = state.allFieldDataArray[j];\n        if (fp.numPostings > 0)\n          allFields.add(fp);\n      }\n    }\n\n    // Sort by field name\n    Collections.sort(allFields);\n    final int numAllFields = allFields.size();\n\n    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,\n                                               termsOut.maxSkipLevels,\n                                               numDocsInRAM, freqOut, proxOut);\n\n    int start = 0;\n    while(start < numAllFields) {\n\n      final String fieldName = ((ThreadState.FieldData) allFields.get(start)).fieldInfo.name;\n\n      int end = start+1;\n      while(end < numAllFields && ((ThreadState.FieldData) allFields.get(end)).fieldInfo.name.equals(fieldName))\n        end++;\n      \n      ThreadState.FieldData[] fields = new ThreadState.FieldData[end-start];\n      for(int i=start;i<end;i++)\n        fields[i-start] = (ThreadState.FieldData) allFields.get(i);\n\n      // If this field has postings then add them to the\n      // segment\n      appendPostings(fields, termsOut, freqOut, proxOut);\n\n      for(int i=0;i<fields.length;i++)\n        fields[i].resetPostingArrays();\n\n      start = end;\n    }\n\n    freqOut.close();\n    proxOut.close();\n    termsOut.close();\n    \n    // Record all files we have flushed\n    List flushedFiles = new ArrayList();\n    flushedFiles.add(segmentFileName(IndexFileNames.FIELD_INFOS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.FREQ_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.PROX_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_INDEX_EXTENSION));\n\n    if (hasNorms) {\n      writeNorms(segmentName, numDocsInRAM);\n      flushedFiles.add(segmentFileName(IndexFileNames.NORMS_EXTENSION));\n    }\n\n    if (infoStream != null) {\n      final long newSegmentSize = segmentSize(segmentName);\n      String message = \"  oldRAMSize=\" + numBytesUsed + \" newFlushedSize=\" + newSegmentSize + \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) + \" new/old=\" + nf.format(100.0*newSegmentSize/numBytesUsed) + \"%\";\n      infoStream.println(message);\n    }\n\n    resetPostingsData();\n\n    nextDocID = 0;\n    nextWriteDocID = 0;\n    numDocsInRAM = 0;\n    files = null;\n\n    // Maybe downsize postingsFreeList array\n    if (postingsFreeList.length > 1.5*postingsFreeCount) {\n      int newSize = postingsFreeList.length;\n      while(newSize > 1.25*postingsFreeCount) {\n        newSize = (int) (newSize*0.8);\n      }\n      Posting[] newArray = new Posting[newSize];\n      System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);\n      postingsFreeList = newArray;\n    }\n\n    return flushedFiles;\n  }\n\n","sourceOld":"  /** Creates a segment from all Postings in the Postings\n   *  hashes across all ThreadStates & FieldDatas. */\n  private List writeSegment() throws IOException {\n\n    assert allThreadsIdle();\n\n    assert nextDocID == numDocsInRAM;\n\n    final String segmentName;\n\n    segmentName = segment;\n\n    TermInfosWriter termsOut = new TermInfosWriter(directory, segmentName, fieldInfos,\n                                                   writer.getTermIndexInterval());\n\n    IndexOutput freqOut = directory.createOutput(segmentName + \".frq\");\n    IndexOutput proxOut = directory.createOutput(segmentName + \".prx\");\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    ArrayList allFields = new ArrayList();\n    assert allThreadsIdle();\n    for(int i=0;i<threadStates.length;i++) {\n      ThreadState state = threadStates[i];\n      state.trimFields();\n      final int numFields = state.numAllFieldData;\n      for(int j=0;j<numFields;j++) {\n        ThreadState.FieldData fp = state.allFieldDataArray[j];\n        if (fp.numPostings > 0)\n          allFields.add(fp);\n      }\n    }\n\n    // Sort by field name\n    Collections.sort(allFields);\n    final int numAllFields = allFields.size();\n\n    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,\n                                               termsOut.maxSkipLevels,\n                                               numDocsInRAM, freqOut, proxOut);\n\n    int start = 0;\n    while(start < numAllFields) {\n\n      final String fieldName = ((ThreadState.FieldData) allFields.get(start)).fieldInfo.name;\n\n      int end = start+1;\n      while(end < numAllFields && ((ThreadState.FieldData) allFields.get(end)).fieldInfo.name.equals(fieldName))\n        end++;\n      \n      ThreadState.FieldData[] fields = new ThreadState.FieldData[end-start];\n      for(int i=start;i<end;i++)\n        fields[i-start] = (ThreadState.FieldData) allFields.get(i);\n\n      // If this field has postings then add them to the\n      // segment\n      appendPostings(fields, termsOut, freqOut, proxOut);\n\n      for(int i=0;i<fields.length;i++)\n        fields[i].resetPostingArrays();\n\n      start = end;\n    }\n\n    freqOut.close();\n    proxOut.close();\n    termsOut.close();\n    \n    // Record all files we have flushed\n    List flushedFiles = new ArrayList();\n    flushedFiles.add(segmentFileName(IndexFileNames.FIELD_INFOS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.FREQ_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.PROX_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_INDEX_EXTENSION));\n\n    if (hasNorms) {\n      writeNorms(segmentName, numDocsInRAM);\n      hasNorms = false;\n      flushedFiles.add(segmentFileName(IndexFileNames.NORMS_EXTENSION));\n    }\n\n    if (infoStream != null) {\n      final long newSegmentSize = segmentSize(segmentName);\n      String message = \"  oldRAMSize=\" + numBytesUsed + \" newFlushedSize=\" + newSegmentSize + \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) + \" new/old=\" + nf.format(100.0*newSegmentSize/numBytesUsed) + \"%\";\n      infoStream.println(message);\n    }\n\n    resetPostingsData();\n\n    nextDocID = 0;\n    nextWriteDocID = 0;\n    numDocsInRAM = 0;\n    files = null;\n\n    // Maybe downsize postingsFreeList array\n    if (postingsFreeList.length > 1.5*postingsFreeCount) {\n      int newSize = postingsFreeList.length;\n      while(newSize > 1.25*postingsFreeCount) {\n        newSize = (int) (newSize*0.8);\n      }\n      Posting[] newArray = new Posting[newSize];\n      System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);\n      postingsFreeList = newArray;\n    }\n\n    return flushedFiles;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be","date":1204801324,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#writeSegment().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#writeSegment().mjava","sourceNew":"  /** Creates a segment from all Postings in the Postings\n   *  hashes across all ThreadStates & FieldDatas. */\n  private List writeSegment() throws IOException {\n\n    assert allThreadsIdle();\n\n    assert nextDocID == numDocsInRAM;\n\n    final String segmentName;\n\n    segmentName = segment;\n\n    TermInfosWriter termsOut = new TermInfosWriter(directory, segmentName, fieldInfos,\n                                                   writer.getTermIndexInterval());\n\n    IndexOutput freqOut = directory.createOutput(segmentName + \".frq\");\n    IndexOutput proxOut = directory.createOutput(segmentName + \".prx\");\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    ArrayList allFields = new ArrayList();\n    assert allThreadsIdle();\n    for(int i=0;i<threadStates.length;i++) {\n      ThreadState state = threadStates[i];\n      state.trimFields();\n      final int numFields = state.numAllFieldData;\n      for(int j=0;j<numFields;j++) {\n        ThreadState.FieldData fp = state.allFieldDataArray[j];\n        if (fp.numPostings > 0)\n          allFields.add(fp);\n      }\n    }\n\n    // Sort by field name\n    Collections.sort(allFields);\n    final int numAllFields = allFields.size();\n\n    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,\n                                               termsOut.maxSkipLevels,\n                                               numDocsInRAM, freqOut, proxOut);\n\n    int start = 0;\n    while(start < numAllFields) {\n\n      final String fieldName = ((ThreadState.FieldData) allFields.get(start)).fieldInfo.name;\n\n      int end = start+1;\n      while(end < numAllFields && ((ThreadState.FieldData) allFields.get(end)).fieldInfo.name.equals(fieldName))\n        end++;\n      \n      ThreadState.FieldData[] fields = new ThreadState.FieldData[end-start];\n      for(int i=start;i<end;i++)\n        fields[i-start] = (ThreadState.FieldData) allFields.get(i);\n\n      // If this field has postings then add them to the\n      // segment\n      appendPostings(fields, termsOut, freqOut, proxOut);\n\n      for(int i=0;i<fields.length;i++)\n        fields[i].resetPostingArrays();\n\n      start = end;\n    }\n\n    freqOut.close();\n    proxOut.close();\n    termsOut.close();\n    \n    // Record all files we have flushed\n    List flushedFiles = new ArrayList();\n    flushedFiles.add(segmentFileName(IndexFileNames.FIELD_INFOS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.FREQ_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.PROX_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_INDEX_EXTENSION));\n\n    if (hasNorms) {\n      writeNorms(segmentName, numDocsInRAM);\n      flushedFiles.add(segmentFileName(IndexFileNames.NORMS_EXTENSION));\n    }\n\n    if (infoStream != null) {\n      final long newSegmentSize = segmentSize(segmentName);\n      String message = \"  oldRAMSize=\" + numBytesUsed + \" newFlushedSize=\" + newSegmentSize + \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) + \" new/old=\" + nf.format(100.0*newSegmentSize/numBytesUsed) + \"%\";\n      infoStream.println(message);\n    }\n\n    resetPostingsData();\n    \n    // Maybe downsize postingsFreeList array\n    if (postingsFreeList.length > 1.5*postingsFreeCount) {\n      int newSize = postingsFreeList.length;\n      while(newSize > 1.25*postingsFreeCount) {\n        newSize = (int) (newSize*0.8);\n      }\n      Posting[] newArray = new Posting[newSize];\n      System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);\n      postingsFreeList = newArray;\n    }\n\n    return flushedFiles;\n  }\n\n","sourceOld":"  /** Creates a segment from all Postings in the Postings\n   *  hashes across all ThreadStates & FieldDatas. */\n  private List writeSegment() throws IOException {\n\n    assert allThreadsIdle();\n\n    assert nextDocID == numDocsInRAM;\n\n    final String segmentName;\n\n    segmentName = segment;\n\n    TermInfosWriter termsOut = new TermInfosWriter(directory, segmentName, fieldInfos,\n                                                   writer.getTermIndexInterval());\n\n    IndexOutput freqOut = directory.createOutput(segmentName + \".frq\");\n    IndexOutput proxOut = directory.createOutput(segmentName + \".prx\");\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    ArrayList allFields = new ArrayList();\n    assert allThreadsIdle();\n    for(int i=0;i<threadStates.length;i++) {\n      ThreadState state = threadStates[i];\n      state.trimFields();\n      final int numFields = state.numAllFieldData;\n      for(int j=0;j<numFields;j++) {\n        ThreadState.FieldData fp = state.allFieldDataArray[j];\n        if (fp.numPostings > 0)\n          allFields.add(fp);\n      }\n    }\n\n    // Sort by field name\n    Collections.sort(allFields);\n    final int numAllFields = allFields.size();\n\n    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,\n                                               termsOut.maxSkipLevels,\n                                               numDocsInRAM, freqOut, proxOut);\n\n    int start = 0;\n    while(start < numAllFields) {\n\n      final String fieldName = ((ThreadState.FieldData) allFields.get(start)).fieldInfo.name;\n\n      int end = start+1;\n      while(end < numAllFields && ((ThreadState.FieldData) allFields.get(end)).fieldInfo.name.equals(fieldName))\n        end++;\n      \n      ThreadState.FieldData[] fields = new ThreadState.FieldData[end-start];\n      for(int i=start;i<end;i++)\n        fields[i-start] = (ThreadState.FieldData) allFields.get(i);\n\n      // If this field has postings then add them to the\n      // segment\n      appendPostings(fields, termsOut, freqOut, proxOut);\n\n      for(int i=0;i<fields.length;i++)\n        fields[i].resetPostingArrays();\n\n      start = end;\n    }\n\n    freqOut.close();\n    proxOut.close();\n    termsOut.close();\n    \n    // Record all files we have flushed\n    List flushedFiles = new ArrayList();\n    flushedFiles.add(segmentFileName(IndexFileNames.FIELD_INFOS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.FREQ_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.PROX_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_INDEX_EXTENSION));\n\n    if (hasNorms) {\n      writeNorms(segmentName, numDocsInRAM);\n      flushedFiles.add(segmentFileName(IndexFileNames.NORMS_EXTENSION));\n    }\n\n    if (infoStream != null) {\n      final long newSegmentSize = segmentSize(segmentName);\n      String message = \"  oldRAMSize=\" + numBytesUsed + \" newFlushedSize=\" + newSegmentSize + \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) + \" new/old=\" + nf.format(100.0*newSegmentSize/numBytesUsed) + \"%\";\n      infoStream.println(message);\n    }\n\n    resetPostingsData();\n\n    nextDocID = 0;\n    nextWriteDocID = 0;\n    numDocsInRAM = 0;\n    files = null;\n\n    // Maybe downsize postingsFreeList array\n    if (postingsFreeList.length > 1.5*postingsFreeCount) {\n      int newSize = postingsFreeList.length;\n      while(newSize > 1.25*postingsFreeCount) {\n        newSize = (int) (newSize*0.8);\n      }\n      Posting[] newArray = new Posting[newSize];\n      System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);\n      postingsFreeList = newArray;\n    }\n\n    return flushedFiles;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a0af3a442be522899177e5e11384a45a6784a3f","date":1205348952,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#writeSegment().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#writeSegment().mjava","sourceNew":"  /** Creates a segment from all Postings in the Postings\n   *  hashes across all ThreadStates & FieldDatas. */\n  private List writeSegment() throws IOException {\n\n    assert allThreadsIdle();\n\n    assert nextDocID == numDocsInRAM;\n\n    final String segmentName;\n\n    segmentName = segment;\n\n    TermInfosWriter termsOut = new TermInfosWriter(directory, segmentName, fieldInfos,\n                                                   writer.getTermIndexInterval());\n\n    IndexOutput freqOut = directory.createOutput(segmentName + \".frq\");\n    IndexOutput proxOut = directory.createOutput(segmentName + \".prx\");\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    ArrayList allFields = new ArrayList();\n    assert allThreadsIdle();\n    for(int i=0;i<threadStates.length;i++) {\n      DocumentsWriterThreadState state = threadStates[i];\n      state.trimFields();\n      final int numFields = state.numAllFieldData;\n      for(int j=0;j<numFields;j++) {\n        DocumentsWriterFieldData fp = state.allFieldDataArray[j];\n        if (fp.numPostings > 0)\n          allFields.add(fp);\n      }\n    }\n\n    // Sort by field name\n    Collections.sort(allFields);\n    final int numAllFields = allFields.size();\n\n    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,\n                                               termsOut.maxSkipLevels,\n                                               numDocsInRAM, freqOut, proxOut);\n\n    int start = 0;\n    while(start < numAllFields) {\n\n      final String fieldName = ((DocumentsWriterFieldData) allFields.get(start)).fieldInfo.name;\n\n      int end = start+1;\n      while(end < numAllFields && ((DocumentsWriterFieldData) allFields.get(end)).fieldInfo.name.equals(fieldName))\n        end++;\n      \n      DocumentsWriterFieldData[] fields = new DocumentsWriterFieldData[end-start];\n      for(int i=start;i<end;i++)\n        fields[i-start] = (DocumentsWriterFieldData) allFields.get(i);\n\n      // If this field has postings then add them to the\n      // segment\n      appendPostings(fields, termsOut, freqOut, proxOut);\n\n      for(int i=0;i<fields.length;i++)\n        fields[i].resetPostingArrays();\n\n      start = end;\n    }\n\n    freqOut.close();\n    proxOut.close();\n    termsOut.close();\n    \n    // Record all files we have flushed\n    List flushedFiles = new ArrayList();\n    flushedFiles.add(segmentFileName(IndexFileNames.FIELD_INFOS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.FREQ_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.PROX_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_INDEX_EXTENSION));\n\n    if (hasNorms) {\n      writeNorms(segmentName, numDocsInRAM);\n      flushedFiles.add(segmentFileName(IndexFileNames.NORMS_EXTENSION));\n    }\n\n    if (infoStream != null) {\n      final long newSegmentSize = segmentSize(segmentName);\n      String message = \"  oldRAMSize=\" + numBytesUsed + \" newFlushedSize=\" + newSegmentSize + \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) + \" new/old=\" + nf.format(100.0*newSegmentSize/numBytesUsed) + \"%\";\n      infoStream.println(message);\n    }\n\n    resetPostingsData();\n    \n    // Maybe downsize postingsFreeList array\n    if (postingsFreeList.length > 1.5*postingsFreeCount) {\n      int newSize = postingsFreeList.length;\n      while(newSize > 1.25*postingsFreeCount) {\n        newSize = (int) (newSize*0.8);\n      }\n      Posting[] newArray = new Posting[newSize];\n      System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);\n      postingsFreeList = newArray;\n    }\n\n    return flushedFiles;\n  }\n\n","sourceOld":"  /** Creates a segment from all Postings in the Postings\n   *  hashes across all ThreadStates & FieldDatas. */\n  private List writeSegment() throws IOException {\n\n    assert allThreadsIdle();\n\n    assert nextDocID == numDocsInRAM;\n\n    final String segmentName;\n\n    segmentName = segment;\n\n    TermInfosWriter termsOut = new TermInfosWriter(directory, segmentName, fieldInfos,\n                                                   writer.getTermIndexInterval());\n\n    IndexOutput freqOut = directory.createOutput(segmentName + \".frq\");\n    IndexOutput proxOut = directory.createOutput(segmentName + \".prx\");\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    ArrayList allFields = new ArrayList();\n    assert allThreadsIdle();\n    for(int i=0;i<threadStates.length;i++) {\n      ThreadState state = threadStates[i];\n      state.trimFields();\n      final int numFields = state.numAllFieldData;\n      for(int j=0;j<numFields;j++) {\n        ThreadState.FieldData fp = state.allFieldDataArray[j];\n        if (fp.numPostings > 0)\n          allFields.add(fp);\n      }\n    }\n\n    // Sort by field name\n    Collections.sort(allFields);\n    final int numAllFields = allFields.size();\n\n    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,\n                                               termsOut.maxSkipLevels,\n                                               numDocsInRAM, freqOut, proxOut);\n\n    int start = 0;\n    while(start < numAllFields) {\n\n      final String fieldName = ((ThreadState.FieldData) allFields.get(start)).fieldInfo.name;\n\n      int end = start+1;\n      while(end < numAllFields && ((ThreadState.FieldData) allFields.get(end)).fieldInfo.name.equals(fieldName))\n        end++;\n      \n      ThreadState.FieldData[] fields = new ThreadState.FieldData[end-start];\n      for(int i=start;i<end;i++)\n        fields[i-start] = (ThreadState.FieldData) allFields.get(i);\n\n      // If this field has postings then add them to the\n      // segment\n      appendPostings(fields, termsOut, freqOut, proxOut);\n\n      for(int i=0;i<fields.length;i++)\n        fields[i].resetPostingArrays();\n\n      start = end;\n    }\n\n    freqOut.close();\n    proxOut.close();\n    termsOut.close();\n    \n    // Record all files we have flushed\n    List flushedFiles = new ArrayList();\n    flushedFiles.add(segmentFileName(IndexFileNames.FIELD_INFOS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.FREQ_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.PROX_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_INDEX_EXTENSION));\n\n    if (hasNorms) {\n      writeNorms(segmentName, numDocsInRAM);\n      flushedFiles.add(segmentFileName(IndexFileNames.NORMS_EXTENSION));\n    }\n\n    if (infoStream != null) {\n      final long newSegmentSize = segmentSize(segmentName);\n      String message = \"  oldRAMSize=\" + numBytesUsed + \" newFlushedSize=\" + newSegmentSize + \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) + \" new/old=\" + nf.format(100.0*newSegmentSize/numBytesUsed) + \"%\";\n      infoStream.println(message);\n    }\n\n    resetPostingsData();\n    \n    // Maybe downsize postingsFreeList array\n    if (postingsFreeList.length > 1.5*postingsFreeCount) {\n      int newSize = postingsFreeList.length;\n      while(newSize > 1.25*postingsFreeCount) {\n        newSize = (int) (newSize*0.8);\n      }\n      Posting[] newArray = new Posting[newSize];\n      System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);\n      postingsFreeList = newArray;\n    }\n\n    return flushedFiles;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84acdfa12c18361ff932244db20470fce117e52d","date":1206384355,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#writeSegment().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#writeSegment().mjava","sourceNew":"  /** Creates a segment from all Postings in the Postings\n   *  hashes across all ThreadStates & FieldDatas. */\n  private List writeSegment() throws IOException {\n\n    assert allThreadsIdle();\n\n    assert nextDocID == numDocsInRAM;\n\n    final String segmentName;\n\n    segmentName = segment;\n\n    TermInfosWriter termsOut = new TermInfosWriter(directory, segmentName, fieldInfos,\n                                                   writer.getTermIndexInterval());\n\n    IndexOutput freqOut = directory.createOutput(segmentName + \".frq\");\n    IndexOutput proxOut = directory.createOutput(segmentName + \".prx\");\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    ArrayList allFields = new ArrayList();\n    assert allThreadsIdle();\n    for(int i=0;i<threadStates.length;i++) {\n      DocumentsWriterThreadState state = threadStates[i];\n      state.trimFields();\n      final int numFields = state.numAllFieldData;\n      for(int j=0;j<numFields;j++) {\n        DocumentsWriterFieldData fp = state.allFieldDataArray[j];\n        if (fp.numPostings > 0)\n          allFields.add(fp);\n      }\n    }\n\n    // Sort by field name\n    Collections.sort(allFields);\n    final int numAllFields = allFields.size();\n\n    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,\n                                               termsOut.maxSkipLevels,\n                                               numDocsInRAM, freqOut, proxOut);\n\n    int start = 0;\n    while(start < numAllFields) {\n\n      final String fieldName = ((DocumentsWriterFieldData) allFields.get(start)).fieldInfo.name;\n\n      int end = start+1;\n      while(end < numAllFields && ((DocumentsWriterFieldData) allFields.get(end)).fieldInfo.name.equals(fieldName))\n        end++;\n      \n      DocumentsWriterFieldData[] fields = new DocumentsWriterFieldData[end-start];\n      for(int i=start;i<end;i++)\n        fields[i-start] = (DocumentsWriterFieldData) allFields.get(i);\n\n      // If this field has postings then add them to the\n      // segment\n      appendPostings(fields, termsOut, freqOut, proxOut);\n\n      for(int i=0;i<fields.length;i++)\n        fields[i].resetPostingArrays();\n\n      start = end;\n    }\n\n    freqOut.close();\n    proxOut.close();\n    termsOut.close();\n    \n    // Record all files we have flushed\n    List flushedFiles = new ArrayList();\n    flushedFiles.add(segmentFileName(IndexFileNames.FIELD_INFOS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.FREQ_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.PROX_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_INDEX_EXTENSION));\n\n    if (hasNorms) {\n      writeNorms(segmentName, numDocsInRAM);\n      flushedFiles.add(segmentFileName(IndexFileNames.NORMS_EXTENSION));\n    }\n\n    if (infoStream != null) {\n      final long newSegmentSize = segmentSize(segmentName);\n      String message = \"  oldRAMSize=\" + numBytesUsed + \" newFlushedSize=\" + newSegmentSize + \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) + \" new/old=\" + nf.format(100.0*newSegmentSize/numBytesUsed) + \"%\";\n      message(message);\n    }\n\n    resetPostingsData();\n    \n    // Maybe downsize postingsFreeList array\n    if (postingsFreeList.length > 1.5*postingsFreeCount) {\n      int newSize = postingsFreeList.length;\n      while(newSize > 1.25*postingsFreeCount) {\n        newSize = (int) (newSize*0.8);\n      }\n      Posting[] newArray = new Posting[newSize];\n      System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);\n      postingsFreeList = newArray;\n    }\n\n    return flushedFiles;\n  }\n\n","sourceOld":"  /** Creates a segment from all Postings in the Postings\n   *  hashes across all ThreadStates & FieldDatas. */\n  private List writeSegment() throws IOException {\n\n    assert allThreadsIdle();\n\n    assert nextDocID == numDocsInRAM;\n\n    final String segmentName;\n\n    segmentName = segment;\n\n    TermInfosWriter termsOut = new TermInfosWriter(directory, segmentName, fieldInfos,\n                                                   writer.getTermIndexInterval());\n\n    IndexOutput freqOut = directory.createOutput(segmentName + \".frq\");\n    IndexOutput proxOut = directory.createOutput(segmentName + \".prx\");\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    ArrayList allFields = new ArrayList();\n    assert allThreadsIdle();\n    for(int i=0;i<threadStates.length;i++) {\n      DocumentsWriterThreadState state = threadStates[i];\n      state.trimFields();\n      final int numFields = state.numAllFieldData;\n      for(int j=0;j<numFields;j++) {\n        DocumentsWriterFieldData fp = state.allFieldDataArray[j];\n        if (fp.numPostings > 0)\n          allFields.add(fp);\n      }\n    }\n\n    // Sort by field name\n    Collections.sort(allFields);\n    final int numAllFields = allFields.size();\n\n    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,\n                                               termsOut.maxSkipLevels,\n                                               numDocsInRAM, freqOut, proxOut);\n\n    int start = 0;\n    while(start < numAllFields) {\n\n      final String fieldName = ((DocumentsWriterFieldData) allFields.get(start)).fieldInfo.name;\n\n      int end = start+1;\n      while(end < numAllFields && ((DocumentsWriterFieldData) allFields.get(end)).fieldInfo.name.equals(fieldName))\n        end++;\n      \n      DocumentsWriterFieldData[] fields = new DocumentsWriterFieldData[end-start];\n      for(int i=start;i<end;i++)\n        fields[i-start] = (DocumentsWriterFieldData) allFields.get(i);\n\n      // If this field has postings then add them to the\n      // segment\n      appendPostings(fields, termsOut, freqOut, proxOut);\n\n      for(int i=0;i<fields.length;i++)\n        fields[i].resetPostingArrays();\n\n      start = end;\n    }\n\n    freqOut.close();\n    proxOut.close();\n    termsOut.close();\n    \n    // Record all files we have flushed\n    List flushedFiles = new ArrayList();\n    flushedFiles.add(segmentFileName(IndexFileNames.FIELD_INFOS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.FREQ_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.PROX_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_INDEX_EXTENSION));\n\n    if (hasNorms) {\n      writeNorms(segmentName, numDocsInRAM);\n      flushedFiles.add(segmentFileName(IndexFileNames.NORMS_EXTENSION));\n    }\n\n    if (infoStream != null) {\n      final long newSegmentSize = segmentSize(segmentName);\n      String message = \"  oldRAMSize=\" + numBytesUsed + \" newFlushedSize=\" + newSegmentSize + \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) + \" new/old=\" + nf.format(100.0*newSegmentSize/numBytesUsed) + \"%\";\n      infoStream.println(message);\n    }\n\n    resetPostingsData();\n    \n    // Maybe downsize postingsFreeList array\n    if (postingsFreeList.length > 1.5*postingsFreeCount) {\n      int newSize = postingsFreeList.length;\n      while(newSize > 1.25*postingsFreeCount) {\n        newSize = (int) (newSize*0.8);\n      }\n      Posting[] newArray = new Posting[newSize];\n      System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);\n      postingsFreeList = newArray;\n    }\n\n    return flushedFiles;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5350389bf83287111f7760b9e3db3af8e3648474","date":1216372812,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#writeSegment().mjava","sourceNew":null,"sourceOld":"  /** Creates a segment from all Postings in the Postings\n   *  hashes across all ThreadStates & FieldDatas. */\n  private List writeSegment() throws IOException {\n\n    assert allThreadsIdle();\n\n    assert nextDocID == numDocsInRAM;\n\n    final String segmentName;\n\n    segmentName = segment;\n\n    TermInfosWriter termsOut = new TermInfosWriter(directory, segmentName, fieldInfos,\n                                                   writer.getTermIndexInterval());\n\n    IndexOutput freqOut = directory.createOutput(segmentName + \".frq\");\n    IndexOutput proxOut = directory.createOutput(segmentName + \".prx\");\n\n    // Gather all FieldData's that have postings, across all\n    // ThreadStates\n    ArrayList allFields = new ArrayList();\n    assert allThreadsIdle();\n    for(int i=0;i<threadStates.length;i++) {\n      DocumentsWriterThreadState state = threadStates[i];\n      state.trimFields();\n      final int numFields = state.numAllFieldData;\n      for(int j=0;j<numFields;j++) {\n        DocumentsWriterFieldData fp = state.allFieldDataArray[j];\n        if (fp.numPostings > 0)\n          allFields.add(fp);\n      }\n    }\n\n    // Sort by field name\n    Collections.sort(allFields);\n    final int numAllFields = allFields.size();\n\n    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,\n                                               termsOut.maxSkipLevels,\n                                               numDocsInRAM, freqOut, proxOut);\n\n    int start = 0;\n    while(start < numAllFields) {\n\n      final String fieldName = ((DocumentsWriterFieldData) allFields.get(start)).fieldInfo.name;\n\n      int end = start+1;\n      while(end < numAllFields && ((DocumentsWriterFieldData) allFields.get(end)).fieldInfo.name.equals(fieldName))\n        end++;\n      \n      DocumentsWriterFieldData[] fields = new DocumentsWriterFieldData[end-start];\n      for(int i=start;i<end;i++)\n        fields[i-start] = (DocumentsWriterFieldData) allFields.get(i);\n\n      // If this field has postings then add them to the\n      // segment\n      appendPostings(fields, termsOut, freqOut, proxOut);\n\n      for(int i=0;i<fields.length;i++)\n        fields[i].resetPostingArrays();\n\n      start = end;\n    }\n\n    freqOut.close();\n    proxOut.close();\n    termsOut.close();\n    \n    // Record all files we have flushed\n    List flushedFiles = new ArrayList();\n    flushedFiles.add(segmentFileName(IndexFileNames.FIELD_INFOS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.FREQ_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.PROX_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_EXTENSION));\n    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_INDEX_EXTENSION));\n\n    if (hasNorms) {\n      writeNorms(segmentName, numDocsInRAM);\n      flushedFiles.add(segmentFileName(IndexFileNames.NORMS_EXTENSION));\n    }\n\n    if (infoStream != null) {\n      final long newSegmentSize = segmentSize(segmentName);\n      String message = \"  oldRAMSize=\" + numBytesUsed + \" newFlushedSize=\" + newSegmentSize + \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) + \" new/old=\" + nf.format(100.0*newSegmentSize/numBytesUsed) + \"%\";\n      message(message);\n    }\n\n    resetPostingsData();\n    \n    // Maybe downsize postingsFreeList array\n    if (postingsFreeList.length > 1.5*postingsFreeCount) {\n      int newSize = postingsFreeList.length;\n      while(newSize > 1.25*postingsFreeCount) {\n        newSize = (int) (newSize*0.8);\n      }\n      Posting[] newArray = new Posting[newSize];\n      System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);\n      postingsFreeList = newArray;\n    }\n\n    return flushedFiles;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6754b3085e8633adf1ed1ac8bac35fb34a06cf97":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["6754b3085e8633adf1ed1ac8bac35fb34a06cf97"],"5a0af3a442be522899177e5e11384a45a6784a3f":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"5350389bf83287111f7760b9e3db3af8e3648474":["84acdfa12c18361ff932244db20470fce117e52d"],"84acdfa12c18361ff932244db20470fce117e52d":["5a0af3a442be522899177e5e11384a45a6784a3f"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5350389bf83287111f7760b9e3db3af8e3648474"]},"commit2Childs":{"6754b3085e8633adf1ed1ac8bac35fb34a06cf97":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["5a0af3a442be522899177e5e11384a45a6784a3f"],"5a0af3a442be522899177e5e11384a45a6784a3f":["84acdfa12c18361ff932244db20470fce117e52d"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["6754b3085e8633adf1ed1ac8bac35fb34a06cf97"],"5350389bf83287111f7760b9e3db3af8e3648474":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"84acdfa12c18361ff932244db20470fce117e52d":["5350389bf83287111f7760b9e3db3af8e3648474"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}