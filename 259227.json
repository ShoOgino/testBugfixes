{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","commits":[{"id":"c177b66783fe11c18553f8b57e8b745098cc7607","date":1412798789,"type":1,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55980207f1977bd1463465de1659b821347e2fa8","date":1413336386,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a56958d7f71a28824f20031ffbb2e13502a0274e","date":1425573902,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    Analyzer analyzer = new ClassicAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n    analyzer.close();\n    sa.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    Analyzer analyzer = new ClassicAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n    analyzer.close();\n    sa.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    Analyzer analyzer = new ClassicAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n    analyzer.close();\n    sa.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    Analyzer analyzer = new ClassicAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n    analyzer.close();\n    sa.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    Analyzer analyzer = new ClassicAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiTerms.getTermPostingsEnum(reader,\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n    analyzer.close();\n    sa.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    Analyzer analyzer = new ClassicAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n    analyzer.close();\n    sa.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d77dafd89756a5161d244985903e3487ca109182","date":1548679743,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = new ByteBuffersDirectory();\n    Analyzer analyzer = new ClassicAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiTerms.getTermPostingsEnum(reader,\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n    analyzer.close();\n    sa.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    Analyzer analyzer = new ClassicAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiTerms.getTermPostingsEnum(reader,\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n    analyzer.close();\n    sa.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"313c36388b6cae6118f75a1860ad0ba0af7e1344","date":1601279368,"type":5,"author":"Tomoko Uchida","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/classic/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = new ByteBuffersDirectory();\n    Analyzer analyzer = new ClassicAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiTerms.getTermPostingsEnum(reader,\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n    analyzer.close();\n    sa.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    Directory dir = new ByteBuffersDirectory();\n    Analyzer analyzer = new ClassicAnalyzer();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(analyzer));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    PostingsEnum tps = MultiTerms.getTermPostingsEnum(reader,\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n    analyzer.close();\n    sa.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["51f5280f31484820499077f41fcdfe92d527d9dc","a56958d7f71a28824f20031ffbb2e13502a0274e"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["a56958d7f71a28824f20031ffbb2e13502a0274e"],"55980207f1977bd1463465de1659b821347e2fa8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c177b66783fe11c18553f8b57e8b745098cc7607"],"313c36388b6cae6118f75a1860ad0ba0af7e1344":["d77dafd89756a5161d244985903e3487ca109182"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d77dafd89756a5161d244985903e3487ca109182":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"c177b66783fe11c18553f8b57e8b745098cc7607":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a56958d7f71a28824f20031ffbb2e13502a0274e":["51f5280f31484820499077f41fcdfe92d527d9dc"],"51f5280f31484820499077f41fcdfe92d527d9dc":["c177b66783fe11c18553f8b57e8b745098cc7607"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["313c36388b6cae6118f75a1860ad0ba0af7e1344"]},"commit2Childs":{"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"0f4464508ee83288c8c4585b533f9faaa93aa314":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"55980207f1977bd1463465de1659b821347e2fa8":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["55980207f1977bd1463465de1659b821347e2fa8","c177b66783fe11c18553f8b57e8b745098cc7607"],"313c36388b6cae6118f75a1860ad0ba0af7e1344":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d77dafd89756a5161d244985903e3487ca109182":["313c36388b6cae6118f75a1860ad0ba0af7e1344"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["d77dafd89756a5161d244985903e3487ca109182"],"c177b66783fe11c18553f8b57e8b745098cc7607":["55980207f1977bd1463465de1659b821347e2fa8","51f5280f31484820499077f41fcdfe92d527d9dc"],"a56958d7f71a28824f20031ffbb2e13502a0274e":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","0f4464508ee83288c8c4585b533f9faaa93aa314"],"51f5280f31484820499077f41fcdfe92d527d9dc":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","a56958d7f71a28824f20031ffbb2e13502a0274e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","55980207f1977bd1463465de1659b821347e2fa8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}