{"path":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setUseCompoundFile(useCompoundFile);\n      }\n      \n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newField(\"contents\", docs[j], TextField.TYPE_STORED));\n        d.add(newField(\"id\", \"\"+j, StringField.TYPE_UNSTORED));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = IndexReader.open(directory);\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          Document d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setUseCompoundFile(useCompoundFile);\n      }\n      \n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newField(\"contents\", docs[j], TextField.TYPE_STORED));\n        d.add(newField(\"id\", \"\"+j, StringField.TYPE_UNSTORED));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = IndexReader.open(directory);\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          Document d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setUseCompoundFile(useCompoundFile);\n      }\n      \n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newField(\"contents\", docs[j], TextField.TYPE_STORED));\n        d.add(newField(\"id\", \"\"+j, StringField.TYPE_UNSTORED));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          Document d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setUseCompoundFile(useCompoundFile);\n      }\n      \n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newField(\"contents\", docs[j], TextField.TYPE_STORED));\n        d.add(newField(\"id\", \"\"+j, StringField.TYPE_UNSTORED));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = IndexReader.open(directory);\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          Document d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setUseCompoundFile(useCompoundFile);\n      }\n      \n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(newStringField(\"id\", \"\"+j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          Document d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setUseCompoundFile(useCompoundFile);\n      }\n      \n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newField(\"contents\", docs[j], TextField.TYPE_STORED));\n        d.add(newField(\"id\", \"\"+j, StringField.TYPE_UNSTORED));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          Document d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","date":1341839195,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setUseCompoundFile(useCompoundFile);\n      }\n      \n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(newStringField(\"id\", \"\"+j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setUseCompoundFile(useCompoundFile);\n      }\n      \n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(newStringField(\"id\", \"\"+j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          Document d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setUseCompoundFile(useCompoundFile);\n      }\n      \n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(newStringField(\"id\", \"\"+j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setUseCompoundFile(useCompoundFile);\n      }\n      \n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(newStringField(\"id\", \"\"+j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          Document d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173","date":1365631993,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setUseCompoundFile(useCompoundFile);\n      }\n      \n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(newStringField(\"id\", \"\"+j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setUseCompoundFile(useCompoundFile);\n      }\n      \n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(newStringField(\"id\", \"\"+j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"088a7ef694fd43d5d9a4d200c4005865f773d1e7","date":1371136274,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(newStringField(\"id\", \"\"+j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setUseCompoundFile(useCompoundFile);\n      }\n      \n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(newStringField(\"id\", \"\"+j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"59a0020b413d44dd79d85d7a66ed5004265fb453","date":1371758877,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new IntField(\"id\", j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(newStringField(\"id\", \"\"+j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new IntField(\"id\", j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(newStringField(\"id\", \"\"+j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new IntField(\"id\", j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.shutdown();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new IntField(\"id\", j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new IntField(\"id\", j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.shutdown();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new IntField(\"id\", j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.shutdown();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new IntField(\"id\", j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new IntField(\"id\", j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.shutdown();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ed6b1234af94a2693d3e6550e7b3ee41fd3f51c","date":1416362965,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new IntField(\"id\", j, Field.Store.NO));\n        d.add(new NumericDocValuesField(\"id\", j));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new IntField(\"id\", j, Field.Store.NO));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc","date":1424799790,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new IntField(\"id\", j, Field.Store.NO));\n        d.add(new NumericDocValuesField(\"id\", j));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new IntField(\"id\", j, Field.Store.NO));\n        d.add(new NumericDocValuesField(\"id\", j));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, null, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"770342641f7b505eaa8dccdc666158bff2419109","date":1449868421,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new NumericDocValuesField(\"id\", j));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new IntField(\"id\", j, Field.Store.NO));\n        d.add(new NumericDocValuesField(\"id\", j));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/TestSearch#doTestSearch(Random,PrintWriter,boolean).mjava","sourceNew":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new NumericDocValuesField(\"id\", j));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          Document d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","sourceOld":"    private void doTestSearch(Random random, PrintWriter out, boolean useCompoundFile)\n    throws Exception {\n      Directory directory = newDirectory();\n      Analyzer analyzer = new MockAnalyzer(random);\n      IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n      MergePolicy mp = conf.getMergePolicy();\n      mp.setNoCFSRatio(useCompoundFile ? 1.0 : 0.0);\n      IndexWriter writer = new IndexWriter(directory, conf);\n\n      String[] docs = {\n        \"a b c d e\",\n        \"a b c d e a b c d e\",\n        \"a b c d e f g h i j\",\n        \"a c e\",\n        \"e c a\",\n        \"a c e a c e\",\n        \"a c e a b c\"\n      };\n      for (int j = 0; j < docs.length; j++) {\n        Document d = new Document();\n        d.add(newTextField(\"contents\", docs[j], Field.Store.YES));\n        d.add(new NumericDocValuesField(\"id\", j));\n        writer.addDocument(d);\n      }\n      writer.close();\n\n      IndexReader reader = DirectoryReader.open(directory);\n      IndexSearcher searcher = newSearcher(reader);\n\n      ScoreDoc[] hits = null;\n\n      Sort sort = new Sort(SortField.FIELD_SCORE,\n                           new SortField(\"id\", SortField.Type.INT));\n\n      for (Query query : buildQueries()) {\n        out.println(\"Query: \" + query.toString(\"contents\"));\n        if (VERBOSE) {\n          System.out.println(\"TEST: query=\" + query);\n        }\n\n        hits = searcher.search(query, 1000, sort).scoreDocs;\n\n        out.println(hits.length + \" total results\");\n        for (int i = 0 ; i < hits.length && i < 10; i++) {\n          StoredDocument d = searcher.doc(hits[i].doc);\n          out.println(i + \" \" + hits[i].score + \" \" + d.get(\"contents\"));\n        }\n      }\n      reader.close();\n      directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["088a7ef694fd43d5d9a4d200c4005865f773d1e7","59a0020b413d44dd79d85d7a66ed5004265fb453"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173":["1d028314cced5858683a1bb4741423d0f934257b"],"770342641f7b505eaa8dccdc666158bff2419109":["f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc"],"0ed6b1234af94a2693d3e6550e7b3ee41fd3f51c":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"1d028314cced5858683a1bb4741423d0f934257b":["04f07771a2a7dd3a395700665ed839c3dae2def2","8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["3b0e8c39ca08b5a02de6edcd33d6f3b90b865173"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc":["0ed6b1234af94a2693d3e6550e7b3ee41fd3f51c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"59a0020b413d44dd79d85d7a66ed5004265fb453":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["59a0020b413d44dd79d85d7a66ed5004265fb453"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["770342641f7b505eaa8dccdc666158bff2419109"]},"commit2Childs":{"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4":["1d028314cced5858683a1bb4741423d0f934257b"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"770342641f7b505eaa8dccdc666158bff2419109":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","1d028314cced5858683a1bb4741423d0f934257b"],"0ed6b1234af94a2693d3e6550e7b3ee41fd3f51c":["f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc"],"1d028314cced5858683a1bb4741423d0f934257b":["3b0e8c39ca08b5a02de6edcd33d6f3b90b865173"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["37a0f60745e53927c4c876cfe5b5a58170f0646c","59a0020b413d44dd79d85d7a66ed5004265fb453"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc":["770342641f7b505eaa8dccdc666158bff2419109"],"59a0020b413d44dd79d85d7a66ed5004265fb453":["37a0f60745e53927c4c876cfe5b5a58170f0646c","ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["0ed6b1234af94a2693d3e6550e7b3ee41fd3f51c"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}