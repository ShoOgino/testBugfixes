{"path":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4d3e8520fd031bab31fd0e4d480e55958bc45efe","date":1340901565,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":["3cc749c053615f5871f3b95715fe292f34e70a53"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"322360ac5185a8446d3e0b530b2068bef67cd3d5","date":1343669494,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069","date":1348430063,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"815287248ca7a77db68038baad5698c5767f36a7","date":1350761762,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream generated from a {@link Terms}. This\n   * can be used to feed the highlighter with a pre-parsed token\n   * stream.  The {@link Terms} must have offsets available.\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   *\n   * @throws IllegalArgumentException if no offsets are available\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":["3cc749c053615f5871f3b95715fe292f34e70a53"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"62e52115b56781006682fd92c6938efaf174304d","date":1351014780,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream generated from a {@link Terms}. This\n   * can be used to feed the highlighter with a pre-parsed token\n   * stream.  The {@link Terms} must have offsets available.\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   *\n   * @throws IllegalArgumentException if no offsets are available\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7530de27b87b961b51f01bd1299b7004d46e8823","date":1355236261,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream generated from a {@link Terms}. This\n   * can be used to feed the highlighter with a pre-parsed token\n   * stream.  The {@link Terms} must have offsets available.\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   *\n   * @throws IllegalArgumentException if no offsets are available\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        @Override\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream generated from a {@link Terms}. This\n   * can be used to feed the highlighter with a pre-parsed token\n   * stream.  The {@link Terms} must have offsets available.\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   *\n   * @throws IllegalArgumentException if no offsets are available\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream generated from a {@link Terms}. This\n   * can be used to feed the highlighter with a pre-parsed token\n   * stream.  The {@link Terms} must have offsets available.\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   *\n   * @throws IllegalArgumentException if no offsets are available\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        @Override\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream generated from a {@link Terms}. This\n   * can be used to feed the highlighter with a pre-parsed token\n   * stream.  The {@link Terms} must have offsets available.\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   *\n   * @throws IllegalArgumentException if no offsets are available\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dcc555744b1a581a4beccd0b75f8d3fe49735a2f","date":1367588265,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream generated from a {@link Terms}. This\n   * can be used to feed the highlighter with a pre-parsed token\n   * stream.  The {@link Terms} must have offsets available.\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   *\n   * @throws IllegalArgumentException if no offsets are available\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.timSort(tokensInOriginalOrder, new Comparator<Token>() {\n        @Override\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream generated from a {@link Terms}. This\n   * can be used to feed the highlighter with a pre-parsed token\n   * stream.  The {@link Terms} must have offsets available.\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   *\n   * @throws IllegalArgumentException if no offsets are available\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        @Override\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"000498895a9d8c442dd10d03121bd753ec00bc0e","date":1389468193,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream generated from a {@link Terms}. This\n   * can be used to feed the highlighter with a pre-parsed token\n   * stream.  The {@link Terms} must have offsets available.\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   *\n   * @throws IllegalArgumentException if no offsets are available\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      PayloadAttribute payloadAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n        payloadAtt = addAttribute(PayloadAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        BytesRef payload = token.getPayload();\n        if (payload != null) {\n          payloadAtt.setPayload(payload);\n        }\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n\n    boolean hasPayloads = tpv.hasPayloads();\n\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (hasPayloads) {\n          // Must make a deep copy of the returned payload,\n          // since D&PEnum API is allowed to re-use on every\n          // call:\n          token.setPayload(BytesRef.deepCopyOf(dpEnum.getPayload()));\n        }\n\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.timSort(tokensInOriginalOrder, new Comparator<Token>() {\n        @Override\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) {\n            return t1.endOffset() - t2.endOffset();\n          } else {\n            return t1.startOffset() - t2.startOffset();\n          }\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream generated from a {@link Terms}. This\n   * can be used to feed the highlighter with a pre-parsed token\n   * stream.  The {@link Terms} must have offsets available.\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   *\n   * @throws IllegalArgumentException if no offsets are available\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.timSort(tokensInOriginalOrder, new Comparator<Token>() {\n        @Override\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":["3cc749c053615f5871f3b95715fe292f34e70a53"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream generated from a {@link Terms}. This\n   * can be used to feed the highlighter with a pre-parsed token\n   * stream.  The {@link Terms} must have offsets available.\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   *\n   * @throws IllegalArgumentException if no offsets are available\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      PayloadAttribute payloadAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n        payloadAtt = addAttribute(PayloadAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        BytesRef payload = token.getPayload();\n        if (payload != null) {\n          payloadAtt.setPayload(payload);\n        }\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n\n    boolean hasPayloads = tpv.hasPayloads();\n\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (hasPayloads) {\n          // Must make a deep copy of the returned payload,\n          // since D&PEnum API is allowed to re-use on every\n          // call:\n          token.setPayload(BytesRef.deepCopyOf(dpEnum.getPayload()));\n        }\n\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.timSort(tokensInOriginalOrder, new Comparator<Token>() {\n        @Override\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) {\n            return t1.endOffset() - t2.endOffset();\n          } else {\n            return t1.startOffset() - t2.startOffset();\n          }\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream generated from a {@link Terms}. This\n   * can be used to feed the highlighter with a pre-parsed token\n   * stream.  The {@link Terms} must have offsets available.\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   *\n   * @throws IllegalArgumentException if no offsets are available\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      PayloadAttribute payloadAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n        payloadAtt = addAttribute(PayloadAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        BytesRef payload = token.getPayload();\n        if (payload != null) {\n          payloadAtt.setPayload(payload);\n        }\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n\n    boolean hasPayloads = tpv.hasPayloads();\n\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (hasPayloads) {\n          // Must make a deep copy of the returned payload,\n          // since D&PEnum API is allowed to re-use on every\n          // call:\n          token.setPayload(BytesRef.deepCopyOf(dpEnum.getPayload()));\n        }\n\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.timSort(tokensInOriginalOrder, new Comparator<Token>() {\n        @Override\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) {\n            return t1.endOffset() - t2.endOffset();\n          } else {\n            return t1.startOffset() - t2.startOffset();\n          }\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae230518a1a68acc124bef8df61ef94bd7c1295e","date":1417181719,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /** Simply calls {@link #getTokenStream(org.apache.lucene.index.Terms)} now. */\n  @Deprecated\n  public static TokenStream getTokenStream(Terms vector,\n                                           boolean tokenPositionsGuaranteedContiguous) throws IOException {\n    return getTokenStream(vector);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream generated from a {@link Terms}. This\n   * can be used to feed the highlighter with a pre-parsed token\n   * stream.  The {@link Terms} must have offsets available.\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   *\n   * @throws IllegalArgumentException if no offsets are available\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n\n    if (!tpv.hasOffsets()) {\n      throw new IllegalArgumentException(\"Cannot create TokenStream from Terms without offsets\");\n    }\n\n    if (!tokenPositionsGuaranteedContiguous && tpv.hasPositions()) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      PayloadAttribute payloadAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n        payloadAtt = addAttribute(PayloadAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        BytesRef payload = token.getPayload();\n        if (payload != null) {\n          payloadAtt.setPayload(payload);\n        }\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n\n    boolean hasPayloads = tpv.hasPayloads();\n\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n      final String term = text.utf8ToString();\n\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        if (dpEnum.startOffset() < 0) {\n          throw new IllegalArgumentException(\n              \"Required TermVector Offset information was not found\");\n        }\n        final Token token = new Token(term,\n                                      dpEnum.startOffset(),\n                                      dpEnum.endOffset());\n        if (hasPayloads) {\n          // Must make a deep copy of the returned payload,\n          // since D&PEnum API is allowed to re-use on every\n          // call:\n          token.setPayload(BytesRef.deepCopyOf(dpEnum.getPayload()));\n        }\n\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.timSort(tokensInOriginalOrder, new Comparator<Token>() {\n        @Override\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) {\n            return t1.endOffset() - t2.endOffset();\n          } else {\n            return t1.startOffset() - t2.startOffset();\n          }\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d62e4938659e263e96ae8188e11aea8a940aea5","date":1430230314,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","sourceNew":"  /** Simply calls {@link #getTokenStream(org.apache.lucene.index.Terms)} now. */\n  @Deprecated // maintenance reasons LUCENE-6445\n  public static TokenStream getTokenStream(Terms vector,\n                                           boolean tokenPositionsGuaranteedContiguous) throws IOException {\n    return getTokenStream(vector);\n  }\n\n","sourceOld":"  /** Simply calls {@link #getTokenStream(org.apache.lucene.index.Terms)} now. */\n  @Deprecated\n  public static TokenStream getTokenStream(Terms vector,\n                                           boolean tokenPositionsGuaranteedContiguous) throws IOException {\n    return getTokenStream(vector);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069":["322360ac5185a8446d3e0b530b2068bef67cd3d5"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["815287248ca7a77db68038baad5698c5767f36a7","7530de27b87b961b51f01bd1299b7004d46e8823"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["000498895a9d8c442dd10d03121bd753ec00bc0e"],"5d62e4938659e263e96ae8188e11aea8a940aea5":["ae230518a1a68acc124bef8df61ef94bd7c1295e"],"000498895a9d8c442dd10d03121bd753ec00bc0e":["dcc555744b1a581a4beccd0b75f8d3fe49735a2f"],"815287248ca7a77db68038baad5698c5767f36a7":["c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ae230518a1a68acc124bef8df61ef94bd7c1295e":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"62e52115b56781006682fd92c6938efaf174304d":["c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069","815287248ca7a77db68038baad5698c5767f36a7"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["b89678825b68eccaf09e6ab71675fc0b0af1e099","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["fe33227f6805edab2036cbb80645cc4e2d1fa424","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"dcc555744b1a581a4beccd0b75f8d3fe49735a2f":["7530de27b87b961b51f01bd1299b7004d46e8823"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["4d3e8520fd031bab31fd0e4d480e55958bc45efe","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"7530de27b87b961b51f01bd1299b7004d46e8823":["815287248ca7a77db68038baad5698c5767f36a7"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5d62e4938659e263e96ae8188e11aea8a940aea5"]},"commit2Childs":{"c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069":["815287248ca7a77db68038baad5698c5767f36a7","62e52115b56781006682fd92c6938efaf174304d"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae230518a1a68acc124bef8df61ef94bd7c1295e"],"5d62e4938659e263e96ae8188e11aea8a940aea5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"000498895a9d8c442dd10d03121bd753ec00bc0e":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"815287248ca7a77db68038baad5698c5767f36a7":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","62e52115b56781006682fd92c6938efaf174304d","7530de27b87b961b51f01bd1299b7004d46e8823"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["fe33227f6805edab2036cbb80645cc4e2d1fa424","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"ae230518a1a68acc124bef8df61ef94bd7c1295e":["5d62e4938659e263e96ae8188e11aea8a940aea5"],"62e52115b56781006682fd92c6938efaf174304d":[],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":[],"dcc555744b1a581a4beccd0b75f8d3fe49735a2f":["000498895a9d8c442dd10d03121bd753ec00bc0e"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["fe33227f6805edab2036cbb80645cc4e2d1fa424","d6f074e73200c07d54f242d3880a8da5a35ff97b","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"7530de27b87b961b51f01bd1299b7004d46e8823":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","dcc555744b1a581a4beccd0b75f8d3fe49735a2f"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["c7bd1fdddb8e84c1857d1a55c32ced51f0ed2069","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","62e52115b56781006682fd92c6938efaf174304d","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}