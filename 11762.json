{"path":"contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#NGramTokenizer(Reader,int,int).mjava","commits":[{"id":"962b2790491d84618a387a768d018c54e9809f91","date":1166830997,"type":0,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#NGramTokenizer(Reader,int,int).mjava","pathOld":"/dev/null","sourceNew":"    /**\n     * Creates NGramTokenizer with given min and max n-grams.\n     * @param input Reader holding the input to be tokenized\n     * @param minGram the smallest n-gram to generate\n     * @param maxGram the largest n-gram to generate\n     */\n    public NGramTokenizer(Reader input, int minGram, int maxGram) {\n        super(input);\n        if (minGram < 1) {\n            throw new IllegalArgumentException(\"minGram must be greater than zero\");\n        }\n        if (minGram > maxGram) {\n            throw new IllegalArgumentException(\"minGram must not be greater than maxGram\");\n        }\n        this.minGram = minGram;\n        this.maxGram = maxGram;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2dc0aae18c8a2691ff696acb78f04d0b3587e575","date":1172858067,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#NGramTokenizer(Reader,int,int).mjava","pathOld":"contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#NGramTokenizer(Reader,int,int).mjava","sourceNew":"  /**\n   * Creates NGramTokenizer with given min and max n-grams.\n   * @param input Reader holding the input to be tokenized\n   * @param minGram the smallest n-gram to generate\n   * @param maxGram the largest n-gram to generate\n   */\n  public NGramTokenizer(Reader input, int minGram, int maxGram) {\n    super(input);\n    if (minGram < 1) {\n      throw new IllegalArgumentException(\"minGram must be greater than zero\");\n    }\n    if (minGram > maxGram) {\n      throw new IllegalArgumentException(\"minGram must not be greater than maxGram\");\n    }\n    this.minGram = minGram;\n    this.maxGram = maxGram;\n  }\n\n","sourceOld":"    /**\n     * Creates NGramTokenizer with given min and max n-grams.\n     * @param input Reader holding the input to be tokenized\n     * @param minGram the smallest n-gram to generate\n     * @param maxGram the largest n-gram to generate\n     */\n    public NGramTokenizer(Reader input, int minGram, int maxGram) {\n        super(input);\n        if (minGram < 1) {\n            throw new IllegalArgumentException(\"minGram must be greater than zero\");\n        }\n        if (minGram > maxGram) {\n            throw new IllegalArgumentException(\"minGram must not be greater than maxGram\");\n        }\n        this.minGram = minGram;\n        this.maxGram = maxGram;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dd745d580729e528151b58aeda87ef82f1b95c9b","date":1248369082,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#NGramTokenizer(Reader,int,int).mjava","pathOld":"contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer#NGramTokenizer(Reader,int,int).mjava","sourceNew":"  /**\n   * Creates NGramTokenizer with given min and max n-grams.\n   * @param input Reader holding the input to be tokenized\n   * @param minGram the smallest n-gram to generate\n   * @param maxGram the largest n-gram to generate\n   */\n  public NGramTokenizer(Reader input, int minGram, int maxGram) {\n    super(input);\n    if (minGram < 1) {\n      throw new IllegalArgumentException(\"minGram must be greater than zero\");\n    }\n    if (minGram > maxGram) {\n      throw new IllegalArgumentException(\"minGram must not be greater than maxGram\");\n    }\n    this.minGram = minGram;\n    this.maxGram = maxGram;\n  }\n\n","sourceOld":"  /**\n   * Creates NGramTokenizer with given min and max n-grams.\n   * @param input Reader holding the input to be tokenized\n   * @param minGram the smallest n-gram to generate\n   * @param maxGram the largest n-gram to generate\n   */\n  public NGramTokenizer(Reader input, int minGram, int maxGram) {\n    super(input);\n    if (minGram < 1) {\n      throw new IllegalArgumentException(\"minGram must be greater than zero\");\n    }\n    if (minGram > maxGram) {\n      throw new IllegalArgumentException(\"minGram must not be greater than maxGram\");\n    }\n    this.minGram = minGram;\n    this.maxGram = maxGram;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"dd745d580729e528151b58aeda87ef82f1b95c9b":["2dc0aae18c8a2691ff696acb78f04d0b3587e575"],"2dc0aae18c8a2691ff696acb78f04d0b3587e575":["962b2790491d84618a387a768d018c54e9809f91"],"962b2790491d84618a387a768d018c54e9809f91":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["dd745d580729e528151b58aeda87ef82f1b95c9b"]},"commit2Childs":{"dd745d580729e528151b58aeda87ef82f1b95c9b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2dc0aae18c8a2691ff696acb78f04d0b3587e575":["dd745d580729e528151b58aeda87ef82f1b95c9b"],"962b2790491d84618a387a768d018c54e9809f91":["2dc0aae18c8a2691ff696acb78f04d0b3587e575"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["962b2790491d84618a387a768d018c54e9809f91"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}