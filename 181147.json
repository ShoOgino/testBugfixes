{"path":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","commits":[{"id":"3e8715d826e588419327562287d5d6a8040d63d6","date":1427987148,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>());\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>());\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2638f781be724518ff6c2263d14a48cf6e68017","date":1427989059,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#buildIndex(Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>());\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  private FieldsProducer buildIndex(Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    Codec codec = getCodec();\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random(), 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>());\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca792c26af46bd6c4a08d81117c60440cf6a7e3d","date":1445938295,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>());\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ceaef6cfc68c8ab22a684192e469a8280f9e6e70","date":1462354657,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31741cf1390044e38a2ec3127cf302ba841bfd75","date":1491292636,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92212fd254551a0b1156aafc3a1a6ed1a43932ad","date":1491296431,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"622a708571e534680618b3c5e0c28ac539a47776","date":1517406892,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eeba0a4d0845889a402dd225793d62f009d029c9","date":1527938093,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab548c8f96022b4780f7500a30b19b4f4a5feeb6","date":1527940044,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7e4ca6dc9612ff741d8713743e2bccfae5eadac","date":1528093718,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6652c943595e92c187ee904c382863013eae28f","date":1539042663,"type":3,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790693f23f4e88a59fbb25e47cc25f6d493b03cb","date":1553077690,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, false, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"763da4a9605e47013078edc323b9d4b608f0f9e0","date":1555353576,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, false, IOContext.READ, Collections.emptyMap());\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, false, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a4e83191a3e02851a0b67e5335e6922f3e9ea86d","date":1583489709,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, false, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, false, IOContext.READ, Collections.emptyMap());\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bec68e7c41fed133827595747d853cad504e481e","date":1583501052,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/RandomPostingsTester#buildIndex(Codec,Directory,IndexOptions,boolean,boolean).mjava","sourceNew":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","sourceOld":"  // maxAllowed = the \"highest\" we can index, but we will still\n  // randomly index at lower IndexOption\n  public FieldsProducer buildIndex(Codec codec, Directory dir, IndexOptions maxAllowed, boolean allowPayloads, boolean alwaysTestMax) throws IOException {\n    SegmentInfo segmentInfo = new SegmentInfo(dir, Version.LATEST, Version.LATEST, \"_0\", maxDoc, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    int maxIndexOption = Arrays.asList(IndexOptions.values()).indexOf(maxAllowed);\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"\\nTEST: now build index\");\n    }\n\n    // TODO use allowPayloads\n\n    FieldInfo[] newFieldInfoArray = new FieldInfo[fields.size()];\n    for(int fieldUpto=0;fieldUpto<fields.size();fieldUpto++) {\n      FieldInfo oldFieldInfo = fieldInfos.fieldInfo(fieldUpto);\n    \n      // Randomly picked the IndexOptions to index this\n      // field with:\n      IndexOptions indexOptions = IndexOptions.values()[alwaysTestMax ? maxIndexOption : TestUtil.nextInt(random, 1, maxIndexOption)];\n      boolean doPayloads = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 && allowPayloads;\n\n      newFieldInfoArray[fieldUpto] = new FieldInfo(oldFieldInfo.name,\n                                                   fieldUpto,\n                                                   false,\n                                                   false,\n                                                   doPayloads,\n                                                   indexOptions,\n                                                   DocValuesType.NONE,\n                                                   -1,\n                                                   new HashMap<>(),\n                                                   0, 0, 0, false);\n    }\n\n    FieldInfos newFieldInfos = new FieldInfos(newFieldInfoArray);\n\n    // Estimate that flushed segment size will be 25% of\n    // what we use in RAM:\n    long bytes =  totalPostings * 8 + totalPayloadBytes;\n\n    SegmentWriteState writeState = new SegmentWriteState(null, dir,\n                                                         segmentInfo, newFieldInfos,\n                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));\n\n    Fields seedFields = new SeedFields(fields, newFieldInfos, maxAllowed, allowPayloads);\n\n    NormsProducer fakeNorms = new NormsProducer() {\n\n      @Override\n      public void close() throws IOException {}\n\n      @Override\n      public long ramBytesUsed() {\n        return 0;\n      }\n\n      @Override\n      public NumericDocValues getNorms(FieldInfo field) throws IOException {\n        if (newFieldInfos.fieldInfo(field.number).hasNorms()) {\n          return new NumericDocValues() {\n            \n            int doc = -1;\n            \n            @Override\n            public int nextDoc() throws IOException {\n              if (++doc == segmentInfo.maxDoc()) {\n                return doc = NO_MORE_DOCS;\n              }\n              return doc;\n            }\n            \n            @Override\n            public int docID() {\n              return doc;\n            }\n            \n            @Override\n            public long cost() {\n              return segmentInfo.maxDoc();\n            }\n            \n            @Override\n            public int advance(int target) throws IOException {\n              return doc = target >= segmentInfo.maxDoc() ? DocIdSetIterator.NO_MORE_DOCS : target;\n            }\n            \n            @Override\n            public boolean advanceExact(int target) throws IOException {\n              doc = target;\n              return true;\n            }\n            \n            @Override\n            public long longValue() throws IOException {\n              return DOC_TO_NORM.applyAsLong(doc);\n            }\n          };\n        } else {\n          return null;\n        }\n      }\n\n      @Override\n      public void checkIntegrity() throws IOException {}\n      \n    };\n    FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(writeState);\n    boolean success = false;\n    try {\n      consumer.write(seedFields, fakeNorms);\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(consumer);\n      } else {\n        IOUtils.closeWhileHandlingException(consumer);\n      }\n    }\n\n    if (LuceneTestCase.VERBOSE) {\n      System.out.println(\"TEST: after indexing: files=\");\n      for(String file : dir.listAll()) {\n        System.out.println(\"  \" + file + \": \" + dir.fileLength(file) + \" bytes\");\n      }\n    }\n\n    currentFieldInfos = newFieldInfos;\n\n    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, false, IOContext.READ);\n\n    return codec.postingsFormat().fieldsProducer(readState);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["f6652c943595e92c187ee904c382863013eae28f"],"f6652c943595e92c187ee904c382863013eae28f":["b7e4ca6dc9612ff741d8713743e2bccfae5eadac"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["790693f23f4e88a59fbb25e47cc25f6d493b03cb"],"bec68e7c41fed133827595747d853cad504e481e":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"0ad30c6a479e764150a3316e57263319775f1df2":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d","3d33e731a93d4b57e662ff094f64f94a745422d4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d","d470c8182e92b264680e34081b75e70a9f2b3c89"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d","0ad30c6a479e764150a3316e57263319775f1df2"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d"],"ab548c8f96022b4780f7500a30b19b4f4a5feeb6":["eeba0a4d0845889a402dd225793d62f009d029c9"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["3e8715d826e588419327562287d5d6a8040d63d6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70042a8a492f7054d480ccdd2be9796510d4327":["622a708571e534680618b3c5e0c28ac539a47776","b7e4ca6dc9612ff741d8713743e2bccfae5eadac"],"d2638f781be724518ff6c2263d14a48cf6e68017":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3e8715d826e588419327562287d5d6a8040d63d6"],"b7e4ca6dc9612ff741d8713743e2bccfae5eadac":["ab548c8f96022b4780f7500a30b19b4f4a5feeb6"],"eeba0a4d0845889a402dd225793d62f009d029c9":["622a708571e534680618b3c5e0c28ac539a47776"],"622a708571e534680618b3c5e0c28ac539a47776":["31741cf1390044e38a2ec3127cf302ba841bfd75"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"3e8715d826e588419327562287d5d6a8040d63d6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d","ceaef6cfc68c8ab22a684192e469a8280f9e6e70"],"f592209545c71895260367152601e9200399776d":["ab548c8f96022b4780f7500a30b19b4f4a5feeb6","b7e4ca6dc9612ff741d8713743e2bccfae5eadac"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["bec68e7c41fed133827595747d853cad504e481e"]},"commit2Childs":{"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"f6652c943595e92c187ee904c382863013eae28f":["790693f23f4e88a59fbb25e47cc25f6d493b03cb"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"bec68e7c41fed133827595747d853cad504e481e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["bec68e7c41fed133827595747d853cad504e481e"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","31741cf1390044e38a2ec3127cf302ba841bfd75","92212fd254551a0b1156aafc3a1a6ed1a43932ad"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["622a708571e534680618b3c5e0c28ac539a47776"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["0ad30c6a479e764150a3316e57263319775f1df2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","ceaef6cfc68c8ab22a684192e469a8280f9e6e70","3d33e731a93d4b57e662ff094f64f94a745422d4"],"ab548c8f96022b4780f7500a30b19b4f4a5feeb6":["b7e4ca6dc9612ff741d8713743e2bccfae5eadac","f592209545c71895260367152601e9200399776d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d2638f781be724518ff6c2263d14a48cf6e68017","3e8715d826e588419327562287d5d6a8040d63d6"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"d2638f781be724518ff6c2263d14a48cf6e68017":[],"b7e4ca6dc9612ff741d8713743e2bccfae5eadac":["f6652c943595e92c187ee904c382863013eae28f","b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d"],"eeba0a4d0845889a402dd225793d62f009d029c9":["ab548c8f96022b4780f7500a30b19b4f4a5feeb6"],"622a708571e534680618b3c5e0c28ac539a47776":["b70042a8a492f7054d480ccdd2be9796510d4327","eeba0a4d0845889a402dd225793d62f009d029c9"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":[],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"3e8715d826e588419327562287d5d6a8040d63d6":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d","d2638f781be724518ff6c2263d14a48cf6e68017"],"f592209545c71895260367152601e9200399776d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","b70042a8a492f7054d480ccdd2be9796510d4327","d2638f781be724518ff6c2263d14a48cf6e68017","92212fd254551a0b1156aafc3a1a6ed1a43932ad","f592209545c71895260367152601e9200399776d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}