{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","commits":[{"id":"8895dc5c560508f33baa8753218d4745200a0b85","date":1395207031,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","pathOld":"/dev/null","sourceNew":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(TEST_VERSION_CURRENT, stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff4227bb146f97aabae888091c19e48c88dbb0db","date":1406758576,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","sourceNew":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(TEST_VERSION_CURRENT, stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n  }\n\n","sourceOld":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(TEST_VERSION_CURRENT, stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5cdab62f058ea765dd33deb05b4f19b7d626c801","date":1406803479,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","sourceNew":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(TEST_VERSION_CURRENT, stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n  }\n\n","sourceOld":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(TEST_VERSION_CURRENT, stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"379db3ad24c4f0214f30a122265a6d6be003a99d","date":1407537768,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","sourceNew":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n  }\n\n","sourceOld":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(TEST_VERSION_CURRENT, stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a56958d7f71a28824f20031ffbb2e13502a0274e","date":1425573902,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","sourceNew":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n    a.close();\n  }\n\n","sourceOld":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","sourceNew":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n    a.close();\n  }\n\n","sourceOld":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9","date":1574619880,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","sourceNew":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @SuppressWarnings(\"deprecation\")\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n    a.close();\n  }\n\n","sourceOld":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n    a.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb9c3baacabd473e8ecd6c4948aabacead49b88e","date":1574700980,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","sourceNew":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n    a.close();\n  }\n\n","sourceOld":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @SuppressWarnings(\"deprecation\")\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n    a.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f95c0e33e58652b2a4d8560c8297dbe86ff5b1f2","date":1591961131,"type":3,"author":"Michael Sokolov","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething#testCuriousWikipediaString().mjava","sourceNew":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    @SuppressWarnings(\"deprecation\")\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n    a.close();\n  }\n\n","sourceOld":"  public void testCuriousWikipediaString() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(\n        Arrays.asList(\"rrdpafa\", \"pupmmlu\", \"xlq\", \"dyy\", \"zqrxrrck\", \"o\", \"hsrlfvcha\")), false);\n    final byte table[] = new byte[] { \n        -57, 26, 1, 48, 63, -23, 55, -84, 18, 120, -97, 103, 58, 13, 84, 89, 57, -13, -63, \n        5, 28, 97, -54, -94, 102, -108, -5, 5, 46, 40, 43, 78, 43, -72, 36, 29, 124, -106, \n        -22, -51, 65, 5, 31, -42, 6, -99, 97, 14, 81, -128, 74, 100, 54, -55, -25, 53, -71, \n        -98, 44, 33, 86, 106, -42, 47, 115, -89, -18, -26, 22, -95, -43, 83, -125, 105, -104,\n        -24, 106, -16, 126, 115, -105, 97, 65, -33, 57, 44, -1, 123, -68, 100, 13, -41, -64, \n        -119, 0, 92, 94, -36, 53, -9, -102, -18, 90, 94, -26, 31, 71, -20\n    };\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new WikipediaTokenizer();\n        TokenStream stream = new SopTokenFilter(tokenizer);\n        stream = new WordDelimiterFilter(stream, table, -50, protWords);\n        stream = new SopTokenFilter(stream);\n        return new TokenStreamComponents(tokenizer, stream);\n      }  \n    };\n    checkAnalysisConsistency(random(), a, false, \"B\\u28c3\\ue0f8[ \\ud800\\udfc2 </p> jb\");\n    a.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ff4227bb146f97aabae888091c19e48c88dbb0db":["8895dc5c560508f33baa8753218d4745200a0b85"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["379db3ad24c4f0214f30a122265a6d6be003a99d","a56958d7f71a28824f20031ffbb2e13502a0274e"],"a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9":["a56958d7f71a28824f20031ffbb2e13502a0274e"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"bb9c3baacabd473e8ecd6c4948aabacead49b88e":["a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9"],"8895dc5c560508f33baa8753218d4745200a0b85":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f95c0e33e58652b2a4d8560c8297dbe86ff5b1f2":["bb9c3baacabd473e8ecd6c4948aabacead49b88e"],"a56958d7f71a28824f20031ffbb2e13502a0274e":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["ff4227bb146f97aabae888091c19e48c88dbb0db"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f95c0e33e58652b2a4d8560c8297dbe86ff5b1f2"]},"commit2Childs":{"ff4227bb146f97aabae888091c19e48c88dbb0db":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9":["bb9c3baacabd473e8ecd6c4948aabacead49b88e"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","a56958d7f71a28824f20031ffbb2e13502a0274e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8895dc5c560508f33baa8753218d4745200a0b85"],"8895dc5c560508f33baa8753218d4745200a0b85":["ff4227bb146f97aabae888091c19e48c88dbb0db"],"bb9c3baacabd473e8ecd6c4948aabacead49b88e":["f95c0e33e58652b2a4d8560c8297dbe86ff5b1f2"],"f95c0e33e58652b2a4d8560c8297dbe86ff5b1f2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a56958d7f71a28824f20031ffbb2e13502a0274e":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9"],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}