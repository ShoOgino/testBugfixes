{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenOffsetFilter#test().mjava","commits":[{"id":"64cf07d2eb6bab312c43411d6a3e0373ed15a245","date":1429731531,"type":0,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenOffsetFilter#test().mjava","pathOld":"/dev/null","sourceNew":"  public void test() throws Exception {\n    for (final boolean consumeAll : new boolean[]{true, false}) {\n      MockTokenizer tokenizer = whitespaceMockTokenizer(\"A1 B2 C3 D4 E5 F6\");\n      tokenizer.setEnableChecks(consumeAll);\n      //note with '3', this test would fail if erroneously the filter used endOffset instead\n      TokenStream stream = new LimitTokenOffsetFilter(tokenizer, 3, consumeAll);\n      assertTokenStreamContents(stream, new String[]{\"A1\", \"B2\"});\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"64cf07d2eb6bab312c43411d6a3e0373ed15a245":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["64cf07d2eb6bab312c43411d6a3e0373ed15a245"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["64cf07d2eb6bab312c43411d6a3e0373ed15a245"],"64cf07d2eb6bab312c43411d6a3e0373ed15a245":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}