{"path":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","commits":[{"id":"ceaef6cfc68c8ab22a684192e469a8280f9e6e70","date":1462354657,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":1,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":1,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader],List[Integer]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers, List<Integer> docIDBases) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    //System.out.println(\"BKDW.merge segs=\" + readers.size());\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap, docIDBases.get(i));\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docIDBase + reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6","date":1468339076,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        final IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n          final BytesRef scratch = new BytesRef();\n\n          {\n            scratch.length = packedBytesLength;\n            scratch.offset = 0;\n          }\n\n          @Override\n          public BytesRef apply(int i) {\n            scratch.bytes = leafBlockPackedValues[i];\n            return scratch;\n          }\n        };\n        writeLeafBlockPackedValues(out, commonPrefixLengths, leafCount, 0, packedValues);\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        // Write the full values:\n        for (int i=0;i<leafCount;i++) {\n          writeLeafBlockPackedValue(out, commonPrefixLengths, leafBlockPackedValues[i], 0);\n        }\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d561885e9bb6238af1ff8afe8630dcfe49b66ac7","date":1469780634,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(out);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      oneDimWriter.add(reader.state.scratchPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        final IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n          final BytesRef scratch = new BytesRef();\n\n          {\n            scratch.length = packedBytesLength;\n            scratch.offset = 0;\n          }\n\n          @Override\n          public BytesRef apply(int i) {\n            scratch.bytes = leafBlockPackedValues[i];\n            return scratch;\n          }\n        };\n        writeLeafBlockPackedValues(out, commonPrefixLengths, leafCount, 0, packedValues);\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b013574eedcdbac35dc7e35b0ee616ffc38895d","date":1470897818,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(out);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      oneDimWriter.add(reader.state.scratchPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    if (numDims != 1) {\n      throw new UnsupportedOperationException(\"numDims must be 1 but got \" + numDims);\n    }\n    if (pointCount != 0) {\n      throw new IllegalStateException(\"cannot mix add and merge\");\n    }\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    // Mark that we already finished:\n    heapPointWriter = null;\n\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    if (queue.size() == 0) {\n      return -1;\n    }\n\n    int leafCount = 0;\n    List<Long> leafBlockFPs = new ArrayList<>();\n    List<byte[]> leafBlockStartValues = new ArrayList<>();\n\n    // Target halfway between min and max allowed for the leaf:\n    int pointsPerLeafBlock = (int) (0.75 * maxPointsInLeafNode);\n    //System.out.println(\"POINTS PER: \" + pointsPerLeafBlock);\n\n    byte[] lastPackedValue = new byte[bytesPerDim];\n    byte[] firstPackedValue = new byte[bytesPerDim];\n    long valueCount = 0;\n\n    // Buffer up each leaf block's docs and values\n    int[] leafBlockDocIDs = new int[maxPointsInLeafNode];\n    byte[][] leafBlockPackedValues = new byte[maxPointsInLeafNode][];\n    for(int i=0;i<maxPointsInLeafNode;i++) {\n      leafBlockPackedValues[i] = new byte[packedBytesLength];\n    }\n    Arrays.fill(commonPrefixLengths, bytesPerDim);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      int docID = reader.docID;\n      leafBlockDocIDs[leafCount] = docID;\n      System.arraycopy(reader.state.scratchPackedValue, 0, leafBlockPackedValues[leafCount], 0, packedBytesLength);\n      docsSeen.set(docID);\n\n      if (valueCount == 0) {\n        System.arraycopy(reader.state.scratchPackedValue, 0, minPackedValue, 0, packedBytesLength);\n      }\n      System.arraycopy(reader.state.scratchPackedValue, 0, maxPackedValue, 0, packedBytesLength);\n\n      assert numDims > 1 || valueInOrder(valueCount, lastPackedValue, reader.state.scratchPackedValue, 0);\n      valueCount++;\n      if (pointCount > totalPointCount) {\n        throw new IllegalStateException(\"totalPointCount=\" + totalPointCount + \" was passed when we were created, but we just hit \" + pointCount + \" values\");\n      }\n\n      if (leafCount == 0) {\n        if (leafBlockFPs.size() > 0) {\n          // Save the first (minimum) value in each leaf block except the first, to build the split value index in the end:\n          leafBlockStartValues.add(Arrays.copyOf(reader.state.scratchPackedValue, bytesPerDim));\n        }\n        Arrays.fill(commonPrefixLengths, bytesPerDim);\n        System.arraycopy(reader.state.scratchPackedValue, 0, firstPackedValue, 0, bytesPerDim);\n      } else {\n        // Find per-dim common prefix:\n        for(int dim=0;dim<numDims;dim++) {\n          int offset = dim * bytesPerDim;\n          for(int j=0;j<commonPrefixLengths[dim];j++) {\n            if (firstPackedValue[offset+j] != reader.state.scratchPackedValue[offset+j]) {\n              commonPrefixLengths[dim] = j;\n              break;\n            }\n          }\n        }\n      }\n\n      leafCount++;\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n\n      // We write a block once we hit exactly the max count ... this is different from\n      // when we flush a new segment, where we write between max/2 and max per leaf block,\n      // so merged segments will behave differently from newly flushed segments:\n      if (leafCount == pointsPerLeafBlock || queue.size() == 0) {\n        leafBlockFPs.add(out.getFilePointer());\n        checkMaxLeafNodeCount(leafBlockFPs.size());\n\n        writeLeafBlockDocs(out, leafBlockDocIDs, 0, leafCount);\n        writeCommonPrefixes(out, commonPrefixLengths, firstPackedValue);\n\n        final IntFunction<BytesRef> packedValues = new IntFunction<BytesRef>() {\n          final BytesRef scratch = new BytesRef();\n\n          {\n            scratch.length = packedBytesLength;\n            scratch.offset = 0;\n          }\n\n          @Override\n          public BytesRef apply(int i) {\n            scratch.bytes = leafBlockPackedValues[i];\n            return scratch;\n          }\n        };\n        writeLeafBlockPackedValues(out, commonPrefixLengths, leafCount, 0, packedValues);\n\n        leafCount = 0;\n      }\n    }\n\n    pointCount = valueCount;\n\n    long indexFP = out.getFilePointer();\n\n    int numInnerNodes = leafBlockStartValues.size();\n\n    //System.out.println(\"BKDW: now rotate numInnerNodes=\" + numInnerNodes + \" leafBlockStarts=\" + leafBlockStartValues.size());\n\n    byte[] index = new byte[(1+numInnerNodes) * (1+bytesPerDim)];\n    rotateToTree(1, 0, numInnerNodes, index, leafBlockStartValues);\n    long[] arr = new long[leafBlockFPs.size()];\n    for(int i=0;i<leafBlockFPs.size();i++) {\n      arr[i] = leafBlockFPs.get(i);\n    }\n    writeIndex(out, arr, index);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"/dev/null","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(out);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      oneDimWriter.add(reader.state.scratchPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a7a864301b26df0e75ce5a305cc1f6de6023638c","date":1482316486,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(out);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      oneDimWriter.add(reader.state.scratchPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(out);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      oneDimWriter.add(reader.state.scratchPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(out);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      oneDimWriter.add(reader.state.scratchPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(out);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      // NOTE: doesn't work with subclasses (e.g. SimpleText!)\n      oneDimWriter.add(reader.state.scratchPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d774afea15a18cc375fae2ffaa8319935d0a7b2a","date":1536414150,"type":3,"author":"iverase","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(out);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      oneDimWriter.add(reader.state.scratchPackedValue1, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(out);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      oneDimWriter.add(reader.state.scratchPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6652c943595e92c187ee904c382863013eae28f","date":1539042663,"type":3,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(out);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      oneDimWriter.add(reader.state.scratchDataPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(out);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      oneDimWriter.add(reader.state.scratchPackedValue1, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"78e689a3b60e84c75dc6dd7b181a71fc19ef8482","date":1591689554,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,IndexOutput,IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#merge(IndexOutput,List[MergeState.DocMap],List[BKDReader]).mjava","sourceNew":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public Runnable merge(IndexOutput metaOut, IndexOutput indexOut, IndexOutput dataOut, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(metaOut, indexOut, dataOut);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      oneDimWriter.add(reader.state.scratchDataPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","sourceOld":"  /** More efficient bulk-add for incoming {@link BKDReader}s.  This does a merge sort of the already\n   *  sorted values and currently only works when numDims==1.  This returns -1 if all documents containing\n   *  dimensional values were deleted. */\n  public long merge(IndexOutput out, List<MergeState.DocMap> docMaps, List<BKDReader> readers) throws IOException {\n    assert docMaps == null || readers.size() == docMaps.size();\n\n    BKDMergeQueue queue = new BKDMergeQueue(bytesPerDim, readers.size());\n\n    for(int i=0;i<readers.size();i++) {\n      BKDReader bkd = readers.get(i);\n      MergeState.DocMap docMap;\n      if (docMaps == null) {\n        docMap = null;\n      } else {\n        docMap = docMaps.get(i);\n      }\n      MergeReader reader = new MergeReader(bkd, docMap);\n      if (reader.next()) {\n        queue.add(reader);\n      }\n    }\n\n    OneDimensionBKDWriter oneDimWriter = new OneDimensionBKDWriter(out);\n\n    while (queue.size() != 0) {\n      MergeReader reader = queue.top();\n      // System.out.println(\"iter reader=\" + reader);\n\n      oneDimWriter.add(reader.state.scratchDataPackedValue, reader.docID);\n\n      if (reader.next()) {\n        queue.updateTop();\n      } else {\n        // This segment was exhausted\n        queue.pop();\n      }\n    }\n\n    return oneDimWriter.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f6652c943595e92c187ee904c382863013eae28f":["d774afea15a18cc375fae2ffaa8319935d0a7b2a"],"a7a864301b26df0e75ce5a305cc1f6de6023638c":["d561885e9bb6238af1ff8afe8630dcfe49b66ac7"],"0ad30c6a479e764150a3316e57263319775f1df2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3d33e731a93d4b57e662ff094f64f94a745422d4"],"d774afea15a18cc375fae2ffaa8319935d0a7b2a":["a7a864301b26df0e75ce5a305cc1f6de6023638c"],"d561885e9bb6238af1ff8afe8630dcfe49b66ac7":["3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d561885e9bb6238af1ff8afe8630dcfe49b66ac7"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0ad30c6a479e764150a3316e57263319775f1df2"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","a7a864301b26df0e75ce5a305cc1f6de6023638c"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"78e689a3b60e84c75dc6dd7b181a71fc19ef8482":["f6652c943595e92c187ee904c382863013eae28f"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ceaef6cfc68c8ab22a684192e469a8280f9e6e70"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":["3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6","d561885e9bb6238af1ff8afe8630dcfe49b66ac7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["78e689a3b60e84c75dc6dd7b181a71fc19ef8482"]},"commit2Childs":{"f6652c943595e92c187ee904c382863013eae28f":["78e689a3b60e84c75dc6dd7b181a71fc19ef8482"],"a7a864301b26df0e75ce5a305cc1f6de6023638c":["d774afea15a18cc375fae2ffaa8319935d0a7b2a","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"d774afea15a18cc375fae2ffaa8319935d0a7b2a":["f6652c943595e92c187ee904c382863013eae28f"],"d561885e9bb6238af1ff8afe8630dcfe49b66ac7":["a7a864301b26df0e75ce5a305cc1f6de6023638c","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["f03e4bed5023ec3ef93a771b8888cae991cf448d"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":[],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0ad30c6a479e764150a3316e57263319775f1df2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","ceaef6cfc68c8ab22a684192e469a8280f9e6e70","3d33e731a93d4b57e662ff094f64f94a745422d4"],"3ca40baa99f9578eb8408ee5b9177f7ffe6f65d6":["d561885e9bb6238af1ff8afe8630dcfe49b66ac7","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"78e689a3b60e84c75dc6dd7b181a71fc19ef8482":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["f03e4bed5023ec3ef93a771b8888cae991cf448d","3b013574eedcdbac35dc7e35b0ee616ffc38895d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}