{"path":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","commits":[{"id":"ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d","date":1426480823,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","pathOld":"/dev/null","sourceNew":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n\n\n    createAccs(-1, 1);\n    prepareForCollection();\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int defaultMinDf = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    int minDfFilterCache = freq.cacheDf == 0 ? defaultMinDf : freq.cacheDf;\n    if (minDfFilterCache == -1) minDfFilterCache = Integer.MAX_VALUE;  // -1 means never cache\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["dd0759e8803a09424422a329163d5900f6b10c42"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":0,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","pathOld":"/dev/null","sourceNew":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n\n\n    createAccs(-1, 1);\n    prepareForCollection();\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int defaultMinDf = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    int minDfFilterCache = freq.cacheDf == 0 ? defaultMinDf : freq.cacheDf;\n    if (minDfFilterCache == -1) minDfFilterCache = Integer.MAX_VALUE;  // -1 means never cache\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","sourceNew":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n\n\n    createAccs(-1, 1);\n    prepareForCollection();\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int defaultMinDf = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    int minDfFilterCache = freq.cacheDf == 0 ? defaultMinDf : freq.cacheDf;\n    if (minDfFilterCache == -1) minDfFilterCache = Integer.MAX_VALUE;  // -1 means never cache\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n\n\n  }\n\n","sourceOld":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n\n\n    createAccs(-1, 1);\n    prepareForCollection();\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int defaultMinDf = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    int minDfFilterCache = freq.cacheDf == 0 ? defaultMinDf : freq.cacheDf;\n    if (minDfFilterCache == -1) minDfFilterCache = Integer.MAX_VALUE;  // -1 means never cache\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator(null);\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dd0759e8803a09424422a329163d5900f6b10c42","date":1431227616,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","sourceNew":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n\n\n    createAccs(-1, 1);\n    prepareForCollection();\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int defaultMinDf = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    int minDfFilterCache = freq.cacheDf == 0 ? defaultMinDf : freq.cacheDf;\n    if (minDfFilterCache == -1) minDfFilterCache = Integer.MAX_VALUE;  // -1 means never cache\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n\n\n    createAccs(-1, 1);\n    prepareForCollection();\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int defaultMinDf = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    int minDfFilterCache = freq.cacheDf == 0 ? defaultMinDf : freq.cacheDf;\n    if (minDfFilterCache == -1) minDfFilterCache = Integer.MAX_VALUE;  // -1 means never cache\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n\n\n  }\n\n","bugFix":["ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9e13d0d4d8b6dc352cb304974502b9a36c153f78","date":1436492687,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","sourceNew":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int defaultMinDf = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    int minDfFilterCache = freq.cacheDf == 0 ? defaultMinDf : freq.cacheDf;\n    if (minDfFilterCache == -1) minDfFilterCache = Integer.MAX_VALUE;  // -1 means never cache\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n\n\n    createAccs(-1, 1);\n    prepareForCollection();\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int defaultMinDf = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    int minDfFilterCache = freq.cacheDf == 0 ? defaultMinDf : freq.cacheDf;\n    if (minDfFilterCache == -1) minDfFilterCache = Integer.MAX_VALUE;  // -1 means never cache\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"48529f421973605be8a95edc29ff0fa4341d228f","date":1470666999,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","sourceNew":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int defaultMinDf = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    int minDfFilterCache = freq.cacheDf == 0 ? defaultMinDf : freq.cacheDf;\n    if (minDfFilterCache == -1) minDfFilterCache = Integer.MAX_VALUE;  // -1 means never cache\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8bca25eefa1f2205e2b0ef713701dc3a0fecd702","date":1470810578,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","sourceNew":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int defaultMinDf = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    int minDfFilterCache = freq.cacheDf == 0 ? defaultMinDf : freq.cacheDf;\n    if (minDfFilterCache == -1) minDfFilterCache = Integer.MAX_VALUE;  // -1 means never cache\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b013574eedcdbac35dc7e35b0ee616ffc38895d","date":1470897818,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","sourceNew":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int defaultMinDf = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    int minDfFilterCache = freq.cacheDf == 0 ? defaultMinDf : freq.cacheDf;\n    if (minDfFilterCache == -1) minDfFilterCache = Integer.MAX_VALUE;  // -1 means never cache\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79759974460bc59933cd169acc94f5c6b16368d5","date":1471318443,"type":5,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","sourceNew":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","sourceNew":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","sourceNew":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","sourceNew":null,"sourceOld":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    int defaultMinDf = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    int minDfFilterCache = freq.cacheDf == 0 ? defaultMinDf : freq.cacheDf;\n    if (minDfFilterCache == -1) minDfFilterCache = Integer.MAX_VALUE;  // -1 means never cache\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["8bca25eefa1f2205e2b0ef713701dc3a0fecd702","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["3b013574eedcdbac35dc7e35b0ee616ffc38895d","79759974460bc59933cd169acc94f5c6b16368d5"],"dd0759e8803a09424422a329163d5900f6b10c42":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"48529f421973605be8a95edc29ff0fa4341d228f":["9e13d0d4d8b6dc352cb304974502b9a36c153f78"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["9e13d0d4d8b6dc352cb304974502b9a36c153f78","403d05f7f8d69b65659157eff1bc1d2717f04c66"],"8bca25eefa1f2205e2b0ef713701dc3a0fecd702":["9e13d0d4d8b6dc352cb304974502b9a36c153f78","48529f421973605be8a95edc29ff0fa4341d228f"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d"],"9e13d0d4d8b6dc352cb304974502b9a36c153f78":["dd0759e8803a09424422a329163d5900f6b10c42"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"79759974460bc59933cd169acc94f5c6b16368d5":["3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":["9e13d0d4d8b6dc352cb304974502b9a36c153f78","8bca25eefa1f2205e2b0ef713701dc3a0fecd702"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["403d05f7f8d69b65659157eff1bc1d2717f04c66"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["dd0759e8803a09424422a329163d5900f6b10c42"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"dd0759e8803a09424422a329163d5900f6b10c42":["9e13d0d4d8b6dc352cb304974502b9a36c153f78"],"48529f421973605be8a95edc29ff0fa4341d228f":["8bca25eefa1f2205e2b0ef713701dc3a0fecd702"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"8bca25eefa1f2205e2b0ef713701dc3a0fecd702":["403d05f7f8d69b65659157eff1bc1d2717f04c66","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"9e13d0d4d8b6dc352cb304974502b9a36c153f78":["48529f421973605be8a95edc29ff0fa4341d228f","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","8bca25eefa1f2205e2b0ef713701dc3a0fecd702","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d"],"ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"79759974460bc59933cd169acc94f5c6b16368d5":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","79759974460bc59933cd169acc94f5c6b16368d5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}