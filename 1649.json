{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random, 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random, 4000, 8000) : _TestUtil.nextInt(random, 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":["302be0cc5e6a28ebcebcac98aa81a92be2e94370"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4356000e349e38c9fb48034695b7c309abd54557","date":1337460341,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE)\n        System.out.println(\"TEST: iter=\" + iter);\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = IndexReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = IndexReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = IndexReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d19974432be9aed28ee7dca73bdf01d139e763a9","date":1342822166,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":["84b590669deb3d3a471cec6cb13b104b2ee94418"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","date":1343059585,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"360305e2c581a6b0d491602f847fd8f76e8718f0","date":1345817101,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"001b25b42373b22a52f399dbf072f1224632e8e6","date":1345889167,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fc4053f65ade14b2ce1979911bc398299cc9c9ef","date":1365632447,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"68a0364646d8ac738da2a85894804beef9d42794","date":1365687804,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          _TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        _TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? _TestUtil.nextInt(random(), 4000, 8000) : _TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.shutdown();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.shutdown();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.shutdown();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.shutdown();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.shutdown();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setOpenMode(OpenMode.APPEND)\n                                         .setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.shutdown();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.shutdown();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.shutdown();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.shutdown();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setOpenMode(OpenMode.APPEND)\n                                         .setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.shutdown();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.shutdown();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setOpenMode(OpenMode.APPEND)\n                                         .setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.shutdown();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa","date":1420599177,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setOpenMode(OpenMode.APPEND)\n                                         .setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              DirectoryReader readers[] = new DirectoryReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                TestUtil.addIndexesSlowly(writer, readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setOpenMode(OpenMode.APPEND)\n                                         .setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              IndexReader readers[] = new IndexReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                writer.addIndexes(readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":["84b590669deb3d3a471cec6cb13b104b2ee94418"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"302be0cc5e6a28ebcebcac98aa81a92be2e94370","date":1423848654,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setOpenMode(OpenMode.APPEND)\n                                         .setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              DirectoryReader readers[] = new DirectoryReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                TestUtil.addIndexesSlowly(writer, readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), new RAMDirectory(startDir, newIOContext(random())));\n        writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setOpenMode(OpenMode.APPEND)\n                                         .setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              DirectoryReader readers[] = new DirectoryReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                TestUtil.addIndexesSlowly(writer, readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc","date":1424799790,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setOpenMode(OpenMode.APPEND)\n                                         .setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              DirectoryReader readers[] = new DirectoryReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                TestUtil.addIndexesSlowly(writer, readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setOpenMode(OpenMode.APPEND)\n                                         .setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              DirectoryReader readers[] = new DirectoryReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                TestUtil.addIndexesSlowly(writer, readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d9e22bdf0692bfa61e342b04a6ac7078670c1e16","date":1436866730,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(startDir));\n        dir.setPreventDoubleWrite(false);\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n          .setOpenMode(OpenMode.APPEND)\n          .setMergePolicy(newLogMergePolicy(false));\n        writer = new IndexWriter(dir, iwc);\n        Exception err = null;\n\n        for(int x=0;x<2;x++) {\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          if (ms instanceof ConcurrentMergeScheduler) {\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x) {\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            } else {\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n            }\n          }\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            }\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + testName);\n          }\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              DirectoryReader readers[] = new DirectoryReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                TestUtil.addIndexesSlowly(writer, readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IllegalStateException | IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit Exception: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          if (x == 1) {\n            // Make sure all threads from ConcurrentMergeScheduler are done\n            TestUtil.syncConcurrentMerges(writer);\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            writer.rollback();\n            writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                     .setOpenMode(OpenMode.APPEND)\n                                     .setMergePolicy(newLogMergePolicy(false)));\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException\n  {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(startDir));\n        writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setOpenMode(OpenMode.APPEND)\n                                         .setMergePolicy(newLogMergePolicy(false)));\n        IOException err = null;\n\n        MergeScheduler ms = writer.getConfig().getMergeScheduler();\n        for(int x=0;x<2;x++) {\n          if (ms instanceof ConcurrentMergeScheduler)\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x)\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            else\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE)\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n          }\n          \n          if (VERBOSE)\n            System.out.println(\"\\ncycle: \" + testName);\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              DirectoryReader readers[] = new DirectoryReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                TestUtil.addIndexesSlowly(writer, readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit IOException: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          // Make sure all threads from\n          // ConcurrentMergeScheduler are done\n          TestUtil.syncConcurrentMerges(writer);\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        // Wait for all BG threads to finish else\n        // dir.close() will throw IOException because\n        // there are still open files\n        TestUtil.syncConcurrentMerges(ms);\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":["11c6df42fb3eba174c3ca0d9a5194eaecd893b77"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"11c6df42fb3eba174c3ca0d9a5194eaecd893b77","date":1465931757,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(startDir));\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n          .setOpenMode(OpenMode.APPEND)\n          .setMergePolicy(newLogMergePolicy(false));\n        writer = new IndexWriter(dir, iwc);\n        Exception err = null;\n\n        for(int x=0;x<2;x++) {\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          if (ms instanceof ConcurrentMergeScheduler) {\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x) {\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            } else {\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n            }\n          }\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            }\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + testName);\n          }\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              DirectoryReader readers[] = new DirectoryReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                TestUtil.addIndexesSlowly(writer, readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IllegalStateException | IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit Exception: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          if (x == 1) {\n            // Make sure all threads from ConcurrentMergeScheduler are done\n            TestUtil.syncConcurrentMerges(writer);\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            writer.rollback();\n            writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                     .setOpenMode(OpenMode.APPEND)\n                                     .setMergePolicy(newLogMergePolicy(false)));\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(startDir));\n        dir.setPreventDoubleWrite(false);\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n          .setOpenMode(OpenMode.APPEND)\n          .setMergePolicy(newLogMergePolicy(false));\n        writer = new IndexWriter(dir, iwc);\n        Exception err = null;\n\n        for(int x=0;x<2;x++) {\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          if (ms instanceof ConcurrentMergeScheduler) {\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x) {\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            } else {\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n            }\n          }\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            }\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + testName);\n          }\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              DirectoryReader readers[] = new DirectoryReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                TestUtil.addIndexesSlowly(writer, readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IllegalStateException | IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit Exception: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          if (x == 1) {\n            // Make sure all threads from ConcurrentMergeScheduler are done\n            TestUtil.syncConcurrentMerges(writer);\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            writer.rollback();\n            writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                     .setOpenMode(OpenMode.APPEND)\n                                     .setMergePolicy(newLogMergePolicy(false)));\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":["d9e22bdf0692bfa61e342b04a6ac7078670c1e16"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(startDir));\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n          .setOpenMode(OpenMode.APPEND)\n          .setMergePolicy(newLogMergePolicy(false));\n        writer = new IndexWriter(dir, iwc);\n        Exception err = null;\n\n        for(int x=0;x<2;x++) {\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          if (ms instanceof ConcurrentMergeScheduler) {\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x) {\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            } else {\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n            }\n          }\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            }\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + testName);\n          }\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              DirectoryReader readers[] = new DirectoryReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                TestUtil.addIndexesSlowly(writer, readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IllegalStateException | IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit Exception: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          if (x == 1) {\n            // Make sure all threads from ConcurrentMergeScheduler are done\n            TestUtil.syncConcurrentMerges(writer);\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            writer.rollback();\n            writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                     .setOpenMode(OpenMode.APPEND)\n                                     .setMergePolicy(newLogMergePolicy(false)));\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(startDir));\n        dir.setPreventDoubleWrite(false);\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n          .setOpenMode(OpenMode.APPEND)\n          .setMergePolicy(newLogMergePolicy(false));\n        writer = new IndexWriter(dir, iwc);\n        Exception err = null;\n\n        for(int x=0;x<2;x++) {\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          if (ms instanceof ConcurrentMergeScheduler) {\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x) {\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            } else {\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n            }\n          }\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            }\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + testName);\n          }\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              DirectoryReader readers[] = new DirectoryReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                TestUtil.addIndexesSlowly(writer, readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IllegalStateException | IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit Exception: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          if (x == 1) {\n            // Make sure all threads from ConcurrentMergeScheduler are done\n            TestUtil.syncConcurrentMerges(writer);\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            writer.rollback();\n            writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                     .setOpenMode(OpenMode.APPEND)\n                                     .setMergePolicy(newLogMergePolicy(false)));\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24f89e8a6aac05753cde4c83d62a74356098200d","date":1525768331,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull#testAddIndexOnDiskFull().mjava","sourceNew":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(startDir));\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n          .setOpenMode(OpenMode.APPEND)\n          .setMergePolicy(newLogMergePolicy(false));\n        writer = new IndexWriter(dir, iwc);\n        Exception err = null;\n\n        for(int x=0;x<2;x++) {\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          if (ms instanceof ConcurrentMergeScheduler) {\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x) {\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            } else {\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n            }\n          }\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            }\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + testName);\n          }\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              DirectoryReader readers[] = new DirectoryReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                TestUtil.addIndexesSlowly(writer, readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IllegalStateException | IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit Exception: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          if (x == 1) {\n            // Make sure all threads from ConcurrentMergeScheduler are done\n            TestUtil.syncConcurrentMerges(writer);\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            writer.rollback();\n            writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                     .setOpenMode(OpenMode.APPEND)\n                                     .setMergePolicy(newLogMergePolicy(false)));\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","sourceOld":"  /*\n  Test: make sure when we run out of disk space or hit\n  random IOExceptions in any of the addIndexes(*) calls\n  that 1) index is not corrupt (searcher can open/search\n  it) and 2) transactional semantics are followed:\n  either all or none of the incoming documents were in\n  fact added.\n   */\n  public void testAddIndexOnDiskFull() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n\n    int START_COUNT = 57;\n    int NUM_DIR = TEST_NIGHTLY ? 50 : 5;\n    int END_COUNT = START_COUNT + NUM_DIR* (TEST_NIGHTLY ? 25 : 5);\n    \n    // Build up a bunch of dirs that have indexes which we\n    // will then merge together by calling addIndexes(*):\n    Directory[] dirs = new Directory[NUM_DIR];\n    long inputDiskUsage = 0;\n    for(int i=0;i<NUM_DIR;i++) {\n      dirs[i] = newDirectory();\n      IndexWriter writer = new IndexWriter(dirs[i], newIndexWriterConfig(new MockAnalyzer(random())));\n      for(int j=0;j<25;j++) {\n        addDocWithIndex(writer, 25*i+j);\n      }\n      writer.close();\n      String[] files = dirs[i].listAll();\n      for(int j=0;j<files.length;j++) {\n        inputDiskUsage += dirs[i].fileLength(files[j]);\n      }\n    }\n    \n    // Now, build a starting index that has START_COUNT docs.  We\n    // will then try to addIndexes into a copy of this:\n    MockDirectoryWrapper startDir = newMockDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(new MockAnalyzer(random())));\n    for(int j=0;j<START_COUNT;j++) {\n      addDocWithIndex(writer, j);\n    }\n    writer.close();\n    \n    // Make sure starting index seems to be working properly:\n    Term searchTerm = new Term(\"content\", \"aaa\");        \n    IndexReader reader = DirectoryReader.open(startDir);\n    assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n    \n    IndexSearcher searcher = newSearcher(reader);\n    ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), 1000).scoreDocs;\n    assertEquals(\"first number of hits\", 57, hits.length);\n    reader.close();\n    \n    // Iterate with larger and larger amounts of free\n    // disk space.  With little free disk space,\n    // addIndexes will certainly run out of space &\n    // fail.  Verify that when this happens, index is\n    // not corrupt and index in fact has added no\n    // documents.  Then, we increase disk space by 2000\n    // bytes each iteration.  At some point there is\n    // enough free disk space and addIndexes should\n    // succeed and index should show all documents were\n    // added.\n    \n    // String[] files = startDir.listAll();\n    long diskUsage = startDir.sizeInBytes();\n    \n    long startDiskUsage = 0;\n    String[] files = startDir.listAll();\n    for(int i=0;i<files.length;i++) {\n      startDiskUsage += startDir.fileLength(files[i]);\n    }\n\n    for(int iter=0;iter<3;iter++) {\n      \n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n      \n      // Start with 100 bytes more than we are currently using:\n      long diskFree = diskUsage+ TestUtil.nextInt(random(), 50, 200);\n      \n      int method = iter;\n      \n      boolean success = false;\n      boolean done = false;\n      \n      String methodName;\n      if (0 == method) {\n        methodName = \"addIndexes(Directory[]) + forceMerge(1)\";\n      } else if (1 == method) {\n        methodName = \"addIndexes(IndexReader[])\";\n      } else {\n        methodName = \"addIndexes(Directory[])\";\n      }\n      \n      while(!done) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: cycle...\");\n        }\n        \n        // Make a new dir that will enforce disk usage:\n        MockDirectoryWrapper dir = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(startDir));\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()))\n          .setOpenMode(OpenMode.APPEND)\n          .setMergePolicy(newLogMergePolicy(false));\n        writer = new IndexWriter(dir, iwc);\n        Exception err = null;\n\n        for(int x=0;x<2;x++) {\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          if (ms instanceof ConcurrentMergeScheduler) {\n            // This test intentionally produces exceptions\n            // in the threads that CMS launches; we don't\n            // want to pollute test output with these.\n            if (0 == x) {\n              ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n            } else {\n              ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n            }\n          }\n          \n          // Two loops: first time, limit disk space &\n          // throw random IOExceptions; second time, no\n          // disk space limit:\n          \n          double rate = 0.05;\n          double diskRatio = ((double) diskFree)/diskUsage;\n          long thisDiskFree;\n          \n          String testName = null;\n          \n          if (0 == x) {\n            dir.setRandomIOExceptionRateOnOpen(random().nextDouble()*0.01);\n            thisDiskFree = diskFree;\n            if (diskRatio >= 2.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 4.0) {\n              rate /= 2;\n            }\n            if (diskRatio >= 6.0) {\n              rate = 0.0;\n            }\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            }\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            thisDiskFree = 0;\n            rate = 0.0;\n            if (VERBOSE) {\n              testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + testName);\n          }\n          \n          dir.setTrackDiskUsage(true);\n          dir.setMaxSizeInBytes(thisDiskFree);\n          dir.setRandomIOExceptionRate(rate);\n          \n          try {\n            \n            if (0 == method) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: now addIndexes count=\" + dirs.length);\n              }\n              writer.addIndexes(dirs);\n              if (VERBOSE) {\n                System.out.println(\"TEST: now forceMerge\");\n              }\n              writer.forceMerge(1);\n            } else if (1 == method) {\n              DirectoryReader readers[] = new DirectoryReader[dirs.length];\n              for(int i=0;i<dirs.length;i++) {\n                readers[i] = DirectoryReader.open(dirs[i]);\n              }\n              try {\n                TestUtil.addIndexesSlowly(writer, readers);\n              } finally {\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i].close();\n                }\n              }\n            } else {\n              writer.addIndexes(dirs);\n            }\n            \n            success = true;\n            if (VERBOSE) {\n              System.out.println(\"  success!\");\n            }\n            \n            if (0 == x) {\n              done = true;\n            }\n            \n          } catch (IllegalStateException | IOException e) {\n            success = false;\n            err = e;\n            if (VERBOSE) {\n              System.out.println(\"  hit Exception: \" + e);\n              e.printStackTrace(System.out);\n            }\n            \n            if (1 == x) {\n              e.printStackTrace(System.out);\n              fail(methodName + \" hit IOException after disk space was freed up\");\n            }\n          }\n          \n          if (x == 1) {\n            // Make sure all threads from ConcurrentMergeScheduler are done\n            TestUtil.syncConcurrentMerges(writer);\n          } else {\n            dir.setRandomIOExceptionRateOnOpen(0.0);\n            writer.rollback();\n            writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))\n                                     .setOpenMode(OpenMode.APPEND)\n                                     .setMergePolicy(newLogMergePolicy(false)));\n          }\n          \n          if (VERBOSE) {\n            System.out.println(\"  now test readers\");\n          }\n          \n          // Finally, verify index is not corrupt, and, if\n          // we succeeded, we see all docs added, and if we\n          // failed, we see either all docs or no docs added\n          // (transactional semantics):\n          dir.setRandomIOExceptionRateOnOpen(0.0);\n          try {\n            reader = DirectoryReader.open(dir);\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when creating IndexReader: \" + e);\n          }\n          int result = reader.docFreq(searchTerm);\n          if (success) {\n            if (result != START_COUNT) {\n              fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result != START_COUNT && result != END_COUNT) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n            }\n          }\n          \n          searcher = newSearcher(reader);\n          try {\n            hits = searcher.search(new TermQuery(searchTerm), END_COUNT).scoreDocs;\n          } catch (IOException e) {\n            e.printStackTrace(System.out);\n            fail(testName + \": exception when searching: \" + e);\n          }\n          int result2 = hits.length;\n          if (success) {\n            if (result2 != result) {\n              fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          } else {\n            // On hitting exception we still may have added\n            // all docs:\n            if (result2 != result) {\n              err.printStackTrace(System.out);\n              fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n            }\n          }\n          \n          reader.close();\n          if (VERBOSE) {\n            System.out.println(\"  count is \" + result);\n          }\n          \n          if (done || result == END_COUNT) {\n            break;\n          }\n        }\n        \n        if (VERBOSE) {\n          System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n        }\n        \n        if (done) {\n          // Javadocs state that temp free Directory space\n          // required is at most 2X total input size of\n          // indices so let's make sure:\n          assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                     \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes vs limit=\" + (2*(startDiskUsage + inputDiskUsage)) +\n                     \"; starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                     \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                     (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n        }\n        \n        // Make sure we don't hit disk full during close below:\n        dir.setMaxSizeInBytes(0);\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setRandomIOExceptionRateOnOpen(0.0);\n        \n        writer.close();\n        \n        dir.close();\n        \n        // Try again with more free space:\n        diskFree += TEST_NIGHTLY ? TestUtil.nextInt(random(), 4000, 8000) : TestUtil.nextInt(random(), 40000, 80000);\n      }\n    }\n    \n    startDir.close();\n    for (Directory dir : dirs)\n      dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"001b25b42373b22a52f399dbf072f1224632e8e6":["aba371508186796cc6151d8223a5b4e16d02e26e","360305e2c581a6b0d491602f847fd8f76e8718f0"],"360305e2c581a6b0d491602f847fd8f76e8718f0":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"11c6df42fb3eba174c3ca0d9a5194eaecd893b77":["d9e22bdf0692bfa61e342b04a6ac7078670c1e16"],"68a0364646d8ac738da2a85894804beef9d42794":["fc4053f65ade14b2ce1979911bc398299cc9c9ef"],"fc4053f65ade14b2ce1979911bc398299cc9c9ef":["360305e2c581a6b0d491602f847fd8f76e8718f0"],"4356000e349e38c9fb48034695b7c309abd54557":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"aba371508186796cc6151d8223a5b4e16d02e26e":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","d19974432be9aed28ee7dca73bdf01d139e763a9"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"24f89e8a6aac05753cde4c83d62a74356098200d":["11c6df42fb3eba174c3ca0d9a5194eaecd893b77"],"f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc":["302be0cc5e6a28ebcebcac98aa81a92be2e94370"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","d19974432be9aed28ee7dca73bdf01d139e763a9"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"6613659748fe4411a7dcf85266e55db1f95f7315":["68a0364646d8ac738da2a85894804beef9d42794"],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["d9e22bdf0692bfa61e342b04a6ac7078670c1e16","11c6df42fb3eba174c3ca0d9a5194eaecd893b77"],"d9e22bdf0692bfa61e342b04a6ac7078670c1e16":["f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["629c38c4ae4e303d0617e05fbfe508140b32f0a3","4356000e349e38c9fb48034695b7c309abd54557"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["24f89e8a6aac05753cde4c83d62a74356098200d"],"302be0cc5e6a28ebcebcac98aa81a92be2e94370":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"]},"commit2Childs":{"001b25b42373b22a52f399dbf072f1224632e8e6":[],"360305e2c581a6b0d491602f847fd8f76e8718f0":["001b25b42373b22a52f399dbf072f1224632e8e6","fc4053f65ade14b2ce1979911bc398299cc9c9ef"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"11c6df42fb3eba174c3ca0d9a5194eaecd893b77":["24f89e8a6aac05753cde4c83d62a74356098200d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"68a0364646d8ac738da2a85894804beef9d42794":["6613659748fe4411a7dcf85266e55db1f95f7315"],"fc4053f65ade14b2ce1979911bc398299cc9c9ef":["68a0364646d8ac738da2a85894804beef9d42794"],"aba371508186796cc6151d8223a5b4e16d02e26e":["001b25b42373b22a52f399dbf072f1224632e8e6"],"4356000e349e38c9fb48034695b7c309abd54557":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","d19974432be9aed28ee7dca73bdf01d139e763a9"],"24f89e8a6aac05753cde4c83d62a74356098200d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc":["d9e22bdf0692bfa61e342b04a6ac7078670c1e16"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":[],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["302be0cc5e6a28ebcebcac98aa81a92be2e94370"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"d9e22bdf0692bfa61e342b04a6ac7078670c1e16":["11c6df42fb3eba174c3ca0d9a5194eaecd893b77","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["360305e2c581a6b0d491602f847fd8f76e8718f0","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["4356000e349e38c9fb48034695b7c309abd54557","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"302be0cc5e6a28ebcebcac98aa81a92be2e94370":["f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["001b25b42373b22a52f399dbf072f1224632e8e6","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}