{"path":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","commits":[{"id":"f1802f5d24200a8316d3265c965c644eac5f170f","date":1349121521,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"/dev/null","sourceNew":"  public void test() throws Exception {\n    LinkedList<String> postings = new LinkedList<String>();\n    int numTerms = atLeast(300);\n    int maxTermsPerDoc = 10;\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postings.add(term);\n      }\n    }\n    Collections.shuffle(postings, random());\n    \n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n    Document document = new Document();\n    Field field = newTextField(\"field\", \"\", Field.Store.NO);\n    document.add(field);\n    \n    while (!postings.isEmpty()) {\n      StringBuilder text = new StringBuilder();\n      Set<String> visited = new HashSet<String>();\n      for (int i = 0; i < maxTermsPerDoc; i++) {\n        if (postings.isEmpty() || visited.contains(postings.peek())) {\n          break;\n        }\n        String element = postings.remove();\n        text.append(' ');\n        text.append(element);\n        visited.add(element);\n      }\n      field.setStringValue(text.toString());\n      iw.addDocument(document);\n    }\n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28a18213c24b4c6e2593815802f3751352d60a9b","date":1349125879,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n    //System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n    //System.out.println(\"numTerms=\" + numTerms);\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    //System.out.println(\"threadCount=\" + threadCount);\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    \n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    LinkedList<String> postings = new LinkedList<String>();\n    int numTerms = atLeast(300);\n    int maxTermsPerDoc = 10;\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postings.add(term);\n      }\n    }\n    Collections.shuffle(postings, random());\n    \n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n    Document document = new Document();\n    Field field = newTextField(\"field\", \"\", Field.Store.NO);\n    document.add(field);\n    \n    while (!postings.isEmpty()) {\n      StringBuilder text = new StringBuilder();\n      Set<String> visited = new HashSet<String>();\n      for (int i = 0; i < maxTermsPerDoc; i++) {\n        if (postings.isEmpty() || visited.contains(postings.peek())) {\n          break;\n        }\n        String element = postings.remove();\n        text.append(' ');\n        text.append(element);\n        visited.add(element);\n      }\n      field.setStringValue(text.toString());\n      iw.addDocument(document);\n    }\n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d9773480aa9e800d0a232ab6ccac265e874b0c51","date":1349461188,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n    //System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n    //System.out.println(\"numTerms=\" + numTerms);\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    //System.out.println(\"threadCount=\" + threadCount);\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    \n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f150565c7cebe028e9fe692b9b962cf6e2e2125b","date":1350597682,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = false;\n    Codec defaultCodec = Codec.getDefault();\n\n    if (defaultCodec.getName().equals(\"SimpleText\")) {\n      isSimpleText = true;\n    } else {\n      PostingsFormat defaultPostingsFormat = defaultCodec.postingsFormat();\n      if (defaultPostingsFormat instanceof PerFieldPostingsFormat) {\n        isSimpleText = ((PerFieldPostingsFormat) defaultPostingsFormat).getPostingsFormatForField(\"field\").getName().equals(\"SimpleText\");\n      }\n    }\n\n    if (isSimpleText && TEST_NIGHTLY) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b5e0c4f770f2777e4505710df6ff8e282648a93a","date":1350602969,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(_TestUtil.getPostingsFormat(\"field\"));\n\n    if (isSimpleText && TEST_NIGHTLY) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = false;\n    Codec defaultCodec = Codec.getDefault();\n\n    if (defaultCodec.getName().equals(\"SimpleText\")) {\n      isSimpleText = true;\n    } else {\n      PostingsFormat defaultPostingsFormat = defaultCodec.postingsFormat();\n      if (defaultPostingsFormat instanceof PerFieldPostingsFormat) {\n        isSimpleText = ((PerFieldPostingsFormat) defaultPostingsFormat).getPostingsFormatForField(\"field\").getName().equals(\"SimpleText\");\n      }\n    }\n\n    if (isSimpleText && TEST_NIGHTLY) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"62e52115b56781006682fd92c6938efaf174304d","date":1351014780,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(_TestUtil.getPostingsFormat(\"field\"));\n\n    if (isSimpleText && TEST_NIGHTLY) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74b894c584342b7b718eb81d6318f69cdefc2a70","date":1352996144,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(_TestUtil.getPostingsFormat(\"field\"));\n\n    if (isSimpleText && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(_TestUtil.getPostingsFormat(\"field\"));\n\n    if (isSimpleText && TEST_NIGHTLY) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"429430d016e19235864e88ee07192080f778033d","date":1354488586,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(_TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(_TestUtil.getPostingsFormat(\"field\"));\n\n    if (isSimpleText && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"407687e67faf6e1f02a211ca078d8e3eed631027","date":1355157407,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(_TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(_TestUtil.getPostingsFormat(\"field\"));\n\n    if (isSimpleText && TEST_NIGHTLY) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = _TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(_TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(_TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<String>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<String>(postingsList);\n\n    Directory dir = newFSDirectory(TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<String>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0d579490a72f2e6297eaa648940611234c57cf1","date":1395917140,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(TestUtil.createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c","date":1396633078,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(TestUtil.createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a0f5bb79c600763ffe7b8141df59a3169d31e48","date":1396689440,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(TestUtil.getTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    LeafReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    AtomicReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    LeafReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a postingsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    LeafReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a docsAndPositionsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings#test().mjava","sourceNew":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    LeafReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator();\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a postingsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void test() throws Exception {\n    List<String> postingsList = new ArrayList<>();\n    int numTerms = atLeast(300);\n    final int maxTermsPerDoc = TestUtil.nextInt(random(), 10, 20);\n\n    boolean isSimpleText = \"SimpleText\".equals(TestUtil.getPostingsFormat(\"field\"));\n\n    IndexWriterConfig iwc = newIndexWriterConfig(random(), new MockAnalyzer(random()));\n\n    if ((isSimpleText || iwc.getMergePolicy() instanceof MockRandomMergePolicy) && (TEST_NIGHTLY || RANDOM_MULTIPLIER > 1)) {\n      // Otherwise test can take way too long (> 2 hours)\n      numTerms /= 2;\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"maxTermsPerDoc=\" + maxTermsPerDoc);\n      System.out.println(\"numTerms=\" + numTerms);\n    }\n\n    for (int i = 0; i < numTerms; i++) {\n      String term = Integer.toString(i);\n      for (int j = 0; j < i; j++) {\n        postingsList.add(term);\n      }\n    }\n    Collections.shuffle(postingsList, random());\n\n    final ConcurrentLinkedQueue<String> postings = new ConcurrentLinkedQueue<>(postingsList);\n\n    Directory dir = newFSDirectory(createTempDir(\"bagofpostings\"));\n    final RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);\n\n    int threadCount = TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"config: \" + iw.w.getConfig());\n      System.out.println(\"threadCount=\" + threadCount);\n    }\n\n    Thread[] threads = new Thread[threadCount];\n    final CountDownLatch startingGun = new CountDownLatch(1);\n\n    for(int threadID=0;threadID<threadCount;threadID++) {\n      threads[threadID] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              Document document = new Document();\n              Field field = newTextField(\"field\", \"\", Field.Store.NO);\n              document.add(field);\n              startingGun.await();\n              while (!postings.isEmpty()) {\n                StringBuilder text = new StringBuilder();\n                Set<String> visited = new HashSet<>();\n                for (int i = 0; i < maxTermsPerDoc; i++) {\n                  String token = postings.poll();\n                  if (token == null) {\n                    break;\n                  }\n                  if (visited.contains(token)) {\n                    // Put it back:\n                    postings.add(token);\n                    break;\n                  }\n                  text.append(' ');\n                  text.append(token);\n                  visited.add(token);\n                }\n                field.setStringValue(text.toString());\n                iw.addDocument(document);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[threadID].start();\n    }\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n    \n    iw.forceMerge(1);\n    DirectoryReader ir = iw.getReader();\n    assertEquals(1, ir.leaves().size());\n    LeafReader air = ir.leaves().get(0).reader();\n    Terms terms = air.terms(\"field\");\n    // numTerms-1 because there cannot be a term 0 with 0 postings:\n    assertEquals(numTerms-1, terms.size());\n    TermsEnum termsEnum = terms.iterator(null);\n    BytesRef term;\n    while ((term = termsEnum.next()) != null) {\n      int value = Integer.parseInt(term.utf8ToString());\n      assertEquals(value, termsEnum.docFreq());\n      // don't really need to check more than this, as CheckIndex\n      // will verify that docFreq == actual number of documents seen\n      // from a postingsEnum.\n    }\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["51f5280f31484820499077f41fcdfe92d527d9dc"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6613659748fe4411a7dcf85266e55db1f95f7315"],"d9773480aa9e800d0a232ab6ccac265e874b0c51":["28a18213c24b4c6e2593815802f3751352d60a9b"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c"],"6613659748fe4411a7dcf85266e55db1f95f7315":["429430d016e19235864e88ee07192080f778033d"],"407687e67faf6e1f02a211ca078d8e3eed631027":["b5e0c4f770f2777e4505710df6ff8e282648a93a","429430d016e19235864e88ee07192080f778033d"],"429430d016e19235864e88ee07192080f778033d":["74b894c584342b7b718eb81d6318f69cdefc2a70"],"74b894c584342b7b718eb81d6318f69cdefc2a70":["b5e0c4f770f2777e4505710df6ff8e282648a93a"],"d0d579490a72f2e6297eaa648940611234c57cf1":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"51f5280f31484820499077f41fcdfe92d527d9dc":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"f1802f5d24200a8316d3265c965c644eac5f170f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"28a18213c24b4c6e2593815802f3751352d60a9b":["f1802f5d24200a8316d3265c965c644eac5f170f"],"62e52115b56781006682fd92c6938efaf174304d":["d9773480aa9e800d0a232ab6ccac265e874b0c51","b5e0c4f770f2777e4505710df6ff8e282648a93a"],"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c":["d0d579490a72f2e6297eaa648940611234c57cf1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"b5e0c4f770f2777e4505710df6ff8e282648a93a":["f150565c7cebe028e9fe692b9b962cf6e2e2125b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"f150565c7cebe028e9fe692b9b962cf6e2e2125b":["d9773480aa9e800d0a232ab6ccac265e874b0c51"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["2a0f5bb79c600763ffe7b8141df59a3169d31e48","d0d579490a72f2e6297eaa648940611234c57cf1"],"d9773480aa9e800d0a232ab6ccac265e874b0c51":["62e52115b56781006682fd92c6938efaf174304d","f150565c7cebe028e9fe692b9b962cf6e2e2125b"],"2a0f5bb79c600763ffe7b8141df59a3169d31e48":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"6613659748fe4411a7dcf85266e55db1f95f7315":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"407687e67faf6e1f02a211ca078d8e3eed631027":[],"429430d016e19235864e88ee07192080f778033d":["6613659748fe4411a7dcf85266e55db1f95f7315","407687e67faf6e1f02a211ca078d8e3eed631027"],"74b894c584342b7b718eb81d6318f69cdefc2a70":["429430d016e19235864e88ee07192080f778033d"],"d0d579490a72f2e6297eaa648940611234c57cf1":["a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c"],"51f5280f31484820499077f41fcdfe92d527d9dc":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"f1802f5d24200a8316d3265c965c644eac5f170f":["28a18213c24b4c6e2593815802f3751352d60a9b"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["51f5280f31484820499077f41fcdfe92d527d9dc"],"28a18213c24b4c6e2593815802f3751352d60a9b":["d9773480aa9e800d0a232ab6ccac265e874b0c51"],"62e52115b56781006682fd92c6938efaf174304d":[],"a9a24bae1e63c3bb5ff2fb47b0119240d840ee7c":["2a0f5bb79c600763ffe7b8141df59a3169d31e48"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f1802f5d24200a8316d3265c965c644eac5f170f"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"b5e0c4f770f2777e4505710df6ff8e282648a93a":["407687e67faf6e1f02a211ca078d8e3eed631027","74b894c584342b7b718eb81d6318f69cdefc2a70","62e52115b56781006682fd92c6938efaf174304d"],"f150565c7cebe028e9fe692b9b962cf6e2e2125b":["b5e0c4f770f2777e4505710df6ff8e282648a93a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["407687e67faf6e1f02a211ca078d8e3eed631027","62e52115b56781006682fd92c6938efaf174304d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}