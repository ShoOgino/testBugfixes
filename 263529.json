{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternCaptureGroupTokenFilter#testPatterns(String,String[],String[],int[],int[],int[],boolean).mjava","commits":[{"id":"40e1523ffd1c98c46180e96f5e5e3612267d09d0","date":1366798864,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternCaptureGroupTokenFilter#testPatterns(String,String[],String[],int[],int[],int[],boolean).mjava","pathOld":"/dev/null","sourceNew":"  private void testPatterns(String input, String[] regexes, String[] tokens,\n      int[] startOffsets, int[] endOffsets, int[] positions,\n      boolean preserveOriginal) throws Exception {\n    Pattern[] patterns = new Pattern[regexes.length];\n    for (int i = 0; i < regexes.length; i++) {\n      patterns[i] = Pattern.compile(regexes[i]);\n    }\n    TokenStream ts = new PatternCaptureGroupTokenFilter(new MockTokenizer(\n        new StringReader(input), MockTokenizer.WHITESPACE, false),\n        preserveOriginal, patterns);\n    assertTokenStreamContents(ts, tokens, startOffsets, endOffsets, positions);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternCaptureGroupTokenFilter#testPatterns(String,String[],String[],int[],int[],int[],boolean).mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternCaptureGroupTokenFilter#testPatterns(String,String[],String[],int[],int[],int[],boolean).mjava","sourceNew":"  private void testPatterns(String input, String[] regexes, String[] tokens,\n      int[] startOffsets, int[] endOffsets, int[] positions,\n      boolean preserveOriginal) throws Exception {\n    Pattern[] patterns = new Pattern[regexes.length];\n    for (int i = 0; i < regexes.length; i++) {\n      patterns[i] = Pattern.compile(regexes[i]);\n    }\n\n    Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    tokenizer.setReader( new StringReader(input));\n    TokenStream ts = new PatternCaptureGroupTokenFilter(tokenizer, preserveOriginal, patterns);\n    assertTokenStreamContents(ts, tokens, startOffsets, endOffsets, positions);\n  }\n\n","sourceOld":"  private void testPatterns(String input, String[] regexes, String[] tokens,\n      int[] startOffsets, int[] endOffsets, int[] positions,\n      boolean preserveOriginal) throws Exception {\n    Pattern[] patterns = new Pattern[regexes.length];\n    for (int i = 0; i < regexes.length; i++) {\n      patterns[i] = Pattern.compile(regexes[i]);\n    }\n    TokenStream ts = new PatternCaptureGroupTokenFilter(new MockTokenizer(\n        new StringReader(input), MockTokenizer.WHITESPACE, false),\n        preserveOriginal, patterns);\n    assertTokenStreamContents(ts, tokens, startOffsets, endOffsets, positions);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["40e1523ffd1c98c46180e96f5e5e3612267d09d0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"40e1523ffd1c98c46180e96f5e5e3612267d09d0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"]},"commit2Childs":{"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["40e1523ffd1c98c46180e96f5e5e3612267d09d0"],"40e1523ffd1c98c46180e96f5e5e3612267d09d0":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}