{"path":"src/java/org/apache/lucene/index/NormsWriter#flush(Map,DocumentsWriter.FlushState).mjava","commits":[{"id":"5350389bf83287111f7760b9e3db3af8e3648474","date":1216372812,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/NormsWriter#flush(Map,DocumentsWriter.FlushState).mjava","pathOld":"/dev/null","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  public void flush(Map threadsAndFields, DocumentsWriter.FlushState state) throws IOException {\n\n    final Map byField = new HashMap();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    final Iterator it = threadsAndFields.entrySet().iterator();\n    while(it.hasNext()) {\n      Map.Entry entry = (Map.Entry) it.next();\n\n      Collection fields = (Collection) entry.getValue();\n      Iterator fieldsIt = fields.iterator();\n\n      while(fieldsIt.hasNext()) {\n        NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List l = (List) byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = state.segmentName + \".\" + IndexFileNames.NORMS_EXTENSION;\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List toMerge = (List) byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = (NormsWriterPerField) toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocsInRAM;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocsInRAM;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocsInRAM;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocsInRAM == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocsInRAM) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4d17492f26096e19670d947d1be5e9adc52b1d3d","date":1224931200,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/NormsWriter#flush(Map,SegmentWriteState).mjava","pathOld":"src/java/org/apache/lucene/index/NormsWriter#flush(Map,DocumentsWriter.FlushState).mjava","sourceNew":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  public void flush(Map threadsAndFields, SegmentWriteState state) throws IOException {\n\n    final Map byField = new HashMap();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    final Iterator it = threadsAndFields.entrySet().iterator();\n    while(it.hasNext()) {\n      Map.Entry entry = (Map.Entry) it.next();\n\n      Collection fields = (Collection) entry.getValue();\n      Iterator fieldsIt = fields.iterator();\n\n      while(fieldsIt.hasNext()) {\n        NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List l = (List) byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = state.segmentName + \".\" + IndexFileNames.NORMS_EXTENSION;\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List toMerge = (List) byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = (NormsWriterPerField) toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocs;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocs;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocs == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocs) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","sourceOld":"  /** Produce _X.nrm if any document had a field with norms\n   *  not disabled */\n  public void flush(Map threadsAndFields, DocumentsWriter.FlushState state) throws IOException {\n\n    final Map byField = new HashMap();\n\n    // Typically, each thread will have encountered the same\n    // field.  So first we collate by field, ie, all\n    // per-thread field instances that correspond to the\n    // same FieldInfo\n    final Iterator it = threadsAndFields.entrySet().iterator();\n    while(it.hasNext()) {\n      Map.Entry entry = (Map.Entry) it.next();\n\n      Collection fields = (Collection) entry.getValue();\n      Iterator fieldsIt = fields.iterator();\n\n      while(fieldsIt.hasNext()) {\n        NormsWriterPerField perField = (NormsWriterPerField) fieldsIt.next();\n\n        if (perField.upto > 0) {\n          // It has some norms\n          List l = (List) byField.get(perField.fieldInfo);\n          if (l == null) {\n            l = new ArrayList();\n            byField.put(perField.fieldInfo, l);\n          }\n          l.add(perField);\n        } else\n          // Remove this field since we haven't seen it\n          // since the previous flush\n          fieldsIt.remove();\n      }\n    }\n\n    final String normsFileName = state.segmentName + \".\" + IndexFileNames.NORMS_EXTENSION;\n    state.flushedFiles.add(normsFileName);\n    IndexOutput normsOut = state.directory.createOutput(normsFileName);\n\n    try {\n      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);\n\n      final int numField = fieldInfos.size();\n\n      int normCount = 0;\n\n      for(int fieldNumber=0;fieldNumber<numField;fieldNumber++) {\n\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n        List toMerge = (List) byField.get(fieldInfo);\n        int upto = 0;\n        if (toMerge != null) {\n\n          final int numFields = toMerge.size();\n\n          normCount++;\n\n          final NormsWriterPerField[] fields = new NormsWriterPerField[numFields];\n          int[] uptos = new int[numFields];\n\n          for(int j=0;j<numFields;j++)\n            fields[j] = (NormsWriterPerField) toMerge.get(j);\n\n          int numLeft = numFields;\n              \n          while(numLeft > 0) {\n\n            assert uptos[0] < fields[0].docIDs.length : \" uptos[0]=\" + uptos[0] + \" len=\" + (fields[0].docIDs.length);\n\n            int minLoc = 0;\n            int minDocID = fields[0].docIDs[uptos[0]];\n\n            for(int j=1;j<numLeft;j++) {\n              final int docID = fields[j].docIDs[uptos[j]];\n              if (docID < minDocID) {\n                minDocID = docID;\n                minLoc = j;\n              }\n            }\n\n            assert minDocID < state.numDocsInRAM;\n\n            // Fill hole\n            for(;upto<minDocID;upto++)\n              normsOut.writeByte(defaultNorm);\n\n            normsOut.writeByte(fields[minLoc].norms[uptos[minLoc]]);\n            (uptos[minLoc])++;\n            upto++;\n\n            if (uptos[minLoc] == fields[minLoc].upto) {\n              fields[minLoc].reset();\n              if (minLoc != numLeft-1) {\n                fields[minLoc] = fields[numLeft-1];\n                uptos[minLoc] = uptos[numLeft-1];\n              }\n              numLeft--;\n            }\n          }\n          \n          // Fill final hole with defaultNorm\n          for(;upto<state.numDocsInRAM;upto++)\n            normsOut.writeByte(defaultNorm);\n        } else if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {\n          normCount++;\n          // Fill entire field with default norm:\n          for(;upto<state.numDocsInRAM;upto++)\n            normsOut.writeByte(defaultNorm);\n        }\n\n        assert 4+normCount*state.numDocsInRAM == normsOut.getFilePointer() : \".nrm file size mismatch: expected=\" + (4+normCount*state.numDocsInRAM) + \" actual=\" + normsOut.getFilePointer();\n      }\n\n    } finally {\n      normsOut.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4d17492f26096e19670d947d1be5e9adc52b1d3d":["5350389bf83287111f7760b9e3db3af8e3648474"],"5350389bf83287111f7760b9e3db3af8e3648474":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4d17492f26096e19670d947d1be5e9adc52b1d3d"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5350389bf83287111f7760b9e3db3af8e3648474"],"4d17492f26096e19670d947d1be5e9adc52b1d3d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5350389bf83287111f7760b9e3db3af8e3648474":["4d17492f26096e19670d947d1be5e9adc52b1d3d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}