{"path":"contrib/memory/src/test/org/apache/lucene/index/memory/PatternAnalyzerTest#run(String[]).mjava","commits":[{"id":"50e7972fe4865715af8951d4ba15555e3426fc5d","date":1115024647,"type":0,"author":"Erik Hatcher","isMerge":false,"pathNew":"contrib/memory/src/test/org/apache/lucene/index/memory/PatternAnalyzerTest#run(String[]).mjava","pathOld":"/dev/null","sourceNew":"\tprivate void run(String[] args) throws Throwable {\n\t\tint k = -1;\n\t\t\n\t\tint iters = 1;\n\t\tif (args.length > ++k) iters = Math.max(1, Integer.parseInt(args[k]));\n\t\t\n\t\tint runs = 1;\n\t\tif (args.length > ++k) runs = Math.max(1, Integer.parseInt(args[k]));\n\t\t\n\t\tString cmd = \"patluc\";\n\t\tif (args.length > ++k) cmd = args[k];\n\t\tboolean usePattern = cmd.indexOf(\"pat\") >= 0;\n\t\tboolean useLucene  = cmd.indexOf(\"luc\") >= 0;\n\t\t\n\t\tint maxLetters = 1; // = 2: CharTokenizer.MAX_WORD_LEN issue; see class javadoc\n\t\tif (args.length > ++k) maxLetters = Integer.parseInt(args[k]);\n\t\t\n\t\tint maxToLower = 2;\n\t\tif (args.length > ++k) maxToLower = Integer.parseInt(args[k]);\n\n\t\tint maxStops = 2;\n\t\tif (args.length > ++k) maxStops = Integer.parseInt(args[k]);\n\t\t\n\t\tFile[] files = new File[] {new File(\"CHANGES.txt\"), new File(\"LICENSE.txt\") };\n\t\tif (args.length > ++k) {\n\t\t\tfiles = new File[args.length - k];\n\t\t\tfor (int i=k; i < args.length; i++) {\n\t\t\t\tfiles[i-k] = new File(args[i]);\n\t\t\t}\n\t\t}\n\t\t\n\t\tfor (int iter=0; iter < iters; iter++) {\n\t\t\tSystem.out.println(\"\\n########### iteration=\" + iter);\n\t\t\tlong start = System.currentTimeMillis();\t\t\t\t\t\t\n\t\t\tlong bytes = 0;\n\t\t\t\n\t\t\tfor (int i=0; i < files.length; i++) {\n\t\t\t\tFile file = files[i];\n\t\t\t\tif (!file.exists() || file.isDirectory()) continue; // ignore\n\t\t\t\tbytes += file.length();\n\t\t\t\tString text = toString(new FileInputStream(file), null);\n\t\t\t\tSystem.out.println(\"\\n*********** FILE=\" + file);\n\n\t\t\t\tfor (int letters=0; letters < maxLetters; letters++) {\n\t\t\t\t\tboolean lettersOnly = letters == 0;\n\t\t\t\t\t\n\t\t\t\t\tfor (int stops=0; stops < maxStops; stops++) {\n\t\t\t\t\t\tSet stopWords = null;\n\t\t\t\t\t\tif (stops != 0) stopWords = StopFilter.makeStopSet(StopAnalyzer.ENGLISH_STOP_WORDS);\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\tfor (int toLower=0; toLower < maxToLower; toLower++) {\n\t\t\t\t\t\t\tboolean toLowerCase = toLower != 0;\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tfor (int run=0; run < runs; run++) {\n\t\t\t\t\t\t\t\tList tokens1 = null; List tokens2 = null;\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tif (usePattern) tokens1 = getTokens(patternTokenStream(text, lettersOnly, toLowerCase, stopWords));\n\t\t\t\t\t\t\t\t\tif (useLucene) tokens2 = getTokens(luceneTokenStream(text, lettersOnly, toLowerCase, stopWords));\t\t\t\t\t\n\t\t\t\t\t\t\t\t\tif (usePattern && useLucene) assertEquals(tokens1, tokens2);\n\t\t\t\t\t\t\t\t} catch (Throwable t) {\n\t\t\t\t\t\t\t\t\tif (t instanceof OutOfMemoryError) t.printStackTrace();\n\t\t\t\t\t\t\t\t\tSystem.out.println(\"fatal error at file=\" + file + \", letters=\"+ lettersOnly + \", toLowerCase=\" + toLowerCase + \", stopwords=\" + (stopWords != null ? \"english\" : \"none\"));\n\t\t\t\t\t\t\t\t\tSystem.out.println(\"\\n\\ntokens1=\" + toString(tokens1));\n\t\t\t\t\t\t\t\t\tSystem.out.println(\"\\n\\ntokens2=\" + toString(tokens2));\n\t\t\t\t\t\t\t\t\tthrow t;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tlong end = System.currentTimeMillis();\n\t\t\t\tSystem.out.println(\"\\nsecs = \" + ((end-start)/1000.0f));\n\t\t\t\tSystem.out.println(\"files/sec= \" + \n\t\t\t\t\t\t(1.0f * runs * maxLetters * maxToLower * maxStops * files.length \n\t\t\t\t\t\t/ ((end-start)/1000.0f)));\n\t\t\t\tfloat mb = (1.0f * bytes * runs * maxLetters * maxToLower * maxStops) / (1024.0f * 1024.0f);\n\t\t\t\tSystem.out.println(\"MB/sec = \" + (mb / ((end-start)/1000.0f)));\n\t\t\t}\n\t\t}\n\t\t\n\t\tif (usePattern && useLucene) \n\t\t\tSystem.out.println(\"No bug found. done.\");\n\t\telse \n\t\t\tSystem.out.println(\"Done benchmarking (without checking correctness).\");\n\t}\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f68e24227d5556d33ee6d586fd9010cd9ff8bec","date":1150091176,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"contrib/memory/src/test/org/apache/lucene/index/memory/PatternAnalyzerTest#run(String[]).mjava","pathOld":"contrib/memory/src/test/org/apache/lucene/index/memory/PatternAnalyzerTest#run(String[]).mjava","sourceNew":"  private void run(String[] args) throws Throwable {\n    int k = -1;\n    \n    int iters = 1;\n    if (args.length > ++k) iters = Math.max(1, Integer.parseInt(args[k]));\n    \n    int runs = 1;\n    if (args.length > ++k) runs = Math.max(1, Integer.parseInt(args[k]));\n    \n    String cmd = \"patluc\";\n    if (args.length > ++k) cmd = args[k];\n    boolean usePattern = cmd.indexOf(\"pat\") >= 0;\n    boolean useLucene  = cmd.indexOf(\"luc\") >= 0;\n    \n    int maxLetters = 1; // = 2: CharTokenizer.MAX_WORD_LEN issue; see class javadoc\n    if (args.length > ++k) maxLetters = Integer.parseInt(args[k]);\n    \n    int maxToLower = 2;\n    if (args.length > ++k) maxToLower = Integer.parseInt(args[k]);\n\n    int maxStops = 2;\n    if (args.length > ++k) maxStops = Integer.parseInt(args[k]);\n    \n    File[] files = new File[] {new File(\"CHANGES.txt\"), new File(\"LICENSE.txt\") };\n    if (args.length > ++k) {\n      files = new File[args.length - k];\n      for (int i=k; i < args.length; i++) {\n        files[i-k] = new File(args[i]);\n      }\n    }\n    \n    for (int iter=0; iter < iters; iter++) {\n      System.out.println(\"\\n########### iteration=\" + iter);\n      long start = System.currentTimeMillis();            \n      long bytes = 0;\n      \n      for (int i=0; i < files.length; i++) {\n        File file = files[i];\n        if (!file.exists() || file.isDirectory()) continue; // ignore\n        bytes += file.length();\n        String text = toString(new FileInputStream(file), null);\n        System.out.println(\"\\n*********** FILE=\" + file);\n\n        for (int letters=0; letters < maxLetters; letters++) {\n          boolean lettersOnly = letters == 0;\n          \n          for (int stops=0; stops < maxStops; stops++) {\n            Set stopWords = null;\n            if (stops != 0) stopWords = StopFilter.makeStopSet(StopAnalyzer.ENGLISH_STOP_WORDS);\n                \n            for (int toLower=0; toLower < maxToLower; toLower++) {\n              boolean toLowerCase = toLower != 0;\n                \n              for (int run=0; run < runs; run++) {\n                List tokens1 = null; List tokens2 = null;\n                try {\n                  if (usePattern) tokens1 = getTokens(patternTokenStream(text, lettersOnly, toLowerCase, stopWords));\n                  if (useLucene) tokens2 = getTokens(luceneTokenStream(text, lettersOnly, toLowerCase, stopWords));          \n                  if (usePattern && useLucene) assertEquals(tokens1, tokens2);\n                } catch (Throwable t) {\n                  if (t instanceof OutOfMemoryError) t.printStackTrace();\n                  System.out.println(\"fatal error at file=\" + file + \", letters=\"+ lettersOnly + \", toLowerCase=\" + toLowerCase + \", stopwords=\" + (stopWords != null ? \"english\" : \"none\"));\n                  System.out.println(\"\\n\\ntokens1=\" + toString(tokens1));\n                  System.out.println(\"\\n\\ntokens2=\" + toString(tokens2));\n                  throw t;\n                }\n              }\n            }\n          }\n        }\n        long end = System.currentTimeMillis();\n        System.out.println(\"\\nsecs = \" + ((end-start)/1000.0f));\n        System.out.println(\"files/sec= \" + \n            (1.0f * runs * maxLetters * maxToLower * maxStops * files.length \n            / ((end-start)/1000.0f)));\n        float mb = (1.0f * bytes * runs * maxLetters * maxToLower * maxStops) / (1024.0f * 1024.0f);\n        System.out.println(\"MB/sec = \" + (mb / ((end-start)/1000.0f)));\n      }\n    }\n    \n    if (usePattern && useLucene) \n      System.out.println(\"No bug found. done.\");\n    else \n      System.out.println(\"Done benchmarking (without checking correctness).\");\n  }\n\n","sourceOld":"\tprivate void run(String[] args) throws Throwable {\n\t\tint k = -1;\n\t\t\n\t\tint iters = 1;\n\t\tif (args.length > ++k) iters = Math.max(1, Integer.parseInt(args[k]));\n\t\t\n\t\tint runs = 1;\n\t\tif (args.length > ++k) runs = Math.max(1, Integer.parseInt(args[k]));\n\t\t\n\t\tString cmd = \"patluc\";\n\t\tif (args.length > ++k) cmd = args[k];\n\t\tboolean usePattern = cmd.indexOf(\"pat\") >= 0;\n\t\tboolean useLucene  = cmd.indexOf(\"luc\") >= 0;\n\t\t\n\t\tint maxLetters = 1; // = 2: CharTokenizer.MAX_WORD_LEN issue; see class javadoc\n\t\tif (args.length > ++k) maxLetters = Integer.parseInt(args[k]);\n\t\t\n\t\tint maxToLower = 2;\n\t\tif (args.length > ++k) maxToLower = Integer.parseInt(args[k]);\n\n\t\tint maxStops = 2;\n\t\tif (args.length > ++k) maxStops = Integer.parseInt(args[k]);\n\t\t\n\t\tFile[] files = new File[] {new File(\"CHANGES.txt\"), new File(\"LICENSE.txt\") };\n\t\tif (args.length > ++k) {\n\t\t\tfiles = new File[args.length - k];\n\t\t\tfor (int i=k; i < args.length; i++) {\n\t\t\t\tfiles[i-k] = new File(args[i]);\n\t\t\t}\n\t\t}\n\t\t\n\t\tfor (int iter=0; iter < iters; iter++) {\n\t\t\tSystem.out.println(\"\\n########### iteration=\" + iter);\n\t\t\tlong start = System.currentTimeMillis();\t\t\t\t\t\t\n\t\t\tlong bytes = 0;\n\t\t\t\n\t\t\tfor (int i=0; i < files.length; i++) {\n\t\t\t\tFile file = files[i];\n\t\t\t\tif (!file.exists() || file.isDirectory()) continue; // ignore\n\t\t\t\tbytes += file.length();\n\t\t\t\tString text = toString(new FileInputStream(file), null);\n\t\t\t\tSystem.out.println(\"\\n*********** FILE=\" + file);\n\n\t\t\t\tfor (int letters=0; letters < maxLetters; letters++) {\n\t\t\t\t\tboolean lettersOnly = letters == 0;\n\t\t\t\t\t\n\t\t\t\t\tfor (int stops=0; stops < maxStops; stops++) {\n\t\t\t\t\t\tSet stopWords = null;\n\t\t\t\t\t\tif (stops != 0) stopWords = StopFilter.makeStopSet(StopAnalyzer.ENGLISH_STOP_WORDS);\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\tfor (int toLower=0; toLower < maxToLower; toLower++) {\n\t\t\t\t\t\t\tboolean toLowerCase = toLower != 0;\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tfor (int run=0; run < runs; run++) {\n\t\t\t\t\t\t\t\tList tokens1 = null; List tokens2 = null;\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tif (usePattern) tokens1 = getTokens(patternTokenStream(text, lettersOnly, toLowerCase, stopWords));\n\t\t\t\t\t\t\t\t\tif (useLucene) tokens2 = getTokens(luceneTokenStream(text, lettersOnly, toLowerCase, stopWords));\t\t\t\t\t\n\t\t\t\t\t\t\t\t\tif (usePattern && useLucene) assertEquals(tokens1, tokens2);\n\t\t\t\t\t\t\t\t} catch (Throwable t) {\n\t\t\t\t\t\t\t\t\tif (t instanceof OutOfMemoryError) t.printStackTrace();\n\t\t\t\t\t\t\t\t\tSystem.out.println(\"fatal error at file=\" + file + \", letters=\"+ lettersOnly + \", toLowerCase=\" + toLowerCase + \", stopwords=\" + (stopWords != null ? \"english\" : \"none\"));\n\t\t\t\t\t\t\t\t\tSystem.out.println(\"\\n\\ntokens1=\" + toString(tokens1));\n\t\t\t\t\t\t\t\t\tSystem.out.println(\"\\n\\ntokens2=\" + toString(tokens2));\n\t\t\t\t\t\t\t\t\tthrow t;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tlong end = System.currentTimeMillis();\n\t\t\t\tSystem.out.println(\"\\nsecs = \" + ((end-start)/1000.0f));\n\t\t\t\tSystem.out.println(\"files/sec= \" + \n\t\t\t\t\t\t(1.0f * runs * maxLetters * maxToLower * maxStops * files.length \n\t\t\t\t\t\t/ ((end-start)/1000.0f)));\n\t\t\t\tfloat mb = (1.0f * bytes * runs * maxLetters * maxToLower * maxStops) / (1024.0f * 1024.0f);\n\t\t\t\tSystem.out.println(\"MB/sec = \" + (mb / ((end-start)/1000.0f)));\n\t\t\t}\n\t\t}\n\t\t\n\t\tif (usePattern && useLucene) \n\t\t\tSystem.out.println(\"No bug found. done.\");\n\t\telse \n\t\t\tSystem.out.println(\"Done benchmarking (without checking correctness).\");\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c881464041e282c06fdb34e91f883b83b8d97968","date":1247607562,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"contrib/memory/src/test/org/apache/lucene/index/memory/PatternAnalyzerTest#run(String[]).mjava","pathOld":"contrib/memory/src/test/org/apache/lucene/index/memory/PatternAnalyzerTest#run(String[]).mjava","sourceNew":"  private void run(String[] args) throws Throwable {\n    int k = -1;\n    \n    int iters = 1;\n    if (args.length > ++k) iters = Math.max(1, Integer.parseInt(args[k]));\n    \n    int runs = 1;\n    if (args.length > ++k) runs = Math.max(1, Integer.parseInt(args[k]));\n    \n    String cmd = \"patluc\";\n    if (args.length > ++k) cmd = args[k];\n    boolean usePattern = cmd.indexOf(\"pat\") >= 0;\n    boolean useLucene  = cmd.indexOf(\"luc\") >= 0;\n    \n    int maxLetters = 1; // = 2: CharTokenizer.MAX_WORD_LEN issue; see class javadoc\n    if (args.length > ++k) maxLetters = Integer.parseInt(args[k]);\n    \n    int maxToLower = 2;\n    if (args.length > ++k) maxToLower = Integer.parseInt(args[k]);\n\n    int maxStops = 2;\n    if (args.length > ++k) maxStops = Integer.parseInt(args[k]);\n    \n    File[] files = new File[] {new File(\"CHANGES.txt\"), new File(\"LICENSE.txt\") };\n    if (args.length > ++k) {\n      files = new File[args.length - k];\n      for (int i=k; i < args.length; i++) {\n        files[i-k] = new File(args[i]);\n      }\n    }\n    \n    for (int iter=0; iter < iters; iter++) {\n      System.out.println(\"\\n########### iteration=\" + iter);\n      long start = System.currentTimeMillis();            \n      long bytes = 0;\n      \n      for (int i=0; i < files.length; i++) {\n        File file = files[i];\n        if (!file.exists() || file.isDirectory()) continue; // ignore\n        bytes += file.length();\n        String text = toString(new FileInputStream(file), null);\n        System.out.println(\"\\n*********** FILE=\" + file);\n\n        for (int letters=0; letters < maxLetters; letters++) {\n          boolean lettersOnly = letters == 0;\n          \n          for (int stops=0; stops < maxStops; stops++) {\n            Set stopWords = null;\n            if (stops != 0) stopWords = StopAnalyzer.ENGLISH_STOP_WORDS_SET;\n                \n            for (int toLower=0; toLower < maxToLower; toLower++) {\n              boolean toLowerCase = toLower != 0;\n                \n              for (int run=0; run < runs; run++) {\n                List tokens1 = null; List tokens2 = null;\n                try {\n                  if (usePattern) tokens1 = getTokens(patternTokenStream(text, lettersOnly, toLowerCase, stopWords));\n                  if (useLucene) tokens2 = getTokens(luceneTokenStream(text, lettersOnly, toLowerCase, stopWords));          \n                  if (usePattern && useLucene) assertEquals(tokens1, tokens2);\n                } catch (Throwable t) {\n                  if (t instanceof OutOfMemoryError) t.printStackTrace();\n                  System.out.println(\"fatal error at file=\" + file + \", letters=\"+ lettersOnly + \", toLowerCase=\" + toLowerCase + \", stopwords=\" + (stopWords != null ? \"english\" : \"none\"));\n                  System.out.println(\"\\n\\ntokens1=\" + toString(tokens1));\n                  System.out.println(\"\\n\\ntokens2=\" + toString(tokens2));\n                  throw t;\n                }\n              }\n            }\n          }\n        }\n        long end = System.currentTimeMillis();\n        System.out.println(\"\\nsecs = \" + ((end-start)/1000.0f));\n        System.out.println(\"files/sec= \" + \n            (1.0f * runs * maxLetters * maxToLower * maxStops * files.length \n            / ((end-start)/1000.0f)));\n        float mb = (1.0f * bytes * runs * maxLetters * maxToLower * maxStops) / (1024.0f * 1024.0f);\n        System.out.println(\"MB/sec = \" + (mb / ((end-start)/1000.0f)));\n      }\n    }\n    \n    if (usePattern && useLucene) \n      System.out.println(\"No bug found. done.\");\n    else \n      System.out.println(\"Done benchmarking (without checking correctness).\");\n  }\n\n","sourceOld":"  private void run(String[] args) throws Throwable {\n    int k = -1;\n    \n    int iters = 1;\n    if (args.length > ++k) iters = Math.max(1, Integer.parseInt(args[k]));\n    \n    int runs = 1;\n    if (args.length > ++k) runs = Math.max(1, Integer.parseInt(args[k]));\n    \n    String cmd = \"patluc\";\n    if (args.length > ++k) cmd = args[k];\n    boolean usePattern = cmd.indexOf(\"pat\") >= 0;\n    boolean useLucene  = cmd.indexOf(\"luc\") >= 0;\n    \n    int maxLetters = 1; // = 2: CharTokenizer.MAX_WORD_LEN issue; see class javadoc\n    if (args.length > ++k) maxLetters = Integer.parseInt(args[k]);\n    \n    int maxToLower = 2;\n    if (args.length > ++k) maxToLower = Integer.parseInt(args[k]);\n\n    int maxStops = 2;\n    if (args.length > ++k) maxStops = Integer.parseInt(args[k]);\n    \n    File[] files = new File[] {new File(\"CHANGES.txt\"), new File(\"LICENSE.txt\") };\n    if (args.length > ++k) {\n      files = new File[args.length - k];\n      for (int i=k; i < args.length; i++) {\n        files[i-k] = new File(args[i]);\n      }\n    }\n    \n    for (int iter=0; iter < iters; iter++) {\n      System.out.println(\"\\n########### iteration=\" + iter);\n      long start = System.currentTimeMillis();            \n      long bytes = 0;\n      \n      for (int i=0; i < files.length; i++) {\n        File file = files[i];\n        if (!file.exists() || file.isDirectory()) continue; // ignore\n        bytes += file.length();\n        String text = toString(new FileInputStream(file), null);\n        System.out.println(\"\\n*********** FILE=\" + file);\n\n        for (int letters=0; letters < maxLetters; letters++) {\n          boolean lettersOnly = letters == 0;\n          \n          for (int stops=0; stops < maxStops; stops++) {\n            Set stopWords = null;\n            if (stops != 0) stopWords = StopFilter.makeStopSet(StopAnalyzer.ENGLISH_STOP_WORDS);\n                \n            for (int toLower=0; toLower < maxToLower; toLower++) {\n              boolean toLowerCase = toLower != 0;\n                \n              for (int run=0; run < runs; run++) {\n                List tokens1 = null; List tokens2 = null;\n                try {\n                  if (usePattern) tokens1 = getTokens(patternTokenStream(text, lettersOnly, toLowerCase, stopWords));\n                  if (useLucene) tokens2 = getTokens(luceneTokenStream(text, lettersOnly, toLowerCase, stopWords));          \n                  if (usePattern && useLucene) assertEquals(tokens1, tokens2);\n                } catch (Throwable t) {\n                  if (t instanceof OutOfMemoryError) t.printStackTrace();\n                  System.out.println(\"fatal error at file=\" + file + \", letters=\"+ lettersOnly + \", toLowerCase=\" + toLowerCase + \", stopwords=\" + (stopWords != null ? \"english\" : \"none\"));\n                  System.out.println(\"\\n\\ntokens1=\" + toString(tokens1));\n                  System.out.println(\"\\n\\ntokens2=\" + toString(tokens2));\n                  throw t;\n                }\n              }\n            }\n          }\n        }\n        long end = System.currentTimeMillis();\n        System.out.println(\"\\nsecs = \" + ((end-start)/1000.0f));\n        System.out.println(\"files/sec= \" + \n            (1.0f * runs * maxLetters * maxToLower * maxStops * files.length \n            / ((end-start)/1000.0f)));\n        float mb = (1.0f * bytes * runs * maxLetters * maxToLower * maxStops) / (1024.0f * 1024.0f);\n        System.out.println(\"MB/sec = \" + (mb / ((end-start)/1000.0f)));\n      }\n    }\n    \n    if (usePattern && useLucene) \n      System.out.println(\"No bug found. done.\");\n    else \n      System.out.println(\"Done benchmarking (without checking correctness).\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"439b0fe2f799d1c722151e88e32bdefad8d34ebe","date":1255282509,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/memory/src/test/org/apache/lucene/index/memory/PatternAnalyzerTest#run(String[]).mjava","pathOld":"contrib/memory/src/test/org/apache/lucene/index/memory/PatternAnalyzerTest#run(String[]).mjava","sourceNew":"  private void run(String[] args) throws Throwable {\n    int k = -1;\n    \n    int iters = 1;\n    if (args.length > ++k) iters = Math.max(1, Integer.parseInt(args[k]));\n    \n    int runs = 1;\n    if (args.length > ++k) runs = Math.max(1, Integer.parseInt(args[k]));\n    \n    String cmd = \"patluc\";\n    if (args.length > ++k) cmd = args[k];\n    boolean usePattern = cmd.indexOf(\"pat\") >= 0;\n    boolean useLucene  = cmd.indexOf(\"luc\") >= 0;\n    \n    int maxLetters = 1; // = 2: CharTokenizer.MAX_WORD_LEN issue; see class javadoc\n    if (args.length > ++k) maxLetters = Integer.parseInt(args[k]);\n    \n    int maxToLower = 2;\n    if (args.length > ++k) maxToLower = Integer.parseInt(args[k]);\n\n    int maxStops = 2;\n    if (args.length > ++k) maxStops = Integer.parseInt(args[k]);\n    \n    File[] files = new File[] {new File(\"CHANGES.txt\"), new File(\"LICENSE.txt\") };\n    if (args.length > ++k) {\n      files = new File[args.length - k];\n      for (int i=k; i < args.length; i++) {\n        files[i-k] = new File(args[i]);\n      }\n    }\n    \n    for (int iter=0; iter < iters; iter++) {\n      System.out.println(\"\\n########### iteration=\" + iter);\n      long start = System.currentTimeMillis();            \n      long bytes = 0;\n      \n      for (int i=0; i < files.length; i++) {\n        File file = files[i];\n        if (!file.exists() || file.isDirectory()) continue; // ignore\n        bytes += file.length();\n        String text = toString(new FileInputStream(file), null);\n        System.out.println(\"\\n*********** FILE=\" + file);\n\n        for (int letters=0; letters < maxLetters; letters++) {\n          boolean lettersOnly = letters == 0;\n          \n          for (int stops=0; stops < maxStops; stops++) {\n            Set stopWords = null;\n            if (stops != 0) stopWords = StopAnalyzer.ENGLISH_STOP_WORDS_SET;\n                \n            for (int toLower=0; toLower < maxToLower; toLower++) {\n              boolean toLowerCase = toLower != 0;\n                \n              for (int run=0; run < runs; run++) {\n                TokenStream tokens1 = null; TokenStream tokens2 = null;\n                try {\n                  if (usePattern) tokens1 = patternTokenStream(text, lettersOnly, toLowerCase, stopWords);\n                  if (useLucene) tokens2 = luceneTokenStream(text, lettersOnly, toLowerCase, stopWords);          \n                  if (usePattern && useLucene) {\n                    final TermAttribute termAtt1 = tokens1.addAttribute(TermAttribute.class),\n                      termAtt2 = tokens2.addAttribute(TermAttribute.class);\n                    final OffsetAttribute offsetAtt1 = tokens1.addAttribute(OffsetAttribute.class),\n                      offsetAtt2 = tokens2.addAttribute(OffsetAttribute.class);\n                    final PositionIncrementAttribute posincrAtt1 = tokens1.addAttribute(PositionIncrementAttribute.class),\n                      posincrAtt2 = tokens2.addAttribute(PositionIncrementAttribute.class);\n                    while (tokens1.incrementToken()) {\n                      assertTrue(tokens2.incrementToken());\n                      assertEquals(termAtt1, termAtt2);\n                      assertEquals(offsetAtt1, offsetAtt2);\n                      assertEquals(posincrAtt1, posincrAtt2);\n                    }\n                    assertFalse(tokens2.incrementToken());\n                    tokens1.end(); tokens1.close();\n                    tokens2.end(); tokens2.close();\n                  }\n                } catch (Throwable t) {\n                  if (t instanceof OutOfMemoryError) t.printStackTrace();\n                  System.out.println(\"fatal error at file=\" + file + \", letters=\"+ lettersOnly + \", toLowerCase=\" + toLowerCase + \", stopwords=\" + (stopWords != null ? \"english\" : \"none\"));\n                  throw t;\n                }\n              }\n            }\n          }\n        }\n        long end = System.currentTimeMillis();\n        System.out.println(\"\\nsecs = \" + ((end-start)/1000.0f));\n        System.out.println(\"files/sec= \" + \n            (1.0f * runs * maxLetters * maxToLower * maxStops * files.length \n            / ((end-start)/1000.0f)));\n        float mb = (1.0f * bytes * runs * maxLetters * maxToLower * maxStops) / (1024.0f * 1024.0f);\n        System.out.println(\"MB/sec = \" + (mb / ((end-start)/1000.0f)));\n      }\n    }\n    \n    if (usePattern && useLucene) \n      System.out.println(\"No bug found. done.\");\n    else \n      System.out.println(\"Done benchmarking (without checking correctness).\");\n  }\n\n","sourceOld":"  private void run(String[] args) throws Throwable {\n    int k = -1;\n    \n    int iters = 1;\n    if (args.length > ++k) iters = Math.max(1, Integer.parseInt(args[k]));\n    \n    int runs = 1;\n    if (args.length > ++k) runs = Math.max(1, Integer.parseInt(args[k]));\n    \n    String cmd = \"patluc\";\n    if (args.length > ++k) cmd = args[k];\n    boolean usePattern = cmd.indexOf(\"pat\") >= 0;\n    boolean useLucene  = cmd.indexOf(\"luc\") >= 0;\n    \n    int maxLetters = 1; // = 2: CharTokenizer.MAX_WORD_LEN issue; see class javadoc\n    if (args.length > ++k) maxLetters = Integer.parseInt(args[k]);\n    \n    int maxToLower = 2;\n    if (args.length > ++k) maxToLower = Integer.parseInt(args[k]);\n\n    int maxStops = 2;\n    if (args.length > ++k) maxStops = Integer.parseInt(args[k]);\n    \n    File[] files = new File[] {new File(\"CHANGES.txt\"), new File(\"LICENSE.txt\") };\n    if (args.length > ++k) {\n      files = new File[args.length - k];\n      for (int i=k; i < args.length; i++) {\n        files[i-k] = new File(args[i]);\n      }\n    }\n    \n    for (int iter=0; iter < iters; iter++) {\n      System.out.println(\"\\n########### iteration=\" + iter);\n      long start = System.currentTimeMillis();            \n      long bytes = 0;\n      \n      for (int i=0; i < files.length; i++) {\n        File file = files[i];\n        if (!file.exists() || file.isDirectory()) continue; // ignore\n        bytes += file.length();\n        String text = toString(new FileInputStream(file), null);\n        System.out.println(\"\\n*********** FILE=\" + file);\n\n        for (int letters=0; letters < maxLetters; letters++) {\n          boolean lettersOnly = letters == 0;\n          \n          for (int stops=0; stops < maxStops; stops++) {\n            Set stopWords = null;\n            if (stops != 0) stopWords = StopAnalyzer.ENGLISH_STOP_WORDS_SET;\n                \n            for (int toLower=0; toLower < maxToLower; toLower++) {\n              boolean toLowerCase = toLower != 0;\n                \n              for (int run=0; run < runs; run++) {\n                List tokens1 = null; List tokens2 = null;\n                try {\n                  if (usePattern) tokens1 = getTokens(patternTokenStream(text, lettersOnly, toLowerCase, stopWords));\n                  if (useLucene) tokens2 = getTokens(luceneTokenStream(text, lettersOnly, toLowerCase, stopWords));          \n                  if (usePattern && useLucene) assertEquals(tokens1, tokens2);\n                } catch (Throwable t) {\n                  if (t instanceof OutOfMemoryError) t.printStackTrace();\n                  System.out.println(\"fatal error at file=\" + file + \", letters=\"+ lettersOnly + \", toLowerCase=\" + toLowerCase + \", stopwords=\" + (stopWords != null ? \"english\" : \"none\"));\n                  System.out.println(\"\\n\\ntokens1=\" + toString(tokens1));\n                  System.out.println(\"\\n\\ntokens2=\" + toString(tokens2));\n                  throw t;\n                }\n              }\n            }\n          }\n        }\n        long end = System.currentTimeMillis();\n        System.out.println(\"\\nsecs = \" + ((end-start)/1000.0f));\n        System.out.println(\"files/sec= \" + \n            (1.0f * runs * maxLetters * maxToLower * maxStops * files.length \n            / ((end-start)/1000.0f)));\n        float mb = (1.0f * bytes * runs * maxLetters * maxToLower * maxStops) / (1024.0f * 1024.0f);\n        System.out.println(\"MB/sec = \" + (mb / ((end-start)/1000.0f)));\n      }\n    }\n    \n    if (usePattern && useLucene) \n      System.out.println(\"No bug found. done.\");\n    else \n      System.out.println(\"Done benchmarking (without checking correctness).\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30efa0465403f867cd67289e0668f667ac2d42e9","date":1255523608,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"contrib/memory/src/test/org/apache/lucene/index/memory/PatternAnalyzerTest#run(String[]).mjava","sourceNew":null,"sourceOld":"  private void run(String[] args) throws Throwable {\n    int k = -1;\n    \n    int iters = 1;\n    if (args.length > ++k) iters = Math.max(1, Integer.parseInt(args[k]));\n    \n    int runs = 1;\n    if (args.length > ++k) runs = Math.max(1, Integer.parseInt(args[k]));\n    \n    String cmd = \"patluc\";\n    if (args.length > ++k) cmd = args[k];\n    boolean usePattern = cmd.indexOf(\"pat\") >= 0;\n    boolean useLucene  = cmd.indexOf(\"luc\") >= 0;\n    \n    int maxLetters = 1; // = 2: CharTokenizer.MAX_WORD_LEN issue; see class javadoc\n    if (args.length > ++k) maxLetters = Integer.parseInt(args[k]);\n    \n    int maxToLower = 2;\n    if (args.length > ++k) maxToLower = Integer.parseInt(args[k]);\n\n    int maxStops = 2;\n    if (args.length > ++k) maxStops = Integer.parseInt(args[k]);\n    \n    File[] files = new File[] {new File(\"CHANGES.txt\"), new File(\"LICENSE.txt\") };\n    if (args.length > ++k) {\n      files = new File[args.length - k];\n      for (int i=k; i < args.length; i++) {\n        files[i-k] = new File(args[i]);\n      }\n    }\n    \n    for (int iter=0; iter < iters; iter++) {\n      System.out.println(\"\\n########### iteration=\" + iter);\n      long start = System.currentTimeMillis();            \n      long bytes = 0;\n      \n      for (int i=0; i < files.length; i++) {\n        File file = files[i];\n        if (!file.exists() || file.isDirectory()) continue; // ignore\n        bytes += file.length();\n        String text = toString(new FileInputStream(file), null);\n        System.out.println(\"\\n*********** FILE=\" + file);\n\n        for (int letters=0; letters < maxLetters; letters++) {\n          boolean lettersOnly = letters == 0;\n          \n          for (int stops=0; stops < maxStops; stops++) {\n            Set stopWords = null;\n            if (stops != 0) stopWords = StopAnalyzer.ENGLISH_STOP_WORDS_SET;\n                \n            for (int toLower=0; toLower < maxToLower; toLower++) {\n              boolean toLowerCase = toLower != 0;\n                \n              for (int run=0; run < runs; run++) {\n                TokenStream tokens1 = null; TokenStream tokens2 = null;\n                try {\n                  if (usePattern) tokens1 = patternTokenStream(text, lettersOnly, toLowerCase, stopWords);\n                  if (useLucene) tokens2 = luceneTokenStream(text, lettersOnly, toLowerCase, stopWords);          \n                  if (usePattern && useLucene) {\n                    final TermAttribute termAtt1 = tokens1.addAttribute(TermAttribute.class),\n                      termAtt2 = tokens2.addAttribute(TermAttribute.class);\n                    final OffsetAttribute offsetAtt1 = tokens1.addAttribute(OffsetAttribute.class),\n                      offsetAtt2 = tokens2.addAttribute(OffsetAttribute.class);\n                    final PositionIncrementAttribute posincrAtt1 = tokens1.addAttribute(PositionIncrementAttribute.class),\n                      posincrAtt2 = tokens2.addAttribute(PositionIncrementAttribute.class);\n                    while (tokens1.incrementToken()) {\n                      assertTrue(tokens2.incrementToken());\n                      assertEquals(termAtt1, termAtt2);\n                      assertEquals(offsetAtt1, offsetAtt2);\n                      assertEquals(posincrAtt1, posincrAtt2);\n                    }\n                    assertFalse(tokens2.incrementToken());\n                    tokens1.end(); tokens1.close();\n                    tokens2.end(); tokens2.close();\n                  }\n                } catch (Throwable t) {\n                  if (t instanceof OutOfMemoryError) t.printStackTrace();\n                  System.out.println(\"fatal error at file=\" + file + \", letters=\"+ lettersOnly + \", toLowerCase=\" + toLowerCase + \", stopwords=\" + (stopWords != null ? \"english\" : \"none\"));\n                  throw t;\n                }\n              }\n            }\n          }\n        }\n        long end = System.currentTimeMillis();\n        System.out.println(\"\\nsecs = \" + ((end-start)/1000.0f));\n        System.out.println(\"files/sec= \" + \n            (1.0f * runs * maxLetters * maxToLower * maxStops * files.length \n            / ((end-start)/1000.0f)));\n        float mb = (1.0f * bytes * runs * maxLetters * maxToLower * maxStops) / (1024.0f * 1024.0f);\n        System.out.println(\"MB/sec = \" + (mb / ((end-start)/1000.0f)));\n      }\n    }\n    \n    if (usePattern && useLucene) \n      System.out.println(\"No bug found. done.\");\n    else \n      System.out.println(\"Done benchmarking (without checking correctness).\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"30efa0465403f867cd67289e0668f667ac2d42e9":["439b0fe2f799d1c722151e88e32bdefad8d34ebe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c881464041e282c06fdb34e91f883b83b8d97968":["7f68e24227d5556d33ee6d586fd9010cd9ff8bec"],"7f68e24227d5556d33ee6d586fd9010cd9ff8bec":["50e7972fe4865715af8951d4ba15555e3426fc5d"],"439b0fe2f799d1c722151e88e32bdefad8d34ebe":["c881464041e282c06fdb34e91f883b83b8d97968"],"50e7972fe4865715af8951d4ba15555e3426fc5d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["30efa0465403f867cd67289e0668f667ac2d42e9"]},"commit2Childs":{"30efa0465403f867cd67289e0668f667ac2d42e9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["50e7972fe4865715af8951d4ba15555e3426fc5d"],"c881464041e282c06fdb34e91f883b83b8d97968":["439b0fe2f799d1c722151e88e32bdefad8d34ebe"],"7f68e24227d5556d33ee6d586fd9010cd9ff8bec":["c881464041e282c06fdb34e91f883b83b8d97968"],"439b0fe2f799d1c722151e88e32bdefad8d34ebe":["30efa0465403f867cd67289e0668f667ac2d42e9"],"50e7972fe4865715af8951d4ba15555e3426fc5d":["7f68e24227d5556d33ee6d586fd9010cd9ff8bec"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}