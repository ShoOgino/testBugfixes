{"path":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","commits":[{"id":"004aa3627d484d9bc0a4281c12c40240ceeaf75a","date":1364791776,"type":0,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testReset() throws Exception {\n    Directory indexDir = newDirectory();\n    Directory taxoDir = newDirectory();\n    \n    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);\n    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    \n    FacetFields facetFields = new FacetFields(taxonomyWriter);\n    Document doc = new Document();\n    facetFields.addFields(doc, Arrays.asList(new CategoryPath(\"a/1\", '/'), new CategoryPath(\"b/1\", '/')));\n    iw.addDocument(doc);\n    taxonomyWriter.close();\n    iw.close();\n    \n    DirectoryReader r = DirectoryReader.open(indexDir);\n    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    \n    FacetSearchParams fsp = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"a\"), 10), \n        new CountFacetRequest(new CategoryPath(\"b\"), 10));\n    final FacetsAccumulator fa = random().nextBoolean() ? new FacetsAccumulator(fsp, r, taxo) : new StandardFacetsAccumulator(fsp, r, taxo);\n    final FacetsCollector fc = FacetsCollector.create(fa);\n    // this should populate the cached results, but doing search should clear the cache\n    fc.getFacetResults();\n    new IndexSearcher(r).search(new MatchAllDocsQuery(), fc);\n    \n    List<FacetResult> res1 = fc.getFacetResults();\n    // verify that we didn't get the cached result\n    assertEquals(2, res1.size());\n    for (FacetResult res : res1) {\n      assertEquals(1, res.getFacetResultNode().subResults.size());\n      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);\n    }\n    fc.reset();\n    List<FacetResult> res2 = fc.getFacetResults();\n    assertNotSame(\"reset() should clear the cached results\", res1, res2);\n    \n    IOUtils.close(taxo, taxoDir, r, indexDir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad1f7eb1a6bbf19f3d32b3baf2dee3db844eccdc","date":1366056945,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","sourceNew":"  @Test\n  public void testReset() throws Exception {\n    Directory indexDir = newDirectory();\n    Directory taxoDir = newDirectory();\n    \n    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);\n    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    \n    FacetFields facetFields = new FacetFields(taxonomyWriter);\n    Document doc = new Document();\n    facetFields.addFields(doc, Arrays.asList(new CategoryPath(\"a/1\", '/'), new CategoryPath(\"b/1\", '/')));\n    iw.addDocument(doc);\n    taxonomyWriter.close();\n    iw.close();\n    \n    DirectoryReader r = DirectoryReader.open(indexDir);\n    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    \n    FacetSearchParams fsp = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"a\"), 10), \n        new CountFacetRequest(new CategoryPath(\"b\"), 10));\n    final FacetsAccumulator fa = random().nextBoolean() ? new FacetsAccumulator(fsp, r, taxo) : new StandardFacetsAccumulator(fsp, r, taxo);\n    final FacetsCollector fc = FacetsCollector.create(fa);\n    // this should populate the cached results, but doing search should clear the cache\n    fc.getFacetResults();\n    newSearcher(r).search(new MatchAllDocsQuery(), fc);\n    \n    List<FacetResult> res1 = fc.getFacetResults();\n    // verify that we didn't get the cached result\n    assertEquals(2, res1.size());\n    for (FacetResult res : res1) {\n      assertEquals(1, res.getFacetResultNode().subResults.size());\n      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);\n    }\n    fc.reset();\n    List<FacetResult> res2 = fc.getFacetResults();\n    assertNotSame(\"reset() should clear the cached results\", res1, res2);\n    \n    IOUtils.close(taxo, taxoDir, r, indexDir);\n  }\n\n","sourceOld":"  @Test\n  public void testReset() throws Exception {\n    Directory indexDir = newDirectory();\n    Directory taxoDir = newDirectory();\n    \n    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);\n    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    \n    FacetFields facetFields = new FacetFields(taxonomyWriter);\n    Document doc = new Document();\n    facetFields.addFields(doc, Arrays.asList(new CategoryPath(\"a/1\", '/'), new CategoryPath(\"b/1\", '/')));\n    iw.addDocument(doc);\n    taxonomyWriter.close();\n    iw.close();\n    \n    DirectoryReader r = DirectoryReader.open(indexDir);\n    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    \n    FacetSearchParams fsp = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"a\"), 10), \n        new CountFacetRequest(new CategoryPath(\"b\"), 10));\n    final FacetsAccumulator fa = random().nextBoolean() ? new FacetsAccumulator(fsp, r, taxo) : new StandardFacetsAccumulator(fsp, r, taxo);\n    final FacetsCollector fc = FacetsCollector.create(fa);\n    // this should populate the cached results, but doing search should clear the cache\n    fc.getFacetResults();\n    new IndexSearcher(r).search(new MatchAllDocsQuery(), fc);\n    \n    List<FacetResult> res1 = fc.getFacetResults();\n    // verify that we didn't get the cached result\n    assertEquals(2, res1.size());\n    for (FacetResult res : res1) {\n      assertEquals(1, res.getFacetResultNode().subResults.size());\n      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);\n    }\n    fc.reset();\n    List<FacetResult> res2 = fc.getFacetResults();\n    assertNotSame(\"reset() should clear the cached results\", res1, res2);\n    \n    IOUtils.close(taxo, taxoDir, r, indexDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6249cba93d7ad1bf6f5a225c34fbe3d547ed9f49","date":1375103250,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","sourceNew":"  @Test\n  public void testReset() throws Exception {\n    Directory indexDir = newDirectory();\n    Directory taxoDir = newDirectory();\n    \n    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);\n    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    \n    FacetFields facetFields = new FacetFields(taxonomyWriter);\n    Document doc = new Document();\n    facetFields.addFields(doc, Arrays.asList(new CategoryPath(\"a/1\", '/'), new CategoryPath(\"b/1\", '/')));\n    iw.addDocument(doc);\n    taxonomyWriter.close();\n    iw.close();\n    \n    DirectoryReader r = DirectoryReader.open(indexDir);\n    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    \n    FacetSearchParams fsp = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"a\"), 10), \n        new CountFacetRequest(new CategoryPath(\"b\"), 10));\n    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new StandardFacetsAccumulator(fsp, r, taxo);\n    final FacetsCollector fc = FacetsCollector.create(fa);\n    // this should populate the cached results, but doing search should clear the cache\n    fc.getFacetResults();\n    newSearcher(r).search(new MatchAllDocsQuery(), fc);\n    \n    List<FacetResult> res1 = fc.getFacetResults();\n    // verify that we didn't get the cached result\n    assertEquals(2, res1.size());\n    for (FacetResult res : res1) {\n      assertEquals(1, res.getFacetResultNode().subResults.size());\n      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);\n    }\n    fc.reset();\n    List<FacetResult> res2 = fc.getFacetResults();\n    assertNotSame(\"reset() should clear the cached results\", res1, res2);\n    \n    IOUtils.close(taxo, taxoDir, r, indexDir);\n  }\n\n","sourceOld":"  @Test\n  public void testReset() throws Exception {\n    Directory indexDir = newDirectory();\n    Directory taxoDir = newDirectory();\n    \n    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);\n    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    \n    FacetFields facetFields = new FacetFields(taxonomyWriter);\n    Document doc = new Document();\n    facetFields.addFields(doc, Arrays.asList(new CategoryPath(\"a/1\", '/'), new CategoryPath(\"b/1\", '/')));\n    iw.addDocument(doc);\n    taxonomyWriter.close();\n    iw.close();\n    \n    DirectoryReader r = DirectoryReader.open(indexDir);\n    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    \n    FacetSearchParams fsp = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"a\"), 10), \n        new CountFacetRequest(new CategoryPath(\"b\"), 10));\n    final FacetsAccumulator fa = random().nextBoolean() ? new FacetsAccumulator(fsp, r, taxo) : new StandardFacetsAccumulator(fsp, r, taxo);\n    final FacetsCollector fc = FacetsCollector.create(fa);\n    // this should populate the cached results, but doing search should clear the cache\n    fc.getFacetResults();\n    newSearcher(r).search(new MatchAllDocsQuery(), fc);\n    \n    List<FacetResult> res1 = fc.getFacetResults();\n    // verify that we didn't get the cached result\n    assertEquals(2, res1.size());\n    for (FacetResult res : res1) {\n      assertEquals(1, res.getFacetResultNode().subResults.size());\n      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);\n    }\n    fc.reset();\n    List<FacetResult> res2 = fc.getFacetResults();\n    assertNotSame(\"reset() should clear the cached results\", res1, res2);\n    \n    IOUtils.close(taxo, taxoDir, r, indexDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6","date":1375108983,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","sourceNew":"  @Test\n  public void testReset() throws Exception {\n    Directory indexDir = newDirectory();\n    Directory taxoDir = newDirectory();\n    \n    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);\n    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    \n    FacetFields facetFields = new FacetFields(taxonomyWriter);\n    Document doc = new Document();\n    facetFields.addFields(doc, Arrays.asList(new CategoryPath(\"a/1\", '/'), new CategoryPath(\"b/1\", '/')));\n    iw.addDocument(doc);\n    taxonomyWriter.close();\n    iw.close();\n    \n    DirectoryReader r = DirectoryReader.open(indexDir);\n    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    \n    FacetSearchParams fsp = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"a\"), 10), \n        new CountFacetRequest(new CategoryPath(\"b\"), 10));\n    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);\n    final FacetsCollector fc = FacetsCollector.create(fa);\n    // this should populate the cached results, but doing search should clear the cache\n    fc.getFacetResults();\n    newSearcher(r).search(new MatchAllDocsQuery(), fc);\n    \n    List<FacetResult> res1 = fc.getFacetResults();\n    // verify that we didn't get the cached result\n    assertEquals(2, res1.size());\n    for (FacetResult res : res1) {\n      assertEquals(1, res.getFacetResultNode().subResults.size());\n      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);\n    }\n    fc.reset();\n    List<FacetResult> res2 = fc.getFacetResults();\n    assertNotSame(\"reset() should clear the cached results\", res1, res2);\n    \n    IOUtils.close(taxo, taxoDir, r, indexDir);\n  }\n\n","sourceOld":"  @Test\n  public void testReset() throws Exception {\n    Directory indexDir = newDirectory();\n    Directory taxoDir = newDirectory();\n    \n    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);\n    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    \n    FacetFields facetFields = new FacetFields(taxonomyWriter);\n    Document doc = new Document();\n    facetFields.addFields(doc, Arrays.asList(new CategoryPath(\"a/1\", '/'), new CategoryPath(\"b/1\", '/')));\n    iw.addDocument(doc);\n    taxonomyWriter.close();\n    iw.close();\n    \n    DirectoryReader r = DirectoryReader.open(indexDir);\n    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    \n    FacetSearchParams fsp = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"a\"), 10), \n        new CountFacetRequest(new CategoryPath(\"b\"), 10));\n    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new StandardFacetsAccumulator(fsp, r, taxo);\n    final FacetsCollector fc = FacetsCollector.create(fa);\n    // this should populate the cached results, but doing search should clear the cache\n    fc.getFacetResults();\n    newSearcher(r).search(new MatchAllDocsQuery(), fc);\n    \n    List<FacetResult> res1 = fc.getFacetResults();\n    // verify that we didn't get the cached result\n    assertEquals(2, res1.size());\n    for (FacetResult res : res1) {\n      assertEquals(1, res.getFacetResultNode().subResults.size());\n      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);\n    }\n    fc.reset();\n    List<FacetResult> res2 = fc.getFacetResults();\n    assertNotSame(\"reset() should clear the cached results\", res1, res2);\n    \n    IOUtils.close(taxo, taxoDir, r, indexDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","sourceNew":"  @Test\n  public void testReset() throws Exception {\n    Directory indexDir = newDirectory();\n    Directory taxoDir = newDirectory();\n    \n    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);\n    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    \n    FacetFields facetFields = new FacetFields(taxonomyWriter);\n    Document doc = new Document();\n    facetFields.addFields(doc, Arrays.asList(new CategoryPath(\"a/1\", '/'), new CategoryPath(\"b/1\", '/')));\n    iw.addDocument(doc);\n    taxonomyWriter.close();\n    iw.close();\n    \n    DirectoryReader r = DirectoryReader.open(indexDir);\n    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    \n    FacetSearchParams fsp = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"a\"), 10), \n        new CountFacetRequest(new CategoryPath(\"b\"), 10));\n    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);\n    final FacetsCollector fc = FacetsCollector.create(fa);\n    // this should populate the cached results, but doing search should clear the cache\n    fc.getFacetResults();\n    newSearcher(r).search(new MatchAllDocsQuery(), fc);\n    \n    List<FacetResult> res1 = fc.getFacetResults();\n    // verify that we didn't get the cached result\n    assertEquals(2, res1.size());\n    for (FacetResult res : res1) {\n      assertEquals(1, res.getFacetResultNode().subResults.size());\n      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);\n    }\n    fc.reset();\n    List<FacetResult> res2 = fc.getFacetResults();\n    assertNotSame(\"reset() should clear the cached results\", res1, res2);\n    \n    IOUtils.close(taxo, taxoDir, r, indexDir);\n  }\n\n","sourceOld":"  @Test\n  public void testReset() throws Exception {\n    Directory indexDir = newDirectory();\n    Directory taxoDir = newDirectory();\n    \n    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);\n    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    \n    FacetFields facetFields = new FacetFields(taxonomyWriter);\n    Document doc = new Document();\n    facetFields.addFields(doc, Arrays.asList(new CategoryPath(\"a/1\", '/'), new CategoryPath(\"b/1\", '/')));\n    iw.addDocument(doc);\n    taxonomyWriter.close();\n    iw.close();\n    \n    DirectoryReader r = DirectoryReader.open(indexDir);\n    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    \n    FacetSearchParams fsp = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"a\"), 10), \n        new CountFacetRequest(new CategoryPath(\"b\"), 10));\n    final FacetsAccumulator fa = random().nextBoolean() ? new FacetsAccumulator(fsp, r, taxo) : new StandardFacetsAccumulator(fsp, r, taxo);\n    final FacetsCollector fc = FacetsCollector.create(fa);\n    // this should populate the cached results, but doing search should clear the cache\n    fc.getFacetResults();\n    newSearcher(r).search(new MatchAllDocsQuery(), fc);\n    \n    List<FacetResult> res1 = fc.getFacetResults();\n    // verify that we didn't get the cached result\n    assertEquals(2, res1.size());\n    for (FacetResult res : res1) {\n      assertEquals(1, res.getFacetResultNode().subResults.size());\n      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);\n    }\n    fc.reset();\n    List<FacetResult> res2 = fc.getFacetResults();\n    assertNotSame(\"reset() should clear the cached results\", res1, res2);\n    \n    IOUtils.close(taxo, taxoDir, r, indexDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c190847801a50f4dd20fd639bdc29b54ea3b288b","date":1384461522,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","sourceNew":"  @Test\n  public void testReset() throws Exception {\n    Directory indexDir = newDirectory();\n    Directory taxoDir = newDirectory();\n    \n    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);\n    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    \n    FacetFields facetFields = new FacetFields(taxonomyWriter);\n    Document doc = new Document();\n    facetFields.addFields(doc, Arrays.asList(new FacetLabel(\"a/1\", '/'), new FacetLabel(\"b/1\", '/')));\n    iw.addDocument(doc);\n    taxonomyWriter.close();\n    iw.close();\n    \n    DirectoryReader r = DirectoryReader.open(indexDir);\n    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    \n    FacetSearchParams fsp = new FacetSearchParams(\n        new CountFacetRequest(new FacetLabel(\"a\"), 10), \n        new CountFacetRequest(new FacetLabel(\"b\"), 10));\n    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);\n    final FacetsCollector fc = FacetsCollector.create(fa);\n    // this should populate the cached results, but doing search should clear the cache\n    fc.getFacetResults();\n    newSearcher(r).search(new MatchAllDocsQuery(), fc);\n    \n    List<FacetResult> res1 = fc.getFacetResults();\n    // verify that we didn't get the cached result\n    assertEquals(2, res1.size());\n    for (FacetResult res : res1) {\n      assertEquals(1, res.getFacetResultNode().subResults.size());\n      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);\n    }\n    fc.reset();\n    List<FacetResult> res2 = fc.getFacetResults();\n    assertNotSame(\"reset() should clear the cached results\", res1, res2);\n    \n    IOUtils.close(taxo, taxoDir, r, indexDir);\n  }\n\n","sourceOld":"  @Test\n  public void testReset() throws Exception {\n    Directory indexDir = newDirectory();\n    Directory taxoDir = newDirectory();\n    \n    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);\n    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    \n    FacetFields facetFields = new FacetFields(taxonomyWriter);\n    Document doc = new Document();\n    facetFields.addFields(doc, Arrays.asList(new CategoryPath(\"a/1\", '/'), new CategoryPath(\"b/1\", '/')));\n    iw.addDocument(doc);\n    taxonomyWriter.close();\n    iw.close();\n    \n    DirectoryReader r = DirectoryReader.open(indexDir);\n    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    \n    FacetSearchParams fsp = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"a\"), 10), \n        new CountFacetRequest(new CategoryPath(\"b\"), 10));\n    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);\n    final FacetsCollector fc = FacetsCollector.create(fa);\n    // this should populate the cached results, but doing search should clear the cache\n    fc.getFacetResults();\n    newSearcher(r).search(new MatchAllDocsQuery(), fc);\n    \n    List<FacetResult> res1 = fc.getFacetResults();\n    // verify that we didn't get the cached result\n    assertEquals(2, res1.size());\n    for (FacetResult res : res1) {\n      assertEquals(1, res.getFacetResultNode().subResults.size());\n      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);\n    }\n    fc.reset();\n    List<FacetResult> res2 = fc.getFacetResults();\n    assertNotSame(\"reset() should clear the cached results\", res1, res2);\n    \n    IOUtils.close(taxo, taxoDir, r, indexDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae75def1e2525383b6e1397ed97c44387da9941c","date":1385249238,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","sourceNew":null,"sourceOld":"  @Test\n  public void testReset() throws Exception {\n    Directory indexDir = newDirectory();\n    Directory taxoDir = newDirectory();\n    \n    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);\n    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    \n    FacetFields facetFields = new FacetFields(taxonomyWriter);\n    Document doc = new Document();\n    facetFields.addFields(doc, Arrays.asList(new FacetLabel(\"a/1\", '/'), new FacetLabel(\"b/1\", '/')));\n    iw.addDocument(doc);\n    taxonomyWriter.close();\n    iw.close();\n    \n    DirectoryReader r = DirectoryReader.open(indexDir);\n    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    \n    FacetSearchParams fsp = new FacetSearchParams(\n        new CountFacetRequest(new FacetLabel(\"a\"), 10), \n        new CountFacetRequest(new FacetLabel(\"b\"), 10));\n    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);\n    final FacetsCollector fc = FacetsCollector.create(fa);\n    // this should populate the cached results, but doing search should clear the cache\n    fc.getFacetResults();\n    newSearcher(r).search(new MatchAllDocsQuery(), fc);\n    \n    List<FacetResult> res1 = fc.getFacetResults();\n    // verify that we didn't get the cached result\n    assertEquals(2, res1.size());\n    for (FacetResult res : res1) {\n      assertEquals(1, res.getFacetResultNode().subResults.size());\n      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);\n    }\n    fc.reset();\n    List<FacetResult> res2 = fc.getFacetResults();\n    assertNotSame(\"reset() should clear the cached results\", res1, res2);\n    \n    IOUtils.close(taxo, taxoDir, r, indexDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc728b07df73b197e6d940d27f9b08b63918f13","date":1388834348,"type":4,"author":"Michael McCandless","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector#testReset().mjava","sourceNew":null,"sourceOld":"  @Test\n  public void testReset() throws Exception {\n    Directory indexDir = newDirectory();\n    Directory taxoDir = newDirectory();\n    \n    TaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);\n    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n    \n    FacetFields facetFields = new FacetFields(taxonomyWriter);\n    Document doc = new Document();\n    facetFields.addFields(doc, Arrays.asList(new CategoryPath(\"a/1\", '/'), new CategoryPath(\"b/1\", '/')));\n    iw.addDocument(doc);\n    taxonomyWriter.close();\n    iw.close();\n    \n    DirectoryReader r = DirectoryReader.open(indexDir);\n    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);\n    \n    FacetSearchParams fsp = new FacetSearchParams(\n        new CountFacetRequest(new CategoryPath(\"a\"), 10), \n        new CountFacetRequest(new CategoryPath(\"b\"), 10));\n    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);\n    final FacetsCollector fc = FacetsCollector.create(fa);\n    // this should populate the cached results, but doing search should clear the cache\n    fc.getFacetResults();\n    newSearcher(r).search(new MatchAllDocsQuery(), fc);\n    \n    List<FacetResult> res1 = fc.getFacetResults();\n    // verify that we didn't get the cached result\n    assertEquals(2, res1.size());\n    for (FacetResult res : res1) {\n      assertEquals(1, res.getFacetResultNode().subResults.size());\n      assertEquals(1, (int) res.getFacetResultNode().subResults.get(0).value);\n    }\n    fc.reset();\n    List<FacetResult> res2 = fc.getFacetResults();\n    assertNotSame(\"reset() should clear the cached results\", res1, res2);\n    \n    IOUtils.close(taxo, taxoDir, r, indexDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6":["6249cba93d7ad1bf6f5a225c34fbe3d547ed9f49"],"004aa3627d484d9bc0a4281c12c40240ceeaf75a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["ad1f7eb1a6bbf19f3d32b3baf2dee3db844eccdc"],"3cc728b07df73b197e6d940d27f9b08b63918f13":["d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6","ae75def1e2525383b6e1397ed97c44387da9941c"],"6249cba93d7ad1bf6f5a225c34fbe3d547ed9f49":["ad1f7eb1a6bbf19f3d32b3baf2dee3db844eccdc"],"ad1f7eb1a6bbf19f3d32b3baf2dee3db844eccdc":["004aa3627d484d9bc0a4281c12c40240ceeaf75a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"ae75def1e2525383b6e1397ed97c44387da9941c":["c190847801a50f4dd20fd639bdc29b54ea3b288b"],"c190847801a50f4dd20fd639bdc29b54ea3b288b":["d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6"]},"commit2Childs":{"d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6":["3cc728b07df73b197e6d940d27f9b08b63918f13","c190847801a50f4dd20fd639bdc29b54ea3b288b"],"004aa3627d484d9bc0a4281c12c40240ceeaf75a":["ad1f7eb1a6bbf19f3d32b3baf2dee3db844eccdc"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["004aa3627d484d9bc0a4281c12c40240ceeaf75a"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"3cc728b07df73b197e6d940d27f9b08b63918f13":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6249cba93d7ad1bf6f5a225c34fbe3d547ed9f49":["d9b07d1cdffdee4f4bb3cef8670f6066cf6f64e6"],"ad1f7eb1a6bbf19f3d32b3baf2dee3db844eccdc":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","6249cba93d7ad1bf6f5a225c34fbe3d547ed9f49"],"ae75def1e2525383b6e1397ed97c44387da9941c":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"c190847801a50f4dd20fd639bdc29b54ea3b288b":["ae75def1e2525383b6e1397ed97c44387da9941c"]},"heads":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}