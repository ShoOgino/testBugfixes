{"path":"lucene/test-framework/src/java/org/apache/lucene/analysis/VocabularyAssert#assertVocabulary(Analyzer,InputStream).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/VocabularyAssert#assertVocabulary(Analyzer,InputStream).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/analysis/VocabularyAssert#assertVocabulary(Analyzer,InputStream).mjava","sourceNew":"  /** Run a vocabulary test against one file: tab separated. */\n  public static void assertVocabulary(Analyzer a, InputStream vocOut)\n  throws IOException {\n    BufferedReader vocReader = new BufferedReader(\n        new InputStreamReader(vocOut, \"UTF-8\"));\n    String inputLine = null;\n    while ((inputLine = vocReader.readLine()) != null) {\n      if (inputLine.startsWith(\"#\") || inputLine.trim().length() == 0)\n        continue; /* comment */\n      String words[] = inputLine.split(\"\\t\");\n      BaseTokenStreamTestCase.checkOneTermReuse(a, words[0], words[1]);\n    }\n  }\n\n","sourceOld":"  /** Run a vocabulary test against one file: tab separated. */\n  public static void assertVocabulary(Analyzer a, InputStream vocOut)\n  throws IOException {\n    BufferedReader vocReader = new BufferedReader(\n        new InputStreamReader(vocOut, \"UTF-8\"));\n    String inputLine = null;\n    while ((inputLine = vocReader.readLine()) != null) {\n      if (inputLine.startsWith(\"#\") || inputLine.trim().length() == 0)\n        continue; /* comment */\n      String words[] = inputLine.split(\"\\t\");\n      BaseTokenStreamTestCase.checkOneTermReuse(a, words[0], words[1]);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"df1b735b811bfe6055a98336ee8dfd1e43cf2dc0","date":1379858263,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/VocabularyAssert#assertVocabulary(Analyzer,InputStream).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/VocabularyAssert#assertVocabulary(Analyzer,InputStream).mjava","sourceNew":"  /** Run a vocabulary test against one file: tab separated. */\n  public static void assertVocabulary(Analyzer a, InputStream vocOut)\n  throws IOException {\n    BufferedReader vocReader = new BufferedReader(\n        new InputStreamReader(vocOut, \"UTF-8\"));\n    String inputLine = null;\n    while ((inputLine = vocReader.readLine()) != null) {\n      if (inputLine.startsWith(\"#\") || inputLine.trim().length() == 0)\n        continue; /* comment */\n      String words[] = inputLine.split(\"\\t\");\n      BaseTokenStreamTestCase.checkOneTerm(a, words[0], words[1]);\n    }\n  }\n\n","sourceOld":"  /** Run a vocabulary test against one file: tab separated. */\n  public static void assertVocabulary(Analyzer a, InputStream vocOut)\n  throws IOException {\n    BufferedReader vocReader = new BufferedReader(\n        new InputStreamReader(vocOut, \"UTF-8\"));\n    String inputLine = null;\n    while ((inputLine = vocReader.readLine()) != null) {\n      if (inputLine.startsWith(\"#\") || inputLine.trim().length() == 0)\n        continue; /* comment */\n      String words[] = inputLine.split(\"\\t\");\n      BaseTokenStreamTestCase.checkOneTermReuse(a, words[0], words[1]);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7d89d7e4e5101347833eea558851bf4209218619","date":1396265641,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/VocabularyAssert#assertVocabulary(Analyzer,InputStream).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/VocabularyAssert#assertVocabulary(Analyzer,InputStream).mjava","sourceNew":"  /** Run a vocabulary test against one file: tab separated. */\n  public static void assertVocabulary(Analyzer a, InputStream vocOut)\n  throws IOException {\n    BufferedReader vocReader = new BufferedReader(\n        new InputStreamReader(vocOut, StandardCharsets.UTF_8));\n    String inputLine = null;\n    while ((inputLine = vocReader.readLine()) != null) {\n      if (inputLine.startsWith(\"#\") || inputLine.trim().length() == 0)\n        continue; /* comment */\n      String words[] = inputLine.split(\"\\t\");\n      BaseTokenStreamTestCase.checkOneTerm(a, words[0], words[1]);\n    }\n  }\n\n","sourceOld":"  /** Run a vocabulary test against one file: tab separated. */\n  public static void assertVocabulary(Analyzer a, InputStream vocOut)\n  throws IOException {\n    BufferedReader vocReader = new BufferedReader(\n        new InputStreamReader(vocOut, \"UTF-8\"));\n    String inputLine = null;\n    while ((inputLine = vocReader.readLine()) != null) {\n      if (inputLine.startsWith(\"#\") || inputLine.trim().length() == 0)\n        continue; /* comment */\n      String words[] = inputLine.split(\"\\t\");\n      BaseTokenStreamTestCase.checkOneTerm(a, words[0], words[1]);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/VocabularyAssert#assertVocabulary(Analyzer,InputStream).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/VocabularyAssert#assertVocabulary(Analyzer,InputStream).mjava","sourceNew":"  /** Run a vocabulary test against one file: tab separated. */\n  public static void assertVocabulary(Analyzer a, InputStream vocOut)\n  throws IOException {\n    BufferedReader vocReader = new BufferedReader(\n        new InputStreamReader(vocOut, StandardCharsets.UTF_8));\n    String inputLine = null;\n    while ((inputLine = vocReader.readLine()) != null) {\n      if (inputLine.startsWith(\"#\") || inputLine.trim().length() == 0)\n        continue; /* comment */\n      String words[] = inputLine.split(\"\\t\");\n      BaseTokenStreamTestCase.checkOneTerm(a, words[0], words[1]);\n    }\n  }\n\n","sourceOld":"  /** Run a vocabulary test against one file: tab separated. */\n  public static void assertVocabulary(Analyzer a, InputStream vocOut)\n  throws IOException {\n    BufferedReader vocReader = new BufferedReader(\n        new InputStreamReader(vocOut, \"UTF-8\"));\n    String inputLine = null;\n    while ((inputLine = vocReader.readLine()) != null) {\n      if (inputLine.startsWith(\"#\") || inputLine.trim().length() == 0)\n        continue; /* comment */\n      String words[] = inputLine.split(\"\\t\");\n      BaseTokenStreamTestCase.checkOneTerm(a, words[0], words[1]);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5eb2511ababf862ea11e10761c70ee560cd84510":["df1b735b811bfe6055a98336ee8dfd1e43cf2dc0","7d89d7e4e5101347833eea558851bf4209218619"],"df1b735b811bfe6055a98336ee8dfd1e43cf2dc0":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7d89d7e4e5101347833eea558851bf4209218619":["df1b735b811bfe6055a98336ee8dfd1e43cf2dc0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["7d89d7e4e5101347833eea558851bf4209218619"]},"commit2Childs":{"5eb2511ababf862ea11e10761c70ee560cd84510":[],"df1b735b811bfe6055a98336ee8dfd1e43cf2dc0":["5eb2511ababf862ea11e10761c70ee560cd84510","7d89d7e4e5101347833eea558851bf4209218619"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["df1b735b811bfe6055a98336ee8dfd1e43cf2dc0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"7d89d7e4e5101347833eea558851bf4209218619":["5eb2511ababf862ea11e10761c70ee560cd84510","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5eb2511ababf862ea11e10761c70ee560cd84510","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}