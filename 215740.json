{"path":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","commits":[{"id":"849494cf2f3a96af5c8c84995108ddd8456fcd04","date":1372277913,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED,\n        true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, true);\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      BlockCache blockCache;\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int _1024Size = params.getInt(\"solr.hdfs.blockcache.bufferstore.1024\",\n          8192);\n      int _8192Size = params.getInt(\"solr.hdfs.blockcache.bufferstore.8192\",\n          8192);\n      \n      BufferStore.init(_1024Size, _8192Size, metrics);\n      long totalMemory = (long) bankCount * (long) numberOfBlocksPerBank\n          * (long) blockSize;\n      try {\n        blockCache = new BlockCache(metrics, directAllocation, totalMemory,\n            slabSize, blockSize);\n      } catch (OutOfMemoryError e) {\n        throw new RuntimeException(\n            \"The max direct memory is likely too low.  Either increase it (by adding -XX:MaxDirectMemorySize=<size>g -XX:+UseLargePages to your containers startup args)\"\n                + \" or disable direct allocation using solr.hdfs.blockcache.direct.memory.allocation=false in solrconfig.xml. If you are putting the block cache on the heap,\"\n                + \" your java heap size might not be large enough.\"\n                + \" Failed allocating ~\" + totalMemory / 1000000.0 + \" MB.\", e);\n      }\n      Cache cache = new BlockDirectoryCache(blockCache, metrics);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(\"solrcore\", hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["bba91e4be653c9cabbdd70dd59c7e7379c43f0e8","87df8312996aa71322478e54c0c1c43233e0b491","26bd56bd7f06194390617d646d6b9a24a7a472dd"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED,\n        true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, true);\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      BlockCache blockCache;\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int _1024Size = params.getInt(\"solr.hdfs.blockcache.bufferstore.1024\",\n          8192);\n      int _8192Size = params.getInt(\"solr.hdfs.blockcache.bufferstore.8192\",\n          8192);\n      \n      BufferStore.init(_1024Size, _8192Size, metrics);\n      long totalMemory = (long) bankCount * (long) numberOfBlocksPerBank\n          * (long) blockSize;\n      try {\n        blockCache = new BlockCache(metrics, directAllocation, totalMemory,\n            slabSize, blockSize);\n      } catch (OutOfMemoryError e) {\n        throw new RuntimeException(\n            \"The max direct memory is likely too low.  Either increase it (by adding -XX:MaxDirectMemorySize=<size>g -XX:+UseLargePages to your containers startup args)\"\n                + \" or disable direct allocation using solr.hdfs.blockcache.direct.memory.allocation=false in solrconfig.xml. If you are putting the block cache on the heap,\"\n                + \" your java heap size might not be large enough.\"\n                + \" Failed allocating ~\" + totalMemory / 1000000.0 + \" MB.\", e);\n      }\n      Cache cache = new BlockDirectoryCache(blockCache, metrics);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(\"solrcore\", hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bba91e4be653c9cabbdd70dd59c7e7379c43f0e8","date":1390842189,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED,\n        true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, true);\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      BlockCache blockCache;\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BufferStore.initNewBuffer(bufferSize, bufferCount);\n      long totalMemory = (long) bankCount * (long) numberOfBlocksPerBank\n          * (long) blockSize;\n      try {\n        blockCache = new BlockCache(metrics, directAllocation, totalMemory,\n            slabSize, blockSize);\n      } catch (OutOfMemoryError e) {\n        throw new RuntimeException(\n            \"The max direct memory is likely too low.  Either increase it (by adding -XX:MaxDirectMemorySize=<size>g -XX:+UseLargePages to your containers startup args)\"\n                + \" or disable direct allocation using solr.hdfs.blockcache.direct.memory.allocation=false in solrconfig.xml. If you are putting the block cache on the heap,\"\n                + \" your java heap size might not be large enough.\"\n                + \" Failed allocating ~\" + totalMemory / 1000000.0 + \" MB.\", e);\n      }\n      Cache cache = new BlockDirectoryCache(blockCache, metrics);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(\"solrcore\", hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED,\n        true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, true);\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      BlockCache blockCache;\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int _1024Size = params.getInt(\"solr.hdfs.blockcache.bufferstore.1024\",\n          8192);\n      int _8192Size = params.getInt(\"solr.hdfs.blockcache.bufferstore.8192\",\n          8192);\n      \n      BufferStore.init(_1024Size, _8192Size, metrics);\n      long totalMemory = (long) bankCount * (long) numberOfBlocksPerBank\n          * (long) blockSize;\n      try {\n        blockCache = new BlockCache(metrics, directAllocation, totalMemory,\n            slabSize, blockSize);\n      } catch (OutOfMemoryError e) {\n        throw new RuntimeException(\n            \"The max direct memory is likely too low.  Either increase it (by adding -XX:MaxDirectMemorySize=<size>g -XX:+UseLargePages to your containers startup args)\"\n                + \" or disable direct allocation using solr.hdfs.blockcache.direct.memory.allocation=false in solrconfig.xml. If you are putting the block cache on the heap,\"\n                + \" your java heap size might not be large enough.\"\n                + \" Failed allocating ~\" + totalMemory / 1000000.0 + \" MB.\", e);\n      }\n      Cache cache = new BlockDirectoryCache(blockCache, metrics);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(\"solrcore\", hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"19389fe47925b510b2811e2b385a75f7ad19dcca","date":1393903127,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED,\n        true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, true);\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(path, numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED,\n        true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, true);\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      BlockCache blockCache;\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BufferStore.initNewBuffer(bufferSize, bufferCount);\n      long totalMemory = (long) bankCount * (long) numberOfBlocksPerBank\n          * (long) blockSize;\n      try {\n        blockCache = new BlockCache(metrics, directAllocation, totalMemory,\n            slabSize, blockSize);\n      } catch (OutOfMemoryError e) {\n        throw new RuntimeException(\n            \"The max direct memory is likely too low.  Either increase it (by adding -XX:MaxDirectMemorySize=<size>g -XX:+UseLargePages to your containers startup args)\"\n                + \" or disable direct allocation using solr.hdfs.blockcache.direct.memory.allocation=false in solrconfig.xml. If you are putting the block cache on the heap,\"\n                + \" your java heap size might not be large enough.\"\n                + \" Failed allocating ~\" + totalMemory / 1000000.0 + \" MB.\", e);\n      }\n      Cache cache = new BlockDirectoryCache(blockCache, metrics);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(\"solrcore\", hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":["0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED,\n        true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, true);\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(path, numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED,\n        true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, true);\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      BlockCache blockCache;\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BufferStore.initNewBuffer(bufferSize, bufferCount);\n      long totalMemory = (long) bankCount * (long) numberOfBlocksPerBank\n          * (long) blockSize;\n      try {\n        blockCache = new BlockCache(metrics, directAllocation, totalMemory,\n            slabSize, blockSize);\n      } catch (OutOfMemoryError e) {\n        throw new RuntimeException(\n            \"The max direct memory is likely too low.  Either increase it (by adding -XX:MaxDirectMemorySize=<size>g -XX:+UseLargePages to your containers startup args)\"\n                + \" or disable direct allocation using solr.hdfs.blockcache.direct.memory.allocation=false in solrconfig.xml. If you are putting the block cache on the heap,\"\n                + \" your java heap size might not be large enough.\"\n                + \" Failed allocating ~\" + totalMemory / 1000000.0 + \" MB.\", e);\n      }\n      Cache cache = new BlockDirectoryCache(blockCache, metrics);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(\"solrcore\", hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c9a483962b63ae70a38c025e3de139815a6a221","date":1395710858,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED, true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, true);\n    \n    if (blockCacheWriteEnabled) {\n      LOG.warn(\"Using \" + BLOCKCACHE_WRITE_ENABLED + \" is currently buggy and can result in readers seeing a corrupted view of the index.\");\n    }\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(path, numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED,\n        true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, true);\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(path, numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294","date":1408633409,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED, true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, true);\n    \n    if (blockCacheWriteEnabled) {\n      LOG.warn(\"Using \" + BLOCKCACHE_WRITE_ENABLED + \" is currently buggy and can result in readers seeing a corrupted view of the index.\");\n    }\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED, true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, true);\n    \n    if (blockCacheWriteEnabled) {\n      LOG.warn(\"Using \" + BLOCKCACHE_WRITE_ENABLED + \" is currently buggy and can result in readers seeing a corrupted view of the index.\");\n    }\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(path, numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":["19389fe47925b510b2811e2b385a75f7ad19dcca"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9fdfca640f7fd586475e00772ce20b8c9be6ebf4","date":1408910529,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED, true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, false);\n    \n    if (blockCacheWriteEnabled) {\n      LOG.warn(\"Using \" + BLOCKCACHE_WRITE_ENABLED + \" is currently buggy and can result in readers seeing a corrupted view of the index.\");\n    }\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED, true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, true);\n    \n    if (blockCacheWriteEnabled) {\n      LOG.warn(\"Using \" + BLOCKCACHE_WRITE_ENABLED + \" is currently buggy and can result in readers seeing a corrupted view of the index.\");\n    }\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0a506fe165b26e024afa1aec8a4a7d758e837ff","date":1410971446,"type":5,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,LockFactory,DirContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/core/HdfsDirectoryFactory#create(String,DirContext).mjava","sourceNew":"  @Override\n  protected Directory create(String path, LockFactory lockFactory, DirContext dirContext) throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED, true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, false);\n    \n    if (blockCacheWriteEnabled) {\n      LOG.warn(\"Using \" + BLOCKCACHE_WRITE_ENABLED + \" is currently buggy and can result in readers seeing a corrupted view of the index.\");\n    }\n    \n    final Directory dir;\n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), lockFactory, conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), lockFactory, conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","sourceOld":"  @Override\n  protected Directory create(String path, DirContext dirContext)\n      throws IOException {\n    LOG.info(\"creating directory factory for path {}\", path);\n    Configuration conf = getConf();\n    \n    if (metrics == null) {\n      metrics = new Metrics(conf);\n    }\n    \n    boolean blockCacheEnabled = params.getBool(BLOCKCACHE_ENABLED, true);\n    boolean blockCacheGlobal = params.getBool(BLOCKCACHE_GLOBAL, false); // default to false for back compat\n    boolean blockCacheReadEnabled = params.getBool(BLOCKCACHE_READ_ENABLED, true);\n    boolean blockCacheWriteEnabled = params.getBool(BLOCKCACHE_WRITE_ENABLED, false);\n    \n    if (blockCacheWriteEnabled) {\n      LOG.warn(\"Using \" + BLOCKCACHE_WRITE_ENABLED + \" is currently buggy and can result in readers seeing a corrupted view of the index.\");\n    }\n    Directory dir = null;\n    \n    if (blockCacheEnabled && dirContext != DirContext.META_DATA) {\n      int numberOfBlocksPerBank = params.getInt(NUMBEROFBLOCKSPERBANK, 16384);\n      \n      int blockSize = BlockDirectory.BLOCK_SIZE;\n      \n      int bankCount = params.getInt(BLOCKCACHE_SLAB_COUNT, 1);\n      \n      boolean directAllocation = params.getBool(\n          BLOCKCACHE_DIRECT_MEMORY_ALLOCATION, true);\n      \n      int slabSize = numberOfBlocksPerBank * blockSize;\n      LOG.info(\n          \"Number of slabs of block cache [{}] with direct memory allocation set to [{}]\",\n          bankCount, directAllocation);\n      LOG.info(\n          \"Block cache target memory usage, slab size of [{}] will allocate [{}] slabs and use ~[{}] bytes\",\n          new Object[] {slabSize, bankCount,\n              ((long) bankCount * (long) slabSize)});\n      \n      int bufferSize = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffersize\", 128);\n      int bufferCount = params.getInt(\"solr.hdfs.blockcache.bufferstore.buffercount\", 128 * 128);\n      \n      BlockCache blockCache = getBlockDirectoryCache(numberOfBlocksPerBank,\n          blockSize, bankCount, directAllocation, slabSize,\n          bufferSize, bufferCount, blockCacheGlobal);\n      \n      Cache cache = new BlockDirectoryCache(blockCache, path, metrics, blockCacheGlobal);\n      HdfsDirectory hdfsDirectory = new HdfsDirectory(new Path(path), conf);\n      dir = new BlockDirectory(path, hdfsDirectory, cache, null,\n          blockCacheReadEnabled, blockCacheWriteEnabled);\n    } else {\n      dir = new HdfsDirectory(new Path(path), conf);\n    }\n    \n    boolean nrtCachingDirectory = params.getBool(NRTCACHINGDIRECTORY_ENABLE, true);\n    if (nrtCachingDirectory) {\n      double nrtCacheMaxMergeSizeMB = params.getInt(\n          NRTCACHINGDIRECTORY_MAXMERGESIZEMB, 16);\n      double nrtCacheMaxCacheMB = params.getInt(NRTCACHINGDIRECTORY_MAXCACHEMB,\n          192);\n      \n      return new NRTCachingDirectory(dir, nrtCacheMaxMergeSizeMB,\n          nrtCacheMaxCacheMB);\n    }\n    return dir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a0a506fe165b26e024afa1aec8a4a7d758e837ff":["9fdfca640f7fd586475e00772ce20b8c9be6ebf4"],"19389fe47925b510b2811e2b385a75f7ad19dcca":["bba91e4be653c9cabbdd70dd59c7e7379c43f0e8"],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294":["1c9a483962b63ae70a38c025e3de139815a6a221"],"1c9a483962b63ae70a38c025e3de139815a6a221":["19389fe47925b510b2811e2b385a75f7ad19dcca"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["bba91e4be653c9cabbdd70dd59c7e7379c43f0e8","19389fe47925b510b2811e2b385a75f7ad19dcca"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","849494cf2f3a96af5c8c84995108ddd8456fcd04"],"bba91e4be653c9cabbdd70dd59c7e7379c43f0e8":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9fdfca640f7fd586475e00772ce20b8c9be6ebf4":["0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0a506fe165b26e024afa1aec8a4a7d758e837ff"]},"commit2Childs":{"a0a506fe165b26e024afa1aec8a4a7d758e837ff":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"19389fe47925b510b2811e2b385a75f7ad19dcca":["1c9a483962b63ae70a38c025e3de139815a6a221","96ea64d994d340044e0d57aeb6a5871539d10ca5"],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["37a0f60745e53927c4c876cfe5b5a58170f0646c","bba91e4be653c9cabbdd70dd59c7e7379c43f0e8"],"0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294":["9fdfca640f7fd586475e00772ce20b8c9be6ebf4"],"1c9a483962b63ae70a38c025e3de139815a6a221":["0d29d666b8eaa2fe8efe1e0d42fc8e32876d7294"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"bba91e4be653c9cabbdd70dd59c7e7379c43f0e8":["19389fe47925b510b2811e2b385a75f7ad19dcca","96ea64d994d340044e0d57aeb6a5871539d10ca5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["849494cf2f3a96af5c8c84995108ddd8456fcd04","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"9fdfca640f7fd586475e00772ce20b8c9be6ebf4":["a0a506fe165b26e024afa1aec8a4a7d758e837ff"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}