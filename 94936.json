{"path":"lucene/backwards/src/test/org/apache/lucene/search/TestPhrasePrefixQuery#testPhrasePrefix().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/test/org/apache/lucene/search/TestPhrasePrefixQuery#testPhrasePrefix().mjava","pathOld":"backwards/src/test/org/apache/lucene/search/TestPhrasePrefixQuery#testPhrasePrefix().mjava","sourceNew":"    /**\n     *\n     */\n    public void testPhrasePrefix()\n        throws IOException\n    {\n        RAMDirectory indexStore = new RAMDirectory();\n        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        Document doc1 = new Document();\n        Document doc2 = new Document();\n        Document doc3 = new Document();\n        Document doc4 = new Document();\n        Document doc5 = new Document();\n        doc1.add(new Field(\"body\", \"blueberry pie\", Field.Store.YES, Field.Index.ANALYZED));\n        doc2.add(new Field(\"body\", \"blueberry strudel\", Field.Store.YES, Field.Index.ANALYZED));\n        doc3.add(new Field(\"body\", \"blueberry pizza\", Field.Store.YES, Field.Index.ANALYZED));\n        doc4.add(new Field(\"body\", \"blueberry chewing gum\", Field.Store.YES, Field.Index.ANALYZED));\n        doc5.add(new Field(\"body\", \"piccadilly circus\", Field.Store.YES, Field.Index.ANALYZED));\n        writer.addDocument(doc1);\n        writer.addDocument(doc2);\n        writer.addDocument(doc3);\n        writer.addDocument(doc4);\n        writer.addDocument(doc5);\n        writer.optimize();\n        writer.close();\n\n        IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n        //PhrasePrefixQuery query1 = new PhrasePrefixQuery();\n        MultiPhraseQuery query1 = new MultiPhraseQuery();\n        //PhrasePrefixQuery query2 = new PhrasePrefixQuery();\n        MultiPhraseQuery query2 = new MultiPhraseQuery();\n        query1.add(new Term(\"body\", \"blueberry\"));\n        query2.add(new Term(\"body\", \"strawberry\"));\n\n        LinkedList termsWithPrefix = new LinkedList();\n        IndexReader ir = IndexReader.open(indexStore, true);\n\n        // this TermEnum gives \"piccadilly\", \"pie\" and \"pizza\".\n        String prefix = \"pi\";\n        TermEnum te = ir.terms(new Term(\"body\", prefix + \"*\"));\n        do {\n            if (te.term().text().startsWith(prefix))\n            {\n                termsWithPrefix.add(te.term());\n            }\n        } while (te.next());\n\n        query1.add((Term[])termsWithPrefix.toArray(new Term[0]));\n        query2.add((Term[])termsWithPrefix.toArray(new Term[0]));\n\n        ScoreDoc[] result;\n        result = searcher.search(query1, null, 1000).scoreDocs;\n        assertEquals(2, result.length);\n\n        result = searcher.search(query2, null, 1000).scoreDocs;\n        assertEquals(0, result.length);\n    }\n\n","sourceOld":"    /**\n     *\n     */\n    public void testPhrasePrefix()\n        throws IOException\n    {\n        RAMDirectory indexStore = new RAMDirectory();\n        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        Document doc1 = new Document();\n        Document doc2 = new Document();\n        Document doc3 = new Document();\n        Document doc4 = new Document();\n        Document doc5 = new Document();\n        doc1.add(new Field(\"body\", \"blueberry pie\", Field.Store.YES, Field.Index.ANALYZED));\n        doc2.add(new Field(\"body\", \"blueberry strudel\", Field.Store.YES, Field.Index.ANALYZED));\n        doc3.add(new Field(\"body\", \"blueberry pizza\", Field.Store.YES, Field.Index.ANALYZED));\n        doc4.add(new Field(\"body\", \"blueberry chewing gum\", Field.Store.YES, Field.Index.ANALYZED));\n        doc5.add(new Field(\"body\", \"piccadilly circus\", Field.Store.YES, Field.Index.ANALYZED));\n        writer.addDocument(doc1);\n        writer.addDocument(doc2);\n        writer.addDocument(doc3);\n        writer.addDocument(doc4);\n        writer.addDocument(doc5);\n        writer.optimize();\n        writer.close();\n\n        IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n        //PhrasePrefixQuery query1 = new PhrasePrefixQuery();\n        MultiPhraseQuery query1 = new MultiPhraseQuery();\n        //PhrasePrefixQuery query2 = new PhrasePrefixQuery();\n        MultiPhraseQuery query2 = new MultiPhraseQuery();\n        query1.add(new Term(\"body\", \"blueberry\"));\n        query2.add(new Term(\"body\", \"strawberry\"));\n\n        LinkedList termsWithPrefix = new LinkedList();\n        IndexReader ir = IndexReader.open(indexStore, true);\n\n        // this TermEnum gives \"piccadilly\", \"pie\" and \"pizza\".\n        String prefix = \"pi\";\n        TermEnum te = ir.terms(new Term(\"body\", prefix + \"*\"));\n        do {\n            if (te.term().text().startsWith(prefix))\n            {\n                termsWithPrefix.add(te.term());\n            }\n        } while (te.next());\n\n        query1.add((Term[])termsWithPrefix.toArray(new Term[0]));\n        query2.add((Term[])termsWithPrefix.toArray(new Term[0]));\n\n        ScoreDoc[] result;\n        result = searcher.search(query1, null, 1000).scoreDocs;\n        assertEquals(2, result.length);\n\n        result = searcher.search(query2, null, 1000).scoreDocs;\n        assertEquals(0, result.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6","date":1272983566,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backwards/src/test/org/apache/lucene/search/TestPhrasePrefixQuery#testPhrasePrefix().mjava","sourceNew":null,"sourceOld":"    /**\n     *\n     */\n    public void testPhrasePrefix()\n        throws IOException\n    {\n        RAMDirectory indexStore = new RAMDirectory();\n        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n        Document doc1 = new Document();\n        Document doc2 = new Document();\n        Document doc3 = new Document();\n        Document doc4 = new Document();\n        Document doc5 = new Document();\n        doc1.add(new Field(\"body\", \"blueberry pie\", Field.Store.YES, Field.Index.ANALYZED));\n        doc2.add(new Field(\"body\", \"blueberry strudel\", Field.Store.YES, Field.Index.ANALYZED));\n        doc3.add(new Field(\"body\", \"blueberry pizza\", Field.Store.YES, Field.Index.ANALYZED));\n        doc4.add(new Field(\"body\", \"blueberry chewing gum\", Field.Store.YES, Field.Index.ANALYZED));\n        doc5.add(new Field(\"body\", \"piccadilly circus\", Field.Store.YES, Field.Index.ANALYZED));\n        writer.addDocument(doc1);\n        writer.addDocument(doc2);\n        writer.addDocument(doc3);\n        writer.addDocument(doc4);\n        writer.addDocument(doc5);\n        writer.optimize();\n        writer.close();\n\n        IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n        //PhrasePrefixQuery query1 = new PhrasePrefixQuery();\n        MultiPhraseQuery query1 = new MultiPhraseQuery();\n        //PhrasePrefixQuery query2 = new PhrasePrefixQuery();\n        MultiPhraseQuery query2 = new MultiPhraseQuery();\n        query1.add(new Term(\"body\", \"blueberry\"));\n        query2.add(new Term(\"body\", \"strawberry\"));\n\n        LinkedList termsWithPrefix = new LinkedList();\n        IndexReader ir = IndexReader.open(indexStore, true);\n\n        // this TermEnum gives \"piccadilly\", \"pie\" and \"pizza\".\n        String prefix = \"pi\";\n        TermEnum te = ir.terms(new Term(\"body\", prefix + \"*\"));\n        do {\n            if (te.term().text().startsWith(prefix))\n            {\n                termsWithPrefix.add(te.term());\n            }\n        } while (te.next());\n\n        query1.add((Term[])termsWithPrefix.toArray(new Term[0]));\n        query2.add((Term[])termsWithPrefix.toArray(new Term[0]));\n\n        ScoreDoc[] result;\n        result = searcher.search(query1, null, 1000).scoreDocs;\n        assertEquals(2, result.length);\n\n        result = searcher.search(query2, null, 1000).scoreDocs;\n        assertEquals(0, result.length);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}