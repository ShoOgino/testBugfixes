{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","commits":[{"id":"f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03","date":1377018786,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = BasicAutomata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader);\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = BasicAutomata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader);\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","sourceNew":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = BasicAutomata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = BasicAutomata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader);\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","sourceNew":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = BasicAutomata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.shutdown();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = BasicAutomata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f4ef381bf0c2d618c6db830d3dd668c6901c05a","date":1402592253,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","sourceNew":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final LightAutomaton secondSet = BasicAutomata.makeStringLight(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.shutdown();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = BasicAutomata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.shutdown();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ca1c732df8923f5624f6c06b1dcca9e69d98c96","date":1402957391,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","sourceNew":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = Automata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.shutdown();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final LightAutomaton secondSet = BasicAutomata.makeStringLight(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.shutdown();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c84485629d80d203608e8975a1139de9933cc38","date":1403166128,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","sourceNew":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = Automata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.shutdown();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = BasicAutomata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.shutdown();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","sourceNew":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = Automata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = Automata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.shutdown();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9e1499c5d26c936238506df90a3c02c76707722","date":1434449920,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","sourceNew":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = Automata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery.Builder builder = new PhraseQuery.Builder();\n    builder.add(new Term(\"body\", \"just\"), 0);\n    builder.add(new Term(\"body\", \"test\"), 3);\n    PhraseQuery pq = builder.build();\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = Automata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"body\", \"just\"), 0);\n    pq.add(new Term(\"body\", \"test\"), 3);\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83788ad129a5154d5c6562c4e8ce3db48793aada","date":1532961485,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testStopwordsPosIncHole2().mjava","sourceNew":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = Automata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery.Builder builder = new PhraseQuery.Builder();\n    builder.add(new Term(\"body\", \"just\"), 0);\n    builder.add(new Term(\"body\", \"test\"), 3);\n    PhraseQuery pq = builder.build();\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits.value);\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3849\n  public void testStopwordsPosIncHole2() throws Exception {\n    // use two stopfilters for testing here\n    Directory dir = newDirectory();\n    final Automaton secondSet = Automata.makeString(\"foobar\");\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        TokenStream stream = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);\n        stream = new MockTokenFilter(stream, new CharacterRunAutomaton(secondSet));\n        return new TokenStreamComponents(tokenizer, stream);\n      }\n    };\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, a);\n    Document doc = new Document();\n    doc.add(new TextField(\"body\", \"just a foobar\", Field.Store.NO));\n    doc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\n    iw.addDocument(doc);\n    IndexReader ir = iw.getReader();\n    iw.close();\n    IndexSearcher is = newSearcher(ir);\n    PhraseQuery.Builder builder = new PhraseQuery.Builder();\n    builder.add(new Term(\"body\", \"just\"), 0);\n    builder.add(new Term(\"body\", \"test\"), 3);\n    PhraseQuery pq = builder.build();\n    // body:\"just ? ? test\"\n    assertEquals(1, is.search(pq, 5).totalHits);\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e9e1499c5d26c936238506df90a3c02c76707722":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4ca1c732df8923f5624f6c06b1dcca9e69d98c96":["7f4ef381bf0c2d618c6db830d3dd668c6901c05a"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["5c84485629d80d203608e8975a1139de9933cc38"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"7f4ef381bf0c2d618c6db830d3dd668c6901c05a":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["e9e1499c5d26c936238506df90a3c02c76707722"],"5c84485629d80d203608e8975a1139de9933cc38":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","4ca1c732df8923f5624f6c06b1dcca9e69d98c96"],"f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["83788ad129a5154d5c6562c4e8ce3db48793aada"]},"commit2Childs":{"e9e1499c5d26c936238506df90a3c02c76707722":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03"],"4ca1c732df8923f5624f6c06b1dcca9e69d98c96":["5c84485629d80d203608e8975a1139de9933cc38"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["e9e1499c5d26c936238506df90a3c02c76707722"],"7f4ef381bf0c2d618c6db830d3dd668c6901c05a":["4ca1c732df8923f5624f6c06b1dcca9e69d98c96"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["7f4ef381bf0c2d618c6db830d3dd668c6901c05a","5c84485629d80d203608e8975a1139de9933cc38"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5c84485629d80d203608e8975a1139de9933cc38":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"f3f8a0f8bebc057ea4bdf65150b3fdc539db3d03":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}