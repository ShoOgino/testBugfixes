{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","commits":[{"id":"06663b933cd6a92982d79726136b07915bc86aef","date":1399473227,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-5644: index docs w/ multiple threads but in between flushes we limit how many threads can index concurrently in the next\n  // iteration, and then verify that no more segments were flushed than number of threads:\n  public void testSegmentCountOnFlushRandom() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    int maxThreadStates = TestUtil.nextInt(random(), 1, 12);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: maxThreadStates=\" + maxThreadStates);\n    }\n\n    // Never trigger flushes (so we only flush on getReader):\n    iwc.setMaxBufferedDocs(100000000);\n    iwc.setRAMBufferSizeMB(-1);\n    iwc.setMaxThreadStates(maxThreadStates);\n\n    // Never trigger merges (so we can simplistically count flushed segments):\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    final IndexWriter w = new IndexWriter(dir, iwc);\n\n    // How many threads are indexing in the current cycle:\n    final AtomicInteger indexingCount = new AtomicInteger();\n\n    // How many threads we will use on each cycle:\n    final AtomicInteger maxThreadCount = new AtomicInteger();\n\n    CheckSegmentCount checker = new CheckSegmentCount(w, maxThreadCount, indexingCount);\n\n    // We spin up 10 threads up front, but then in between flushes we limit how many can run on each iteration\n    final int ITERS = 100;\n    Thread[] threads = new Thread[MAX_THREADS_AT_ONCE];\n\n    // We use this to stop all threads once they've indexed their docs in the current iter, and pull a new NRT reader, and verify the\n    // segment count:\n    final CyclicBarrier barrier = new CyclicBarrier(MAX_THREADS_AT_ONCE, checker);\n    \n    for(int i=0;i<threads.length;i++) {\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              for(int iter=0;iter<ITERS;iter++) {\n                if (indexingCount.incrementAndGet() <= maxThreadCount.get()) {\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": do index\");\n                  }\n\n                  // We get to index on this cycle:\n                  Document doc = new Document();\n                  doc.add(newTextField(\"field\", \"here is some text that is a bit longer than normal trivial text\", Field.Store.NO));\n                  for(int j=0;j<200;j++) {\n                    w.addDocument(doc);\n                  }\n                } else {\n                  // We lose: no indexing for us on this cycle\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": don't index\");\n                  }\n                }\n                barrier.await();\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    for(int i=0;i<threads.length;i++) {\n      threads[i].join();\n    }\n\n    IOUtils.close(checker, w, dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d4f1fefe3eaa4cd554cb6ad51a31ef844f15b94","date":1399476468,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","sourceNew":"  // LUCENE-5644: index docs w/ multiple threads but in between flushes we limit how many threads can index concurrently in the next\n  // iteration, and then verify that no more segments were flushed than number of threads:\n  public void testSegmentCountOnFlushRandom() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    int maxThreadStates = TestUtil.nextInt(random(), 1, 12);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: maxThreadStates=\" + maxThreadStates);\n    }\n\n    // Never trigger flushes (so we only flush on getReader):\n    iwc.setMaxBufferedDocs(100000000);\n    iwc.setRAMBufferSizeMB(-1);\n    iwc.setMaxThreadStates(maxThreadStates);\n\n    // Never trigger merges (so we can simplistically count flushed segments):\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    final IndexWriter w = new IndexWriter(dir, iwc);\n\n    // How many threads are indexing in the current cycle:\n    final AtomicInteger indexingCount = new AtomicInteger();\n\n    // How many threads we will use on each cycle:\n    final AtomicInteger maxThreadCount = new AtomicInteger();\n\n    CheckSegmentCount checker = new CheckSegmentCount(w, maxThreadCount, indexingCount);\n\n    // We spin up 10 threads up front, but then in between flushes we limit how many can run on each iteration\n    final int ITERS = 100;\n    Thread[] threads = new Thread[MAX_THREADS_AT_ONCE];\n\n    // We use this to stop all threads once they've indexed their docs in the current iter, and pull a new NRT reader, and verify the\n    // segment count:\n    final CyclicBarrier barrier = new CyclicBarrier(MAX_THREADS_AT_ONCE, checker);\n    \n    for(int i=0;i<threads.length;i++) {\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              for(int iter=0;iter<ITERS;iter++) {\n                if (indexingCount.incrementAndGet() <= maxThreadCount.get()) {\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": do index\");\n                  }\n\n                  // We get to index on this cycle:\n                  Document doc = new Document();\n                  doc.add(new TextField(\"field\", \"here is some text that is a bit longer than normal trivial text\", Field.Store.NO));\n                  for(int j=0;j<200;j++) {\n                    w.addDocument(doc);\n                  }\n                } else {\n                  // We lose: no indexing for us on this cycle\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": don't index\");\n                  }\n                }\n                barrier.await();\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    IOUtils.close(checker, w, dir);\n  }\n\n","sourceOld":"  // LUCENE-5644: index docs w/ multiple threads but in between flushes we limit how many threads can index concurrently in the next\n  // iteration, and then verify that no more segments were flushed than number of threads:\n  public void testSegmentCountOnFlushRandom() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    int maxThreadStates = TestUtil.nextInt(random(), 1, 12);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: maxThreadStates=\" + maxThreadStates);\n    }\n\n    // Never trigger flushes (so we only flush on getReader):\n    iwc.setMaxBufferedDocs(100000000);\n    iwc.setRAMBufferSizeMB(-1);\n    iwc.setMaxThreadStates(maxThreadStates);\n\n    // Never trigger merges (so we can simplistically count flushed segments):\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    final IndexWriter w = new IndexWriter(dir, iwc);\n\n    // How many threads are indexing in the current cycle:\n    final AtomicInteger indexingCount = new AtomicInteger();\n\n    // How many threads we will use on each cycle:\n    final AtomicInteger maxThreadCount = new AtomicInteger();\n\n    CheckSegmentCount checker = new CheckSegmentCount(w, maxThreadCount, indexingCount);\n\n    // We spin up 10 threads up front, but then in between flushes we limit how many can run on each iteration\n    final int ITERS = 100;\n    Thread[] threads = new Thread[MAX_THREADS_AT_ONCE];\n\n    // We use this to stop all threads once they've indexed their docs in the current iter, and pull a new NRT reader, and verify the\n    // segment count:\n    final CyclicBarrier barrier = new CyclicBarrier(MAX_THREADS_AT_ONCE, checker);\n    \n    for(int i=0;i<threads.length;i++) {\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              for(int iter=0;iter<ITERS;iter++) {\n                if (indexingCount.incrementAndGet() <= maxThreadCount.get()) {\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": do index\");\n                  }\n\n                  // We get to index on this cycle:\n                  Document doc = new Document();\n                  doc.add(newTextField(\"field\", \"here is some text that is a bit longer than normal trivial text\", Field.Store.NO));\n                  for(int j=0;j<200;j++) {\n                    w.addDocument(doc);\n                  }\n                } else {\n                  // We lose: no indexing for us on this cycle\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": don't index\");\n                  }\n                }\n                barrier.await();\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    for(int i=0;i<threads.length;i++) {\n      threads[i].join();\n    }\n\n    IOUtils.close(checker, w, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5fcade65c286fca4014dc1dedffed2e912ac7012","date":1399977320,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","sourceNew":"  // LUCENE-5644: index docs w/ multiple threads but in between flushes we limit how many threads can index concurrently in the next\n  // iteration, and then verify that no more segments were flushed than number of threads:\n  public void testSegmentCountOnFlushRandom() throws Exception {\n    Directory dir = newFSDirectory(createTempDir());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    int maxThreadStates = TestUtil.nextInt(random(), 1, 12);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: maxThreadStates=\" + maxThreadStates);\n    }\n\n    // Never trigger flushes (so we only flush on getReader):\n    iwc.setMaxBufferedDocs(100000000);\n    iwc.setRAMBufferSizeMB(-1);\n    iwc.setMaxThreadStates(maxThreadStates);\n\n    // Never trigger merges (so we can simplistically count flushed segments):\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    final IndexWriter w = new IndexWriter(dir, iwc);\n\n    // How many threads are indexing in the current cycle:\n    final AtomicInteger indexingCount = new AtomicInteger();\n\n    // How many threads we will use on each cycle:\n    final AtomicInteger maxThreadCount = new AtomicInteger();\n\n    CheckSegmentCount checker = new CheckSegmentCount(w, maxThreadCount, indexingCount);\n\n    // We spin up 10 threads up front, but then in between flushes we limit how many can run on each iteration\n    final int ITERS = 100;\n    Thread[] threads = new Thread[MAX_THREADS_AT_ONCE];\n\n    // We use this to stop all threads once they've indexed their docs in the current iter, and pull a new NRT reader, and verify the\n    // segment count:\n    final CyclicBarrier barrier = new CyclicBarrier(MAX_THREADS_AT_ONCE, checker);\n    \n    for(int i=0;i<threads.length;i++) {\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              for(int iter=0;iter<ITERS;iter++) {\n                if (indexingCount.incrementAndGet() <= maxThreadCount.get()) {\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": do index\");\n                  }\n\n                  // We get to index on this cycle:\n                  Document doc = new Document();\n                  doc.add(new TextField(\"field\", \"here is some text that is a bit longer than normal trivial text\", Field.Store.NO));\n                  for(int j=0;j<200;j++) {\n                    w.addDocument(doc);\n                  }\n                } else {\n                  // We lose: no indexing for us on this cycle\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": don't index\");\n                  }\n                }\n                barrier.await();\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    IOUtils.close(checker, w, dir);\n  }\n\n","sourceOld":"  // LUCENE-5644: index docs w/ multiple threads but in between flushes we limit how many threads can index concurrently in the next\n  // iteration, and then verify that no more segments were flushed than number of threads:\n  public void testSegmentCountOnFlushRandom() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    int maxThreadStates = TestUtil.nextInt(random(), 1, 12);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: maxThreadStates=\" + maxThreadStates);\n    }\n\n    // Never trigger flushes (so we only flush on getReader):\n    iwc.setMaxBufferedDocs(100000000);\n    iwc.setRAMBufferSizeMB(-1);\n    iwc.setMaxThreadStates(maxThreadStates);\n\n    // Never trigger merges (so we can simplistically count flushed segments):\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    final IndexWriter w = new IndexWriter(dir, iwc);\n\n    // How many threads are indexing in the current cycle:\n    final AtomicInteger indexingCount = new AtomicInteger();\n\n    // How many threads we will use on each cycle:\n    final AtomicInteger maxThreadCount = new AtomicInteger();\n\n    CheckSegmentCount checker = new CheckSegmentCount(w, maxThreadCount, indexingCount);\n\n    // We spin up 10 threads up front, but then in between flushes we limit how many can run on each iteration\n    final int ITERS = 100;\n    Thread[] threads = new Thread[MAX_THREADS_AT_ONCE];\n\n    // We use this to stop all threads once they've indexed their docs in the current iter, and pull a new NRT reader, and verify the\n    // segment count:\n    final CyclicBarrier barrier = new CyclicBarrier(MAX_THREADS_AT_ONCE, checker);\n    \n    for(int i=0;i<threads.length;i++) {\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              for(int iter=0;iter<ITERS;iter++) {\n                if (indexingCount.incrementAndGet() <= maxThreadCount.get()) {\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": do index\");\n                  }\n\n                  // We get to index on this cycle:\n                  Document doc = new Document();\n                  doc.add(new TextField(\"field\", \"here is some text that is a bit longer than normal trivial text\", Field.Store.NO));\n                  for(int j=0;j<200;j++) {\n                    w.addDocument(doc);\n                  }\n                } else {\n                  // We lose: no indexing for us on this cycle\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": don't index\");\n                  }\n                }\n                barrier.await();\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    IOUtils.close(checker, w, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a957bf27202eab1c9ddabc5aa30c7a0db04bbf36","date":1400053604,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","sourceNew":"  // LUCENE-5644: index docs w/ multiple threads but in between flushes we limit how many threads can index concurrently in the next\n  // iteration, and then verify that no more segments were flushed than number of threads:\n  public void testSegmentCountOnFlushRandom() throws Exception {\n    Directory dir = newFSDirectory(createTempDir());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    int maxThreadStates = TestUtil.nextInt(random(), 1, 12);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: maxThreadStates=\" + maxThreadStates);\n    }\n\n    // Never trigger flushes (so we only flush on getReader):\n    iwc.setMaxBufferedDocs(100000000);\n    iwc.setRAMBufferSizeMB(-1);\n    iwc.setMaxThreadStates(maxThreadStates);\n\n    // Never trigger merges (so we can simplistically count flushed segments):\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    final IndexWriter w = new IndexWriter(dir, iwc);\n\n    // How many threads are indexing in the current cycle:\n    final AtomicInteger indexingCount = new AtomicInteger();\n\n    // How many threads we will use on each cycle:\n    final AtomicInteger maxThreadCount = new AtomicInteger();\n\n    CheckSegmentCount checker = new CheckSegmentCount(w, maxThreadCount, indexingCount);\n\n    // We spin up 10 threads up front, but then in between flushes we limit how many can run on each iteration\n    final int ITERS = 100;\n    Thread[] threads = new Thread[MAX_THREADS_AT_ONCE];\n\n    // We use this to stop all threads once they've indexed their docs in the current iter, and pull a new NRT reader, and verify the\n    // segment count:\n    final CyclicBarrier barrier = new CyclicBarrier(MAX_THREADS_AT_ONCE, checker);\n    \n    for(int i=0;i<threads.length;i++) {\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              for(int iter=0;iter<ITERS;iter++) {\n                if (indexingCount.incrementAndGet() <= maxThreadCount.get()) {\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": do index\");\n                  }\n\n                  // We get to index on this cycle:\n                  Document doc = new Document();\n                  doc.add(new TextField(\"field\", \"here is some text that is a bit longer than normal trivial text\", Field.Store.NO));\n                  for(int j=0;j<200;j++) {\n                    w.addDocument(doc);\n                  }\n                } else {\n                  // We lose: no indexing for us on this cycle\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": don't index\");\n                  }\n                }\n                barrier.await();\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    IOUtils.close(checker, w, dir);\n  }\n\n","sourceOld":"  // LUCENE-5644: index docs w/ multiple threads but in between flushes we limit how many threads can index concurrently in the next\n  // iteration, and then verify that no more segments were flushed than number of threads:\n  public void testSegmentCountOnFlushRandom() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    int maxThreadStates = TestUtil.nextInt(random(), 1, 12);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: maxThreadStates=\" + maxThreadStates);\n    }\n\n    // Never trigger flushes (so we only flush on getReader):\n    iwc.setMaxBufferedDocs(100000000);\n    iwc.setRAMBufferSizeMB(-1);\n    iwc.setMaxThreadStates(maxThreadStates);\n\n    // Never trigger merges (so we can simplistically count flushed segments):\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    final IndexWriter w = new IndexWriter(dir, iwc);\n\n    // How many threads are indexing in the current cycle:\n    final AtomicInteger indexingCount = new AtomicInteger();\n\n    // How many threads we will use on each cycle:\n    final AtomicInteger maxThreadCount = new AtomicInteger();\n\n    CheckSegmentCount checker = new CheckSegmentCount(w, maxThreadCount, indexingCount);\n\n    // We spin up 10 threads up front, but then in between flushes we limit how many can run on each iteration\n    final int ITERS = 100;\n    Thread[] threads = new Thread[MAX_THREADS_AT_ONCE];\n\n    // We use this to stop all threads once they've indexed their docs in the current iter, and pull a new NRT reader, and verify the\n    // segment count:\n    final CyclicBarrier barrier = new CyclicBarrier(MAX_THREADS_AT_ONCE, checker);\n    \n    for(int i=0;i<threads.length;i++) {\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              for(int iter=0;iter<ITERS;iter++) {\n                if (indexingCount.incrementAndGet() <= maxThreadCount.get()) {\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": do index\");\n                  }\n\n                  // We get to index on this cycle:\n                  Document doc = new Document();\n                  doc.add(new TextField(\"field\", \"here is some text that is a bit longer than normal trivial text\", Field.Store.NO));\n                  for(int j=0;j<200;j++) {\n                    w.addDocument(doc);\n                  }\n                } else {\n                  // We lose: no indexing for us on this cycle\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": don't index\");\n                  }\n                }\n                barrier.await();\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    IOUtils.close(checker, w, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","sourceNew":"  // LUCENE-5644: index docs w/ multiple threads but in between flushes we limit how many threads can index concurrently in the next\n  // iteration, and then verify that no more segments were flushed than number of threads:\n  public void testSegmentCountOnFlushRandom() throws Exception {\n    Directory dir = newFSDirectory(createTempDir());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n\n    int maxThreadStates = TestUtil.nextInt(random(), 1, 12);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: maxThreadStates=\" + maxThreadStates);\n    }\n\n    // Never trigger flushes (so we only flush on getReader):\n    iwc.setMaxBufferedDocs(100000000);\n    iwc.setRAMBufferSizeMB(-1);\n    iwc.setMaxThreadStates(maxThreadStates);\n\n    // Never trigger merges (so we can simplistically count flushed segments):\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    final IndexWriter w = new IndexWriter(dir, iwc);\n\n    // How many threads are indexing in the current cycle:\n    final AtomicInteger indexingCount = new AtomicInteger();\n\n    // How many threads we will use on each cycle:\n    final AtomicInteger maxThreadCount = new AtomicInteger();\n\n    CheckSegmentCount checker = new CheckSegmentCount(w, maxThreadCount, indexingCount);\n\n    // We spin up 10 threads up front, but then in between flushes we limit how many can run on each iteration\n    final int ITERS = 100;\n    Thread[] threads = new Thread[MAX_THREADS_AT_ONCE];\n\n    // We use this to stop all threads once they've indexed their docs in the current iter, and pull a new NRT reader, and verify the\n    // segment count:\n    final CyclicBarrier barrier = new CyclicBarrier(MAX_THREADS_AT_ONCE, checker);\n    \n    for(int i=0;i<threads.length;i++) {\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              for(int iter=0;iter<ITERS;iter++) {\n                if (indexingCount.incrementAndGet() <= maxThreadCount.get()) {\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": do index\");\n                  }\n\n                  // We get to index on this cycle:\n                  Document doc = new Document();\n                  doc.add(new TextField(\"field\", \"here is some text that is a bit longer than normal trivial text\", Field.Store.NO));\n                  for(int j=0;j<200;j++) {\n                    w.addDocument(doc);\n                  }\n                } else {\n                  // We lose: no indexing for us on this cycle\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": don't index\");\n                  }\n                }\n                barrier.await();\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    IOUtils.close(checker, w, dir);\n  }\n\n","sourceOld":"  // LUCENE-5644: index docs w/ multiple threads but in between flushes we limit how many threads can index concurrently in the next\n  // iteration, and then verify that no more segments were flushed than number of threads:\n  public void testSegmentCountOnFlushRandom() throws Exception {\n    Directory dir = newFSDirectory(createTempDir());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n\n    int maxThreadStates = TestUtil.nextInt(random(), 1, 12);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: maxThreadStates=\" + maxThreadStates);\n    }\n\n    // Never trigger flushes (so we only flush on getReader):\n    iwc.setMaxBufferedDocs(100000000);\n    iwc.setRAMBufferSizeMB(-1);\n    iwc.setMaxThreadStates(maxThreadStates);\n\n    // Never trigger merges (so we can simplistically count flushed segments):\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    final IndexWriter w = new IndexWriter(dir, iwc);\n\n    // How many threads are indexing in the current cycle:\n    final AtomicInteger indexingCount = new AtomicInteger();\n\n    // How many threads we will use on each cycle:\n    final AtomicInteger maxThreadCount = new AtomicInteger();\n\n    CheckSegmentCount checker = new CheckSegmentCount(w, maxThreadCount, indexingCount);\n\n    // We spin up 10 threads up front, but then in between flushes we limit how many can run on each iteration\n    final int ITERS = 100;\n    Thread[] threads = new Thread[MAX_THREADS_AT_ONCE];\n\n    // We use this to stop all threads once they've indexed their docs in the current iter, and pull a new NRT reader, and verify the\n    // segment count:\n    final CyclicBarrier barrier = new CyclicBarrier(MAX_THREADS_AT_ONCE, checker);\n    \n    for(int i=0;i<threads.length;i++) {\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              for(int iter=0;iter<ITERS;iter++) {\n                if (indexingCount.incrementAndGet() <= maxThreadCount.get()) {\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": do index\");\n                  }\n\n                  // We get to index on this cycle:\n                  Document doc = new Document();\n                  doc.add(new TextField(\"field\", \"here is some text that is a bit longer than normal trivial text\", Field.Store.NO));\n                  for(int j=0;j<200;j++) {\n                    w.addDocument(doc);\n                  }\n                } else {\n                  // We lose: no indexing for us on this cycle\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": don't index\");\n                  }\n                }\n                barrier.await();\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    IOUtils.close(checker, w, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8fac5f6bc2cfa00592e549aba4ee7179470f9df3","date":1417028276,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","sourceNew":"  // LUCENE-5644: index docs w/ multiple threads but in between flushes we limit how many threads can index concurrently in the next\n  // iteration, and then verify that no more segments were flushed than number of threads:\n  public void testSegmentCountOnFlushRandom() throws Exception {\n    Directory dir = newFSDirectory(createTempDir());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n\n    int maxThreadStates = TestUtil.nextInt(random(), 1, 12);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: maxThreadStates=\" + maxThreadStates);\n    }\n\n    // Never trigger flushes (so we only flush on getReader):\n    iwc.setMaxBufferedDocs(100000000);\n    iwc.setRAMBufferSizeMB(-1);\n    iwc.setMaxThreadStates(maxThreadStates);\n\n    // Never trigger merges (so we can simplistically count flushed segments):\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    final IndexWriter w = new IndexWriter(dir, iwc);\n\n    // How many threads are indexing in the current cycle:\n    final AtomicInteger indexingCount = new AtomicInteger();\n\n    // How many threads we will use on each cycle:\n    final AtomicInteger maxThreadCount = new AtomicInteger();\n\n    CheckSegmentCount checker = new CheckSegmentCount(w, maxThreadCount, indexingCount);\n\n    // We spin up 10 threads up front, but then in between flushes we limit how many can run on each iteration\n    final int ITERS = TEST_NIGHTLY ? 300 : 10;\n    Thread[] threads = new Thread[MAX_THREADS_AT_ONCE];\n\n    // We use this to stop all threads once they've indexed their docs in the current iter, and pull a new NRT reader, and verify the\n    // segment count:\n    final CyclicBarrier barrier = new CyclicBarrier(MAX_THREADS_AT_ONCE, checker);\n    \n    for(int i=0;i<threads.length;i++) {\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              for(int iter=0;iter<ITERS;iter++) {\n                if (indexingCount.incrementAndGet() <= maxThreadCount.get()) {\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": do index\");\n                  }\n\n                  // We get to index on this cycle:\n                  Document doc = new Document();\n                  doc.add(new TextField(\"field\", \"here is some text that is a bit longer than normal trivial text\", Field.Store.NO));\n                  for(int j=0;j<200;j++) {\n                    w.addDocument(doc);\n                  }\n                } else {\n                  // We lose: no indexing for us on this cycle\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": don't index\");\n                  }\n                }\n                barrier.await();\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    IOUtils.close(checker, w, dir);\n  }\n\n","sourceOld":"  // LUCENE-5644: index docs w/ multiple threads but in between flushes we limit how many threads can index concurrently in the next\n  // iteration, and then verify that no more segments were flushed than number of threads:\n  public void testSegmentCountOnFlushRandom() throws Exception {\n    Directory dir = newFSDirectory(createTempDir());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n\n    int maxThreadStates = TestUtil.nextInt(random(), 1, 12);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: maxThreadStates=\" + maxThreadStates);\n    }\n\n    // Never trigger flushes (so we only flush on getReader):\n    iwc.setMaxBufferedDocs(100000000);\n    iwc.setRAMBufferSizeMB(-1);\n    iwc.setMaxThreadStates(maxThreadStates);\n\n    // Never trigger merges (so we can simplistically count flushed segments):\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    final IndexWriter w = new IndexWriter(dir, iwc);\n\n    // How many threads are indexing in the current cycle:\n    final AtomicInteger indexingCount = new AtomicInteger();\n\n    // How many threads we will use on each cycle:\n    final AtomicInteger maxThreadCount = new AtomicInteger();\n\n    CheckSegmentCount checker = new CheckSegmentCount(w, maxThreadCount, indexingCount);\n\n    // We spin up 10 threads up front, but then in between flushes we limit how many can run on each iteration\n    final int ITERS = 100;\n    Thread[] threads = new Thread[MAX_THREADS_AT_ONCE];\n\n    // We use this to stop all threads once they've indexed their docs in the current iter, and pull a new NRT reader, and verify the\n    // segment count:\n    final CyclicBarrier barrier = new CyclicBarrier(MAX_THREADS_AT_ONCE, checker);\n    \n    for(int i=0;i<threads.length;i++) {\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              for(int iter=0;iter<ITERS;iter++) {\n                if (indexingCount.incrementAndGet() <= maxThreadCount.get()) {\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": do index\");\n                  }\n\n                  // We get to index on this cycle:\n                  Document doc = new Document();\n                  doc.add(new TextField(\"field\", \"here is some text that is a bit longer than normal trivial text\", Field.Store.NO));\n                  for(int j=0;j<200;j++) {\n                    w.addDocument(doc);\n                  }\n                } else {\n                  // We lose: no indexing for us on this cycle\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": don't index\");\n                  }\n                }\n                barrier.await();\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    IOUtils.close(checker, w, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c02b804ab16489b95429791a2d8fb0e0728354d4","date":1436551798,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testSegmentCountOnFlushRandom().mjava","sourceNew":"  // LUCENE-5644: index docs w/ multiple threads but in between flushes we limit how many threads can index concurrently in the next\n  // iteration, and then verify that no more segments were flushed than number of threads:\n  public void testSegmentCountOnFlushRandom() throws Exception {\n    Directory dir = newFSDirectory(createTempDir());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n\n    // Never trigger flushes (so we only flush on getReader):\n    iwc.setMaxBufferedDocs(100000000);\n    iwc.setRAMBufferSizeMB(-1);\n\n    // Never trigger merges (so we can simplistically count flushed segments):\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    final IndexWriter w = new IndexWriter(dir, iwc);\n\n    // How many threads are indexing in the current cycle:\n    final AtomicInteger indexingCount = new AtomicInteger();\n\n    // How many threads we will use on each cycle:\n    final AtomicInteger maxThreadCount = new AtomicInteger();\n\n    CheckSegmentCount checker = new CheckSegmentCount(w, maxThreadCount, indexingCount);\n\n    // We spin up 10 threads up front, but then in between flushes we limit how many can run on each iteration\n    final int ITERS = TEST_NIGHTLY ? 300 : 10;\n    Thread[] threads = new Thread[MAX_THREADS_AT_ONCE];\n\n    // We use this to stop all threads once they've indexed their docs in the current iter, and pull a new NRT reader, and verify the\n    // segment count:\n    final CyclicBarrier barrier = new CyclicBarrier(MAX_THREADS_AT_ONCE, checker);\n    \n    for(int i=0;i<threads.length;i++) {\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              for(int iter=0;iter<ITERS;iter++) {\n                if (indexingCount.incrementAndGet() <= maxThreadCount.get()) {\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": do index\");\n                  }\n\n                  // We get to index on this cycle:\n                  Document doc = new Document();\n                  doc.add(new TextField(\"field\", \"here is some text that is a bit longer than normal trivial text\", Field.Store.NO));\n                  for(int j=0;j<200;j++) {\n                    w.addDocument(doc);\n                  }\n                } else {\n                  // We lose: no indexing for us on this cycle\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": don't index\");\n                  }\n                }\n                barrier.await();\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    IOUtils.close(checker, w, dir);\n  }\n\n","sourceOld":"  // LUCENE-5644: index docs w/ multiple threads but in between flushes we limit how many threads can index concurrently in the next\n  // iteration, and then verify that no more segments were flushed than number of threads:\n  public void testSegmentCountOnFlushRandom() throws Exception {\n    Directory dir = newFSDirectory(createTempDir());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n\n    int maxThreadStates = TestUtil.nextInt(random(), 1, 12);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: maxThreadStates=\" + maxThreadStates);\n    }\n\n    // Never trigger flushes (so we only flush on getReader):\n    iwc.setMaxBufferedDocs(100000000);\n    iwc.setRAMBufferSizeMB(-1);\n    iwc.setMaxThreadStates(maxThreadStates);\n\n    // Never trigger merges (so we can simplistically count flushed segments):\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    final IndexWriter w = new IndexWriter(dir, iwc);\n\n    // How many threads are indexing in the current cycle:\n    final AtomicInteger indexingCount = new AtomicInteger();\n\n    // How many threads we will use on each cycle:\n    final AtomicInteger maxThreadCount = new AtomicInteger();\n\n    CheckSegmentCount checker = new CheckSegmentCount(w, maxThreadCount, indexingCount);\n\n    // We spin up 10 threads up front, but then in between flushes we limit how many can run on each iteration\n    final int ITERS = TEST_NIGHTLY ? 300 : 10;\n    Thread[] threads = new Thread[MAX_THREADS_AT_ONCE];\n\n    // We use this to stop all threads once they've indexed their docs in the current iter, and pull a new NRT reader, and verify the\n    // segment count:\n    final CyclicBarrier barrier = new CyclicBarrier(MAX_THREADS_AT_ONCE, checker);\n    \n    for(int i=0;i<threads.length;i++) {\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              for(int iter=0;iter<ITERS;iter++) {\n                if (indexingCount.incrementAndGet() <= maxThreadCount.get()) {\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": do index\");\n                  }\n\n                  // We get to index on this cycle:\n                  Document doc = new Document();\n                  doc.add(new TextField(\"field\", \"here is some text that is a bit longer than normal trivial text\", Field.Store.NO));\n                  for(int j=0;j<200;j++) {\n                    w.addDocument(doc);\n                  }\n                } else {\n                  // We lose: no indexing for us on this cycle\n                  if (VERBOSE) {\n                    System.out.println(\"TEST: \" + Thread.currentThread().getName() + \": don't index\");\n                  }\n                }\n                barrier.await();\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    IOUtils.close(checker, w, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8fac5f6bc2cfa00592e549aba4ee7179470f9df3":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["5fcade65c286fca4014dc1dedffed2e912ac7012"],"a957bf27202eab1c9ddabc5aa30c7a0db04bbf36":["3d4f1fefe3eaa4cd554cb6ad51a31ef844f15b94","5fcade65c286fca4014dc1dedffed2e912ac7012"],"3d4f1fefe3eaa4cd554cb6ad51a31ef844f15b94":["06663b933cd6a92982d79726136b07915bc86aef"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c02b804ab16489b95429791a2d8fb0e0728354d4"],"c02b804ab16489b95429791a2d8fb0e0728354d4":["8fac5f6bc2cfa00592e549aba4ee7179470f9df3"],"5fcade65c286fca4014dc1dedffed2e912ac7012":["3d4f1fefe3eaa4cd554cb6ad51a31ef844f15b94"],"06663b933cd6a92982d79726136b07915bc86aef":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"8fac5f6bc2cfa00592e549aba4ee7179470f9df3":["c02b804ab16489b95429791a2d8fb0e0728354d4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["06663b933cd6a92982d79726136b07915bc86aef"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["8fac5f6bc2cfa00592e549aba4ee7179470f9df3"],"a957bf27202eab1c9ddabc5aa30c7a0db04bbf36":[],"3d4f1fefe3eaa4cd554cb6ad51a31ef844f15b94":["a957bf27202eab1c9ddabc5aa30c7a0db04bbf36","5fcade65c286fca4014dc1dedffed2e912ac7012"],"5fcade65c286fca4014dc1dedffed2e912ac7012":["d0ef034a4f10871667ae75181537775ddcf8ade4","a957bf27202eab1c9ddabc5aa30c7a0db04bbf36"],"06663b933cd6a92982d79726136b07915bc86aef":["3d4f1fefe3eaa4cd554cb6ad51a31ef844f15b94"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"c02b804ab16489b95429791a2d8fb0e0728354d4":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["a957bf27202eab1c9ddabc5aa30c7a0db04bbf36","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}