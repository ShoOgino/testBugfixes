{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"322360ac5185a8446d3e0b530b2068bef67cd3d5","date":1343669494,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.shutdown();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.shutdown();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff4227bb146f97aabae888091c19e48c88dbb0db","date":1406758576,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.shutdown();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.shutdown();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.shutdown();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.shutdown();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5cdab62f058ea765dd33deb05b4f19b7d626c801","date":1406803479,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.shutdown();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.shutdown();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.shutdown();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.shutdown();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"379db3ad24c4f0214f30a122265a6d6be003a99d","date":1407537768,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.shutdown();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.shutdown();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.shutdown();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.shutdown();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.shutdown();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.shutdown();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c177b66783fe11c18553f8b57e8b745098cc7607","date":1412798789,"type":5,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55980207f1977bd1463465de1659b821347e2fa8","date":1413336386,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(new ClassicAnalyzer()));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents, Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\", Field.Store.NO));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm, Field.Store.NO));\n    ClassicAnalyzer sa = new ClassicAnalyzer();\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ff4227bb146f97aabae888091c19e48c88dbb0db":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"55980207f1977bd1463465de1659b821347e2fa8":["d0ef034a4f10871667ae75181537775ddcf8ade4","c177b66783fe11c18553f8b57e8b745098cc7607"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["04f07771a2a7dd3a395700665ed839c3dae2def2","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["04f07771a2a7dd3a395700665ed839c3dae2def2","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["322360ac5185a8446d3e0b530b2068bef67cd3d5"],"c177b66783fe11c18553f8b57e8b745098cc7607":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["ff4227bb146f97aabae888091c19e48c88dbb0db"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c177b66783fe11c18553f8b57e8b745098cc7607"]},"commit2Childs":{"ff4227bb146f97aabae888091c19e48c88dbb0db":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"55980207f1977bd1463465de1659b821347e2fa8":[],"04f07771a2a7dd3a395700665ed839c3dae2def2":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["55980207f1977bd1463465de1659b821347e2fa8","c177b66783fe11c18553f8b57e8b745098cc7607"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["ff4227bb146f97aabae888091c19e48c88dbb0db"],"c177b66783fe11c18553f8b57e8b745098cc7607":["55980207f1977bd1463465de1659b821347e2fa8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["55980207f1977bd1463465de1659b821347e2fa8","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}