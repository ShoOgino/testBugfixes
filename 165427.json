{"path":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testCreateIndexWithDocValuesUpdates().mjava","commits":[{"id":"ec8882143e40fbd4aaa2cc02fc4abb1217eb24c7","date":1411591737,"type":0,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testCreateIndexWithDocValuesUpdates().mjava","pathOld":"/dev/null","sourceNew":"  // Creates an index with DocValues updates\n  public void testCreateIndexWithDocValuesUpdates() throws Exception {\n    Path indexDir = getIndexDir().resolve(\"dvupdates\");\n    Files.deleteIfExists(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setUseCompoundFile(false).setMergePolicy(NoMergePolicy.INSTANCE);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    // create an index w/ few doc-values fields, some with updates and some without\n    for (int i = 0; i < 30; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"ndv1\", i));\n      doc.add(new NumericDocValuesField(\"ndv1_c\", i*2));\n      doc.add(new NumericDocValuesField(\"ndv2\", i*3));\n      doc.add(new NumericDocValuesField(\"ndv2_c\", i*6));\n      doc.add(new BinaryDocValuesField(\"bdv1\", TestDocValuesUpdatesOnOldSegments.toBytes(i)));\n      doc.add(new BinaryDocValuesField(\"bdv1_c\", TestDocValuesUpdatesOnOldSegments.toBytes(i*2)));\n      doc.add(new BinaryDocValuesField(\"bdv2\", TestDocValuesUpdatesOnOldSegments.toBytes(i*3)));\n      doc.add(new BinaryDocValuesField(\"bdv2_c\", TestDocValuesUpdatesOnOldSegments.toBytes(i*6)));\n      writer.addDocument(doc);\n      if ((i+1) % 10 == 0) {\n        writer.commit(); // flush every 10 docs\n      }\n    }\n    \n    // first segment: no updates\n    \n    // second segment: update two fields, same gen\n    updateNumeric(writer, \"10\", \"ndv1\", \"ndv1_c\", 100L);\n    updateBinary(writer, \"11\", \"bdv1\", \"bdv1_c\", 100L);\n    writer.commit();\n    \n    // third segment: update few fields, different gens, few docs\n    updateNumeric(writer, \"20\", \"ndv1\", \"ndv1_c\", 100L);\n    updateBinary(writer, \"21\", \"bdv1\", \"bdv1_c\", 100L);\n    writer.commit();\n    updateNumeric(writer, \"22\", \"ndv1\", \"ndv1_c\", 200L); // update the field again\n    writer.commit();\n    \n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71387d8cb6923eb831b17a8b734608ba2e21c653","date":1414126093,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testCreateIndexWithDocValuesUpdates().mjava","pathOld":"lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility#testCreateIndexWithDocValuesUpdates().mjava","sourceNew":"  // Creates an index with DocValues updates\n  public void testCreateIndexWithDocValuesUpdates() throws Exception {\n    Path indexDir = getIndexDir().resolve(\"dvupdates\");\n    Files.deleteIfExists(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setUseCompoundFile(false).setMergePolicy(NoMergePolicy.INSTANCE);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    // create an index w/ few doc-values fields, some with updates and some without\n    for (int i = 0; i < 30; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"ndv1\", i));\n      doc.add(new NumericDocValuesField(\"ndv1_c\", i*2));\n      doc.add(new NumericDocValuesField(\"ndv2\", i*3));\n      doc.add(new NumericDocValuesField(\"ndv2_c\", i*6));\n      doc.add(new BinaryDocValuesField(\"bdv1\", toBytes(i)));\n      doc.add(new BinaryDocValuesField(\"bdv1_c\", toBytes(i*2)));\n      doc.add(new BinaryDocValuesField(\"bdv2\", toBytes(i*3)));\n      doc.add(new BinaryDocValuesField(\"bdv2_c\", toBytes(i*6)));\n      writer.addDocument(doc);\n      if ((i+1) % 10 == 0) {\n        writer.commit(); // flush every 10 docs\n      }\n    }\n    \n    // first segment: no updates\n    \n    // second segment: update two fields, same gen\n    updateNumeric(writer, \"10\", \"ndv1\", \"ndv1_c\", 100L);\n    updateBinary(writer, \"11\", \"bdv1\", \"bdv1_c\", 100L);\n    writer.commit();\n    \n    // third segment: update few fields, different gens, few docs\n    updateNumeric(writer, \"20\", \"ndv1\", \"ndv1_c\", 100L);\n    updateBinary(writer, \"21\", \"bdv1\", \"bdv1_c\", 100L);\n    writer.commit();\n    updateNumeric(writer, \"22\", \"ndv1\", \"ndv1_c\", 200L); // update the field again\n    writer.commit();\n    \n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Creates an index with DocValues updates\n  public void testCreateIndexWithDocValuesUpdates() throws Exception {\n    Path indexDir = getIndexDir().resolve(\"dvupdates\");\n    Files.deleteIfExists(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))\n      .setUseCompoundFile(false).setMergePolicy(NoMergePolicy.INSTANCE);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    // create an index w/ few doc-values fields, some with updates and some without\n    for (int i = 0; i < 30; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", \"\" + i, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"ndv1\", i));\n      doc.add(new NumericDocValuesField(\"ndv1_c\", i*2));\n      doc.add(new NumericDocValuesField(\"ndv2\", i*3));\n      doc.add(new NumericDocValuesField(\"ndv2_c\", i*6));\n      doc.add(new BinaryDocValuesField(\"bdv1\", TestDocValuesUpdatesOnOldSegments.toBytes(i)));\n      doc.add(new BinaryDocValuesField(\"bdv1_c\", TestDocValuesUpdatesOnOldSegments.toBytes(i*2)));\n      doc.add(new BinaryDocValuesField(\"bdv2\", TestDocValuesUpdatesOnOldSegments.toBytes(i*3)));\n      doc.add(new BinaryDocValuesField(\"bdv2_c\", TestDocValuesUpdatesOnOldSegments.toBytes(i*6)));\n      writer.addDocument(doc);\n      if ((i+1) % 10 == 0) {\n        writer.commit(); // flush every 10 docs\n      }\n    }\n    \n    // first segment: no updates\n    \n    // second segment: update two fields, same gen\n    updateNumeric(writer, \"10\", \"ndv1\", \"ndv1_c\", 100L);\n    updateBinary(writer, \"11\", \"bdv1\", \"bdv1_c\", 100L);\n    writer.commit();\n    \n    // third segment: update few fields, different gens, few docs\n    updateNumeric(writer, \"20\", \"ndv1\", \"ndv1_c\", 100L);\n    updateBinary(writer, \"21\", \"bdv1\", \"bdv1_c\", 100L);\n    writer.commit();\n    updateNumeric(writer, \"22\", \"ndv1\", \"ndv1_c\", 200L); // update the field again\n    writer.commit();\n    \n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ec8882143e40fbd4aaa2cc02fc4abb1217eb24c7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"71387d8cb6923eb831b17a8b734608ba2e21c653":["ec8882143e40fbd4aaa2cc02fc4abb1217eb24c7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71387d8cb6923eb831b17a8b734608ba2e21c653"]},"commit2Childs":{"ec8882143e40fbd4aaa2cc02fc4abb1217eb24c7":["71387d8cb6923eb831b17a8b734608ba2e21c653"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ec8882143e40fbd4aaa2cc02fc4abb1217eb24c7"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}