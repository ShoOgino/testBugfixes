{"path":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","commits":[{"id":"849494cf2f3a96af5c8c84995108ddd8456fcd04","date":1372277913,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","pathOld":"/dev/null","sourceNew":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n        tlogOutStream.hflush();\n        \n        // we actually need a new reader\n        fis.close();\n        try {\n          FSDataInputStream fdis = fs.open(tlogFile);\n          fis = new FSDataFastInputStream(fdis, pos);\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n        \n      }\n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      tlogOutStream.hflush();\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["d9da6af0d6e0b9ee92e3adbfd2796792453afbef"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","pathOld":"/dev/null","sourceNew":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n        tlogOutStream.hflush();\n        \n        // we actually need a new reader\n        fis.close();\n        try {\n          FSDataInputStream fdis = fs.open(tlogFile);\n          fis = new FSDataFastInputStream(fdis, pos);\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n        \n      }\n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      tlogOutStream.hflush();\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c129f34852d80844343c4826587e2af2dcc7263","date":1408560387,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","sourceNew":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (fis.position() >= sz) {\n        fis.close();\n        tlogOutStream.hflush();\n        try {\n          FSDataInputStream fdis = fs.open(tlogFile);\n          fis = new FSDataFastInputStream(fdis, pos);\n          sz = fs.getFileStatus(tlogFile).getLen();\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","sourceOld":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n        tlogOutStream.hflush();\n        \n        // we actually need a new reader\n        fis.close();\n        try {\n          FSDataInputStream fdis = fs.open(tlogFile);\n          fis = new FSDataFastInputStream(fdis, pos);\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n        \n      }\n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      tlogOutStream.hflush();\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"052e752d13b775cc343d0b8588d38be0113b24ba","date":1454626441,"type":3,"author":"Mark Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","sourceNew":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        synchronized (HdfsTransactionLog.this) {\n          sz = fos.size();\n        }\n        \n        fis.close();\n        tlogOutStream.hflush();\n\n        FSDataInputStream fdis = fs.open(tlogFile);\n        fis = new FSDataFastInputStream(fdis, pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","sourceOld":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (fis.position() >= sz) {\n        fis.close();\n        tlogOutStream.hflush();\n        try {\n          FSDataInputStream fdis = fs.open(tlogFile);\n          fis = new FSDataFastInputStream(fdis, pos);\n          sz = fs.getFileStatus(tlogFile).getLen();\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"85639ac489df34a1af4df5f84b90b46c12d03152","date":1454627270,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","sourceNew":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        synchronized (HdfsTransactionLog.this) {\n          sz = fos.size();\n        }\n        \n        fis.close();\n        tlogOutStream.hflush();\n\n        FSDataInputStream fdis = fs.open(tlogFile);\n        fis = new FSDataFastInputStream(fdis, pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","sourceOld":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (fis.position() >= sz) {\n        fis.close();\n        tlogOutStream.hflush();\n        try {\n          FSDataInputStream fdis = fs.open(tlogFile);\n          fis = new FSDataFastInputStream(fdis, pos);\n          sz = fs.getFileStatus(tlogFile).getLen();\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b1704c078ec59838c9d95d5bf5738b393b537494","date":1454693901,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","sourceNew":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        synchronized (HdfsTransactionLog.this) {\n          sz = fos.size();\n        }\n        \n        fis.close();\n        tlogOutStream.hflush();\n\n        FSDataInputStream fdis = fs.open(tlogFile);\n        fis = new FSDataFastInputStream(fdis, pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","sourceOld":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (fis.position() >= sz) {\n        fis.close();\n        tlogOutStream.hflush();\n        try {\n          FSDataInputStream fdis = fs.open(tlogFile);\n          fis = new FSDataFastInputStream(fdis, pos);\n          sz = fs.getFileStatus(tlogFile).getLen();\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1e6acbaae7af722f17204ceccf0f7db5753eccf3","date":1454775255,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","sourceNew":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        synchronized (HdfsTransactionLog.this) {\n          sz = fos.size();\n        }\n        \n        fis.close();\n        tlogOutStream.hflush();\n\n        FSDataInputStream fdis = fs.open(tlogFile);\n        fis = new FSDataFastInputStream(fdis, pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","sourceOld":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (fis.position() >= sz) {\n        fis.close();\n        tlogOutStream.hflush();\n        try {\n          FSDataInputStream fdis = fs.open(tlogFile);\n          fis = new FSDataFastInputStream(fdis, pos);\n          sz = fs.getFileStatus(tlogFile).getLen();\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd9f6c0ea8e68b9769f3620189f200c9232be6a4","date":1455197557,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","sourceNew":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (fis.position() >= sz) {\n        fis.close();\n        tlogOutStream.hflush();\n        try {\n          FSDataInputStream fdis = fs.open(tlogFile);\n          fis = new FSDataFastInputStream(fdis, pos);\n          sz = fs.getFileStatus(tlogFile).getLen();\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","sourceOld":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        synchronized (HdfsTransactionLog.this) {\n          sz = fos.size();\n        }\n        \n        fis.close();\n        tlogOutStream.hflush();\n\n        FSDataInputStream fdis = fs.open(tlogFile);\n        fis = new FSDataFastInputStream(fdis, pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","bugFix":null,"bugIntro":["d9da6af0d6e0b9ee92e3adbfd2796792453afbef"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1b800611029360525d55dc4797bcdfc2a689b7fe","date":1455310686,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","sourceNew":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        synchronized (HdfsTransactionLog.this) {\n          fos.flushBuffer();\n          sz = fos.size();\n        }\n        \n        tlogOutStream.hflush();\n        fis.close();\n   \n        FSDataInputStream fdis = fs.open(tlogFile);\n        fis = new FSDataFastInputStream(fdis, pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","sourceOld":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (fis.position() >= sz) {\n        fis.close();\n        tlogOutStream.hflush();\n        try {\n          FSDataInputStream fdis = fs.open(tlogFile);\n          fis = new FSDataFastInputStream(fdis, pos);\n          sz = fs.getFileStatus(tlogFile).getLen();\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","bugFix":null,"bugIntro":["d9da6af0d6e0b9ee92e3adbfd2796792453afbef"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d9da6af0d6e0b9ee92e3adbfd2796792453afbef","date":1472233199,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","sourceNew":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+getLogSize());\n        }\n\n        if (pos >= getLogSize()) {\n          return null;\n        }\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        fis.close();\n        initStream(pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= getLogSize()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","sourceOld":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        synchronized (HdfsTransactionLog.this) {\n          fos.flushBuffer();\n          sz = fos.size();\n        }\n        \n        tlogOutStream.hflush();\n        fis.close();\n   \n        FSDataInputStream fdis = fs.open(tlogFile);\n        fis = new FSDataFastInputStream(fdis, pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","bugFix":["1b800611029360525d55dc4797bcdfc2a689b7fe","bd9f6c0ea8e68b9769f3620189f200c9232be6a4","849494cf2f3a96af5c8c84995108ddd8456fcd04"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","sourceNew":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+getLogSize());\n        }\n\n        if (pos >= getLogSize()) {\n          return null;\n        }\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        fis.close();\n        initStream(pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= getLogSize()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","sourceOld":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        synchronized (HdfsTransactionLog.this) {\n          fos.flushBuffer();\n          sz = fos.size();\n        }\n        \n        tlogOutStream.hflush();\n        fis.close();\n   \n        FSDataInputStream fdis = fs.open(tlogFile);\n        fis = new FSDataFastInputStream(fdis, pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","sourceNew":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+getLogSize());\n        }\n\n        if (pos >= getLogSize()) {\n          return null;\n        }\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        fis.close();\n        initStream(pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= getLogSize()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","sourceOld":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        synchronized (HdfsTransactionLog.this) {\n          fos.flushBuffer();\n          sz = fos.size();\n        }\n        \n        tlogOutStream.hflush();\n        fis.close();\n   \n        FSDataInputStream fdis = fs.open(tlogFile);\n        fis = new FSDataFastInputStream(fdis, pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= fos.size()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"740d649f013f07efbeb73ca854f106c60166e7c0","date":1587431295,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsTransactionLog.HDFSLogReader#next().mjava","sourceNew":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos={} currentSize={}\", pos, getLogSize());\n        }\n\n        if (pos >= getLogSize()) {\n          return null;\n        }\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        fis.close();\n        initStream(pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= getLogSize()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","sourceOld":"    /** Returns the next object from the log, or null if none available.\n     *\n     * @return The log record, or null if EOF\n     * @throws IOException If there is a low-level I/O error.\n     */\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+getLogSize());\n        }\n\n        if (pos >= getLogSize()) {\n          return null;\n        }\n      }\n      \n      // we actually need a new reader to \n      // see if any data was added by the writer\n      if (pos >= sz) {\n        log.info(\"Read available inputstream data, opening new inputstream pos={} sz={}\", pos, sz);\n        \n        fis.close();\n        initStream(pos);\n      }\n      \n      if (pos == 0) {\n        readHeader(fis);\n\n        // shouldn't currently happen - header and first record are currently written at the same time\n        synchronized (HdfsTransactionLog.this) {\n          if (fis.position() >= getLogSize()) {\n            return null;\n          }\n          pos = fis.position();\n        }\n      }\n\n      Object o = codec.readVal(fis);\n\n      // skip over record size\n      int size = fis.readInt();\n      assert size == fis.position() - pos - 4;\n\n      return o;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d9da6af0d6e0b9ee92e3adbfd2796792453afbef":["1b800611029360525d55dc4797bcdfc2a689b7fe"],"740d649f013f07efbeb73ca854f106c60166e7c0":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","849494cf2f3a96af5c8c84995108ddd8456fcd04"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":["1c129f34852d80844343c4826587e2af2dcc7263","85639ac489df34a1af4df5f84b90b46c12d03152"],"052e752d13b775cc343d0b8588d38be0113b24ba":["1c129f34852d80844343c4826587e2af2dcc7263"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["1b800611029360525d55dc4797bcdfc2a689b7fe","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"85639ac489df34a1af4df5f84b90b46c12d03152":["1c129f34852d80844343c4826587e2af2dcc7263","052e752d13b775cc343d0b8588d38be0113b24ba"],"1c129f34852d80844343c4826587e2af2dcc7263":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["1b800611029360525d55dc4797bcdfc2a689b7fe","d9da6af0d6e0b9ee92e3adbfd2796792453afbef"],"bd9f6c0ea8e68b9769f3620189f200c9232be6a4":["85639ac489df34a1af4df5f84b90b46c12d03152"],"b1704c078ec59838c9d95d5bf5738b393b537494":["1c129f34852d80844343c4826587e2af2dcc7263","85639ac489df34a1af4df5f84b90b46c12d03152"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["740d649f013f07efbeb73ca854f106c60166e7c0"],"1b800611029360525d55dc4797bcdfc2a689b7fe":["bd9f6c0ea8e68b9769f3620189f200c9232be6a4"]},"commit2Childs":{"d9da6af0d6e0b9ee92e3adbfd2796792453afbef":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"740d649f013f07efbeb73ca854f106c60166e7c0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":[],"052e752d13b775cc343d0b8588d38be0113b24ba":["85639ac489df34a1af4df5f84b90b46c12d03152"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"85639ac489df34a1af4df5f84b90b46c12d03152":["1e6acbaae7af722f17204ceccf0f7db5753eccf3","bd9f6c0ea8e68b9769f3620189f200c9232be6a4","b1704c078ec59838c9d95d5bf5738b393b537494"],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["37a0f60745e53927c4c876cfe5b5a58170f0646c","1c129f34852d80844343c4826587e2af2dcc7263"],"1c129f34852d80844343c4826587e2af2dcc7263":["1e6acbaae7af722f17204ceccf0f7db5753eccf3","052e752d13b775cc343d0b8588d38be0113b24ba","85639ac489df34a1af4df5f84b90b46c12d03152","b1704c078ec59838c9d95d5bf5738b393b537494"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["740d649f013f07efbeb73ca854f106c60166e7c0","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["37a0f60745e53927c4c876cfe5b5a58170f0646c","849494cf2f3a96af5c8c84995108ddd8456fcd04"],"bd9f6c0ea8e68b9769f3620189f200c9232be6a4":["1b800611029360525d55dc4797bcdfc2a689b7fe"],"b1704c078ec59838c9d95d5bf5738b393b537494":[],"1b800611029360525d55dc4797bcdfc2a689b7fe":["d9da6af0d6e0b9ee92e3adbfd2796792453afbef","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","1e6acbaae7af722f17204ceccf0f7db5753eccf3","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","b1704c078ec59838c9d95d5bf5738b393b537494","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}